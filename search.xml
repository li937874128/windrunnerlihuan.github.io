<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[无题]]></title>
    <url>%2F2018%2F09%2F12%2F%E6%97%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;告别魔都，来到杭州，内心没有一丝喜悦，更多的是惋惜和遗憾。 又到了迷茫时间&#160; &#160; &#160; &#160;只身来到杭州，带着一脸不情愿。也许是在大上海呆久了，不喜欢杭州加班成风的氛围，和小家碧玉的格局。论资源，的确上海更胜一筹，体现在方方面面，难怪全国各地的人才都争相流入。很多人说这城市像围城，外面的人想进去，里面的人想出来。但更多的是外面的人进去了，就不想出来了，后来年龄大了，拼不过年轻人了，才不得不出去。&#160; &#160; &#160; &#160;说起来这次工作变更挺悲催的，进了杭州有名的血汗工厂。。。。。。如果996对于上海来说是深恶痛绝的，那么这儿有可能007都是理所当然。我也时常安慰自己，既来之，则安之。但是在时间安排上面还是很难适应，需要一段时间去消化。 艰难的旅程&#160; &#160; &#160; &#160;可能很多人都有感触，应用开发到了一定年头就会出现瓶颈，这时候就面临转型问题。有的人坚持守住一亩三分地，有的人投入专项领域深入研究，也有的人走上层路线打入管理体系。或者遇到对的老大，一人得道，鸡犬升天。树大好乘凉，还有集团利益带来的阵阵凉风，从此一路起飞，直接实现各种自由。当然还有其它情况的，每个人都有不同境遇，结果也是形形色色。这是人生的重大抉择，也是职业难题。&#160; &#160; &#160; &#160;很多人跳不出自己的舒适区，也不愿跳出，毕竟在一个环境呆久了，人就会越来越懒，久而久之，就会形成习惯，很难接受改变。当然，我也是如此。但是有时候大家还是希望能够向内心向往的方向去转变，只是缺少机会。而主动转变的道路又十分艰难，要克服很多苦难，还要克服自己诸多缺点。这对人的意志也是极大的考验，因为稍微一懒惰，就会半途而废，即使偶尔坚持，也是一暴十寒，未见起色，效果微乎其微。 无奈的抉择&#160; &#160; &#160; &#160;我也承认自己是一条咸鱼，工作这么多年，什么也没有积累，倒是心态越来越佛系。多年来一直荣辱不惊，没有翻身的梦想，随遇而安。 小时候心很大，长大后心却很小。没有大目标，没有对物质追求的热衷，也没有对名利的追逐的欲望。唯有自己的一点小兴趣，希望能够长久保持。让生活也有一点色彩，不至于过的浑浑噩噩，换句话说，至少证明自己还活着，而不是行尸走肉。 あの日見た花の名前を僕達はまだ知ら&#160; &#160; &#160; &#160;罗嗦了这么多，也该列出今年未来计划了。目前看来，有如下： Skia WebRTC Chromium Python FFMpeg OpenGL&#160; &#160; &#160; &#160;暂定就是这些，肯定学不完，但是也不能放弃呀。路漫漫其修远兮，贵在兴趣方向的坚持。总之就这样吧，期待自己有所改变。]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>迷茫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHOENIX]]></title>
    <url>%2F2018%2F07%2F24%2FPHOENIX%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;这一拖就是半年，博客几乎要被废弃了。现在重新拾起，为这次归来起一个代号————————————————PHOENIX。 回顾&#160; &#160; &#160; &#160;上一个系列是SurfaceFlinger，距离完结已经过去了半年（其实还有最后一篇关于Fence的内容，但是后来觉得，如果能够顺着这个系列一路踩下去，对于Fence的理解已经完全不用我再出一篇解析了，因此舍弃了续更的想法）。这半半，我的工作和生活发生了翻天覆地的变化。我一度认为已经超出了我个人的承受能力，但是事情来了，还是硬着头皮顶住了。一些事情也没有处理好，我也有反思过，主要还是因为自己责任心不够重。我也一直认为一个人是否成长，就看他的责任心有没有提升。但是提升责任心有一个最大的心里阻碍，就是懒(╯-╰)/众人皆懒，这个很难克服，必须养成良好的习惯才能慢慢改善。 &#160; &#160; &#160; &#160;工作的事情真是一波三折，加入这家公司也是机缘巧合，名字就不透漏了。因为年后公司组织架构调整，人员有所变动，我也不例外。调整之后工作这半年，只能说是受尽心酸。博客停更半年先不说了，技术也一直停滞不前。忙的时候经常一天没喝一口水，或者就是早上刚去接了一杯水，晚上十点多下班后还没喝（＋﹏＋）。忙是一方面，另一方面是公司产品线决定的。因为这是一家纯业务公司，并且在集团快速发展的脚步下，部门也随之急剧变动，可想而知，就会有无休止的业务来折腾。但是却有一个致命的弊端，那就是技术积累严重不足。因为迫于集团的压力，平台老板也是卯足了劲的改革，大刀阔斧的进行业务更替，下面大小领导也是人心惶惶。因为经过几年折腾，平台的业绩本身已经在逐年下滑，但是又不能两耳不闻当咸鱼，所以才有了大小团队各种争相表现举动。产品的需求日日更迭先不提，技术团队也是为出成果各种邀功，因此，各个团队，海量的业务库，组件库，监控库，上报库……等等陆续诞生，先不管有么有用，或者是否已经有重复功能的库已，先集成给主应用再说，然后写个wiki，就证明我们组也有输出了，至少kpi有的写了。至于是否影响主应用性能，或者后续是否仍然使用，或者这个库的维护，这个就不是care的点了(一-一)。可以说，有没有很重要，有没有用不重要。对外宣布这些组件库，感觉他们都说自己是这么个情况： &#160; &#160; &#160; &#160;现在主应用已经越来越卡了，crash率也上升了许多，很多用户都投诉了，每天投诉邮件尽是批评和吐槽。用户数和活跃度也在下降，看着各种V的数据折线，仿佛看着A股的走势，的确有些心寒。 &#160; &#160; &#160; &#160;因为是纯业务公司，再加上忙的不可开交，因此技术实力不进反退(ノ=Д=)ノ┻━┻这个有公司的原因，也有我的原因。本来我是想在音视频的道路上越走越长，顺路拾一拾图形图像的内容。可惜总是事与愿违啊，工作经历一波三折，一直在折腾，一直没有走到正向，反而越走越远。不过说到业务能力，在工作中也得到了锻炼，因为工作量很大，时间周期又很短，所以工作时候不能闷着头就是干，还要学会思考，比如提升工作效率，还有保证质量和复用性。我一直认为一个人的业务能力应该至少有两年时间去沉淀，具体沉淀的厚度就看个人理解程度了。但是总体来说，我还是觉得倒退了，这是无法回避的事实。 展望&#160; &#160; &#160; &#160;上年提到，以后希望能够在音视频方向有所造诣。不求闻达于业界，但求沉浸于兴趣。因为在这家公司工作初期用到了OpenGL相关知识，因此闲暇之余写了个app——狗头相机。 &#160; &#160; &#160; &#160;GitHub传送门如下： https://github.com/windrunnerlihuan/DogCamera &#160; &#160; &#160; &#160;本来想做成抖音的效果，但是抖音的界面太花了，不利于功能的展示，所以UI设计就抄了VUE，但是会加上抖音的功能（因为工作实在太忙，因此很多功能还没有加上。目前已有拍摄、录制、加滤镜、混音等功能）。&#160; &#160; &#160; &#160;这个app目前还有许多bug，以及很多功能还未添加，后续我都会慢慢完善。如果大家有好的点子，欢迎留言或者提issue～]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(十二)----合成Layer]]></title>
    <url>%2F2018%2F01%2F09%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E5%8D%81%E4%BA%8C-%E5%90%88%E6%88%90Layer%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;脱更许久，慢慢步入咸鱼阶段。虽非我愿，但事事不如愿。一切尽在不言中，可与言者无一二。&#160; &#160; &#160; &#160;上一节讲了合成Layer之前的准备工作，主要是就算可视化区域和初始化硬件合成环境，本节就讲讲最后的内容，合成Layer。 回顾&#160; &#160; &#160; &#160;上一节讲合成的时候说过，合成分为两种，离线合成和在线合成： 先将所有图层画到一个最终层（FrameBuffer）上，再将FrameBuffer送到LCD显示。由于合成FrameBuffer与送LCD显示一般是异步的（线下生成FrameBuffer，需要时线上的LCD去取），因此叫离线合成。 不使用FrameBuffer，在LCD需要显示某一行的像素时，用显示控制器将所有图层与该行相关的数据取出，合成一行像素送过去。只有一个图层时，又叫Overlay技术。由于省去合成FrameBuffer时读图层，写FrameBuffer的步骤，大幅降低了内存传输量，减少了功耗，但这个需要硬件支持。 &#160; &#160; &#160; &#160;决定Layer的合成方式是在HWC硬件部分决定的，一般上层看不到代码，不过有幸Intel、三星等等开源了一部分我们还能窥一窥源码。 &#160; &#160; &#160; &#160;如果是Overlay的图层，直接交给显示器硬件系统去在线合成；如果是FRAME_BUFFER类型的图层，需要经过OpenGL处理，然后交给FrameBuffer送给显示器。 合成流程&#160; &#160; &#160; &#160;我们接着上一节的内容，SurfaceFlinger的handleMessageRefresh函数最后的内容。frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp，我们先来看看doComposition函数：123456789101112131415161718192021222324252627282930void SurfaceFlinger::doComposition() &#123; ATRACE_CALL(); //将mRepaintEverything置为0，不用重绘所有区域 const bool repaintEverything = android_atomic_and(0, &amp;mRepaintEverything); //遍历所有的DisplayDevice然后调用doDisplayComposition函数 for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; const sp&lt;DisplayDevice&gt;&amp; hw(mDisplays[dpy]); if (hw-&gt;isDisplayOn()) &#123; // transform the dirty region into this screen's coordinate space //获得屏幕的脏区域 const Region dirtyRegion(hw-&gt;getDirtyRegion(repaintEverything)); //合成，重绘framebuffer // repaint the framebuffer (if needed) doDisplayComposition(hw, dirtyRegion); //清除屏幕脏区域 hw-&gt;dirtyRegion.clear(); //判断系统是否支持软件部分更新 hw-&gt;flip(hw-&gt;swapRegion); //清除交换区域 hw-&gt;swapRegion.clear(); &#125; // inform the h/w that we're done compositing //通知hwc硬件合成结束 hw-&gt;compositionComplete(); &#125; //主要是调用hwc硬件的set函数 //此方法将完成各个图层的合成与显示，等效于EGL标准里面的eglSwapBuffers， //不过eglSwapBuffers是对OpenGL标准/GPU有效，此方法是对硬件合成器有效 postFramebuffer();&#125; &#160; &#160; &#160; &#160;上述代码流程每个模块功能也很清晰，分步骤就是如下：&#160; &#160; &#160; &#160;1）遍历所有的DisplayDevice然后调用合成相关函数；&#160; &#160; &#160; &#160;2）获取每一个屏幕的脏区域；&#160; &#160; &#160; &#160;3）最重要的一步：合成；&#160; &#160; &#160; &#160;4）清除交换区域和屏幕脏区域；&#160; &#160; &#160; &#160;5）通知hwc硬件合成结束；&#160; &#160; &#160; &#160;6）最后主要是调用hwc硬件的set函数。此方法将完成各个图层的合成与显示，等效于EGL标准里面的eglSwapBuffers，不过eglSwapBuffers是对OpenGL标准/GPU有效，此方法是对硬件合成器有效。 &#160; &#160; &#160; &#160;其中核心的就是合成步骤。我们先看看其他几个模块： &#160; &#160; &#160; &#160;获取每一个屏幕的脏区域。frameworks/native/services/surfaceflinger/DisplayDevice.cpp中的getDirtyRegion函数：1234567891011Region DisplayDevice::getDirtyRegion(bool repaintEverything) const &#123; Region dirty; if (repaintEverything) &#123;//0 dirty.set(getBounds()); &#125; else &#123; const Transform&amp; planeTransform(mGlobalTransform); dirty = planeTransform.transform(this-&gt;dirtyRegion); dirty.andSelf(getBounds()); &#125; return dirty;&#125; &#160; &#160; &#160; &#160;如果需要重绘所有内容，就讲脏区域设为整个屏幕（但事实我们的repaintEverything是0，所以不会）；如果不要重绘所有，先对脏区域作矩阵变换（如果屏幕有过几何变换的transform），然后用脏区域与上屏幕大小，求出脏区域在屏幕上大小。 &#160; &#160; &#160; &#160;获取了屏幕的脏区域之后，就开始合成了。 开始合成&#160; &#160; &#160; &#160;我们继续查看doDisplayComposition函数：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071void SurfaceFlinger::doDisplayComposition(const sp&lt;const DisplayDevice&gt;&amp; hw, const Region&amp; inDirtyRegion)&#123; // We only need to actually compose the display if: // 1) It is being handled by hardware composer, which may need this to // keep its virtual display state machine in sync, or // 2) There is work to be done (the dirty region isn't empty) //以下两种情况我们需要去合成显示： //1）需要报保持虚拟显示屏和物理显示屏同步显示的情况； //2）SF需要去渲染的脏区域不为空。 bool isHwcDisplay = hw-&gt;getHwcDisplayId() &gt;= 0; if (!isHwcDisplay &amp;&amp; inDirtyRegion.isEmpty()) &#123; return; &#125; //SF需要渲染的脏区域 Region dirtyRegion(inDirtyRegion); // compute the invalid region //需要渲染到硬件帧缓冲区中去的脏区域的 hw-&gt;swapRegion.orSelf(dirtyRegion); uint32_t flags = hw-&gt;getFlags(); //在这种情况下，系统在软件上支持部分区域更新功能， //同样，这个部分被更新的区域必须要是一个矩形区域。 if (flags &amp; DisplayDevice::SWAP_RECTANGLE) &#123; // we can redraw only what's dirty, but since SWAP_RECTANGLE only // takes a rectangle, we must make sure to update that whole // rectangle in that case dirtyRegion.set(hw-&gt;swapRegion.bounds()); &#125; else &#123; //在这种情况下，系统在硬件上直接支持部分区域更新功能， //不过，这个部分被更新的区域必须要是一个矩形区域。 if (flags &amp; DisplayDevice::PARTIAL_UPDATES) &#123; // We need to redraw the rectangle that will be updated // (pushed to the framebuffer). // This is needed because PARTIAL_UPDATES only takes one // rectangle instead of a region (see DisplayDevice::flip()) dirtyRegion.set(hw-&gt;swapRegion.bounds()); &#125; else &#123; //在这种情况下，系统不支持部分更新区域，这时候就需要更新整个屏幕的内容 // we need to redraw everything (the whole screen) dirtyRegion.set(hw-&gt;bounds()); hw-&gt;swapRegion = dirtyRegion; &#125; &#125; //如果没有开启Daltonize辅助功能（高对比性文字/色彩校正/颜色反转） //并且没有颜色矩阵去混合 if (CC_LIKELY(!mDaltonize &amp;&amp; !mHasColorMatrix)) &#123; //合成 if (!doComposeSurfaces(hw, dirtyRegion)) return; &#125; else &#123;//需要处理一下高对比性文字/色彩校正/颜色反转功能 RenderEngine&amp; engine(getRenderEngine()); mat4 colorMatrix = mColorMatrix; if (mDaltonize) &#123; colorMatrix = colorMatrix * mDaltonizer(); &#125; engine.beginGroup(colorMatrix); doComposeSurfaces(hw, dirtyRegion); engine.endGroup(); &#125; // update the swap region and clear the dirty region //更新与framebuffer交换的脏区域，并清除应用脏区域 hw-&gt;swapRegion.orSelf(dirtyRegion); // swap buffers (presentation) //与fb交换脏区域，会调用eglSwapBuffer函数 //使用egl将egl中的合成好的图像，输出到DisplayDevice的mSurface中 hw-&gt;swapBuffers(getHwComposer());&#125; &#160; &#160; &#160; &#160; DisplayDevice类的成员变量swapRegion用来描述SurfaceFlinger服务需要渲染到硬件帧缓冲区中去的脏区域的。前面提到，inDirtyRegion也是用来描述SurfaceFlinger服务需要渲染的脏区域的，不过，它的作用是用来合成系统中各个应用程序窗口的图形缓冲区的，也就是说，当系统中各个应用程序窗口的图形缓冲区被合成之后，这个成员变量所描述的区域就会被清空，swapRegion会一直等到它的内容被渲染到硬件帧缓冲区中去之后，才会被清空。这样就可能会出现这种情况：上一次合成的图形缓冲区还未被渲染到硬件帧缓冲区中去，SurfaceFlinger服务又会执行新一轮的系统中各个应用程序窗口的图形缓冲区的合并操作。在这种情况下，SurfaceFlinger服务就需要将前面两次需要渲染到硬件帧缓冲区中去的区域合并在一起，以便可以正确地反映被刷新的UI。因此，函数在开头的地方，inDirtyRegion所描述的区域组合到成员变量swapRegion所描述的区域中去。 &#160; &#160; &#160; &#160; 函数接下来调用用来描述系统主显示屏的一个DisplayDevice对象hw的成员函数getFlags来获得系统所支持的渲染方式，并且保存在一个uint32_t变量flags中。接下来，我们就分三种情况来讨论系统所支持的渲染方式： 变量flags的DisplayDevice::SWAP_RECTANGLE位等于1。在这种情况下，系统在软件上支持部分区域更新功能，同样，这个部分被更新的区域必须要是一个矩形区域。 变量flags的DisplayDevice::PARTIAL_UPDATES位等于1。在这种情况下，系统在硬件上直接支持部分区域更新功能，不过，这个部分被更新的区域必须要是一个矩形区域。 变量flags的值等于0。在这种情况下，系统不支持部分更新区域，这时候就需要更新整个屏幕的内容。 &#160; &#160; &#160; &#160; 在第1种和第2种情况中，由于被更新的区域都必须是一个矩形区域，因此，函数就需要变量inDirtyRegion所描述的一个区域设置为包含了所有脏区域的一个最小矩形区域。在第3种情况中，由于需要更新的是整个屏幕的内容，因此，函数就需要inDirtyRegion所描述的一个区域设置为等于屏幕大小的一个矩形区域。 &#160; &#160; &#160; &#160;(在老的版本代码中，比如android2.3，部分更新还支持BUFFER_PRESERVED不规则区域更新。在保留后端图形缓冲区的内容的情况下，系统就可以支持仅仅渲染那些需要更新的脏区域，这些区域可以是不规则的。然而，实现不规则区域部分更新功能是有代价的，因为每次在渲染UI时，都要将后端图形缓冲区的内容拷贝回那些不在那些需要更新的区域中去，这会导致性能低下。因此，系统一般都不支持不规则区域部分更新功能。) &#160; &#160; &#160; &#160; 在安卓L版本中，辅助设置中新加了3个功能，分别是高对比性文字，颜色反转，色彩校正。 This information applies only to devices running Android 5.0 and higher. High contrast makes text easier to read on your device. This feature fixes the text color as either black or white, depending on the original text color. To enable or disable high contrast text, follow these steps:Go to Settings &gt; Accessibility.Select High contrast text. High-contrast text is currently an experimental feature, so it might not work correctly everywhere on your device. &#160; &#160; &#160; &#160; google说，这个功能就是让文字更容易阅读，也就是有点类似与文字高亮。最后的效果就是，把文字变成白色或黑色，把背景文字背景变成黑色或白色。也就是说，如果你之前的文字是白色，背景是黑色，就不会去应用这个功能。 &#160; &#160; &#160; &#160;如果没有开启这个辅助功能，mDaltonize和mHasColorMatrix为null，那么直接调用doComposeSurfaces函数去合成图层；如果开启了辅助功能，那么就需要OpenGL引擎去处理颜色矩阵（高对比性文字主要是修改控件，颜色反转和颜色校正主要是通过操作gpu修改显示效果），这一部分有机会我们以后会研究研究。 &#160; &#160; &#160; &#160;那么接下来两部就是合成图层和交换FrameBuffer了，我们逐步分析。 合成图层doComposeSurfaces&#160; &#160; &#160; &#160;合成图层的函数是doComposeSurfaces，这个函数比较长，我们分段查看：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990bool SurfaceFlinger::doComposeSurfaces(const sp&lt;const DisplayDevice&gt;&amp; hw, const Region&amp; dirty) &#123; RenderEngine&amp; engine(getRenderEngine()); const int32_t id = hw-&gt;getHwcDisplayId(); HWComposer&amp; hwc(getHwComposer()); HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); bool hasGlesComposition = hwc.hasGlesComposition(id); if (hasGlesComposition) &#123;//是否有egl合成 //DisplayDevice类的成员函数makeCurrent的实现很简单， //它只是通过调用函数eglMakeCurrent来将前面已经创建好的绘图表面以及绘图上下文设置为当前线程的绘图表面以及绘图上下文， //即设置为SurfaceFlinger服务的UI渲染线程的绘图表面以及绘图上下文 if (!hw-&gt;makeCurrent(mEGLDisplay, mEGLContext)) &#123; ALOGW("DisplayDevice::makeCurrent failed. Aborting surface composition for display %s", hw-&gt;getDisplayName().string()); //如果绑定失败了，就要取消设置OpenGL库在当前线程的绘图表面以及绘图上下文 eglMakeCurrent(mEGLDisplay, EGL_NO_SURFACE, EGL_NO_SURFACE, EGL_NO_CONTEXT); //那么就要去设置默认屏幕创建好的绘图表面和上下文为当前线程的内容 if(!getDefaultDisplayDevice()-&gt;makeCurrent(mEGLDisplay, mEGLContext)) &#123; ALOGE("DisplayDevice::makeCurrent on default display failed. Aborting."); &#125; return false; &#125; // Never touch the framebuffer if we don't have any framebuffer layers const bool hasHwcComposition = hwc.hasHwcComposition(id); if (hasHwcComposition) &#123;//是否有hwc合成 // when using overlays, we assume a fully transparent framebuffer // NOTE: we could reduce how much we need to clear, for instance // remove where there are opaque FB layers. however, on some // GPUs doing a "clean slate" clear might be more efficient. // We'll revisit later if needed. //如果使用OverLay硬件合成，就要把FrameBuffer全都清除为透明色 engine.clearWithColor(0, 0, 0, 0); &#125; else &#123;//是否有OpenGL合成 //letterbox(信箱模式) // we start with the whole screen area const Region bounds(hw-&gt;getBounds()); // we remove the scissor part // we're left with the letterbox region // (common case is that letterbox ends-up being empty) //移除内容区域，只留下大黑边（剪裁区域） const Region letterbox(bounds.subtract(hw-&gt;getScissor())); // compute the area to clear //计算大黑边区域 Region region(hw-&gt;undefinedRegion.merge(letterbox)); // but limit it to the dirty region //还要限制到更新的脏区域范围内 region.andSelf(dirty); // screen is already cleared here //对大黑边进行挖洞处理，使用OpenGL填成黑色 if (!region.isEmpty()) &#123; // can happen with SurfaceView //SurfaceView的实现原理就是打洞覆盖：另起一个图层（即新建一个Surface）， //并把主图层的相应区域置为透明，然后渲染就发生在新图层中， //最终显示效果自然是依赖SurfaceFlinger的叠加 drawWormhole(hw, region); &#125; &#125; //如果不是主显屏 if (hw-&gt;getDisplayType() != DisplayDevice::DISPLAY_PRIMARY) &#123; // just to be on the safe side, we don't set the // scissor on the main display. It should never be needed // anyways (though in theory it could since the API allows it). //为了安全起见，这个操作不会在主显屏发生 const Rect&amp; bounds(hw-&gt;getBounds()); const Rect&amp; scissor(hw-&gt;getScissor()); if (scissor != bounds) &#123; //如果剪裁区域和屏幕尺寸不匹配，就要清除所有塔之外的内容 //并且enble GL剪裁器，在我们不需要绘制的地方不绘制任何内容 // scissor doesn't match the screen's dimensions, so we // need to clear everything outside of it and enable // the GL scissor so we don't draw anything where we shouldn't // enable scissor for this frame const uint32_t height = hw-&gt;getHeight(); engine.setScissor(scissor.left, height - scissor.bottom, scissor.getWidth(), scissor.getHeight()); &#125; &#125; &#125; //...... &#125; &#160; &#160; &#160; &#160;这部分先调用hasGlesComposition函数和hasHwcComposition函数，就是看其对应的DisplayData中是否有hasFbComp和hasOvComp，对应有无OpenGL合成或者OverLay硬件合成。12345678910bool HWComposer::hasGlesComposition(int32_t id) const &#123; if (!mHwc || uint32_t(id)&gt;31 || !mAllocatedDisplayIDs.hasBit(id)) return true; return mDisplayData[id].hasFbComp; &#125; bool HWComposer::hasHwcComposition(int32_t id) const &#123; if (!mHwc || uint32_t(id)&gt;31 || !mAllocatedDisplayIDs.hasBit(id)) return false; return mDisplayData[id].hasOvComp; &#125; &#160; &#160; &#160; &#160;我们在Android SurfaceFlinger 学习之路(十一)—-合成Layer之准备合成中讲过，而这两个值是在prepare中调用Hwc的prepare函数之后赋值的，具体原理是在Android SurfaceFlinger 学习之路(十一)(PostView)–附：硬件HWC选取合成类型（Intel） 中实现的，如果忘记了可以翻翻上一篇的内容。 &#160; &#160; &#160; &#160;如果有OpenGL的合成，先调用DisplayDevice类的成员函数makeCurrent，通过调用函数eglMakeCurrent来将前面已经创建好的绘图表面以及绘图上下文设置为当前线程的绘图表面以及绘图上下文，即设置为SurfaceFlinger服务的UI渲染线程的绘图表面以及绘图上下文。如果绑定失败了，就要取消设置OpenGL库在当前线程的绘图表面以及绘图上下文。那么就要去设置默认屏幕创建好的绘图表面和上下文为当前线程的内容。 &#160; &#160; &#160; &#160;因为合成有可能是混合合成，所以还要判断是否也有Overlay合成方式。如果有，就要把FrameBuffer全都清除为透明黑色。&#160; &#160; &#160; &#160;如果没有，就是OpenGL单独合成，就要使用信箱模式。&#160; &#160; &#160; &#160;这个只是一种比喻，信箱模式可以查一查资料，比如维基百科)中的解释。我们这里理解为图层的宽高比大于电视的宽高比的情况。从名字来看，letterbox就像邮筒一样，中间是有内容的，上下是没用的。letterbox目标就是保存原有胶片的宽高比，使内容完整的展现在屏幕上。由于内容宽高比较大，所以上下会有黑屏。采用的技术就是：抽调部分扫描线，以满足宽高比，不利的结果就是图像质量下降。 &#160; &#160; &#160; &#160;所以对于这种情况，我们需要对大黑边进行处理：&#160; &#160; &#160; &#160;1）移除内容区域，只留下大黑边（剪裁区域）；&#160; &#160; &#160; &#160;2）计算大黑边区域；&#160; &#160; &#160; &#160;3）还要限制到更新的脏区域范围内；&#160; &#160; &#160; &#160;4）对大黑边进行挖洞处理，使用OpenGL填成黑色&#160; &#160; &#160; &#160;SurfaceView的实现原理就是打洞覆盖：另起一个图层（即新建一个Surface），并把主图层的相应区域置为透明，然后渲染就发生在新图层中，最终显示效果自然是依赖SurfaceFlinger的叠加。 &#160; &#160; &#160; &#160;最后就是一个特殊处理，对于不是主显屏（为了安全起见，这个操作不会在主显屏发生），如果剪裁区域和屏幕尺寸不匹配，就要清除所有塔之外的内容，并且enble GL剪裁器，在我们不需要绘制的地方不绘制任何内容。 &#160; &#160; &#160; &#160;继续往下看：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 /* * and then, render the layers targeted at the framebuffer */ const Vector&lt; sp&lt;Layer&gt; &gt;&amp; layers(hw-&gt;getVisibleLayersSortedByZ()); const size_t count = layers.size(); const Transform&amp; tr = hw-&gt;getTransform(); if (cur != end) &#123;//代表起码有两个以上图层 // we're using h/w composer for (size_t i=0 ; i&lt;count &amp;&amp; cur!=end ; ++i, ++cur) &#123;//遍历图层 const sp&lt;Layer&gt;&amp; layer(layers[i]); //clip用来描述要绘制的区域，而要绘制的区域即为当前需要指定的裁剪区域 //就是全部图层的总体脏区域和这个图层的可视区域的交集 const Region clip(dirty.intersect(tr.transform(layer-&gt;visibleRegion))); if (!clip.isEmpty()) &#123; switch (cur-&gt;getCompositionType()) &#123; //overlay不做处理 case HWC_CURSOR_OVERLAY: case HWC_OVERLAY: &#123; const Layer::State&amp; state(layer-&gt;getDrawingState()); if ((cur-&gt;getHints() &amp; HWC_HINT_CLEAR_FB) &amp;&amp; i &amp;&amp; layer-&gt;isOpaque(state) &amp;&amp; (state.alpha == 0xFF) &amp;&amp; hasGlesComposition) &#123; // never clear the very first layer since we're // guaranteed the FB is already cleared //上面说过，如果是Overlay合成，就要清除FrameBuffer //这里以防万一，确保FB要被清除 layer-&gt;clearWithOpenGL(hw, clip); &#125; break; &#125; //如果是OpenGL合成，就调用Layer的draw函数 case HWC_FRAMEBUFFER: &#123; layer-&gt;draw(hw, clip); break; &#125; //HWC_FRAMEBUFFER_TARGET是OpenGL合成后使用的目标层，直接就跳了 case HWC_FRAMEBUFFER_TARGET: &#123; // this should not happen as the iterator shouldn't // let us get there. ALOGW("HWC_FRAMEBUFFER_TARGET found in hwc list (index=%zu)", i); break; &#125; &#125; &#125; layer-&gt;setAcquireFence(hw, *cur); &#125; &#125; else &#123;//只有一个或者没有图层 就直接使用OpenGL合成，调用Layer的draw // we're not using h/w composer for (size_t i=0 ; i&lt;count ; ++i) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); const Region clip(dirty.intersect( tr.transform(layer-&gt;visibleRegion))); if (!clip.isEmpty()) &#123; layer-&gt;draw(hw, clip); &#125; &#125; &#125; // disable scissor at the end of the frame engine.disableScissor(); return true;&#125; &#160; &#160; &#160; &#160;这一段看注释就知道合成步骤。先根据z-order拿到所有的可视图层，然后分两种情况：&#160; &#160; &#160; &#160;1）起码有两个以上图层 ：遍历图层，用全部图层的总体脏区域和这个图层的可视区域求交集，计算出clip区域。如果clip区域不为空，根据Layer的合成类型分别进行处理： overlay不做处理，但是上面说过，如果是Overlay合成，就要清除FrameBuffer，这里以防万一，确保FB要被清除。 如果是OpenGL合成，就调用Layer的draw函数。 HWC_FRAMEBUFFER_TARGET是OpenGL合成后使用的目标层，直接就跳了。 &#160; &#160; &#160; &#160;2）只有一个或者没有图层 就直接使用OpenGL合成，调用Layer的draw。 &#160; &#160; &#160; &#160;所以合成主要是OpenGL处理，Layer的draw函数。 OpenGL处理合成&#160; &#160; &#160; &#160;我们看看Layer的draw函数实现：123void Layer::draw(const sp&lt;const DisplayDevice&gt;&amp; hw, const Region&amp; clip) const &#123; onDraw(hw, clip, false);&#125; &#160; &#160; &#160; &#160;draw内部又调用了onDraw函数，这个函数有一点点小长，我们依然分段查看：12345678910111213141516171819202122232425262728293031323334353637383940414243void Layer::onDraw(const sp&lt;const DisplayDevice&gt;&amp; hw, const Region&amp; clip, bool useIdentityTransform) const&#123; ATRACE_CALL(); //在纹理未创建好的情况下，一个应用程序窗口是不应该被渲染的 if (CC_UNLIKELY(mActiveBuffer == 0)) &#123; // the texture has not been created yet, this Layer has // in fact never been drawn into. This happens frequently with // SurfaceView because the WindowManager can't know when the client // has drawn the first time. // If there is nothing under us, we paint the screen in black, otherwise // we just skip this update. // figure out if there is something below us //在纹理未创建好的情况下，一个应用程序窗口是不应该被渲染的 //这种情况频繁的发生在SurfaceView身上，因为WindowManger不知道它第一次绘制的时间 //如果没有内容在它下面，我们把整个屏幕都画成黑色 //否则就跳过这次更新 //所以我们需要计算出在它下面的内容 Region under; const SurfaceFlinger::LayerVector&amp; drawingLayers( mFlinger-&gt;mDrawingState.layersSortedByZ); const size_t count = drawingLayers.size(); for (size_t i=0 ; i&lt;count ; ++i) &#123; const sp&lt;Layer&gt;&amp; layer(drawingLayers[i]); if (layer.get() == static_cast&lt;Layer const*&gt;(this)) break; //这时候函数首先将位于当前正在处理的应用程序窗口下面的所有其它应用程序窗口的可见区域组合起来， //并且保存在变量under所描述的区域中 under.orSelf( hw-&gt;getTransform().transform(layer-&gt;visibleRegion) ); &#125; // if not everything below us is covered, we plug the holes! //由于这时候当前正在处理的应用程序窗口不会被绘制， //因此，如果变量under所描述的区域小于参数clip所描述的区域，即变量holes所描述的区域不为空， //那么SurfaceFlinger服务所要求缓制的区域clip就会留下一个洞 Region holes(clip.subtract(under)); //这个洞会被绘制成黑色，这是通过调用函数clearWithOpenGL来实现的 if (!holes.isEmpty()) &#123; clearWithOpenGL(hw, holes, 0, 0, 0, 1); &#125; return; &#125; &#160; &#160; &#160; &#160;这段代码主要是用来处应用程序窗口的纹理尚未创建好的情况。&#160; &#160; &#160; &#160;在纹理未创建好的情况下，一个应用程序窗口是不应该被渲染的。这时候函数首先将位于当前正在处理的应用程序窗口下面的所有其它应用程序窗口的可见区域组合起来，并且保存在变量under所描述的区域中。由于这时候当前正在处理的应用程序窗口不会被绘制，因此，如果变量under所描述的区域小于参数clip所描述的区域，即变量holes所描述的区域不为空，那么SurfaceFlinger服务所要求缓制的区域clip就会留下一个洞。这个洞会被绘制成黑色，这是通过调用函数clearWithOpenGL来实现的。绘制完成之后，函数就可以直接返回了。&#160; &#160; &#160; &#160;这里mActiveBuffer我们在Android SurfaceFlinger 学习之路(十)—-SurfaceFlinger处理Layer更新里面的更新纹理讲过，会为其赋值。 绑定纹理&#160; &#160; &#160; &#160;接着看onDraw函数：12345678// Bind the current buffer to the GL texture, and wait for it to be // ready for us to draw into. status_t err = mSurfaceFlingerConsumer-&gt;bindTextureImage(); if (err != NO_ERROR) &#123; ALOGW("onDraw: bindTextureImage failed (err=%d)", err); // Go ahead and draw the buffer anyway; no matter what we do the screen // is probably going to have something visibly wrong. &#125; &#160; &#160; &#160; &#160;这段代码首先，将buffer加入到GL texture，然后等待底层OpenGL调用GPU绘制完成纹理给我们上层返回。&#160; &#160; &#160; &#160;我们看看SurfaceFlingerConsumer的bindTextureImage函数：12345status_t SurfaceFlingerConsumer::bindTextureImage()&#123; Mutex::Autolock lock(mMutex); return bindTextureImageLocked();&#125; &#160; &#160; &#160; &#160;又调用了父类的GLConsumer的bindTextureImageLocked函数：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253status_t GLConsumer::bindTextureImageLocked() &#123; if (mEglDisplay == EGL_NO_DISPLAY) &#123; ALOGE("bindTextureImage: invalid display"); return INVALID_OPERATION; &#125; GLint error; while ((error = glGetError()) != GL_NO_ERROR) &#123; ST_LOGW("bindTextureImage: clearing GL error: %#04x", error); &#125; //绑定一个Texture纹理，如果没有就创建 glBindTexture(mTexTarget, mTexName); if (mCurrentTexture == BufferQueue::INVALID_BUFFER_SLOT &amp;&amp; mCurrentTextureImage == NULL) &#123; ST_LOGE("bindTextureImage: no currently-bound texture"); return NO_INIT; &#125; //创建一个Image绘制 status_t err = mCurrentTextureImage-&gt;createIfNeeded(mEglDisplay, mCurrentCrop); if (err != NO_ERROR) &#123; ST_LOGW("bindTextureImage: can't create image on display=%p slot=%d", mEglDisplay, mCurrentTexture); return UNKNOWN_ERROR; &#125; //将Image绑定到Texure mCurrentTextureImage-&gt;bindToTextureTarget(mTexTarget); // In the rare case that the display is terminated and then initialized // again, we can't detect that the display changed (it didn't), but the // image is invalid. In this case, repeat the exact same steps while // forcing the creation of a new image. if ((error = glGetError()) != GL_NO_ERROR) &#123; glBindTexture(mTexTarget, mTexName); status_t err = mCurrentTextureImage-&gt;createIfNeeded(mEglDisplay, mCurrentCrop, true); if (err != NO_ERROR) &#123; ST_LOGW("bindTextureImage: can't create image on display=%p slot=%d", mEglDisplay, mCurrentTexture); return UNKNOWN_ERROR; &#125; mCurrentTextureImage-&gt;bindToTextureTarget(mTexTarget); if ((error = glGetError()) != GL_NO_ERROR) &#123; ST_LOGE("bindTextureImage: error binding external image: %#04x", error); return UNKNOWN_ERROR; &#125; &#125; // Wait for the new buffer to be ready. //等待OpenGL处理完成，返回fence return doGLFenceWaitLocked();&#125; &#160; &#160; &#160; &#160;OpenGL绑定纹理需要四步：&#160; &#160; &#160; &#160;1）绑定一个Texture纹理，如果没有就创建；&#160; &#160; &#160; &#160;2）创建一个Image用于绘制；&#160; &#160; &#160; &#160;3）将Image绑定到Texure；&#160; &#160; &#160; &#160;4）等待OpenGL处理完成，返回fence。 &#160; &#160; &#160; &#160;我们依然分不查看：&#160; &#160; &#160; &#160;1）绑定纹理：glBindTexture。&#160; &#160; &#160; &#160;这里有一个特别尴尬的地方，就是我们这里的代码也看不到，因为这和GPU硬件相关，厂商不会开放这些代码的ToT。。。之前我们能看看高通、TI、甚至Intel的硬件模块代码实属运气，Google将其也放入了AOSP部分。但是对于GPU的源代码却看不到了，想想就明白了，现在显卡卖的一点都不比CPU便宜，尤其这两年人工智能、机器学习火起来，还有比特币的高涨，挖矿越来越猛烈，这些都需要超高速密集运算硬件支持，所以GPU价值和价格也有水涨船高之势头。所以指望这些厂商开发源代码实属异想天开=。=&#160; &#160; &#160; &#160;虽然硬件代码看不到，但是我们可以曲线救国，从软件实现去分析功能。软件实现位于/frameworks/native/opengl/libagl/texture.cpp：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253void glBindTexture(GLenum target, GLuint texture)&#123; ogles_context_t* c = ogles_context_t::get(); if (target != GL_TEXTURE_2D &amp;&amp; target != GL_TEXTURE_EXTERNAL_OES) &#123; ogles_error(c, GL_INVALID_ENUM); return; &#125; // Bind or create a texture //绑定或创建一个纹理 sp&lt;EGLTextureObject&gt; tex; if (texture == 0) &#123; // 0 is our local texture object //0表示是本地的纹理对象 tex = c-&gt;textures.defaultTexture; &#125; else &#123; tex = c-&gt;surfaceManager-&gt;texture(texture); if (ggl_unlikely(tex == 0)) &#123; //创建一个纹理 tex = c-&gt;surfaceManager-&gt;createTexture(texture); if (tex == 0) &#123; ogles_error(c, GL_OUT_OF_MEMORY); return; &#125; &#125; &#125; bindTextureTmu(c, c-&gt;textures.active, texture, tex);&#125;void bindTextureTmu( ogles_context_t* c, int tmu, GLuint texture, const sp&lt;EGLTextureObject&gt;&amp; tex)&#123; if (tex.get() == c-&gt;textures.tmu[tmu].texture) return; // free the reference to the previously bound object //释放对之前纹理对象的引用 texture_unit_t&amp; u(c-&gt;textures.tmu[tmu]); if (u.texture) u.texture-&gt;decStrong(c); // bind this texture to the current active texture unit // and add a reference to this texture object //绑定这个纹理对象到当前活跃的纹理单元 //并且增加引用计数 u.texture = tex.get(); u.texture-&gt;incStrong(c); u.name = texture; invalidate_texture(c, tmu);&#125;staticvoid invalidate_texture(ogles_context_t* c, int tmu, uint8_t flags = 0xFF) &#123; c-&gt;textures.tmu[tmu].dirty = flags;&#125; &#160; &#160; &#160; &#160;软件实现也是先创建一个纹理texture对象，然后释放掉对之前纹理对象的引用，然后将引用指向新得纹理，再将引用计数加一，这样完成对当前纹理的绑定操作。 &#160; &#160; &#160; &#160;2）创建一个Image用于绘制：&#160; &#160; &#160; &#160;mCurrentTextureImage是一个 EglImage，我们在Android SurfaceFlinger 学习之路(十)—-SurfaceFlinger处理Layer更新里面的更新纹理讲过，acquireBuffer时候会创建一个EglImage。所以这里调用EglImage的createIfNeeded函数：1234567891011121314151617181920212223242526272829303132333435363738status_t GLConsumer::EglImage::createIfNeeded(EGLDisplay eglDisplay, const Rect&amp; cropRect, bool forceCreation) &#123; // If there's an image and it's no longer valid, destroy it. //如果之前的image不在有效了，就要销毁它 bool haveImage = mEglImage != EGL_NO_IMAGE_KHR; bool displayInvalid = mEglDisplay != eglDisplay; bool cropInvalid = hasEglAndroidImageCrop() &amp;&amp; mCropRect != cropRect; if (haveImage &amp;&amp; (displayInvalid || cropInvalid || forceCreation)) &#123; if (!eglDestroyImageKHR(mEglDisplay, mEglImage)) &#123; ALOGE("createIfNeeded: eglDestroyImageKHR failed"); &#125; mEglImage = EGL_NO_IMAGE_KHR; mEglDisplay = EGL_NO_DISPLAY; &#125; // If there's no image, create one. //如果没有image，就要创建一个 if (mEglImage == EGL_NO_IMAGE_KHR) &#123; mEglDisplay = eglDisplay; mCropRect = cropRect; mEglImage = createImage(mEglDisplay, mGraphicBuffer, mCropRect); &#125; // Fail if we can't create a valid image. //创建失败了，就只能GG了 if (mEglImage == EGL_NO_IMAGE_KHR) &#123; mEglDisplay = EGL_NO_DISPLAY; mCropRect.makeInvalid(); const sp&lt;GraphicBuffer&gt;&amp; buffer = mGraphicBuffer; ALOGE("Failed to create image. size=%ux%u st=%u usage=0x%x fmt=%d", buffer-&gt;getWidth(), buffer-&gt;getHeight(), buffer-&gt;getStride(), buffer-&gt;getUsage(), buffer-&gt;getPixelFormat()); return UNKNOWN_ERROR; &#125; return OK;&#125; &#160; &#160; &#160; &#160;如果之前的image不在有效了，就要销毁它；如果没有image，就要创建一个；创建失败了，就只能GG了。&#160; &#160; &#160; &#160;我们可以继续看createImage函数，如何去创建一个image：123456789101112131415161718192021222324252627282930EGLImageKHR GLConsumer::EglImage::createImage(EGLDisplay dpy, const sp&lt;GraphicBuffer&gt;&amp; graphicBuffer, const Rect&amp; crop) &#123; EGLClientBuffer cbuf = (EGLClientBuffer)graphicBuffer-&gt;getNativeBuffer(); EGLint attrs[] = &#123; EGL_IMAGE_PRESERVED_KHR, EGL_TRUE, EGL_IMAGE_CROP_LEFT_ANDROID, crop.left, EGL_IMAGE_CROP_TOP_ANDROID, crop.top, EGL_IMAGE_CROP_RIGHT_ANDROID, crop.right, EGL_IMAGE_CROP_BOTTOM_ANDROID, crop.bottom, EGL_NONE, &#125;; if (!crop.isValid()) &#123; // No crop rect to set, so terminate the attrib array before the crop. attrs[2] = EGL_NONE; &#125; else if (!isEglImageCroppable(crop)) &#123; // The crop rect is not at the origin, so we can't set the crop on the // EGLImage because that's not allowed by the EGL_ANDROID_image_crop // extension. In the future we can add a layered extension that // removes this restriction if there is hardware that can support it. attrs[2] = EGL_NONE; &#125; /*此句为创建Image的代码*/ EGLImageKHR image = eglCreateImageKHR(dpy, EGL_NO_CONTEXT, EGL_NATIVE_BUFFER_ANDROID, cbuf, attrs); if (image == EGL_NO_IMAGE_KHR) &#123; EGLint error = eglGetError(); ALOGE("error creating EGLImage: %#x", error); &#125; return image;&#125; &#160; &#160; &#160; &#160;上面创建Image主要代码就是eglCreateImageKHR函数，不过依然很悲催，还是和GPU相关的硬件代码。我们只能继续曲线救国了，查看软件创建Image的实现，位于/frameworks/native/opengl/libagl/egl.cpp：1234567891011121314151617181920212223242526272829303132333435EGLImageKHR eglCreateImageKHR(EGLDisplay dpy, EGLContext ctx, EGLenum target, EGLClientBuffer buffer, const EGLint* /*attrib_list*/)&#123; if (egl_display_t::is_valid(dpy) == EGL_FALSE) &#123; return setError(EGL_BAD_DISPLAY, EGL_NO_IMAGE_KHR); &#125; if (ctx != EGL_NO_CONTEXT) &#123; return setError(EGL_BAD_CONTEXT, EGL_NO_IMAGE_KHR); &#125; if (target != EGL_NATIVE_BUFFER_ANDROID) &#123; return setError(EGL_BAD_PARAMETER, EGL_NO_IMAGE_KHR); &#125; //这个buffer还是我们上层acquireBuffer时候获取的图形缓冲区buffer ANativeWindowBuffer* native_buffer = (ANativeWindowBuffer*)buffer; if (native_buffer-&gt;common.magic != ANDROID_NATIVE_BUFFER_MAGIC) return setError(EGL_BAD_PARAMETER, EGL_NO_IMAGE_KHR); if (native_buffer-&gt;common.version != sizeof(ANativeWindowBuffer)) return setError(EGL_BAD_PARAMETER, EGL_NO_IMAGE_KHR); switch (native_buffer-&gt;format) &#123; case HAL_PIXEL_FORMAT_RGBA_8888: case HAL_PIXEL_FORMAT_RGBX_8888: case HAL_PIXEL_FORMAT_RGB_888: case HAL_PIXEL_FORMAT_RGB_565: case HAL_PIXEL_FORMAT_BGRA_8888: break; default: return setError(EGL_BAD_PARAMETER, EGL_NO_IMAGE_KHR); &#125; native_buffer-&gt;common.incRef(&amp;native_buffer-&gt;common); return (EGLImageKHR)native_buffer;&#125; &#160; &#160; &#160; &#160;说是创建一个EGLImageKHR，其实还是对我们上层acquireBuffer时候获取的图形缓冲区buffer，进行一些处理包装。因为这是软件实现，所以参考价值不大吧。 &#160; &#160; &#160; &#160;3）将Image绑定到Texure：123void GLConsumer::EglImage::bindToTextureTarget(uint32_t texTarget) &#123; glEGLImageTargetTexture2DOES(texTarget, (GLeglImageOES)mEglImage);&#125; &#160; &#160; &#160; &#160;这次就真尴尬了，别说没硬件实现了，连软件都没得看了。。。。。。曲线救国没戏了，大清要亡啊。。。。。&#160; &#160; &#160; &#160;中华民国到来，学医救不了中国。。。。。。java救不了中国，但是php能=。=&#160; &#160; &#160; &#160;沦落到看模拟器实现绑定，看看goldfish的实现，位于device/generic/goldfish/opengl/system/GLESv2/gl2.cpp：1234567891011121314151617181920212223242526//GL extensionsvoid glEGLImageTargetTexture2DOES(void * self, GLenum target, GLeglImageOES image)&#123; DBG("glEGLImageTargetTexture2DOES v2 target=%#x img=%p\n", target, image); //TODO: check error - we don't have a way to set gl error android_native_buffer_t* native_buffer = (android_native_buffer_t*)image; if (native_buffer-&gt;common.magic != ANDROID_NATIVE_BUFFER_MAGIC) &#123; return; &#125; if (native_buffer-&gt;common.version != sizeof(android_native_buffer_t)) &#123; return; &#125; GET_CONTEXT; DEFINE_AND_VALIDATE_HOST_CONNECTION(); //openGL的context上下文覆盖纹理坐标 ctx-&gt;override2DTextureTarget(target); //然后用渲染引擎绑定Image的缓冲区引用 rcEnc-&gt;rcBindTexture(rcEnc, ((cb_handle_t *)(native_buffer-&gt;handle))-&gt;hostHandle); //然后恢复纹理 ctx-&gt;restore2DTextureTarget(); return;&#125; &#160; &#160; &#160; &#160;看代码核心也是三步：openGL的context上下文覆盖纹理坐标；然后用渲染引擎绑定Image的缓冲区引用；然后恢复纹理。 &#160; &#160; &#160; &#160;4）等待OpenGL处理完成，返回fence：&#160; &#160; &#160; &#160;上一节更新纹理时候讲过，里面调用acquireBufferLocked，buffer状态迁移到acquire，获得要显示出来的Buffer。这里会创建一个Fence，目前acquire fencefd还没使用，因为还未去合成这个layer，没到用layer中数据的时候。&#160; &#160; &#160; &#160;如果到了合成步骤，这个Fence才是使用的时机，就是接下来第四部做的操作：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 等待生产者的acquire fence触发status_t GLConsumer::doGLFenceWaitLocked() const &#123; EGLDisplay dpy = eglGetCurrentDisplay(); EGLContext ctx = eglGetCurrentContext(); if (mEglDisplay != dpy || mEglDisplay == EGL_NO_DISPLAY) &#123; ST_LOGE("doGLFenceWait: invalid current EGLDisplay"); return INVALID_OPERATION; &#125; if (mEglContext != ctx || mEglContext == EGL_NO_CONTEXT) &#123; ST_LOGE("doGLFenceWait: invalid current EGLContext"); return INVALID_OPERATION; &#125; //等待生产者的acquire fence触发， if (mCurrentFence-&gt;isValid()) &#123; //检查EGL库是否支持EGL_ANDROID_native_fence_sync或者EGL_KHR_fence_sync属性的fence if (SyncFeatures::getInstance().useWaitSync()) &#123; // Create an EGLSyncKHR from the current fence. //dup一份上层传来的fence int fenceFd = mCurrentFence-&gt;dup(); if (fenceFd == -1) &#123; ST_LOGE("doGLFenceWait: error dup'ing fence fd: %d", errno); return -errno; &#125; EGLint attribs[] = &#123; EGL_SYNC_NATIVE_FENCE_FD_ANDROID, fenceFd, EGL_NONE &#125;; //创建一个GPU的fence EGLSyncKHR sync = eglCreateSyncKHR(dpy, EGL_SYNC_NATIVE_FENCE_ANDROID, attribs); if (sync == EGL_NO_SYNC_KHR) &#123; close(fenceFd); ST_LOGE("doGLFenceWait: error creating EGL fence: %#x", eglGetError()); return UNKNOWN_ERROR; &#125; // XXX: The spec draft is inconsistent as to whether this should // return an EGLint or void. Ignore the return value for now, as // it's not strictly needed. //GPU的fence等待，绘制完成会唤醒 eglWaitSyncKHR(dpy, sync, 0); EGLint eglErr = eglGetError(); //唤醒后移除fence围栏 eglDestroySyncKHR(dpy, sync); if (eglErr != EGL_SUCCESS) &#123; ST_LOGE("doGLFenceWait: error waiting for EGL fence: %#x", eglErr); return UNKNOWN_ERROR; &#125; &#125; else &#123;//不支持则上层fence去wait，一般都是-1 status_t err = mCurrentFence-&gt;waitForever( "GLConsumer::doGLFenceWaitLocked"); if (err != NO_ERROR) &#123; ST_LOGE("doGLFenceWait: error waiting for fence: %d", err); return err; &#125; &#125; &#125; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;等待逻辑步骤如下：&#160; &#160; &#160; &#160;1）这里会先等待生产者的acquire fence触发，检查这个Fence是否生效；&#160; &#160; &#160; &#160;2）然后查询EGL库是否支持EGL_ANDROID_native_fence_sync或者EGL_KHR_fence_sync属性的fence，这些都是从OpenGL相关的so库中获取，我们看看实现，位于frameworks/native/libs/gui/SyncFeatures.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051SyncFeatures::SyncFeatures() : Singleton&lt;SyncFeatures&gt;(), mHasNativeFenceSync(false), mHasFenceSync(false), mHasWaitSync(false) &#123; EGLDisplay dpy = eglGetDisplay(EGL_DEFAULT_DISPLAY); // This can only be called after EGL has been initialized; otherwise the // check below will abort. const char* exts = eglQueryStringImplementationANDROID(dpy, EGL_EXTENSIONS); LOG_ALWAYS_FATAL_IF(exts == NULL, "eglQueryStringImplementationANDROID failed"); if (strstr(exts, "EGL_ANDROID_native_fence_sync")) &#123; // This makes GLConsumer use the EGL_ANDROID_native_fence_sync // extension to create Android native fences to signal when all // GLES reads for a given buffer have completed. mHasNativeFenceSync = true; &#125; if (strstr(exts, "EGL_KHR_fence_sync")) &#123; mHasFenceSync = true; &#125; if (strstr(exts, "EGL_KHR_wait_sync")) &#123; mHasWaitSync = true; &#125; mString.append("[using:"); if (useNativeFenceSync()) &#123; mString.append(" EGL_ANDROID_native_fence_sync"); &#125; if (useFenceSync()) &#123; mString.append(" EGL_KHR_fence_sync"); &#125; if (useWaitSync()) &#123; mString.append(" EGL_KHR_wait_sync"); &#125; mString.append("]");&#125;bool SyncFeatures::useNativeFenceSync() const &#123; // EGL_ANDROID_native_fence_sync is not compatible with using the // EGL_KHR_fence_sync extension for the same purpose. return mHasNativeFenceSync;&#125;bool SyncFeatures::useFenceSync() const &#123;#ifdef DONT_USE_FENCE_SYNC // on some devices it's better to not use EGL_KHR_fence_sync // even if they have it return false;#endif // currently we shall only attempt to use EGL_KHR_fence_sync if // USE_FENCE_SYNC is set in our makefile return !mHasNativeFenceSync &amp;&amp; mHasFenceSync;&#125;bool SyncFeatures::useWaitSync() const &#123; return (useNativeFenceSync() || useFenceSync()) &amp;&amp; mHasWaitSync;&#125; &#160; &#160; &#160; &#160;查询GPU相关属性在eglQueryStringImplementationANDROID函数，不过这次我们曲线救国都没用了，中华民国也GG了=。=想想也是，Fence实现每家GPU都不同，并且也算比较重要的技术，肯定不会给我们开放的。。。 &#160; &#160; &#160; &#160;3）如果支持硬件Fence围栏，那么就用上层传入的fence去dup一份copy，然后根据这个copy创建一个GPU的fence。eglCreateSyncKHR依然GG，php也救不了中国～ &#160; &#160; &#160; &#160;4）GPU的fence等待，绘制完成会唤醒。等待操作GG。。。唤醒后就要销毁这个fence，放开栅栏； &#160; &#160; &#160; &#160;5）fence的合法性验证为是否为-1，如果上层使用OpenGL绘制，那么fence就不为-1，下层也需要OpenGL绘制纹理；如果上层使用canvas绘制，那么fence就为-1，则gpu不等待，直接合成ok。 &#160; &#160; &#160; &#160;这就是OpenGL绑定纹理的操作，其中比较重要的是Fence在中间穿插的作用，这里只是简单分析，如果有机会，我会抽出一章专门分析一下。 drawWithOpenGL&#160; &#160; &#160; &#160;我们继续回到Layer的onDraw函数，继续分析合成步骤。&#160; &#160; &#160; &#160;继续看下一部分：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//这个Layer是否是受保护的，比如截屏的Layerbool blackOutLayer = isProtected() || (isSecure() &amp;&amp; !hw-&gt;isSecure()); RenderEngine&amp; engine(mFlinger-&gt;getRenderEngine()); //不是受保护的Layer if (!blackOutLayer) &#123; // TODO: we could be more subtle with isFixedSize() //是否是规则的形状，或者上层应用程序有能力确认缓冲区的有效性 //如果isFixedSize为true，那么就很容易处理图形的变换 const bool useFiltering = getFiltering() || needsFiltering(hw) || isFixedSize(); // Query the texture matrix given our current filtering mode. float textureMatrix[16]; mSurfaceFlingerConsumer-&gt;setFilteringEnabled(useFiltering); mSurfaceFlingerConsumer-&gt;getTransformMatrix(textureMatrix); //如果这个图形缓冲区之前曾经被旋转过，例如，被水平翻转或者垂直翻转过， //那么在对它进行合并之前，还需要将它的旋转方向恢复回来 if (mSurfaceFlingerConsumer-&gt;getTransformToDisplayInverse()) &#123; /* * the code below applies the display's inverse transform to the texture transform */ // create a 4x4 transform matrix from the display transform flags const mat4 flipH(-1,0,0,0, 0,1,0,0, 0,0,1,0, 1,0,0,1); const mat4 flipV( 1,0,0,0, 0,-1,0,0, 0,0,1,0, 0,1,0,1); const mat4 rot90( 0,1,0,0, -1,0,0,0, 0,0,1,0, 1,0,0,1); mat4 tr; uint32_t transform = hw-&gt;getOrientationTransform(); if (transform &amp; NATIVE_WINDOW_TRANSFORM_ROT_90) tr = tr * rot90; if (transform &amp; NATIVE_WINDOW_TRANSFORM_FLIP_H) tr = tr * flipH; if (transform &amp; NATIVE_WINDOW_TRANSFORM_FLIP_V) tr = tr * flipV; // calculate the inverse //对它进行合并之前，还需要将它的旋转方向恢复回来 //所以需要反转变换矩阵 tr = inverse(tr); // and finally apply it to the original texture matrix //然后将它应用到纹理中 const mat4 texTransform(mat4(static_cast&lt;const float*&gt;(textureMatrix)) * tr); memcpy(textureMatrix, texTransform.asArray(), sizeof(textureMatrix)); &#125; // Set things up for texturing. //将上面对纹理的配置设置进来 mTexture.setDimensions(mActiveBuffer-&gt;getWidth(), mActiveBuffer-&gt;getHeight()); mTexture.setFiltering(useFiltering); mTexture.setMatrix(textureMatrix); engine.setupLayerTexturing(mTexture); &#125; else &#123; engine.setupLayerBlackedOut(); &#125; &#160; &#160; &#160; &#160;我们处理合成的Layer一般都是不受保护的Layer，如果是截屏或者其他被保护的Layer，blackOutLayer就为true。&#160; &#160; &#160; &#160;这里再说一下isFixedSize，这个在讲Android SurfaceFlinger 学习之路(九)—-SurfaceFlinger事务处理的时候提到过，我们再复习一下。 &#160; &#160; &#160; &#160;Layer类的成员变量mFixedSize是一个布尔变量，它的值可以通过Layer类的成员函数isFixedSize来获得。从前面Android SurfaceFlinger 学习之路(六)—-SurfaceFlinger创建Surface可以知道，当Android应用程序请求SurfaceFlinger服务分配一块图形缓冲区时，Android应用程序会传递两个参数reqWidth和reqHeight过来，表示请求分配的图形缓冲区的宽度和高度。这两个参数是可以同时等于0的，表示使用默认的宽度和高度值来创建所请求的图形缓冲区。这两个默认的宽度和高度值即等于当前所处理的应用程序窗口的宽度和高度值，而后者的宽度和高度值是在其创建的时候指定的。&#160; &#160; &#160; &#160;Layer类的成员函数requestBuffer的参数reqWidth和reqHeight的值等于0意味着什么呢？从前面Android SurfaceFlinger 学习之路(八)—-Surface管理图形缓冲区可以知道，Android应用程序在请求SurfaceFlinger服务分配一块图形缓冲区之前，会通过在Surface类内部来检查当前所处理的应用程序窗口的大小是否发生了变化。如果发生了变化，那么Android应用程序就会忽略掉缓存自己一侧的图形缓冲区，而去SurfaceFlinger服务请求新的图形缓冲区，因为那些缓存的图形缓冲区由于与它们所关联的应用程序窗口大小发生了变化而变为无效了。但是有一种特殊情况，在Android应用程序这一侧，用来描述应用程序窗口的Surface类可以不维护应用程序窗口的大小值。在这种情况下，Surface类就会将与它所关联的应用程序窗口的大小值设置为0，这意味着Android应用程序每次为这个应用程序窗口向SurfaceFlinger服务请求分配图形缓冲区之前，都认为这个应用程序窗口的大小值没有发生变化，同时传递给Layer类的参数reqWidth和reqHeight的值会等于0。事实上，一个应用程序窗口的大小是随时都可以发生变化的，比如，我们可以通过调用用来在Android应用程序和SurfaceFlinger服务建立连接的一个类型为Client的Binder对象的成员函数setState来改变一个应用程序窗口的大小，而一旦一个应用程序窗口的大小发生了变化，Layer类的成员函数doTransaction就会被调用。&#160; &#160; &#160; &#160;Layer类的成员函数doTransaction在处理应用程序窗口大小变化时，需要考虑Android应用程序每次在为该应用程序窗口向SurfaceFlinger服务请求分配图形缓冲区之前，是否有能力去判断之前为该应用程序窗口缓存的图形缓冲区的有效性。如果没有的话，那么Layer类的成员函数doTransaction就需要将为该应用程序窗口缓存的图形缓冲区设置为无效，以便以后Android应用程序可以请求SurfaceFlinger服务分配新的、大小正确的图形缓冲区。从前面的分析还可以知道，当Android应用程序没有能力去判断之前为一个应用程序窗口所缓存的图形缓冲区的有效性时，那么之前在请求分配这些图形缓冲区时，传递给Layer类的的参数reqWidth和reqHeight的值就会等于0，这时候Layer类就会将Layer类的成员变量mFixedSize的值设置为false。&#160; &#160; &#160; &#160;当Layer类的成员变量mFixedSize的值等于false时，由于Android应用程序没有能力去判断之前为一个应用程序窗口所缓存的图形缓冲区的有效性，因此，Layer类的成员函数doTransaction除了会调用外一个成员函数setBuffers来将新的应用程序窗口大小记录下来之外，还会通知BufferQueueProducer将当前正在处理的应用程序窗口所缓存的图形缓冲区设置为无效。&#160; &#160; &#160; &#160;当Layer类的成员变量mFixedSize的值等于false时，Layer类的成员函数doTransaction还会提前将成员变量mCurrentState所描述的一个State对象的成员变量requested_w和requested_h的值保存到成员变量mDrawingState所描述的一个State对象的成员变量requested_w和requested_h中去，这是为了避免后面调用父类Layer的成员函数doTransaction时，会返回一个Layer::eVisibleRegion位不等于0的标志值给前面，而这将会导致SurfaceFlinger服务马上重新计算各个应用程序窗口的可见区域。现在不返回一个Layer::eVisibleRegion位不等于0的标志值给前面，就会等到下次渲染当前正在处理的应用程序窗口时再重新计算各个应用程序窗口的可见区域。 &#160; &#160; &#160; &#160;太长了，跳过~&#160; &#160; &#160; &#160;GLConsumer的构造函数中mFilteringEnabled(true)，所以filter为true。filter参数是OpenGL中使用纹理坐标映射到纹素数组，比如我们在doTransatin时候提到的线性滤波方法(linear filtering)。要理解这个也是需要OpenGL基础啊，奈何我也是小白=。=不过我们知道这里就是打开了滤波设置。 &#160; &#160; &#160; &#160;接下来代码用来检查当前正在处理的应用程序窗口的图形缓冲区是否是一个可以跳过合成阶段的图形缓冲区。本来这种图形缓冲区是可以直接渲染到硬件帧缓冲区中去的，但是由于它不是全屏显示的，因此就需要与其它应用程序窗口的图形缓冲区进行合并操作。如果这个图形缓冲区之前曾经被旋转过，例如，被水平翻转或者垂直翻转过，那么在对它进行合并之前，还需要将它的旋转方向恢复回来。&#160; &#160; &#160; &#160;当用来描述一个图形缓冲区的一个GraphicBuffer对象的成员变量transform的值不等于0时，那么就说明这个图形缓冲区是被旋转过的，这时候函数就会对这个成员变量的值的NATIVE_WINDOW_TRANSFORM_FLIP_H位或者NATIVE_WINDOW_TRANSFORM_FLIP_V位进行取反，目的就是为了恢复它之前的旋转方向。 &#160; &#160; &#160; &#160;然后进入最后一部分，drawWithOpenGL：12drawWithOpenGL(hw, clip, useIdentityTransform);engine.disableTexturing(); &#160; &#160; &#160; &#160;drawWithOpenGL来将这个图形缓冲区的内容绘制在系统的主显示屏的指定区域上来，这个图形缓冲区的内容是使用mesh对象来描述的，包括顶点缓冲,索引缓冲,纹理对象等，而指定的主显示屏区域是由参数clip来描述的。123456789101112131415161718192021222324252627282930313233343536373839404142void Layer::drawWithOpenGL(const sp&lt;const DisplayDevice&gt;&amp; hw, const Region&amp; /* clip */, bool useIdentityTransform) const &#123; const uint32_t fbHeight = hw-&gt;getHeight(); const State&amp; s(getDrawingState()); computeGeometry(hw, mMesh, useIdentityTransform); /* * NOTE: the way we compute the texture coordinates here produces * different results than when we take the HWC path -- in the later case * the "source crop" is rounded to texel boundaries. * This can produce significantly different results when the texture * is scaled by a large amount. * * The GL code below is more logical (imho), and the difference with * HWC is due to a limitation of the HWC API to integers -- a question * is suspend is whether we should ignore this problem or revert to * GL composition when a buffer scaling is applied (maybe with some * minimal value)? Or, we could make GL behave like HWC -- but this feel * like more of a hack. */ const Rect win(computeBounds()); float left = float(win.left) / float(s.active.w); float top = float(win.top) / float(s.active.h); float right = float(win.right) / float(s.active.w); float bottom = float(win.bottom) / float(s.active.h); // TODO: we probably want to generate the texture coords with the mesh // here we assume that we only have 4 vertices Mesh::VertexArray&lt;vec2&gt; texCoords(mMesh.getTexCoordArray&lt;vec2&gt;()); texCoords[0] = vec2(left, 1.0f - top); texCoords[1] = vec2(left, 1.0f - bottom); texCoords[2] = vec2(right, 1.0f - bottom); texCoords[3] = vec2(right, 1.0f - top); RenderEngine&amp; engine(mFlinger-&gt;getRenderEngine()); engine.setupLayerBlending(mPremultipliedAlpha, isOpaque(s), s.alpha); engine.drawMesh(mMesh); engine.disableBlending();&#125; &#160; &#160; &#160; &#160;这个我们也是分步查看：1234567891011121314151617181920212223242526272829303132 //主显示屏高度 const uint32_t fbHeight = hw-&gt;getHeight(); //当前正在处理的应用程序窗口状态的一个State对象s const State&amp; s(getDrawingState()); computeGeometry(hw, mMesh, useIdentityTransform);/*-----------------*/void Layer::computeGeometry(const sp&lt;const DisplayDevice&gt;&amp; hw, Mesh&amp; mesh, bool useIdentityTransform) const&#123; const Layer::State&amp; s(getDrawingState()); //上一步反转后的变换transform const Transform tr(useIdentityTransform ? hw-&gt;getTransform() : hw-&gt;getTransform() * s.transform); const uint32_t hw_h = hw-&gt;getHeight(); Rect win(s.active.w, s.active.h); if (!s.active.crop.isEmpty()) &#123; win.intersect(s.active.crop, &amp;win); &#125; // subtract the transparent region and snap to the bounds //应用程序窗口减去透明区域 win = reduce(win, s.activeTransparentRegion); //获取纹理的位置坐标 Mesh::VertexArray&lt;vec2&gt; position(mesh.getPositionArray&lt;vec2&gt;()); position[0] = tr.transform(win.left, win.top); position[1] = tr.transform(win.left, win.bottom); position[2] = tr.transform(win.right, win.bottom); position[3] = tr.transform(win.right, win.top); for (size_t i=0 ; i&lt;4 ; i++) &#123; position[i].y = hw_h - position[i].y; &#125;&#125; &#160; &#160; &#160; &#160;这段代码首先得到主显示屏的高度fbHeight、要绘制的区域mesh的位置坐标，以及用来描述当前正在处理的应用程序窗口状态的一个State对象s。 &#160; &#160; &#160; &#160;接着往下看：123456789101112131415161718192021222324 const Rect win(computeBounds()); float left = float(win.left) / float(s.active.w); float top = float(win.top) / float(s.active.h); float right = float(win.right) / float(s.active.w); float bottom = float(win.bottom) / float(s.active.h); // TODO: we probably want to generate the texture coords with the mesh // here we assume that we only have 4 vertices Mesh::VertexArray&lt;vec2&gt; texCoords(mMesh.getTexCoordArray&lt;vec2&gt;()); texCoords[0] = vec2(left, 1.0f - top); texCoords[1] = vec2(left, 1.0f - bottom); texCoords[2] = vec2(right, 1.0f - bottom); texCoords[3] = vec2(right, 1.0f - top);/*-----------------*/Rect Layer::computeBounds() const &#123; const Layer::State&amp; s(getDrawingState()); Rect win(s.active.w, s.active.h); if (!s.active.crop.isEmpty()) &#123; win.intersect(s.active.crop, &amp;win); &#125; // subtract the transparent region and snap to the bounds return reduce(win, s.activeTransparentRegion);&#125; &#160; &#160; &#160; &#160;上面是根据mesh的位置计算出纹理顶点的坐标。&#160; &#160; &#160; &#160;如果Android应用程序没有指定一个窗口的纹理坐标，那么这个窗口的纹理坐标的默认值就使用要绘制的纹理的四个角的坐标来描述。注意，在计算纹理坐标的时候，还要考虑纹理的大小，以及纹理本身所设置的缩放因子，以便可以正确地将纹理绘制在应用程序窗口中。 &#160; &#160; &#160; &#160;继续往下，先看看这一步：12RenderEngine&amp; engine(mFlinger-&gt;getRenderEngine());engine.setupLayerBlending(mPremultipliedAlpha, isOpaque(s), s.alpha); &#160; &#160; &#160; &#160;Android在此新增一个RenderEngine类，用来屏蔽OpenGL ES1.0、1.1和2.0的用法差异。基本用法和opengl是一样的，没有简化太多。&#160; &#160; &#160; &#160;这一步是对颜色的处理，我们看看OpenGL E2.0的实现，位于frameworks/native/services/surfaceflinger/RenderEngine/GLES20RenderEngine.cpp：1234567891011121314void GLES20RenderEngine::setupDimLayerBlending(int alpha) &#123; mState.setPlaneAlpha(1.0f); mState.setPremultipliedAlpha(true); mState.setOpaque(false); mState.setColor(0, 0, 0, alpha/255.0f); mState.disableTexture(); if (alpha == 0xFF) &#123; glDisable(GL_BLEND); &#125; else &#123; glEnable(GL_BLEND); glBlendFunc(GL_ONE, GL_ONE_MINUS_SRC_ALPHA); &#125;&#125; &#160; &#160; &#160; &#160;这里是设定图层混合的模式（mPremultipliedAlpha表示该图层是否已经做过预乘处理，Opaque表示该图层像素是否无视本图层的透明度，s.alpha表示该图层的整体透明度）。是否需要以混合模式来绘制，这是通过调用函数glEnable(GL_BLEND)来实现的。在需要混合模式来绘制纹理texture的情况下，还需要调用函数glBlendFunc来指定混合函数。&#160; &#160; &#160; &#160;当前正在处理的应用程序窗口的Alpha通道的值小于0xFF，即State对象s的成员变量alpha的值小于0xFF，这表明该窗口的背景是半透明的。此时就需要使用混合模式来绘制纹理。 &#160; &#160; &#160; &#160;我们看最后一部分：1234567891011121314151617181920212223242526272829303132 engine.drawMesh(mMesh); engine.disableBlending();/*---------GLES20RenderEngine.cpp---------*/void GLES20RenderEngine::drawMesh(const Mesh&amp; mesh) &#123; ProgramCache::getInstance().useProgram(mState); //指定要绘制的顶点数组 if (mesh.getTexCoordsSize()) &#123; glEnableVertexAttribArray(Program::texCoords); glVertexAttribPointer(Program::texCoords, mesh.getTexCoordsSize(), GL_FLOAT, GL_FALSE, mesh.getByteStride(), mesh.getTexCoords()); &#125; //指定要绘制的纹理坐标 glVertexAttribPointer(Program::position, mesh.getVertexSize(), GL_FLOAT, GL_FALSE, mesh.getByteStride(), mesh.getPositions()); //绘制前面指定的顶点数组以及纹理 glDrawArrays(mesh.getPrimitive(), 0, mesh.getVertexCount()); if (mesh.getTexCoordsSize()) &#123; glDisableVertexAttribArray(Program::texCoords); &#125;&#125;void GLES20RenderEngine::disableBlending() &#123; glDisable(GL_BLEND);&#125; &#160; &#160; &#160; &#160;分别调用函数glVertexPointer和glTexCoordPointer来指定要绘制的顶点数组以及纹理坐标。设置好要绘制的顶点数组以及纹理坐标之后，最后调用函数glDrawArrays来绘制前面指定的顶点数组以及纹理。&#160; &#160; &#160; &#160;其中这三个函数依然让我们曲线救国失败啊，大清彻底亡了=。= &#160; &#160; &#160; &#160;doComposeSurfaces流程就分析完了，我们回到SF的doDisplayComposition函数继续。 &#160; &#160; &#160; &#160;到此合成流程就完了，接下来到了和FB交换buffer。 交换合成缓冲区&#160; &#160; &#160; &#160;沿着上面到了doDisplayComposition的最后一步。 与fb交换脏区域，会调用eglSwapBuffer函数，使用egl将egl中的合成好的图像，输出到DisplayDevice的mSurface中。&#160; &#160; &#160; &#160;继续查看，调用DisplayDevice的swapBuffers函数：123456789101112131415161718192021222324252627282930313233343536void DisplayDevice::swapBuffers(HWComposer&amp; hwc) const &#123; // We need to call eglSwapBuffers() if: // (1) we don't have a hardware composer, or // (2) we did GLES composition this frame, and either // (a) we have framebuffer target support (not present on legacy // devices, where HWComposer::commit() handles things); or // (b) this is a virtual display //当以下几种情况我们会调用eglSwapBuffers： //1.没有硬件合成器 //2.使用OpenGL合成这一帧时候，以下两种情况之一 //a.有FRAME_BUFFER_TARGET支持（overlay合成时候还有遗留的设备） //b.这是一个虚拟显屏 if (hwc.initCheck() != NO_ERROR || (hwc.hasGlesComposition(mHwcDisplayId) &amp;&amp; (hwc.supportsFramebufferTarget() || mType &gt;= DISPLAY_VIRTUAL))) &#123; //调用OpenGL库中的函数eglSwapBuffers来将系统的UI渲染到系统的主绘图表面上去的 EGLBoolean success = eglSwapBuffers(mDisplay, mSurface); if (!success) &#123; EGLint error = eglGetError(); if (error == EGL_CONTEXT_LOST || mType == DisplayDevice::DISPLAY_PRIMARY) &#123; LOG_ALWAYS_FATAL("eglSwapBuffers(%p, %p) failed with 0x%08x", mDisplay, mSurface, error); &#125; else &#123; ALOGE("eglSwapBuffers(%p, %p) failed with 0x%08x", mDisplay, mSurface, error); &#125; &#125; &#125; status_t result = mDisplaySurface-&gt;advanceFrame(); if (result != NO_ERROR) &#123; ALOGE("[%s] failed pushing new frame to HWC: %d", mDisplayName.string(), result); &#125;&#125; &#160; &#160; &#160; &#160;调用OpenGL库中的函数eglSwapBuffers来将系统的UI渲染到系统的主绘图表面上去的，即渲染到系统的硬件帧缓冲区上去的。&#160; &#160; &#160; &#160;这里天不亡大清~~继续曲线救国，我们看看软件的实现，位于frameworks/native/opengl/libagl/egl.cpp：1234567891011121314151617181920212223242526272829EGLBoolean eglSwapBuffers(EGLDisplay dpy, EGLSurface draw)&#123; if (egl_display_t::is_valid(dpy) == EGL_FALSE) return setError(EGL_BAD_DISPLAY, EGL_FALSE); //将EGLSurface 强转为egl_surface_t结构体指针 egl_surface_t* d = static_cast&lt;egl_surface_t*&gt;(draw); if (!d-&gt;isValid()) return setError(EGL_BAD_SURFACE, EGL_FALSE); if (d-&gt;dpy != dpy) return setError(EGL_BAD_DISPLAY, EGL_FALSE); // post the surface //然后调用swapBuffers完成交换操作 d-&gt;swapBuffers(); // if it's bound to a context, update the buffer if (d-&gt;ctx != EGL_NO_CONTEXT) &#123; d-&gt;bindDrawSurface((ogles_context_t*)d-&gt;ctx); // if this surface is also the read surface of the context // it is bound to, make sure to update the read buffer as well. // The EGL spec is a little unclear about this. egl_context_t* c = egl_context_t::context(d-&gt;ctx); if (c-&gt;read == draw) &#123; d-&gt;bindReadSurface((ogles_context_t*)d-&gt;ctx); &#125; &#125; return EGL_TRUE;&#125; &#160; &#160; &#160; &#160;先将将EGLSurface 强转为egl_surface_t结构体指针，这里为什么能强转，我们下面一个小节会讲到。接着调用swapBuffers无参函数完成交换。同样位于frameworks/native/opengl/libagl/egl.cpp下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106EGLBoolean egl_window_surface_v2_t::swapBuffers()&#123; if (!buffer) &#123; return setError(EGL_BAD_ACCESS, EGL_FALSE); &#125; /* * Handle eglSetSwapRectangleANDROID() * We copyback from the front buffer */ //如果合成区域的脏区域不为空 if (!dirtyRegion.isEmpty()) &#123; //首先通过andSelf（）函数，算出在buffer中的dirtyRegion的区域 dirtyRegion.andSelf(Rect(buffer-&gt;width, buffer-&gt;height)); //如果上一次buffer不为空 if (previousBuffer) &#123; // This was const Region copyBack, but that causes an // internal compile error on simulator builds //然后调用subtract将oldDirtyRegion中去掉了dirtyRegion区域， //然后见这块区域从previousBuffer拷贝到当前的buffer中。 /*const*/ Region copyBack(Region::subtract(oldDirtyRegion, dirtyRegion)); if (!copyBack.isEmpty()) &#123; void* prevBits; if (lock(previousBuffer, GRALLOC_USAGE_SW_READ_OFTEN, &amp;prevBits) == NO_ERROR) &#123; // copy from previousBuffer to buffer copyBlt(buffer, bits, previousBuffer, prevBits, copyBack); unlock(previousBuffer); &#125; &#125; &#125; //将新的脏区域赋值给oldDirtyRegion oldDirtyRegion = dirtyRegion; &#125; //将少之前buffer的对gralloc的引用 if (previousBuffer) &#123; previousBuffer-&gt;common.decRef(&amp;previousBuffer-&gt;common); previousBuffer = 0; &#125; unlock(buffer); //完成buffer内容的填充，然后将previousBuffer指向buffer previousBuffer = buffer; //这里的nativeWindow是egl_window_surface_v2_t结构体的成员变量 //是一个ANativeWindow*类型 //在DisplayDevice构造函数里调用eglCreateWindowSurface创建egl_window_surface_v2_t实例时候， //new了一个Surface传入的 //这里还调用了Surface的queueBuffer函数 //从之前将Surface管理缓冲区得知，这会回调consumer的onFrameAvailable函数 //调用surface的queueBuffer，agl的实现fencefd输入为-1 //硬件平台不为-1，比如抓高通的log //肯定在前面先创建了fence同步对象，经过merge后肯定不再为-1了 nativeWindow-&gt;queueBuffer(nativeWindow, buffer, -1); buffer = 0; // dequeue a new buffer int fenceFd = -1; //然后dequeue一个新的buffer，并等待fence ///第一次被申请的buffer slot。返回-1 //假设不是。有release fence，则会dup该fencefd if (nativeWindow-&gt;dequeueBuffer(nativeWindow, &amp;buffer, &amp;fenceFd) == NO_ERROR) &#123; sp&lt;Fence&gt; fence(new Fence(fenceFd)); //wait，等待release fence触发 //如果fence超时，就把buffer cancel掉。 if (fence-&gt;wait(Fence::TIMEOUT_NEVER)) &#123; nativeWindow-&gt;cancelBuffer(nativeWindow, buffer, fenceFd); return setError(EGL_BAD_ALLOC, EGL_FALSE); &#125; // reallocate the depth-buffer if needed //按需重新计算buffer if ((width != buffer-&gt;width) || (height != buffer-&gt;height)) &#123; // TODO: we probably should reset the swap rect here // if the window size has changed width = buffer-&gt;width; height = buffer-&gt;height; if (depth.data) &#123; free(depth.data); depth.width = width; depth.height = height; depth.stride = buffer-&gt;stride; depth.data = (GGLubyte*)malloc(depth.stride*depth.height*2); if (depth.data == 0) &#123; setError(EGL_BAD_ALLOC, EGL_FALSE); return EGL_FALSE; &#125; &#125; &#125; // keep a reference on the buffer buffer-&gt;common.incRef(&amp;buffer-&gt;common); // finally pin the buffer down if (lock(buffer, GRALLOC_USAGE_SW_READ_OFTEN | GRALLOC_USAGE_SW_WRITE_OFTEN, &amp;bits) != NO_ERROR) &#123; ALOGE("eglSwapBuffers() failed to lock buffer %p (%ux%u)", buffer, buffer-&gt;width, buffer-&gt;height); return setError(EGL_BAD_ACCESS, EGL_FALSE); // FIXME: we should make sure we're not accessing the buffer anymore &#125; &#125; else &#123; return setError(EGL_BAD_CURRENT_SURFACE, EGL_FALSE); &#125; return EGL_TRUE;&#125; &#160; &#160; &#160; &#160;软件实现大致如下：&#160; &#160; &#160; &#160;1）如果合成区域的脏区域不为空，首先通过andSelf（）函数，算出在buffer中的dirtyRegion的区域，然后调用subtract将oldDirtyRegion中去掉了dirtyRegion区域，然后见这块区域从previousBuffer拷贝到当前的buffer中。这一部分和我门之前讲Android SurfaceFlinger 学习之路(八)—-Surface管理图形缓冲区中Surface申请图形缓冲区过程类似，也是没有变化的区域就用copyBlt拷贝像素。&#160; &#160; &#160; &#160;2）将新的脏区域赋值给oldDirtyRegion，将少之前buffer的对gralloc的引用（这个我们在讲Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现中提到过，这是C语言中的一种继承方式），完成buffer内容的填充，然后将previousBuffer指向buffer。&#160; &#160; &#160; &#160;3）接着调用调用nativeBuffer变量的queueBuffer。这里的nativeWindow是egl_window_surface_v2_t结构体的成员变量，是一个ANativeWindow*类型。在DisplayDevice构造函数里调用eglCreateWindowSurface创建egl_window_surface_v2_t实例时候，new了一个Surface传入的。之前Android SurfaceFlinger 学习之路(六)—-SurfaceFlinger创建Surface讲过，Surface间接继承与ANativeWindow。&#160; &#160; &#160; &#160;从之前将Surface管理缓冲区得知，这会回调consumer的onFrameAvailable函数，后面会讲到，这会触发FrameBufferSurface的onFrameAvailable函数回调。&#160; &#160; &#160; &#160;调用surface的queueBuffer，agl的实现fencefd输入为-1。硬件平台不为-1，比如抓高通的log，肯定在前面先创建了fence同步对象，经过merge后肯定不再为-1了。&#160; &#160; &#160; &#160;4）Dequeue一块新的buffer，并wait，等待release fence触发。如果等待超时，就将buffer cancel掉。第一次被申请的buffer slot，返回-1。假设不是。有release fence，则会dup该fencefd。&#160; &#160; &#160; &#160;5）按需重新计算buffer。 &#160; &#160; &#160; &#160;既然提到交换缓冲区，就是讲生产者的内容post到消费者去，所以我们应该关注nativeWindow的queueBuffer过程。在讲这一部分之前，我们要看看之前讲到的DisplayDevice和FrameBufferSurface关系，还要引入一点OpenGL相关的内容。 OpenGL环境创建&#160; &#160; &#160; &#160;使用OpenGL标准，用GPU把图层画到统一的FrameBuffer上，然后送显。毫无疑问这是离线合成的一种。EGL标准下，OpenGL环境创建的一般流程如下图所示： &#160; &#160; &#160; &#160;这部分工作在SurfaceFlinger::init函数完成，也即服务初起之时。我们在讲Android SurfaceFlinger 学习之路(五)—-VSync 工作原理时候提到过这一部分：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// initialize EGL for the default displaymEGLDisplay = eglGetDisplay(EGL_DEFAULT_DISPLAY);eglInitialize(mEGLDisplay, NULL, NULL);//初始化硬件合成器（这个和3D合成无关）mHwc = new HWComposer(this, *static_cast&lt;HWComposer::EventHandler *&gt;(this));//创建渲染引擎，主要是选择EGL配置，选择OpenGL版本，创建OpenGL上下文mRenderEngine = RenderEngine::create(mEGLDisplay, mHwc-&gt;getVisualID());// retrieve the EGL context that was selected/createdmEGLContext = mRenderEngine-&gt;getEGLContext();LOG_ALWAYS_FATAL_IF(mEGLContext == EGL_NO_CONTEXT, "couldn't create EGLContext");//创建OpenGL的渲染目标Surfacefor (size_t i=0 ; i&lt;DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES ; i++) &#123; DisplayDevice::DisplayType type((DisplayDevice::DisplayType)i); // set-up the displays that are already connected if (mHwc-&gt;isConnected(i) || type==DisplayDevice::DISPLAY_PRIMARY) &#123; // All non-virtual displays are currently considered secure. bool isSecure = true; createBuiltinDisplayLocked(type); wp&lt;IBinder&gt; token = mBuiltinDisplays[i]; sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferConsumer&gt; consumer; BufferQueue::createBufferQueue(&amp;producer, &amp;consumer, new GraphicBufferAlloc()); /*创建窗口Surface所需要的window句柄，注意这里面window句柄是FramebufferSurface*/ sp&lt;FramebufferSurface&gt; fbs = new FramebufferSurface(*mHwc, i, consumer); int32_t hwcId = allocateHwcDisplayId(type); /*在构造函数中，调用 eglCreateSurface 创建了OpenGL渲染的目标Surface*/ sp&lt;DisplayDevice&gt; hw = new DisplayDevice(this, type, hwcId, mHwc-&gt;getFormat(hwcId), isSecure, token, fbs, producer, mRenderEngine-&gt;getEGLConfig()); if (i &gt; DisplayDevice::DISPLAY_PRIMARY) &#123; // FIXME: currently we don't get blank/unblank requests // for displays other than the main display, so we always // assume a connected display is unblanked. ALOGD("marking display %zu as acquired/unblanked", i); hw-&gt;setPowerMode(HWC_POWER_MODE_NORMAL); &#125; mDisplays.add(token, hw); &#125;&#125;// make the GLContext current so that we can create textures when creating Layers// (which may happens before we render something)/*绑定上下文和Surface，以便绘制，这一步在调用OpenGL的drawcall之前就可以，这里调一次貌似是没必要的*/getDefaultDisplayDevice()-&gt;makeCurrent(mEGLDisplay, mEGLContext); &#160; &#160; &#160; &#160;对使用opengl合成的layer将合成结果放置到HWC_FRAMEBUFFER_TARGET layer中，然后再交给HWComposer处理。在surfaceflinger的init函数中，定义了HWC_FRAMEBUFFER_TARGET layer合成时相应的生成者和消费者，每一个display相应有一个DisplayDevice作为生产者(opengl合成数据)，而FramebufferSurface是相应的消费者(注意这个消费者仅仅是处理opengl合成相关的，overlay全然由HAL层的hwcomposer处理)。&#160; &#160; &#160; &#160;注意到，创建的窗口是FramebufferSurface。为了更直观的查看Surface创建流程，我们再把DisplayDevice构造函数贴出来：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677DisplayDevice::DisplayDevice( const sp&lt;SurfaceFlinger&gt;&amp; flinger, DisplayType type, int32_t hwcId, bool isSecure, const wp&lt;IBinder&gt;&amp; displayToken, const sp&lt;DisplaySurface&gt;&amp; displaySurface, const sp&lt;IGraphicBufferProducer&gt;&amp; producer, EGLConfig config) : mFlinger(flinger), mType(type), mHwcDisplayId(hwcId), mDisplayToken(displayToken), mDisplaySurface(displaySurface), mDisplay(EGL_NO_DISPLAY), mSurface(EGL_NO_SURFACE), mDisplayWidth(), mDisplayHeight(), mFormat(), mFlags(), mPageFlipCount(), mIsSecure(isSecure), mSecureLayerVisible(false), mScreenAcquired(false), mLayerStack(NO_LAYER_STACK), mOrientation()&#123; //利用bq创建surface mNativeWindow = new Surface(producer, false); ANativeWindow* const window = mNativeWindow.get(); int format; window-&gt;query(window, NATIVE_WINDOW_FORMAT, &amp;format); // Make sure that composition can never be stalled by a virtual display // consumer that isn't processing buffers fast enough. We have to do this // in two places: // * Here, in case the display is composed entirely by HWC. // * In makeCurrent(), using eglSwapInterval. Some EGL drivers set the // window's swap interval in eglMakeCurrent, so they'll override the // interval we set here. if (mType &gt;= DisplayDevice::DISPLAY_VIRTUAL) window-&gt;setSwapInterval(window, 0); /* * Create our display's surface */ // 利用EGL创建本地opengl环境，要用opengl 合成layer EGLSurface surface; EGLint w, h; EGLDisplay display = eglGetDisplay(EGL_DEFAULT_DISPLAY); //调用eglCreateWindowSurface创建EGLSurface surface = eglCreateWindowSurface(display, config, window, NULL); eglQuerySurface(display, surface, EGL_WIDTH, &amp;mDisplayWidth); eglQuerySurface(display, surface, EGL_HEIGHT, &amp;mDisplayHeight); mDisplay = display; mSurface = surface; mFormat = format; mPageFlipCount = 0; mViewport.makeInvalid(); mFrame.makeInvalid(); // virtual displays are always considered enabled mScreenAcquired = (mType &gt;= DisplayDevice::DISPLAY_VIRTUAL); // Name the display. The name will be replaced shortly if the display // was created with createDisplay(). switch (mType) &#123; case DISPLAY_PRIMARY: mDisplayName = "Built-in Screen"; break; case DISPLAY_EXTERNAL: mDisplayName = "HDMI Screen"; break; default: mDisplayName = "Virtual Screen"; // e.g. Overlay #n break; &#125;&#125; &#160; &#160; &#160; &#160;在DisplayDevice构造函数里，利用从SF的init 函数里传入的BufferQueueProducer构造了一个Surface，然后调用eglCreateWindowSurface创建EGLSurface。&#160; &#160; &#160; &#160;我们知道surface继承了RefBase，所以get()实际上RefBase提供的函数。返回了surface的对象引用。而surface继承了ANativeObjectBase模版，通过ANativeObjectBase模版，可以理解成surface类也继承了AnativeWindow和RefBase。那么surface.get()作为AnativeWindow的类型参数传递给CreateWindowSurface也就好理解咯。也就是window本质上就是surface。&#160; &#160; &#160; &#160;为了解决上面留下的迷惑，我们看看eglCreateWindowSurface的实现。依然需要曲线救国，看看软件实现，位于frameworks/native/opengl/libagl/egl.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657EGLSurface eglCreateWindowSurface( EGLDisplay dpy, EGLConfig config, NativeWindowType window, const EGLint *attrib_list)&#123; return createWindowSurface(dpy, config, window, attrib_list);&#125;static EGLSurface createWindowSurface(EGLDisplay dpy, EGLConfig config, NativeWindowType window, const EGLint* /*attrib_list*/)&#123; /*------前面都是一些错误检查------*/ if (egl_display_t::is_valid(dpy) == EGL_FALSE) return setError(EGL_BAD_DISPLAY, EGL_NO_SURFACE); if (window == 0) return setError(EGL_BAD_MATCH, EGL_NO_SURFACE); EGLint surfaceType; if (getConfigAttrib(dpy, config, EGL_SURFACE_TYPE, &amp;surfaceType) == EGL_FALSE) return EGL_FALSE; if (!(surfaceType &amp; EGL_WINDOW_BIT)) return setError(EGL_BAD_MATCH, EGL_NO_SURFACE); if (static_cast&lt;ANativeWindow*&gt;(window)-&gt;common.magic != ANDROID_NATIVE_WINDOW_MAGIC) &#123; return setError(EGL_BAD_NATIVE_WINDOW, EGL_NO_SURFACE); &#125; EGLint configID; if (getConfigAttrib(dpy, config, EGL_CONFIG_ID, &amp;configID) == EGL_FALSE) return EGL_FALSE; int32_t depthFormat; int32_t pixelFormat; if (getConfigFormatInfo(configID, pixelFormat, depthFormat) != NO_ERROR) &#123; return setError(EGL_BAD_MATCH, EGL_NO_SURFACE); &#125; // FIXME: we don't have access to the pixelFormat here just yet. // (it's possible that the surface is not fully initialized) // maybe this should be done after the page-flip //if (EGLint(info.format) != pixelFormat) // return setError(EGL_BAD_MATCH, EGL_NO_SURFACE); egl_surface_t* surface; //egl_surface_t的详细实现为egl_window_surface_v2_t surface = new egl_window_surface_v2_t(dpy, config, depthFormat, static_cast&lt;ANativeWindow*&gt;(window)); if (!surface-&gt;initCheck()) &#123; // there was a problem in the ctor, the error // flag has been set. delete surface; surface = 0; &#125; return surface;&#125; &#160; &#160; &#160; &#160;前面都是一些错误检测，最后面new了一个egl_window_surface_v2_t结构体实现，egl_surface_t的详细实现为egl_window_surface_v2_t。&#160; &#160; &#160; &#160;我们再看看这个结构体部分内容：1234567891011121314151617181920212223242526272829303132333435363738394041struct egl_window_surface_v2_t : public egl_surface_t&#123; egl_window_surface_v2_t( EGLDisplay dpy, EGLConfig config, int32_t depthFormat, ANativeWindow* window); ~egl_window_surface_v2_t();private: status_t lock(ANativeWindowBuffer* buf, int usage, void** vaddr); status_t unlock(ANativeWindowBuffer* buf); ANativeWindow* nativeWindow; ANativeWindowBuffer* buffer; ANativeWindowBuffer* previousBuffer; gralloc_module_t const* module; int width; int height; void* bits; GGLFormat const* pixelFormatTable;&#125;;egl_window_surface_v2_t::egl_window_surface_v2_t(EGLDisplay dpy, EGLConfig config, int32_t depthFormat, ANativeWindow* window) : egl_surface_t(dpy, config, depthFormat), nativeWindow(window), buffer(0), previousBuffer(0), module(0), bits(NULL)&#123; hw_module_t const* pModule; hw_get_module(GRALLOC_HARDWARE_MODULE_ID, &amp;pModule); module = reinterpret_cast&lt;gralloc_module_t const*&gt;(pModule); pixelFormatTable = gglGetPixelFormatTable(); // keep a reference on the window nativeWindow-&gt;common.incRef(&amp;nativeWindow-&gt;common); nativeWindow-&gt;query(nativeWindow, NATIVE_WINDOW_WIDTH, &amp;width); nativeWindow-&gt;query(nativeWindow, NATIVE_WINDOW_HEIGHT, &amp;height);&#125; &#160; &#160; &#160; &#160;上面提到了，这里的nativeWindow是egl_window_surface_v2_t结构体的成员变量，是一个ANativeWindow*类型。在DisplayDevice构造函数里调用eglCreateWindowSurface创建egl_window_surface_v2_t实例时候，new了一个Surface传入的。&#160; &#160; &#160; &#160;然后就是上面调用Surface的dequeueBuffer，忘了的话可以往上翻=。= &#160; &#160; &#160; &#160;前面提到过。eglSwapBuffers会触发DisplayDevice这个producer去dequeue buffer和queue buffer。这里的opengl合成和上层的opengl画图相似，在queuebuffer中就会为该buffer设置一个acquire buffer，传递给消费者consumer。从SF的init函数中得知，创建的窗口是FramebufferSurface，这个FrameBufferSurface包了一个BufferQueueConsumer，就像我们在Android SurfaceFlinger 学习之路(八)—-Surface管理图形缓冲区中讲到上层绘制通知SF消费时候的SurfaceFlingerConsumer类的功能一样。所以最后queue buffer会触发FramebufferSurface::onFrameAvailable()。&#160; &#160; &#160; &#160;FrameBufrerSurface的类图如下： &#160; &#160; &#160; &#160;我们再看看FrameBufferSurface的构造函数，位于frameworks/native/services/surfaceflinger/DisplayHardware/FramebufferSurface.cpp：123456789101112131415161718192021#ifndef NUM_FRAMEBUFFER_SURFACE_BUFFERS#define NUM_FRAMEBUFFER_SURFACE_BUFFERS (2)#endifFramebufferSurface::FramebufferSurface(HWComposer&amp; hwc, int disp, const sp&lt;IGraphicBufferConsumer&gt;&amp; consumer) : ConsumerBase(consumer), mDisplayType(disp), mCurrentBufferSlot(-1), mCurrentBuffer(0), mHwc(hwc)&#123; mName = "FramebufferSurface"; mConsumer-&gt;setConsumerName(mName); mConsumer-&gt;setConsumerUsageBits(GRALLOC_USAGE_HW_FB | GRALLOC_USAGE_HW_RENDER | GRALLOC_USAGE_HW_COMPOSER); mConsumer-&gt;setDefaultBufferFormat(mHwc.getFormat(disp)); mConsumer-&gt;setDefaultBufferSize(mHwc.getWidth(disp), mHwc.getHeight(disp)); mConsumer-&gt;setDefaultMaxBufferCount(NUM_FRAMEBUFFER_SURFACE_BUFFERS);&#125; &#160; &#160; &#160; &#160;我们注意到mConsumer的setConsumerUsageBits函数中设置的标志位：GRALLOC_USAGE_HW_FB | GRALLOC_USAGE_HW_RENDER | GRALLOC_USAGE_HW_COMPOSER。从之前的Android SurfaceFlinger 学习之路(八)—-Surface管理图形缓冲区讲过，如果是上层应用程序申请图形缓冲区，应用程序进程使用的图形缓冲区一般都是在匿名共享内存里面分配的，这个图形缓冲区填好数据之后，就会再交给SurfaceFlinger服务来合成到硬件帧缓冲区上去渲染。因此，从前面传过来给函数gralloc_alloc的参数usage的GRALLOC_USAGE_HW_FB位会被设置为0，以便可以在匿名共享内存中分配一个图形缓冲区。&#160; &#160; &#160; &#160;而这里使用的标志位不同，GRALLOC_USAGE_HW_FB | GRALLOC_USAGE_HW_RENDER | GRALLOC_USAGE_HW_COMPOSER，从之前Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现可以得知，FrameBufferSurface类使用的图形缓冲区是直接在硬件帧缓冲区分配的，并且它可以直接将这些图形缓冲区渲染到硬件帧缓冲区中去。要从硬件帧缓冲区中分配和渲染图形缓冲区，就必须要将HAL层中的Gralloc模块加载到当前的进程空间来，并且打开里面的gralloc设备和fb设备，其中，gralloc设备用来分配图形缓冲区，而fb设备用来渲染图形缓冲区。 &#160; &#160; &#160; &#160;还有一个注意点就是consumer的setDefaultMaxBufferCount函数，设置大小为NUM_FRAMEBUFFER_SURFACE_BUFFERS，为2。硬件帧缓冲区能够提供的图形缓冲区的个数等于2，这意味着Android系统可以使用双缓冲区技术来渲染系统的UI。&#160; &#160; &#160; &#160;这里有个误区，上面说的双缓冲并不是指4.1之后的Triple Buffer。三缓冲是对于Producer这一侧的生产者来说，我们在Layer的onFirstRef函数中有设置过，在Android SurfaceFlinger 学习之路(六)—-SurfaceFlinger创建Surface中也分析过，回顾一下：123456789101112131415161718192021222324252627void Layer::onFirstRef() &#123; // Creates a custom BufferQueue for SurfaceFlingerConsumer to use sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferConsumer&gt; consumer; //BufferQueue创建图形缓冲区管理成员，我们以后分析图形缓冲区管理会讲到 BufferQueue::createBufferQueue(&amp;producer, &amp;consumer); //mProducer 不为空了，赋值 mProducer = new MonitoredProducer(producer, mFlinger); //mSurfaceFlingerConsumer 不为空了，赋值 mSurfaceFlingerConsumer = new SurfaceFlingerConsumer(consumer, mTextureName); //设置消费者相关设置 mSurfaceFlingerConsumer-&gt;setConsumerUsageBits(getEffectiveUsage(0)); mSurfaceFlingerConsumer-&gt;setContentsChangedListener(this); mSurfaceFlingerConsumer-&gt;setName(mName); // TARGET_DISABLE_TRIPLE_BUFFERING为false，所以使用了三缓冲，project butter计划嘛#ifdef TARGET_DISABLE_TRIPLE_BUFFERING#warning "disabling triple buffering" mSurfaceFlingerConsumer-&gt;setDefaultMaxBufferCount(2);#else mSurfaceFlingerConsumer-&gt;setDefaultMaxBufferCount(3);#endif //获取默认显示器 const sp&lt;const DisplayDevice&gt; hw(mFlinger-&gt;getDefaultDisplayDevice()); //更新显示图像方向 updateTransformHint(hw);&#125; &#160; &#160; &#160; &#160;根据“黄油计划”定义了三缓冲。因为Producer定义了三缓冲，为了避免junk现象，让屏幕始终有一块buffer拉去离屏渲染。所以在producer这一侧，一块给cpu调度，一块给gpu绘制，看看谁有空闲的送给显示屏。如果默认的cpu调度和gpu性能都没有问题，在一个vsync周期内都能完成各自工作，那么以前仅有两块buffer情况下都不会出问题，因为，等他们用完后，总会空出一块buffer送给显示屏。但是如果其中任何一个出了问题，占用时间周期超过了一个vsync，或者更严重，两个都拖堂了，那么显示屏就没有空闲buffer拉取了，就会有很严重的掉帧。所以从4.1之后又加了一块buffer在producer这一侧，保证出问题时候还有一块buffer送显屏幕，即使从时间周期的pipeline来看，就这一个出问题的帧多显示了一下，但是后续的都显示正常，虽然仅仅延时了一帧，但是整体不会像以前那样严重的连续掉下帧。&#160; &#160; &#160; &#160;这就是triple buffer的简单概述，如果有机会，后面专门讲讲。如果我理解有问题，也欢迎大家指正。 &#160; &#160; &#160; &#160;因为对于屏幕显示器来说，上层app和SF都是属于producer生产者，而自己display才是消费者。屏幕是双缓冲显示，一块back buffer作为离屏渲染，一块front buffer作为在线显示，然后用完了又回到后台拉去producer送显的内容作为离屏渲染，同时之前的back buffer交换到前台作为front buffer在线显示，这样周而复始不挺循环。 &#160; &#160; &#160; &#160;这里扯的有点多，如果有误请轻喷，欢迎指出，我会在第一时间修改。 触发渲染&#160; &#160; &#160; &#160;继续回到上面内容，由eglSwapBuffers触发queueBuffer，进而触发FramebufferSurface中的onFrameAvailable方法：12345678910111213141516void FramebufferSurface::onFrameAvailable() &#123; sp&lt;GraphicBuffer&gt; buf; sp&lt;Fence&gt; acquireFence; /*acquireBuffer，取得一块生产完成（OpenGL合成好）的Buffer*/ status_t err = nextBuffer(buf, acquireFence); if (err != NO_ERROR) &#123; ALOGE("error latching nnext FramebufferSurface buffer: %s (%d)", strerror(-err), err); return; &#125; /*最终调用 gralloc 模块中的 post方法，该此Buffer送显*/ err = mHwc.fbPost(mDisplayType, acquireFence, buf); if (err != NO_ERROR) &#123; ALOGE("error posting framebuffer: %d", err); &#125;&#125; &#160; &#160; &#160; &#160;我们先看看nextBuffer函数，取得一块生产完成（OpenGL合成好）的Buffer。（由于surfaceflinger是用opengl合成HWC_FRAMEBUFFER_TARGET layer的，所以有可能“合成”这个生产还未完毕）：12345678910111213141516171819202122232425262728293031323334353637383940414243status_t FramebufferSurface::nextBuffer(sp&lt;GraphicBuffer&gt;&amp; outBuffer, sp&lt;Fence&gt;&amp; outFence) &#123; Mutex::Autolock lock(mMutex); BufferQueue::BufferItem item; //通过acquireBufferLocked获取BufferItem，其中的mBuf就是buffer了 //acquireBufferLocked由之前父类传入的BufferQueueConsumer调用 status_t err = acquireBufferLocked(&amp;item, 0); if (err == BufferQueue::NO_BUFFER_AVAILABLE) &#123; outBuffer = mCurrentBuffer; return NO_ERROR; &#125; else if (err != NO_ERROR) &#123; ALOGE("error acquiring buffer: %s (%d)", strerror(-err), err); return err; &#125; // If the BufferQueue has freed and reallocated a buffer in mCurrentSlot // then we may have acquired the slot we already own. If we had released // our current buffer before we call acquireBuffer then that release call // would have returned STALE_BUFFER_SLOT, and we would have called // freeBufferLocked on that slot. Because the buffer slot has already // been overwritten with the new buffer all we have to do is skip the // releaseBuffer call and we should be in the same state we'd be in if we // had released the old buffer first. //把老的buffer先release掉，还给BufferQueue，release时肯定得加入个release fence //如果在acquireBuffer之前调用releaseBuffer，就会返回一个STALE_BUFFER_SLOT状态，就不得不调用freeBufferLocked释放掉那个过时的buffer回到slot中。 //为了避免获老的buffer被新的buffer内容覆盖，就要先acquire新的buffer然后release旧的buffer if (mCurrentBufferSlot != BufferQueue::INVALID_BUFFER_SLOT &amp;&amp; item.mBuf != mCurrentBufferSlot) &#123; // Release the previous buffer. err = releaseBufferLocked(mCurrentBufferSlot, mCurrentBuffer, EGL_NO_DISPLAY, EGL_NO_SYNC_KHR); if (err &lt; NO_ERROR) &#123; ALOGE("error releasing buffer: %s (%d)", strerror(-err), err); return err; &#125; &#125; mCurrentBufferSlot = item.mBuf; mCurrentBuffer = mSlots[mCurrentBufferSlot].mGraphicBuffer; outFence = item.mFence; outBuffer = mCurrentBuffer; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;获取一块合成好的buffer，就是之前HWC_FRAMEBUFFER标志的所有layer合成到HWC_FRAMEBUFFER_TARGET这个图层（有可能还没有合成好）。通过acquireBufferLocked获取BufferItem，其中的mBuf就是buffer了，acquireBufferLocked由之前父类传入的BufferQueueConsumer调用。&#160; &#160; &#160; &#160;然后把老的buffer先release掉，还给BufferQueue，release时肯定得加入个release fence。如果在acquireBuffer之前调用releaseBuffer，就会返回一个STALE_BUFFER_SLOT状态，就不得不调用freeBufferLocked释放掉那个过时的buffer回到slot中。为了避免获老的buffer被新的buffer内容覆盖，就要先acquire新的buffer然后release旧的buffer。这个我们在Android SurfaceFlinger 学习之路(十)—-SurfaceFlinger处理Layer更新讲过。 &#160; &#160; &#160; &#160;继续回到剩下的内容，最终调用 gralloc 模块中的 post方法，该此Buffer送显。这一部分可以参考Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现，其中的图形缓冲区的渲染过程模块。 &#160; &#160; &#160; &#160;到这里万里长征终于快到尽头了，doDisplayComposition函数流程分析完了。合成和送显示差不多完成了，还剩下最后一点杂物。 硬件模块渲染过程&#160; &#160; &#160; &#160;我们回到SF的doComposition函数，查看剩下内容：12345678910111213141516 //清除屏幕脏区域 hw-&gt;dirtyRegion.clear(); //判断系统是否支持软件部分更新 hw-&gt;flip(hw-&gt;swapRegion); //清除交换区域 hw-&gt;swapRegion.clear(); &#125; // inform the h/w that we're done compositing //通知hwc硬件合成结束 hw-&gt;compositionComplete(); &#125; //主要是调用hwc硬件的set函数 //此方法将完成各个图层的合成与显示，等效于EGL标准里面的eglSwapBuffers， //不过eglSwapBuffers是对OpenGL标准/GPU有效，此方法是对硬件合成器有效 postFramebuffer();&#125; &#160; &#160; &#160; &#160;主要就三个功能：&#160; &#160; &#160; &#160;1）判断系统是否支持软件部分更新；&#160; &#160; &#160; &#160;2）通知hwc硬件合成结束；&#160; &#160; &#160; &#160;3）调用hwc硬件的set函数，各个overlay图层的合成与显示。 &#160; &#160; &#160; &#160;老规矩，分步骤分析：&#160; &#160; &#160; &#160;1）判断系统是否支持软件部分更新。调用了DisplayDevice的flip函数，参数时要交换的合成区域：1234567891011121314151617181920void DisplayDevice::flip(const Region&amp; dirty) const&#123; mFlinger-&gt;getRenderEngine().checkErrors(); EGLDisplay dpy = mDisplay; EGLSurface surface = mSurface;#ifdef EGL_ANDROID_swap_rectangle if (mFlags &amp; SWAP_RECTANGLE) &#123; const Region newDirty(dirty.intersect(bounds())); const Rect b(newDirty.getBounds()); eglSetSwapRectangleANDROID(dpy, surface, b.left, b.top, b.width(), b.height()); &#125;#else (void) dirty; // Eliminate unused parameter warning#endif mPageFlipCount++;&#125; &#160; &#160; &#160; &#160;这段代码主要用来检查系统的主绘图表面是否支持EGL_ANDROID_swap_rectangle扩展属性。如果支持的话，那么每次在调用函数eglSwapBuffers来渲染UI时，都会使用软件的方式来支持部分更新区域功能，即：先得到不在新脏区域里面的那部分旧脏区域的内容，然后再将得到的这部分旧脏区域的内容拷贝回到要渲染的新图形缓冲区中去，这要求每次在渲染UI时，都要将被渲染的图形缓冲区以及对应的脏区域保存下来。&#160; &#160; &#160; &#160;函数会首先判断系统的主绘图表面是否支持EGL_ANDROID_swap_rectangle扩展属性。如果支持EGL_ANDROID_swap_rectangle扩展属性，即DisplayDevice类的成员变量mFlags的SWAP_RECTANGLE位等于1，那么就需要调用函数eglSetSwapRectangleANDROID来设置要渲染的区域，以便在渲染UI时，可以通过软件的方式来支持部分更新。&#160; &#160; &#160; &#160;不过从上一段选择部分更新的内容来看，是直接在硬件上支持部分更新，因而性能会更好。这一段软件的eglSetSwapRectangleANDROID就不看了曲线救国了，大清亡就亡了=。= &#160; &#160; &#160; &#160;2）通知hwc硬件合成结束。go on：123status_t DisplayDevice::compositionComplete() const &#123; return mDisplaySurface-&gt;compositionComplete();&#125; &#160; &#160; &#160; &#160;之前从SF的init函数和DisplayDevice的构造函数得知，mDisplaySurface是一个FrameBufferSurface对象，所以我们继续看看它的compositionComplete函数：1234status_t FramebufferSurface::compositionComplete()&#123; return mHwc.fbCompositionComplete();&#125; &#160; &#160; &#160; &#160;它又调用了HWComposer的fbCompositionComplete函数：1234567891011int HWComposer::fbCompositionComplete() &#123; //如果支持HWC硬件1.1版本，就不用通知任何消息 if (mHwc &amp;&amp; hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) return NO_ERROR; //否则通知FrameBuffer驱动FrameBufferTarget合成结束 if (mFbDev-&gt;compositionComplete) &#123; return mFbDev-&gt;compositionComplete(mFbDev); &#125; else &#123; return INVALID_OPERATION; &#125;&#125; &#160; &#160; &#160; &#160;如果支持HWC硬件1.1版本，就不用通知任何消息，否则通知FrameBuffer驱动FrameBufferTarget合成结束。 &#160; &#160; &#160; &#160;3）调用hwc硬件的set函数，各个overlay图层的合成与显示。这一步是postFramebuffer函数内完成的，我们看看实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455void SurfaceFlinger::postFramebuffer()&#123; ATRACE_CALL(); const nsecs_t now = systemTime(); mDebugInSwapBuffers = now; HWComposer&amp; hwc(getHwComposer()); if (hwc.initCheck() == NO_ERROR) &#123; if (!hwc.supportsFramebufferTarget()) &#123; // EGL spec says: // "surface must be bound to the calling thread's current context, // for the current rendering API." getDefaultDisplayDevice()-&gt;makeCurrent(mEGLDisplay, mEGLContext); &#125; //调用HWComposer的commit函数 hwc.commit(); &#125; // make the default display current because the VirtualDisplayDevice code cannot // deal with dequeueBuffer() being called outside of the composition loop; however // the code below can call glFlush() which is allowed (and does in some case) call // dequeueBuffer(). //为了虚拟显示屏做的规避 getDefaultDisplayDevice()-&gt;makeCurrent(mEGLDisplay, mEGLContext); for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers(hw-&gt;getVisibleLayersSortedByZ()); //完毕了framebuffertarget layer的swapbuffers。 hw-&gt;onSwapBuffersCompleted(hwc); const size_t count = currentLayers.size(); int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;=0 &amp;&amp; hwc.initCheck() == NO_ERROR) &#123; HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i = 0; cur != end &amp;&amp; i &lt; count; ++i, ++cur) &#123; //最后回调每个Layer的onLayerDisplayed函数 currentLayers[i]-&gt;onLayerDisplayed(hw, &amp;*cur); &#125; &#125; else &#123; for (size_t i = 0; i &lt; count; i++) &#123; currentLayers[i]-&gt;onLayerDisplayed(hw, NULL); &#125; &#125; &#125; mLastSwapBufferTime = systemTime() - now; mDebugInSwapBuffers = 0; uint32_t flipCount = getDefaultDisplayDevice()-&gt;getPageFlipCount(); if (flipCount % LOG_FRAME_STATS_PERIOD == 0) &#123; logFrameStats(); &#125;&#125; &#160; &#160; &#160; &#160;也是三部操作：1）调用调用HWComposer的commit函数完成OverLay合成；2）完毕了framebuffertarget layer的swapbuffers；3）最后回调每个Layer的onLayerDisplayed函数。&#160; &#160; &#160; &#160;我们先看第一步：123456789101112131415161718192021222324252627282930313233343536373839status_t HWComposer::commit() &#123; int err = NO_ERROR; if (mHwc) &#123; if (!hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &#123; // On version 1.0, the OpenGL ES target surface is communicated // by the (dpy, sur) fields and we are guaranteed to have only // a single display. mLists[0]-&gt;dpy = eglGetCurrentDisplay(); mLists[0]-&gt;sur = eglGetCurrentSurface(EGL_DRAW); &#125; for (size_t i=VIRTUAL_DISPLAY_ID_BASE; i&lt;mNumDisplays; i++) &#123; DisplayData&amp; disp(mDisplayData[i]); if (disp.outbufHandle) &#123; mLists[i]-&gt;outbuf = disp.outbufHandle; mLists[i]-&gt;outbufAcquireFenceFd = disp.outbufAcquireFence-&gt;dup(); &#125; &#125; //主要是调用hwc硬件的set函数 //此方法将完成各个图层的合成与显示，等效于EGL标准里面的eglSwapBuffers， //不过eglSwapBuffers是对OpenGL标准/GPU有效，此方法是对硬件合成器有效 err = mHwc-&gt;set(mHwc, mNumDisplays, mLists); for (size_t i=0 ; i&lt;mNumDisplays ; i++) &#123; DisplayData&amp; disp(mDisplayData[i]); disp.lastDisplayFence = disp.lastRetireFence; disp.lastRetireFence = Fence::NO_FENCE; if (disp.list) &#123; if (disp.list-&gt;retireFenceFd != -1) &#123; disp.lastRetireFence = new Fence(disp.list-&gt;retireFenceFd); disp.list-&gt;retireFenceFd = -1; &#125; disp.list-&gt;flags &amp;= ~HWC_GEOMETRY_CHANGED; &#125; &#125; &#125; return (status_t)err;&#125; &#160; &#160; &#160; &#160;主要是调用hwc硬件的set函数，此方法将完成各个图层的合成与显示，等效于EGL标准里面的eglSwapBuffers，不过eglSwapBuffers是对OpenGL标准/GPU有效，此方法是对硬件合成器有效。&#160; &#160; &#160; &#160;不过这是hwc硬件的实现，我们也看不到代码。感兴趣的同学可以搜一搜AOSP里qcom、boardcom、ti、intel、samsung等公司开源出来的实现。 &#160; &#160; &#160; &#160;后两部都是和Fence相关，这里不是重点：&#160; &#160; &#160; &#160;第二步是完毕了framebuffertarget layer的swapbuffers，调用DisplayDevice的onSwapBuffersCompleted函数：12345void DisplayDevice::onSwapBuffersCompleted(HWComposer&amp; hwc) const &#123; if (hwc.initCheck() == NO_ERROR) &#123; mDisplaySurface-&gt;onFrameCommitted(); &#125;&#125; &#160; &#160; &#160; &#160;依然调用FrameBufferSurface的onFrameCommitted函数：1234567891011121314151617181920212223242526void FramebufferSurface::onFrameCommitted() &#123; sp&lt;Fence&gt; fence = mHwc.getAndResetReleaseFence(mDisplayType); if (fence-&gt;isValid() &amp;&amp; mCurrentBufferSlot != BufferQueue::INVALID_BUFFER_SLOT) &#123; status_t err = addReleaseFence(mCurrentBufferSlot, mCurrentBuffer, fence); ALOGE_IF(err, "setReleaseFenceFd: failed to add the fence: %s (%d)", strerror(-err), err); &#125;&#125;sp&lt;Fence&gt; HWComposer::getAndResetReleaseFence(int32_t id) &#123; if (uint32_t(id)&gt;31 || !mAllocatedDisplayIDs.hasBit(id)) return Fence::NO_FENCE; int fd = INVALID_OPERATION; if (mHwc &amp;&amp; hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &#123; const DisplayData&amp; disp(mDisplayData[id]); // 这里的disp.framebufferTarget-&gt;releaseFenceFd应该就是底层hwcomposer设置的 if (disp.framebufferTarget) &#123; fd = disp.framebufferTarget-&gt;releaseFenceFd; disp.framebufferTarget-&gt;acquireFenceFd = -1; disp.framebufferTarget-&gt;releaseFenceFd = -1; &#125; &#125; return fd &gt;= 0 ? new Fence(fd) : Fence::NO_FENCE;&#125; &#160; &#160; &#160; &#160;onFrameCommitted主要就是获取hwcomposer设置的release fence，然后设置到slot中。 &#160; &#160; &#160; &#160;第三部回调Layer的onLayerDisplayed函数：1234567891011121314151617181920212223242526272829void Layer::onLayerDisplayed(const sp&lt;const DisplayDevice&gt;&amp; hw, HWComposer::HWCLayerInterface* layer) &#123; if (layer) &#123; //HWCLayerVersion1的onDisplayed函数，位于HWComposer.cpp中 layer-&gt;onDisplayed(); //将fence设置到slot中 mSurfaceFlingerConsumer-&gt;setReleaseFence(layer-&gt;getAndResetReleaseFence()); &#125;&#125;/*----------- HWCLayerVersion1 ----------*/virtual sp&lt;Fence&gt; getAndResetReleaseFence() &#123; //获取layer的releaseFenceFd int fd = getLayer()-&gt;releaseFenceFd; getLayer()-&gt;releaseFenceFd = -1; //new 一个Fence return fd &gt;= 0 ? new Fence(fd) : Fence::NO_FENCE;&#125;virtual void onDisplayed() &#123; hwc_region_t&amp; visibleRegion = getLayer()-&gt;visibleRegionScreen; SharedBuffer const* sb = SharedBuffer::bufferFromData(visibleRegion.rects); if (sb) &#123; sb-&gt;release(); // not technically needed but safer visibleRegion.numRects = 0; visibleRegion.rects = NULL; &#125; getLayer()-&gt;acquireFenceFd = -1;&#125; &#160; &#160; &#160; &#160;而对overlay相应的layer而言，前面仅仅设置了acquire fence，在hwcomposer HAL处理后肯定会给加入一个release fence，而这一部分代码我们看不到实现。 &#160; &#160; &#160; &#160;到此硬件模块渲染过程就分析完了。SF的doComposition函数的流程就结束了。 更新SW Vsync误差&#160; &#160; &#160; &#160;最后一部分就是SF中handleMessageRefresh最后一部，postComposition。主要用于调试，调Layer的onPostComposition方法。我们看看实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172void SurfaceFlinger::postComposition()&#123; const LayerVector&amp; layers(mDrawingState.layersSortedByZ); const size_t count = layers.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; //调Layer的onPostComposition方法 layers[i]-&gt;onPostComposition(); &#125; // 通过 HWComposer 获得 Fence const HWComposer&amp; hwc = getHwComposer(); sp&lt;Fence&gt; presentFence = hwc.getDisplayFence(HWC_DISPLAY_PRIMARY); //注意，如果硬件vsync已经被打开了，那么fence是无效了，只有它在关闭的情况下，它才有效 //矫正更新Vsync，是否打开或关闭vsync信号 if (presentFence-&gt;isValid()) &#123; if (mPrimaryDispSync.addPresentFence(presentFence)) &#123; enableHardwareVsync(); &#125; else &#123; disableHardwareVsync(false); &#125; &#125; if (kIgnorePresentFences) &#123; const sp&lt;const DisplayDevice&gt; hw(getDefaultDisplayDevice()); if (hw-&gt;isDisplayOn()) &#123; enableHardwareVsync(); &#125; &#125; //动画相关 if (mAnimCompositionPending) &#123; mAnimCompositionPending = false; if (presentFence-&gt;isValid()) &#123; mAnimFrameTracker.setActualPresentFence(presentFence); &#125; else &#123; // The HWC doesn't support present fences, so use the refresh // timestamp instead. nsecs_t presentTime = hwc.getRefreshTimestamp(HWC_DISPLAY_PRIMARY); mAnimFrameTracker.setActualPresentTime(presentTime); &#125; mAnimFrameTracker.advanceFrame(); &#125;&#125;/*--------Layer.cpp--------*/void Layer::onPostComposition() &#123; if (mFrameLatencyNeeded) &#123; nsecs_t desiredPresentTime = mSurfaceFlingerConsumer-&gt;getTimestamp(); mFrameTracker.setDesiredPresentTime(desiredPresentTime); sp&lt;Fence&gt; frameReadyFence = mSurfaceFlingerConsumer-&gt;getCurrentFence(); if (frameReadyFence-&gt;isValid()) &#123; mFrameTracker.setFrameReadyFence(frameReadyFence); &#125; else &#123; // There was no fence for this frame, so assume that it was ready // to be presented at the desired present time. mFrameTracker.setFrameReadyTime(desiredPresentTime); &#125; const HWComposer&amp; hwc = mFlinger-&gt;getHwComposer(); sp&lt;Fence&gt; presentFence = hwc.getDisplayFence(HWC_DISPLAY_PRIMARY); if (presentFence-&gt;isValid()) &#123; mFrameTracker.setActualPresentFence(presentFence); &#125; else &#123; // The HWC doesn't support present fences, so use the refresh // timestamp instead. nsecs_t presentTime = hwc.getRefreshTimestamp(HWC_DISPLAY_PRIMARY); mFrameTracker.setActualPresentTime(presentTime); &#125; mFrameTracker.advanceFrame(); mFrameLatencyNeeded = false; &#125;&#125; &#160; &#160; &#160; &#160;postComposition函数中比较重要的是更新SW Vsync的误差值。注意，如果硬件vsync已经被打开了，那么fence是无效了，只有它在关闭的情况下，它才有效。当更新SW Vsync模型后，就会关闭硬件Vsync信号，这时候Fence就有效了。所以我们进入第一个if条件内部，查看DispSync的addPresentFence函数：1234567891011121314151617181920212223242526272829bool DispSync::addPresentFence(const sp&lt;Fence&gt;&amp; fence) &#123; Mutex::Autolock lock(mMutex); // 将当前硬件vsync的fence保存在 mPresentFences里, 目的是为了计算偏移 // mPresentFences 最多保存8个硬件 偏移 mPresentFences[mPresentSampleOffset] = fence; mPresentTimes[mPresentSampleOffset] = 0; mPresentSampleOffset = (mPresentSampleOffset + 1) % NUM_PRESENT_SAMPLES; mNumResyncSamplesSincePresent = 0; // 将 mNumResyncSamplesSincePresent 置为0， for (size_t i = 0; i &lt; NUM_PRESENT_SAMPLES; i++) &#123; const sp&lt;Fence&gt;&amp; f(mPresentFences[i]); if (f != NULL) &#123; //这里 f 是有可能为NULL, 即只有一个 硬件 vsync 偏移时 nsecs_t t = f-&gt;getSignalTime(); //猜测这个就是硬件 vsync的时间 if (t &lt; INT64_MAX) &#123; mPresentFences[i].clear(); //将每个vsync时间戳记录在 mPresentTimes 里，这里 kPresentTimeOffset是可以配置的，即可调的 mPresentTimes[i] = t + kPresentTimeOffset; &#125; &#125; &#125; //更新错误信息 updateErrorLocked(); // 这里，一般的情况是 mModelUpdated 已经被更新了，然后硬件vsync被disable了， // 所以这里只需要看SW vsync的真实的硬件vsync的误差是否在可 // 允许的范围内即可 return !mModelUpdated || mError &gt; kErrorThreshold;&#125; &#160; &#160; &#160; &#160;ddPresentFence最后的返回, mError是方差，见下面分析，当方差大于 kErrorThreshold后就返回true：12345678910111213141516171819202122232425262728293031323334353637383940void DispSync::updateErrorLocked() &#123; if (!mModelUpdated) &#123; return; &#125; // Need to compare present fences against the un-adjusted refresh period, // since they might arrive between two events. //得到真实的 period, 具体见 5.2.4 updateModelLocked 里的分析 nsecs_t period = mPeriod / (1 + mRefreshSkipCount); int numErrSamples = 0; nsecs_t sqErrSum = 0; //这里的 mReferenceTime 是第一个硬件vsync的时间戳 见 addResyncSample里的 mReferenceTime for (size_t i = 0; i &lt; NUM_PRESENT_SAMPLES; i++) &#123; nsecs_t sample = mPresentTimes[i] - mReferenceTime; // 这里 sample 一般来说是大于偏移的 if (sample &gt; mPhase) &#123; nsecs_t sampleErr = (sample - mPhase) % period; if (sampleErr &gt; period / 2) &#123; sampleErr -= period; &#125; //记录 偏移差的平方和 sqErrSum += sampleErr * sampleErr; numErrSamples++; &#125; &#125; // 说到底mError就是方差 if (numErrSamples &gt; 0) &#123; mError = sqErrSum / numErrSamples; &#125; else &#123; mError = 0; &#125; if (kTraceDetailedInfo) &#123; ATRACE_INT64("DispSync:Error", mError); &#125;&#125; &#160; &#160; &#160; &#160;如果 addPresentFence返回true, 那么就说明SW vsync和硬件Vsync的误差已经无法接受了，那么这时就得重新打开硬件Vsync，来重新调节SW vsync模型了。 &#160; &#160; &#160; &#160;合成全部流程到此结束，打完收工~~ 小结&#160; &#160; &#160; &#160;合成流程中比较重要的就是Layer与纹理，我们贴两幅图，一张是GraphicBuffer上传为纹理，一张为更新纹理： &#160; &#160; &#160; &#160;GraphicBuffer上传为纹理 &#160; &#160; &#160; &#160;更新纹理 &#160; &#160; &#160; &#160;合成流程是整个SurfaceFlinger中最重要的环节，这个大山我们推到之后，对于Android显示系统的理解豁然开朗。后面我们有机会继续研究研究其他内容，比如Fence相关的东西（可能没有时间=。=）&#160; &#160; &#160; &#160;细数2017年每一件事，真是事事不如意。这一年，也是我整个人生中最动荡的一年，尝遍酸甜苦辣，受尽世态炎凉ToT~&#160; &#160; &#160; &#160;希望2018年能对我好一点。。。。。。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(十一)(PostView)--附：硬件HWC选取合成类型（Intel）]]></title>
    <url>%2F2017%2F12%2F19%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E5%8D%81%E4%B8%80-PostView-%E9%99%84%EF%BC%9A%E7%A1%AC%E4%BB%B6HWC%E9%80%89%E5%8F%96%E5%90%88%E6%88%90%E7%B1%BB%E5%9E%8B%EF%BC%88Intel%EF%BC%89%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;这里说点题外话，高通最近势头正猛啊，回拒了博通的恶意收购，又出了骁龙835进军PC市场，拉着AMD一起干英特尔。英特尔在移动设备市场上面还是没有ARM老谋深算，人家简单指令集一路走到底，授权卖指令集或者自己出公版，得到了苹果、三星、高通、MTK等许多大厂的青睐（苹果、高通、三星等厂商都是向ARM买指令集，再自己设计电路，大概是嫌ARM自己的芯片效能不够；然而联发科和其他几家厂商买的是IP（公版）来兜自己的芯片，不自己从头设计电路），细水长流，进过多年顺风打野，最后打出一身神装出来carry。&#160; &#160; &#160; &#160;反观intel，执着于自己x86架构，也用复杂指令集芯片打入过移动市场，奈何功耗扛不住啊，（充电两小时，装逼5分钟，只是个夸张比喻，勿当真）虽然后来又出了Atom的低功耗处理器，奈何市场已被ARM吞食大半，自己名气也没有打出，最后只能默默凉凉了。&#160; &#160; &#160; &#160;intel在PC市场上霸主这么多年，实力肯定不是盖的，所以我们这次选个intel在hwc模块的实现来看看（其实是因为TI、高通的实现有些复杂，比较晦涩，三星也倒是比较容易看懂，但是也没有intel代码这么人性化=。=） HWC选取合成类型&#160; &#160; &#160; &#160;上面讲到mHwc-&gt;prepare(mHwc, mNumDisplays, mLists);对所有显示屏中所有Layer作合成准备（此时也可以开始发送合成的命令码下去，启动硬件合成，但不需要等待完成），hwcomposer需要正确汇报每个Layer的composetype，以告知SurfaceFlinger是否需要额外处理。&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;实现位于/hardware/intel/img/hwcomposer/common/base/Hwcomposer.cpp，我们找一个公共模块：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253bool Hwcomposer::prepare(size_t numDisplays, hwc_display_contents_1_t** displays)&#123; bool ret = true; RETURN_FALSE_IF_NOT_INIT(); ALOGTRACE("display count = %d", numDisplays); if (!numDisplays || !displays) &#123; ELOGTRACE("invalid parameters"); return false; &#125; //检查显示屏相关信息，处理热插拔消息 mDisplayAnalyzer-&gt;analyzeContents(numDisplays, displays); // disable reclaimed planes //disable掉所有回收的显示平面 mPlaneManager-&gt;disableReclaimedPlanes(); // reclaim all allocated planes if possible //如果可能，回首所有已经申请过的显示平面 for (size_t i = 0; i &lt; numDisplays; i++) &#123; if (i &gt;= mDisplayDevices.size()) &#123; continue; &#125; IDisplayDevice *device = mDisplayDevices.itemAt(i); if (!device) &#123; VLOGTRACE("device %d doesn't exist", i); continue; &#125; //检查显示屏几何区域是否有变化 device-&gt;prePrepare(displays[i]); &#125; for (size_t i = 0; i &lt; numDisplays; i++) &#123; if (i &gt;= mDisplayDevices.size()) &#123; continue; &#125; IDisplayDevice *device = mDisplayDevices.itemAt(i); if (!device) &#123; VLOGTRACE("device %d doesn't exist", i); continue; &#125; //最重要的一行：更新HwcLayer，选择合成类型 ret = device-&gt;prepare(displays[i]); if (ret == false) &#123; ELOGTRACE("failed to do prepare for device %d", i); continue; &#125; &#125; return ret;&#125; &#160; &#160; &#160; &#160;上述就是选取合成类型的步骤：&#160; &#160; &#160; &#160;1）检查显示屏相关信息，处理热插拔消息；&#160; &#160; &#160; &#160;2）disable掉所有回收的显示平面；&#160; &#160; &#160; &#160;3）检查显示屏几何区域是否有变化；&#160; &#160; &#160; &#160;4）最重要的一行：更新HwcLayer，选择合成类型。 流程细节&#160; &#160; &#160; &#160;我们分步查看：&#160; &#160; &#160; &#160;1）检查显示屏相关信息，处理热插拔消息。位于hardware/intel/img/hwcomposer/common/base/DisplayAnalyzer.cpp：12345678910111213141516171819202122232425262728293031323334void DisplayAnalyzer::analyzeContents( size_t numDisplays, hwc_display_contents_1_t** displays)&#123; // cache and use them only in this context during analysis mCachedNumDisplays = numDisplays; mCachedDisplays = displays; //处理pending的事件 handlePendingEvents();&#125;void DisplayAnalyzer::handlePendingEvents()&#123; // handle one event per analysis to avoid blocking surface flinger // some event may take lengthy time to process Event e; if (!getEvent(e)) &#123; return; &#125; switch (e.type) &#123; case HOTPLUG_EVENT: //处理热插拔 handleHotplugEvent(e.bValue); break; &#125;&#125;void DisplayAnalyzer::handleHotplugEvent(bool connected)&#123; if (connected) &#123; for (int i = 0; i &lt; mCachedNumDisplays; i++) &#123; //将合成类型设置成HWC_FRAMEBUFFER setCompositionType(i, HWC_FRAMEBUFFER, true); &#125; &#125;&#125; &#160; &#160; &#160; &#160;这就是处理热插拔消息，然后将cache的显示屏合成类型设置成HWC_FRAMEBUFFER，表示用Open GL合成。 &#160; &#160; &#160; &#160;2）disable掉所有回收的显示平面。位于hardware/intel/img/hwcomposer/common/planes/DisplayPlaneManager.cpp：123456789101112131415161718192021222324252627282930void DisplayPlaneManager::disableReclaimedPlanes()&#123; int i, j; bool ret; RETURN_VOID_IF_NOT_INIT(); for (i = 0; i &lt; DisplayPlane::PLANE_MAX; i++) &#123; // disable reclaimed planes if (mReclaimedPlanes[i]) &#123; for (j = 0; j &lt; mPlaneCount[i]; j++) &#123; int bit = (1 &lt;&lt; j); if (mReclaimedPlanes[i] &amp; bit) &#123; DisplayPlane* plane = mPlanes[i].itemAt(j); // check plane state first ret = plane-&gt;isDisabled(); // reset plane if (ret) ret = plane-&gt;reset(); if (ret) &#123; // only merge into free bitmap if it is successfully disabled and reset // otherwise, plane will be disabled and reset again. mFreePlanes[i] |=bit; mReclaimedPlanes[i] &amp;= ~bit; &#125; &#125; &#125; &#125; &#125;&#125; &#160; &#160; &#160; &#160;disable所有已经回收planes，检查plane是否已经disable，然后将其reset，如果成功了，就合并进入空闲的bitmap，如果没有就要重新disabled and reset。 &#160; &#160; &#160; &#160;3）检查显示屏几何区域是否有变化，遍历每一个显示屏，调用device-&gt;prePrepare(displays[i]);我们看一下物理屏幕，位于hardware/intel/img/hwcomposer/common/devices/PhysicalDevice.cpp：123456789101112131415161718bool PhysicalDevice::prePrepare(hwc_display_contents_1_t *display)&#123; RETURN_FALSE_IF_NOT_INIT(); // for a null list, delete hwc list if (!mConnected || !display || mBlank) &#123; if (mLayerList) &#123; DEINIT_AND_DELETE_OBJ(mLayerList); &#125; return true; &#125; // check if geometry is changed, if changed delete list if ((display-&gt;flags &amp; HWC_GEOMETRY_CHANGED) &amp;&amp; mLayerList) &#123; DEINIT_AND_DELETE_OBJ(mLayerList); &#125; return true;&#125; &#160; &#160; &#160; &#160;如果是一个空的hwc list，就将其删除；然后检查几何区域是否变化，如果变了，也要删除hwc list。 &#160; &#160; &#160; &#160;4）最重要的一行：更新HwcLayer，选择合成类型。依然是遍历每一个显示屏，然后调用prepare函数，依然位于hardware/intel/img/hwcomposer/common/devices/PhysicalDevice.cpp：1234567891011121314151617181920212223242526272829303132333435363738394041424344bool PhysicalDevice::prepare(hwc_display_contents_1_t *display)&#123; RETURN_FALSE_IF_NOT_INIT(); if (!mConnected || !display || mBlank) return true; // check if geometry is changed if (display-&gt;flags &amp; HWC_GEOMETRY_CHANGED) &#123; //上一步处理了几何区域变化，上出了hwc list，这一步重新创建 onGeometryChanged(display); &#125; if (!mLayerList) &#123; WLOGTRACE("null HWC layer list"); return true; &#125; // update list with new list //更新hwc list使用重新new的，选取合成类型 return mLayerList-&gt;update(display);&#125;void PhysicalDevice::onGeometryChanged(hwc_display_contents_1_t *list)&#123; if (!list) &#123; ELOGTRACE("list is NULL"); return; &#125; ALOGTRACE("disp = %d, layer number = %d", mType, list-&gt;numHwLayers); // NOTE: should NOT be here if (mLayerList) &#123; WLOGTRACE("mLayerList exists"); DEINIT_AND_DELETE_OBJ(mLayerList); &#125; // create a new layer list //上一步处理了几何区域变化，上出了hwc list，这一步重新创建 mLayerList = new HwcLayerList(list, mType); if (!mLayerList) &#123; WLOGTRACE("failed to create layer list"); &#125;&#125; &#160; &#160; &#160; &#160;上一步处理了几何区域变化，上出了hwc list，这一步重新创建，在onGeometryChanged函数中得以体现。 更新HwcLayerList&#160; &#160; &#160; &#160;那么选取合成类型就位于mLayerList-&gt;update(display);，位于hardware/intel/img/hwcomposer/common/base/HwcLayerList.cpp：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970bool HwcLayerList::update(hwc_display_contents_1_t *list)&#123; bool ret; // basic check to make sure the consistance if (!list) &#123; ELOGTRACE("null layer list"); return false; &#125; if ((int)list-&gt;numHwLayers != mLayerCount) &#123; ELOGTRACE("layer count doesn't match (%d, %d)", list-&gt;numHwLayers, mLayerCount); return false; &#125; // update list mList = list; bool ok = true; // update all layers, call each layer's update() for (int i = 0; i &lt; mLayerCount; i++) &#123; HwcLayer *hwcLayer = mLayers.itemAt(i); if (!hwcLayer) &#123; ELOGTRACE("no HWC layer for layer %d", i); continue; &#125; //更新layer，如果更新返回false，则需要fallback回GLES去合成 if (!hwcLayer-&gt;update(&amp;list-&gt;hwLayers[i])) &#123; ok = false; hwcLayer-&gt;setCompositionType(HWC_FORCE_FRAMEBUFFER); &#125; &#125; if (!ok) &#123; //如果上一步update有返回false的，就要把它fallback回GLES合成 ILOGTRACE("overlay fallback to GLES. flags: %#x", list-&gt;flags); for (int i = 0; i &lt; mLayerCount - 1; i++) &#123; HwcLayer *hwcLayer = mLayers.itemAt(i); if (hwcLayer-&gt;getPlane() &amp;&amp; (hwcLayer-&gt;getCompositionType() == HWC_OVERLAY || hwcLayer-&gt;getCompositionType() == HWC_CURSOR_OVERLAY)) &#123; hwcLayer-&gt;setCompositionType(HWC_FRAMEBUFFER); &#125; &#125; //最后一个layer是HWC_FRAMEBUFFER_TARGET合成目标，上面讲过 mLayers.itemAt(mLayerCount - 1)-&gt;setCompositionType(HWC_FRAMEBUFFER_TARGET); //反初始化 deinitialize(); mList = list; //初始化LayerList每一项 initialize(); // update all layers again after plane re-allocation //再次更新所有的layer，在plane重新申请之后 for (int i = 0; i &lt; mLayerCount; i++) &#123; HwcLayer *hwcLayer = mLayers.itemAt(i); if (!hwcLayer) &#123; ELOGTRACE("no HWC layer for layer %d", i); continue; &#125; if (!hwcLayer-&gt;update(&amp;list-&gt;hwLayers[i])) &#123; DLOGTRACE("fallback to GLES update failed on layer[%d]!\n", i); &#125; &#125; &#125; //建立只能合成选项，即选择OverLay还是FB setupSmartComposition(); return true;&#125; 更新HwcLayer&#160; &#160; &#160; &#160;这里依然要分步查看：&#160; &#160; &#160; &#160;Part.1：我们可以看到，选取合成类型为Overlay还是FB就在hwcLayer-&gt;update函数，如果返回false，就要fallback回GLES，剩余的都是OverLay。我们看看这个实现，位于hardware/intel/img/hwcomposer/common/base/HwcLayer.cpp：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647bool HwcLayer::update(hwc_layer_1_t *layer)&#123; // update layer mLayer = layer; //创建基本属性，从DataBuffer获取，也可是说是GraphicBuffer setupAttributes(); // if not a FB layer &amp; a plane was attached update plane's data buffer //如果不是一个FB layer并且plane已经被连接到更新data buffer if (mPlane) &#123; //设置position位置信息 mPlane-&gt;setPosition(layer-&gt;displayFrame.left, layer-&gt;displayFrame.top, layer-&gt;displayFrame.right - layer-&gt;displayFrame.left, layer-&gt;displayFrame.bottom - layer-&gt;displayFrame.top); //设置sourceCrop区域 mPlane-&gt;setSourceCrop(layer-&gt;sourceCropf.left, layer-&gt;sourceCropf.top, layer-&gt;sourceCropf.right - layer-&gt;sourceCropf.left, layer-&gt;sourceCropf.bottom - layer-&gt;sourceCropf.top); //设置矩阵变换 mPlane-&gt;setTransform(layer-&gt;transform); //透明度 mPlane-&gt;setPlaneAlpha(layer-&gt;planeAlpha, layer-&gt;blending); //将layer的句柄设置给plane bool ret = mPlane-&gt;setDataBuffer((uint32_t)layer-&gt;handle); if (ret == true) &#123; return true; &#125; DLOGTRACE("failed to set data buffer, reset handle to 0!!"); mHandle = 0; if (!mIsProtected) &#123;//如果不是被保护的buffer //典型的例子：旋转的buffer没有准备好或者句柄为null，就不能用OverLay // typical case: rotated buffer is not ready or handle is null return false; &#125; else &#123; //被保护的视频需要使用overlay去渲染，return true // protected video has to be rendered using overlay. // if buffer is not ready overlay will still be attached to this layer // but rendering needs to be skipped. WLOGTRACE("ignoring result of data buffer setting for protected video"); return true; &#125; &#125; return true;&#125; &#160; &#160; &#160; &#160;先创建基本属性，从DataBuffer获取，也可是说是GraphicBuffer，我们看看setupAttributes函数：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859void HwcLayer::setupAttributes()&#123; //如果如下属性发生改变，将mUpdated 置为true if ((mLayer-&gt;flags &amp; HWC_SKIP_LAYER) || mTransform != mLayer-&gt;transform || mSourceCropf != mLayer-&gt;sourceCropf || mDisplayFrame != mLayer-&gt;displayFrame || mHandle != (uint32_t)mLayer-&gt;handle || DisplayQuery::isVideoFormat(mFormat)) &#123; // TODO: same handle does not mean there is always no update mUpdated = true; &#125; // update handle always as it can become "NULL" // if the given layer is not ready //如果指定的layer没有准备好，那么handle句柄就会变成null，所以要更新句柄的值 mTransform = mLayer-&gt;transform; mSourceCropf = mLayer-&gt;sourceCropf; mDisplayFrame = mLayer-&gt;displayFrame; mHandle = (uint32_t)mLayer-&gt;handle; //其实属性已经被设置，return if (mFormat != DataBuffer::FORMAT_INVALID) &#123; // other attributes have been set. return; &#125; //句柄为null，return if (mLayer-&gt;handle == NULL) &#123; VLOGTRACE("invalid handle"); return; &#125; //获取BufferManager BufferManager *bm = Hwcomposer::getInstance().getBufferManager(); if (bm == NULL) &#123; // TODO: this check is redundant return; &#125; //锁定图形缓冲区 DataBuffer *buffer = bm-&gt;lockDataBuffer((uint32_t)mLayer-&gt;handle); if (!buffer) &#123; ELOGTRACE("failed to get buffer"); &#125; else &#123;//获取图形缓冲区属性，并设置给全局变量 mFormat = buffer-&gt;getFormat(); mWidth = buffer-&gt;getWidth(); mHeight = buffer-&gt;getHeight(); mStride = buffer-&gt;getStride(); mPriority = (mSourceCropf.right - mSourceCropf.left) * (mSourceCropf.bottom - mSourceCropf.top); mPriority &lt;&lt;= LAYER_PRIORITY_SIZE_OFFSET; mPriority |= mIndex; GraphicBuffer *gBuffer = (GraphicBuffer*)buffer; mUsage = gBuffer-&gt;getUsage(); mIsProtected = GraphicBuffer::isProtectedBuffer((GraphicBuffer*)buffer); if (mIsProtected) &#123; mPriority |= LAYER_PRIORITY_PROTECTED; &#125; else if (PlaneCapabilities::isFormatSupported(DisplayPlane::PLANE_OVERLAY, this)) &#123; mPriority |= LAYER_PRIORITY_OVERLAY; &#125; //获取完之后释放 bm-&gt;unlockDataBuffer(buffer); &#125;&#125; &#160; &#160; &#160; &#160;这段代码要是参考之前Gralloc模块应该有点印象，就是锁定图形缓冲区，获取图形缓冲区属性，并设置给全局变量，获取完之后释放。&#160; &#160; &#160; &#160;回到HwcLayer::update，之后判断如果不是一个FB layer并且plane已经被连接到更新data buffer，就要设置position位置信息、设置sourceCrop区域、设置矩阵变换 、透明度。 映射过程&#160; &#160; &#160; &#160;然后将layer的句柄设置给plane，我们看看这个实现，位于hardware/intel/img/hwcomposer/common/planes/DisplayPlane.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172bool DisplayPlane::setDataBuffer(uint32_t handle)&#123; DataBuffer *buffer; BufferMapper *mapper; ssize_t index; bool ret; bool isCompression; //获取buffermanager BufferManager *bm = Hwcomposer::getInstance().getBufferManager(); RETURN_FALSE_IF_NOT_INIT(); ALOGTRACE("handle = %#x", handle); if (!handle) &#123; WLOGTRACE("invalid buffer handle"); return false; &#125; // do not need to update the buffer handle //如果不是当前保存句柄，加上CHANGE标签 if (mCurrentDataBuffer != handle) mUpdateMasks |= PLANE_BUFFER_CHANGED; // if no update then do Not need set data buffer if (!mUpdateMasks) return true; //锁定缓冲区 buffer = bm-&gt;lockDataBuffer(handle); if (!buffer) &#123; ELOGTRACE("failed to get buffer"); return false; &#125; mIsProtectedBuffer = GraphicBuffer::isProtectedBuffer((GraphicBuffer*)buffer); isCompression = GraphicBuffer::isCompressionBuffer((GraphicBuffer*)buffer); // map buffer if it's not in cache //如果映射区域不在缓存cache中，就要重新映射 index = mDataBuffers.indexOfKey(buffer-&gt;getKey()); if (index &lt; 0) &#123; VLOGTRACE("unmapped buffer, mapping..."); //映射buffer，并加入缓存 mapper = mapBuffer(buffer); if (!mapper) &#123; ELOGTRACE("failed to map buffer %#x", handle); bm-&gt;unlockDataBuffer(buffer); return false; &#125; &#125; else &#123; VLOGTRACE("got mapper in saved data buffers and update source Crop"); mapper = mDataBuffers.valueAt(index); &#125; // always update source crop to mapper //更新crop区域 mapper-&gt;setCrop(mSrcCrop.x, mSrcCrop.y, mSrcCrop.w, mSrcCrop.h); mapper-&gt;setIsCompression(isCompression); // unlock buffer after getting mapper //映射区域之后释放缓冲区 bm-&gt;unlockDataBuffer(buffer); buffer = NULL; //将映射区域设置给plane ret = setDataBuffer(*mapper); if (ret) &#123;//如果成功了，就将handle赋给mCurrentDataBuffer ，并且更新活跃的buffer mCurrentDataBuffer = handle; // update active buffers updateActiveBuffers(mapper); &#125; return ret;&#125; &#160; &#160; &#160; &#160;核心就是获取显示屏的BufferManager，然后锁定图形缓冲区，然后映射buffer，并加入缓存，映射区域之后释放缓冲区，将映射区域设置给plane。&#160; &#160; &#160; &#160;映射实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243BufferMapper* DisplayPlane::mapBuffer(DataBuffer *buffer)&#123; BufferManager *bm = Hwcomposer::getInstance().getBufferManager(); // invalidate buffer cache if cache is full //cache满了就要清除 if ((int)mDataBuffers.size() &gt;= mCacheCapacity) &#123; invalidateBufferCache(); &#125; //映射plane内存区域 BufferMapper *mapper = bm-&gt;map(*buffer); if (!mapper) &#123; ELOGTRACE("failed to map buffer"); return NULL; &#125; // add it to data buffers //映射成功后就将这个句柄加入缓存 ssize_t index = mDataBuffers.add(buffer-&gt;getKey(), mapper); if (index &lt; 0) &#123; ELOGTRACE("failed to add mapper"); bm-&gt;unmap(mapper); return NULL; &#125; return mapper;&#125;void DisplayPlane::invalidateBufferCache()&#123; BufferManager *bm = Hwcomposer::getInstance().getBufferManager(); BufferMapper* mapper; RETURN_VOID_IF_NOT_INIT(); for (size_t i = 0; i &lt; mDataBuffers.size(); i++) &#123; mapper = mDataBuffers.valueAt(i); bm-&gt;unmap(mapper); &#125; mDataBuffers.clear(); // reset current buffer mCurrentDataBuffer = 0;&#125; &#160; &#160; &#160; &#160;之后将映射区域设置给plane，如果成功了，就将handle赋给mCurrentDataBuffer ，并且更新活跃的buffer：1234567891011121314151617void DisplayPlane::updateActiveBuffers(BufferMapper *mapper)&#123; BufferManager *bm = Hwcomposer::getInstance().getBufferManager(); // unmap the first entry (oldest buffer) if (mActiveBuffers.size() &gt;= MIN_DATA_BUFFER_COUNT) &#123; BufferMapper *oldest = mActiveBuffers.itemAt(0); bm-&gt;unmap(oldest); mActiveBuffers.removeAt(0); &#125; // queue it to active buffers if (!isActiveBuffer(mapper)) &#123; mapper-&gt;incRef(); mActiveBuffers.push_back(mapper); &#125;&#125; &#160; &#160; &#160; &#160;也就是移除最老的映射buffer，然后新增加一个最新的。 &#160; &#160; &#160; &#160;如果设置给plane不成功，即没有可用的内存区域或者不匹配，则不能使用OverLay合成，返回false，就要是OpenGL了。 初始化和反初始化&#160; &#160; &#160; &#160;回到上面HwcLayerList::update，继续往下走。 &#160; &#160; &#160; &#160;然后就是先反初始化deinitialize：123456789101112131415161718192021222324252627void HwcLayerList::deinitialize()&#123; if (mLayerCount == 0) &#123; return; &#125; DisplayPlaneManager *planeManager = Hwcomposer::getInstance().getPlaneManager(); for (int i = 0; i &lt; mLayerCount; i++) &#123; HwcLayer *hwcLayer = mLayers.itemAt(i); if (hwcLayer) &#123; DisplayPlane *plane = hwcLayer-&gt;detachPlane(); if (plane) &#123; planeManager-&gt;reclaimPlane(mDisplayIndex, *plane); &#125; &#125; delete hwcLayer; &#125; mLayers.clear(); mFBLayers.clear(); mOverlayCandidates.clear(); mSpriteCandidates.clear(); mCursorCandidates.clear(); mZOrderConfig.clear(); mFrameBufferTarget = NULL; mLayerCount = 0;&#125; &#160; &#160; &#160; &#160;反初始化就是清除一些变量。然后是初始化initialize：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104bool HwcLayerList::initialize()&#123; if (!mList || mList-&gt;numHwLayers == 0) &#123; ELOGTRACE("invalid hwc list"); return false; &#125; //设置一些变量 mLayerCount = (int)mList-&gt;numHwLayers; mLayers.setCapacity(mLayerCount); mFBLayers.setCapacity(mLayerCount); mSpriteCandidates.setCapacity(mLayerCount); mOverlayCandidates.setCapacity(mLayerCount); mCursorCandidates.setCapacity(mLayerCount); mZOrderConfig.setCapacity(mLayerCount); Hwcomposer&amp; hwc = Hwcomposer::getInstance(); PriorityVector rgbOverlayLayers; rgbOverlayLayers.setCapacity(mLayerCount); //根据不同合成类型设置hwcLayer的type for (int i = 0; i &lt; mLayerCount; i++) &#123; hwc_layer_1_t *layer = &amp;mList-&gt;hwLayers[i]; if (!layer) &#123; DEINIT_AND_RETURN_FALSE("layer %d is null", i); &#125; HwcLayer *hwcLayer = new HwcLayer(i, layer); if (!hwcLayer) &#123; DEINIT_AND_RETURN_FALSE("failed to allocate hwc layer %d", i); &#125; if (layer-&gt;compositionType == HWC_FRAMEBUFFER_TARGET) &#123;//HWC_FRAMEBUFFER_TARGET hwcLayer-&gt;setType(HwcLayer::LAYER_FRAMEBUFFER_TARGET); mFrameBufferTarget = hwcLayer; &#125; else if (layer-&gt;compositionType == HWC_OVERLAY)&#123; // skipped layer, filtered by Display Analyzer hwcLayer-&gt;setType(HwcLayer::LAYER_SKIPPED); &#125; else if (layer-&gt;compositionType == HWC_FORCE_FRAMEBUFFER) &#123;//HWC_FORCE_FRAMEBUFFER layer-&gt;compositionType = HWC_FRAMEBUFFER; hwcLayer-&gt;setType(HwcLayer::LAYER_FORCE_FB); // add layer to FB layer list for zorder check during plane assignment mFBLayers.add(hwcLayer); &#125; else if (layer-&gt;compositionType == HWC_FRAMEBUFFER) &#123;//HWC_FRAMEBUFFER // by default use GPU composition //默认由GPU合成 hwcLayer-&gt;setType(HwcLayer::LAYER_FB); mFBLayers.add(hwcLayer); //检查是否支持cursor overlay if (checkCursorSupported(hwcLayer)) &#123; mCursorCandidates.add(hwcLayer); &#125; else if (checkRgbOverlaySupported(hwcLayer)) &#123;//检查是否支持rgb overlay rgbOverlayLayers.add(hwcLayer); &#125; else if (checkSupported(DisplayPlane::PLANE_SPRITE, hwcLayer)) &#123;//检查是否支持PLANE_SPRITE mSpriteCandidates.add(hwcLayer); &#125; else if (checkSupported(DisplayPlane::PLANE_OVERLAY, hwcLayer)) &#123;//是否支持PLANE_OVERLAY mOverlayCandidates.add(hwcLayer); &#125; else &#123; // noncandidate layer &#125; &#125; else if (layer-&gt;compositionType == HWC_SIDEBAND)&#123;//HWC_SIDEBAND hwcLayer-&gt;setType(HwcLayer::LAYER_SIDEBAND); &#125; else &#123; DEINIT_AND_RETURN_FALSE("invalid composition type %d", layer-&gt;compositionType); &#125; // add layer to layer list mLayers.add(hwcLayer); &#125; if (mFrameBufferTarget == NULL) &#123; ELOGTRACE("no frame buffer target?"); return false; &#125; // If has layer besides of FB_Target, but no FBLayers, skip plane allocation // Note: There is case that SF passes down a layerlist with only FB_Target // layer; we need to have this FB_Target to be flipped as well, otherwise it // will have the buffer queue blocked. (The buffer hold by driver cannot be // released if new buffers' flip is skipped). if ((mFBLayers.size() == 0) &amp;&amp; (mLayers.size() &gt; 1)) &#123; VLOGTRACE("no FB layers, skip plane allocation"); return true; &#125; bool hasOverlay = mOverlayCandidates.size() != 0; while (rgbOverlayLayers.size()) &#123; HwcLayer *hwcLayer = rgbOverlayLayers.top(); if (hasOverlay) &#123; mSpriteCandidates.add(hwcLayer); &#125; else &#123; mOverlayCandidates.add(hwcLayer); &#125; rgbOverlayLayers.removeItemsAt(0); &#125; if (!DisplayQuery::forceFbScaling(mDisplayIndex)) &#123; allocatePlanes(); &#125; else &#123; // force GLES composition on all layers, then use GPU or hardware // overlay to scale buffer to match display resolution assignPrimaryPlane(); &#125; //dump(); return true;&#125; &#160; &#160; &#160; &#160;这一段核心就是根据不同合成类型设置hwcLayer的type，合成类型有HWC_FRAMEBUFFER_TARGET、HWC_FORCE_FRAMEBUFFER、HWC_FRAMEBUFFER、HWC_SIDEBAND，又在HWC_FRAMEBUFFER检测：检查是否支持cursor overlay、检查是否支持rgb overlay、检查是否支持PLANE_SPRITE、是否支持PLANE_OVERLAY，如果支持就要加入OverLay合成。 开始智能合成&#160; &#160; &#160; &#160;继续回到上面，进行最后一步setupSmartComposition，建立只能合成选项，即选择OverLay还是FB：12345678910111213141516171819202122232425262728293031void HwcLayerList::setupSmartComposition()&#123; //默认都是HWC_OVERLAY uint32_t compositionType = HWC_OVERLAY; HwcLayer *hwcLayer = NULL; // setup smart composition only there's no update on all FB layers //如果所有的FB layer没有更新了就开始建立智能合成 for (size_t i = 0; i &lt; mFBLayers.size(); i++) &#123; hwcLayer = mFBLayers.itemAt(i); if (hwcLayer-&gt;isUpdated()) &#123; compositionType = HWC_FRAMEBUFFER; &#125; &#125; VLOGTRACE("smart composition enabled %s", (compositionType == HWC_OVERLAY) ? "TRUE" : "FALSE"); //开始只能合成，当compositionType == HWC_OVERLAY就是硬件合成，否则其他都是FB模式 for (size_t i = 0; i &lt; mFBLayers.size(); i++) &#123; hwcLayer = mFBLayers.itemAt(i); switch (hwcLayer-&gt;getType()) &#123; case HwcLayer::LAYER_FB: case HwcLayer::LAYER_FORCE_FB: hwcLayer-&gt;setCompositionType(compositionType); break; default: ELOGTRACE("Invalid layer type %d", hwcLayer-&gt;getType()); break; &#125; &#125;&#125; &#160; &#160; &#160; &#160;上面就是开始智能合成，当compositionType == HWC_OVERLAY就是硬件合成，否则其他都是FB模式。 小结&#160; &#160; &#160; &#160;上述就是hwc硬件选取合成类型的模块，流程分析比较粗略，估计有好多不正确的地方，欢迎大家指正，我好在第一时间修改。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(十一)----合成Layer之准备合成]]></title>
    <url>%2F2017%2F12%2F01%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E5%8D%81%E4%B8%80-%E5%90%88%E6%88%90Layer%E4%B9%8B%E5%87%86%E5%A4%87%E5%90%88%E6%88%90%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;上一篇我们主要分析了计算Layer的脏区域流程，其中涉及了Buffer状态迁移的后两个步骤：ACQUIRED、RELEADED。本届开始就是正式的合成步骤——REFRESH。但是这个步骤比较复杂，所以先分析合成前的准备工作：重建Layer栈与初始化硬件合成器。 合成准备&#160; &#160; &#160; &#160;当Vsync到来时候，SF会处理MessageQueue的INVALIDATE消息，消息最后一部再次触发REFRESH消息，进入合成阶段。&#160; &#160; &#160; &#160;这个流程在SF的handleMessageRefresh函数，位于frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp中：123456789101112131415void SurfaceFlinger::handleMessageRefresh() &#123; ATRACE_CALL(); /*调Layer的onPreComposition方法，主要是标志一下Layer已经被用于合成*/ preComposition(); /*若Layer的位置/先后顺序/可见性发生变化，重新计算Layer的目标合成区域和先后顺序*/ rebuildLayerStacks(); /*配置硬件合成器，调hwc的prepare方法*/ setUpHWComposer(); /*当打开开发者选项中的“显示Surface刷新”时，额外为产生变化的图层绘制闪烁动画*/ doDebugFlashRegions();//ignore /*执行合成主体，对3D合成而言，调opengl的drawcall，对硬件合成而言，调hwc的set方法*/ doComposition(); /*主要用于调试，调Layer的onPostComposition方法*/ postComposition();&#125; &#160; &#160; &#160; &#160;这个函数每一步的功能都很清晰，看起来简单粗暴。综合上面几节，流程图如下： &#160; &#160; &#160; &#160;每一部的功能，我们会在接下来每一步详细分析。 流程分析preComposition&#160; &#160; &#160; &#160;调Layer的onPreComposition方法，主要是标志一下Layer已经被用于合成。我们看看preComposition函数：1234567891011121314void SurfaceFlinger::preComposition()&#123; bool needExtraInvalidate = false; const LayerVector&amp; layers(mDrawingState.layersSortedByZ); const size_t count = layers.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; if (layers[i]-&gt;onPreComposition()) &#123; needExtraInvalidate = true; &#125; &#125; if (needExtraInvalidate) &#123; signalLayerUpdate(); &#125;&#125; &#160; &#160; &#160; &#160;代码很简单，一共就三步：&#160; &#160; &#160; &#160;1）获取全部经过z-order排序的layer；&#160; &#160; &#160; &#160;2）每个layer调用onPrecomposition判断是否需要触发vsync更新；&#160; &#160; &#160; &#160;3）Layer update。&#160; &#160; &#160; &#160;我们看看第二步的判断逻辑，Layer的onPrecomposition函数，位于frameworks/native/services/surfaceflinger/Layer.cpp：123456bool Layer::onPreComposition() &#123; mRefreshPending = false;//标志一下Layer已经被用于合成 //当layer里存放被queue的frame以后，就会出发layer update return mQueuedFrames &gt; 0 || mSidebandStreamChanged;&#125; &#160; &#160; &#160; &#160;我们在Android SurfaceFlinger 学习之路(八)—-Surface管理图形缓冲区中讲过，当Buffer被queue回BufferQueue，状态迁移到QUEUED，就会回调Layer的onFrameAvailable函数，将mQueuedFrames自动+1。所以当vsync到来时候，就会根据这个判断时候更新layer。 rebuildLayerStacks&#160; &#160; &#160; &#160;若Layer的位置/先后顺序/可见性发生变化，重新计算Layer的目标合成区域和先后顺序。我们要看看rebuildLayerStacks函数：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455void SurfaceFlinger::rebuildLayerStacks() &#123; // rebuild the visible layer list per screen //系统显示屏以及各个应用程序窗口的属性变化时 if (CC_UNLIKELY(mVisibleRegionsDirty)) &#123; ATRACE_CALL(); mVisibleRegionsDirty = false; //将mHwWorkListDirty 标记为true invalidateHwcGeometry(); //获取当前应用程序所有按照z-order排列的layer const LayerVector&amp; layers(mDrawingState.layersSortedByZ); //遍历每一个显示屏 for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; Region opaqueRegion;//全局不透明区域 Region dirtyRegion;//需要重新render的脏区域 Vector&lt; sp&lt;Layer&gt; &gt; layersSortedByZ; const sp&lt;DisplayDevice&gt;&amp; hw(mDisplays[dpy]); const Transform&amp; tr(hw-&gt;getTransform()); const Rect bounds(hw-&gt;getBounds()); if (hw-&gt;isDisplayOn()) &#123; //计算当前显示屏各个应用程序的可见区域 SurfaceFlinger::computeVisibleRegions(layers, hw-&gt;getLayerStack(), dirtyRegion, opaqueRegion); const size_t count = layers.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); const Layer::State&amp; s(layer-&gt;getDrawingState()); if (s.layerStack == hw-&gt;getLayerStack()) &#123; //可见不透明的区域 Region drawRegion(tr.transform( layer-&gt;visibleNonTransparentRegion)); drawRegion.andSelf(bounds); if (!drawRegion.isEmpty()) &#123; //添加进入z-order排列的layer layersSortedByZ.add(layer); &#125; &#125; &#125; &#125; //z-order排列的layer hw-&gt;setVisibleLayersSortedByZ(layersSortedByZ); //显示屏大小 hw-&gt;undefinedRegion.set(bounds); //减去不透明区域 hw-&gt;undefinedRegion.subtractSelf(tr.transform(opaqueRegion)); //累加脏区域 hw-&gt;dirtyRegion.orSelf(dirtyRegion); &#125; &#125;&#125;void SurfaceFlinger::invalidateHwcGeometry()&#123; mHwWorkListDirty = true;&#125; &#160; &#160; &#160; &#160;流程如下：&#160; &#160; &#160; &#160;1）从上一节的内容可以知道，当SurfaceFlinger服务在处理系统显示屏以及各个应用程序窗口的属性变化时，如果发现需要重新计算各个应用程序窗口的可见区域，那么就会将SurfaceFlinger类的成员变量mVisibleRegionsDirty的值设置为true。&#160; &#160; &#160; &#160;进入判断逻辑之后，将成员变量mVisibleRegionsDirty的值设置为false了，因为这时候系统中各个应用程序窗口的可见区域开始计算了。&#160; &#160; &#160; &#160;2）函数接下来通过SurfaceFlinger类的成员变量mDrawingState所描述的一个State对象的成员变量layersSortedByZ来获得系统当前所有的应用程序窗口，并且保存在一个类型为LayerVector的向量layers中。&#160; &#160; &#160; &#160;3）SurfaceFlinger类的成员函数computeVisibleRegions在计算完成各个应用程序窗口的可见区域之后，会得到一个全局不透明区域，保存在输出参数opaqueRegion中。这个全局不透明区域就是接下来需要渲染的，一般情况下，它的大小就应该等于显示屏的大小，即变量hw-&gt;getBounds()所描述的区域。在异常情况下，可能会导致显示屏区域bounds大于全局不透明区域opaqueRegion，这时候前者减去后者就可以得到一些称为“虫洞”的区域。由于这些虫洞区域不会被各个应用程序窗口覆盖，因此，SurfaceFlinger服务需要对它们进行特殊处理，即以一种特殊的方式来渲染它们。在后面的部分内容中，我们就会看到SurfaceFlinger服务是通过调用SurfaceFlinger类的成员函数drawWormhole来渲染这些虫洞的。&#160; &#160; &#160; &#160;4）dirtyRegion是需要被刷新的。 opaqueRegion 不透明区域，layer是按Z-order排序的，左右排在前面的opaqueRegion 会挡住后面的layer。Region drawRegion(tr.transform( layer-&gt;visibleNonTransparentRegion));程序需要进一步得出每个layer 绘制的区域。&#160; &#160; &#160; &#160;5）将结果保存到hw中。 &#160; &#160; &#160; &#160;所以上述流程的重点就是computeVisibleRegions就算过程，我们详细分析这个流程。 &#160; &#160; &#160; &#160;再计算可见区域之前，我们要了解一些概念。 相关区域概述&#160; &#160; &#160; &#160;在分析源码前，我们自己先来想一下，图层中什么样的区域是可见的呢？ Z-order&#160; &#160; &#160; &#160;layer的z-order无疑是第一考虑的要素。因为排在越前面的图层，其获得曝光的机率越大，可见的区域也可能越大，如下图所示： &#160; &#160; &#160; &#160;所以在计算可见性时，是按照Z-order由上而下进行的。假如一个layer的某个区域被确定为可见，那么与之相对应的它下面的所有图层区域都会被遮盖而不可见。 透明度&#160; &#160; &#160; &#160;虽然越前面的layer优先级越高，但这并不代表后面的图层完全没有机会。只要前一个layer不是完全不透明的，那么从理论上来讲用户就应该能“透过”这部分区域看到后面的内容。 图层大小&#160; &#160; &#160; &#160;与透明度一样，图层大小也直接影响到其可见区域。因为每个layer都是有大有小的，即便前一个layer是完全不透明的，但只要它的尺寸没有达到“满屏”，那么比它z-order小的图层还是有机会暴露出来的。这也是我们需要考虑的因素之一。 &#160; &#160; &#160; &#160;因此我们要我们首先解释一些与应用程序窗口相关的概念：可见区域（Visible Region）、透明区域（Transparent Region）、半透明区域（Translucent Region）、完全不透明区域（Opaque Region）和被覆盖区域（Covered Region）。 &#160; &#160; &#160; &#160;假设一个应用程序窗口的宽度和高度分别为w和h，如图所示： &#160; &#160; &#160; &#160;那么我们就可以将由(0,0)、(0, w)、(0, h)和(w,h)四个点组成的区域称为应用程序窗口的可见区域。 &#160; &#160; &#160; &#160; 接下来，我们可以在一个应用程序窗口的可见区域挖一个洞出来，如下图所示： &#160; &#160; &#160; &#160;这时候应用程序窗口真正的可见区域就需要减去中间被挖出来的洞。这个被挖出来的洞就称为应用程序窗口的透明可见区域。&#160; &#160; &#160; &#160;如果应用程序窗口的可见区域的Alpha通道大于0并且小255，那么我们就认为应用程序窗口的可见区域是半透明的。有两种极端情况，即当应用程序窗口的可见区域的Alpha通道等于0或者255的时候。当等于0的时候，我们就认为应用程序窗口的可见区域是透明的，就如图5所示的洞一样，而当等于255的时候，我们就认为应用程序窗口的可见区域是完全不透明的。 &#160; &#160; &#160; &#160;上面我们讨论的应用程序窗口的可见区域是基于单个应用程序窗口而言的，当多个应用程序窗口叠加在一起的时候，在讨论一个应用程序窗口的可见区域的时候，就需要考虑位于它上面的其它应用程序窗口的可见区域的影响了。注意，一个应用程序窗口的可见区域只受位于它上面的其它应用程序窗口影响，而不会受到位于它下面的其它的应用程序窗口影响，因此，我们是按照从上到下的顺序（z-order）来计算系统中各个应用程序窗口的可见区域的。 &#160; &#160; &#160; &#160;为了方便描述，我们假设位于一个应用程序窗口上面的所有应用程序窗口组成了一个整体的可见区域（Above Covered Layers），并且这个可见区域与我们所要讨论的应用程序窗口相交，即它们叠加在一起，如图所示： &#160; &#160; &#160; &#160;由蓝色矩形组成的区域即为上层所有应用程序窗口所组成的一个整体可见区域，这个整体可见区域与下面绿色矩形组成的一个应用程序窗口相交的部分，即由虚线所围成的区域，就是下面的一个应用程序窗口的被覆盖区域。 &#160; &#160; &#160; &#160;一个应用程序窗口的被覆盖区域有可能是半透明的，也有可能是完全不透明的，但是不可能是透明的，如图所示： &#160; &#160; &#160; &#160;在原来由虚线围成的区域中，深蓝色的那部分区域就是完全不透明的（Above Opaque Layers），这时候由绿色矩形组成的应用程序窗口的可见区域除了要减去中间的洞（透明区域）之外，还要减去被覆盖的完全不透明区域，如下图所示： &#160; &#160; &#160; &#160;从上面的讨论我们就可以清楚地知道，为了计算一个应用程序窗口的最终可见区域，我们需要知道： 应用程序窗口的左上角位置，以及宽度和高度，以便可以获得应用程序窗口的原始可见区域。 应用程序窗口的透明区域。 应用程序窗口的被覆盖完全不透明区域。 &#160; &#160; &#160; &#160;用第1步到的原始可见区域减去第2步的透明区域和第3步的被覆盖完全不透明区域，就可以得到一个应用程序窗口的最终可见区域。&#160; &#160; &#160; &#160;为了获得第3步的被覆盖完全不透明区域，我们在计算一个应用程序窗口的最终可见区域的过程中，还需要将此前得到的应用程序窗口的完全不透明区域组合起来，形成一个覆盖完全不透明区域（Above Opaque Layers），因此，我们还需要知道： 应用程序窗口的完全不透明区域。 &#160; &#160; &#160; &#160;此外，由于一个应用程序窗口的被覆盖半透明区域是需要与上层的应用程序窗口可见区域执行混合计算的，因此，我们在计算系统中各个应用程序窗口的可见区域的过程中，还需要将所有上层的应用程序窗口可见区域组合起来形成一个覆盖区域（Above Covered Layers）。 &#160; &#160; &#160; &#160;所以计算所有Layer的可见区域逻辑， 按照Z-order逐个计算各layer的可见区域。对于Z-order值最大的layer，显然没有其它图层会遮盖它。所以它的可见区域(visibleRegion)应该是(当然，前提是这个layer没有超过屏幕区域)自身的大小再减去完全透明的部分(transparentRegion)，由此计算出来的结果我们把它称为aboveCoveredLayers。这个变量应该是全局的，因为它需要被传递到后面的layers中，然后不断地累积运算，直到覆盖整个屏幕区域。 &#160; &#160; &#160; &#160;有了这些背景知识之后，接下来我们就可以分析SurfaceFlinger类的成员函数computeVisibleRegions的实现了。 computeVisibleRegions&#160; &#160; &#160; &#160;computeVisibleRegions代码有点长，我们分段阅读：1234567891011void SurfaceFlinger::computeVisibleRegions( const LayerVector&amp; currentLayers, uint32_t layerStack, Region&amp; outDirtyRegion, Region&amp; outOpaqueRegion)&#123; ATRACE_CALL(); Region aboveOpaqueLayers; Region aboveCoveredLayers; Region dirty; outDirtyRegion.clear(); &#160; &#160; &#160; &#160;这段代码定义了另外两个区域aboveOpaqueLayers，分别用来描述上层覆盖完全不透明区域（Above Opaque Layers）和上层覆盖区域（Above Covered Layers），上面讲过，这两个计算累加的结果。&#160; &#160; &#160; &#160; 我们接着往下阅读代码：12345678910size_t i = currentLayers.size(); while (i--) &#123; const sp&lt;Layer&gt;&amp; layer = currentLayers[i]; // start with the whole surface at its current location const Layer::State&amp; s(layer-&gt;getDrawingState()); // only consider the layers on the given layer stack if (s.layerStack != layerStack) continue; &#160; &#160; &#160; &#160;这段代码是一个while循环的前面几行。系统中所有需要计算可见区域的应用程序窗口都保存在参数currentLayers所描述的一个向量中。这段代码的while循环就是用来逐个地这些应用程序窗口的可见区域的。注意，这个while是先计算是按照从上到下的顺序来计算系统中各个应用程序窗口的可见区域的。&#160; &#160; &#160; &#160;然后获得用来描述当前正在处理的应用程序窗口的当前渲染状态的一个State对象s，然后判断是否是当前应用程序的layerStack，否则就continue。&#160; &#160; &#160; &#160;我们继续往下阅读代码：12345678910111213141516171819202122232425262728/* * opaqueRegion: area of a surface that is fully opaque. */Region opaqueRegion;/* * visibleRegion: area of a surface that is visible on screen * and not fully transparent. This is essentially the layer's * footprint minus the opaque regions above it. * Areas covered by a translucent surface are considered visible. */Region visibleRegion;/* * coveredRegion: area of a surface that is covered by all * visible regions above it (which includes the translucent areas). */Region coveredRegion;/* * transparentRegion: area of a surface that is hinted to be completely * transparent. This is only used to tell when the layer has no visible * non-transparent regions and can be removed from the layer list. It * does not affect the visibleRegion of this layer or any layers * beneath it. The hint may not be correct if apps don't respect the * SurfaceView restrictions (which, sadly, some don't). */Region transparentRegion; &#160; &#160; &#160; &#160;接着再定义了四个Region对象opaqueRegion、visibleRegion、coveredRegion和transparentRegion，分别用来描述当前正在处理的应用程序窗口的完全不透明区域、可见区域、被覆盖区域和透明区域。这四个区域的含义和作用如前所述。&#160; &#160; &#160; &#160;go on:1234567891011121314151617181920212223242526272829303132333435363738394041424344// handle hidden surfaces by setting the visible region to empty if (CC_LIKELY(layer-&gt;isVisible())) &#123; //是否是半透明 const bool translucent = !layer-&gt;isOpaque(s); //layer的可见区域 Rect bounds(s.transform.transform(layer-&gt;computeBounds())); //以便可以得到当前正在处理的应用程序窗口的初始可见区域visibleRegion visibleRegion.set(bounds); if (!visibleRegion.isEmpty()) &#123;//计算透明区域 // Remove the transparent area from the visible region if (translucent) &#123; //当前渲染状态的一个State对象s的成员变量transform指向的也是一个变换矩阵 //用来描述当前正在处理的应用程序窗口的位置、旋转方向和缩放因子等 const Transform tr(s.transform); //函数接着判断当前正在处理的应用程序窗口是否被旋转过或者被缩放过 if (tr.transformed()) &#123; //判断当前正在处理的应用程序窗口是否被旋转和缩放得不规则 if (tr.preserveRects()) &#123; // transform the transparent region transparentRegion = tr.transform(s.activeTransparentRegion); &#125; else &#123; //即当当前正在处理的应用程序窗口被旋转和缩放得不规则时， //这时候对应用程序窗口的透明区域进行旋转或者缩放就会很复杂， //于是函数就干脆将它的透明区域忽略掉 // transformation too complex, can't do the // transparent region optimization. transparentRegion.clear(); &#125; &#125; else &#123;//如果没有经过矩阵变换处理 transparentRegion = s.activeTransparentRegion; &#125; &#125; // compute the opaque region //当当前正在处理的应用程序窗口是完全不透明，并且旋转方向也是规则时， //那么它的完全不透明区域opaqueRegion就等于前面计算所得到的可见区域visibleRegion const int32_t layerOrientation = s.transform.getOrientation(); if (s.alpha==255 &amp;&amp; !translucent &amp;&amp; ((layerOrientation &amp; Transform::ROT_INVALID) == false)) &#123; // the opaque region is the layer's footprint opaqueRegion = visibleRegion; &#125; &#125; &#125; &#160; &#160; &#160; &#160;这段代码用来计算当前正在处理的应用程序窗口的可见区域和完全不透明区域。只有在当前正在处理的应用程序窗口处于可见状态，并且它不是完全透明时，才需要计算这两个区域。我们看看Layer的isVisible函数，位于frameworks/native/services/surfaceflinger/Layer.cpp：123456bool Layer::isVisible() const &#123; const Layer::State&amp; s(mDrawingState); return !(s.flags &amp; layer_state_t::eLayerHidden) &amp;&amp; s.alpha &amp;&amp; (mActiveBuffer != NULL || mSidebandStream != NULL);&#125; &#160; &#160; &#160; &#160;当State对象s的成员变量flags的ISurfaceComposer::eLayerHidden位等于0时，就说明当前正在处理的应用程序窗口是处于可见状态的，而当它的另外一个成员变量alpha的值不等于0的时候，就说明当前正在处理的应用程序窗口不是完全透明的。&#160; &#160; &#160; &#160;回到上面部分，接着计算透明区域。用来描述当前正在处理的应用程序窗口的当前渲染状态的一个State对象s的成员变量transform指向的也是一个变换矩阵，用来描述当前正在处理的应用程序窗口的位置、旋转方向和缩放因子等。&#160; &#160; &#160; &#160;函数接着判断当前正在处理的应用程序窗口是否被旋转过或者被缩放过。如果是的话，那么前面调用变换矩阵tr的成员函数transformed的返回值就会等于true，即变量transformed的等于true。在这种情况下，函数就要相应地对当前正在处理的应用程序窗口的透明区域进行旋转或者缩放。但是有一种特殊情况，即当当前正在处理的应用程序窗口被旋转和缩放得不规则时，这时候对应用程序窗口的透明区域进行旋转或者缩放就会很复杂，于是函数就干脆将它的透明区域忽略掉。判断当前正在处理的应用程序窗口是否被旋转和缩放得不规则是通过调用变换矩阵tr的成员函数preserveRects来实现的，当它的返回值等于true的时候，就说明当前正在处理的应用程序窗口是否被旋转和缩放之后还是规则，否则就是不规则的。&#160; &#160; &#160; &#160;当前正在处理的应用程序窗口的透明区域保存在State对象s的成员变量transparentRegion中，按照上述原理，函数按照以下规则来对它进行处理：&#160; &#160; &#160; &#160;1）当变量transformed的等于false时，说明当前正在处理的应用程序窗口的透明区域就不需要进行旋转或者缩放，这时候就可以将这个透明区域保存在变量transparentRegionScreen中。&#160; &#160; &#160; &#160;2）当变量transformed的等于true，并且变换矩阵tr的成员函数preserveRects的返回值也等于true时，那么就说明当前正在处理的应用程序窗口的透明区域需要进行旋转或者缩放，这时候通过调用变换矩阵tr的成员函数transform来实现的。 最终得到的透明区域同样是保存在变量transparentRegionScreen中。&#160; &#160; &#160; &#160;3） 当变量transformed的等于true，并且变换矩阵tr的成员函数preserveRects的返回值等于false时，那么就说明需要忽略掉当前正在处理的应用程序窗口的透明区域，通过成员函数clear来实现的。 &#160; &#160; &#160; &#160;我们接着来看当前正在处理的应用程序窗口的完全不透明区域的计算过程。&#160; &#160; &#160; &#160;当当前正在处理的应用程序窗口是完全不透明，并且旋转方向也是规则时，那么它的完全不透明区域opaqueRegion就等于前面计算所得到的可见区域visibleRegion。当当前正在处理的应用程序窗口的Alpha通道等于255，即当State对象s的成员变量alpha的值等于255，并且变量translucent的值等于false时，就说明它是完全不透明的，而当当前正在处理的应用程序窗口的旋转方向layerOrientation的Transform::ROT_INVALID位等于0的时候，就说明它的旋转方向是规则的。 &#160; &#160; &#160; &#160;go on：12345678// Clip the covered region to the visible region coveredRegion = aboveCoveredLayers.intersect(visibleRegion); // Update aboveCoveredLayers for next (lower) layer aboveCoveredLayers.orSelf(visibleRegion); // subtract the opaque region covered by the layers above us visibleRegion.subtractSelf(aboveOpaqueLayers); &#160; &#160; &#160; &#160;这段代码用来计算当前正在处理的应用程序窗口的被覆盖区域，以及再进一步计算它的可见区域，主要考虑是否被上层的不透明区域覆盖了。&#160; &#160; &#160; &#160;变量aboveCoveredLayers用来描述当前正在处理的应用程序窗口的所有上层应用程序窗口所组成的可见区域，将这个区域与当前正在处理的应用程序窗口的可见区域visibleRegion相交，就可以得到当前正在处理的应用程序窗口的被覆盖区域coveredRegion，而将这个区域与当前正在处理的应用程序窗口的可见区域visibleRegion相或一下，就可以得到下一个应用程序窗口的所有上层应用程序窗口所组成的可见区域aboveCoveredLayers。&#160; &#160; &#160; &#160;变量aboveOpaqueLayers用来描述当前正在处理的应用程序窗口的所有上层应用程序窗口所组成的完全不透明区域，这个区域从当前正在处理的应用程序窗口的可见区域visibleRegion减去后，就可以得到当前正在处理的应用程序窗口的最终可见区域visibleRegion。 &#160; &#160; &#160; &#160;go on:123456789101112131415161718192021222324252627282930// compute this layer's dirty region if (layer-&gt;contentDirty) &#123; // we need to invalidate the whole region dirty = visibleRegion; // as well, as the old visible region dirty.orSelf(layer-&gt;visibleRegion); layer-&gt;contentDirty = false; &#125; else &#123; /* compute the exposed region: * the exposed region consists of two components: * 1) what's VISIBLE now and was COVERED before * 2) what's EXPOSED now less what was EXPOSED before * * note that (1) is conservative, we start with the whole * visible region but only keep what used to be covered by * something -- which mean it may have been exposed. * * (2) handles areas that were not covered by anything but got * exposed because of a resize. */ const Region newExposed = visibleRegion - coveredRegion; const Region oldVisibleRegion = layer-&gt;visibleRegion; const Region oldCoveredRegion = layer-&gt;coveredRegion; const Region oldExposed = oldVisibleRegion - oldCoveredRegion; dirty = (visibleRegion&amp;oldCoveredRegion) | (newExposed-oldExposed); &#125; dirty.subtractSelf(aboveOpaqueLayers); // accumulate to the screen dirty region outDirtyRegion.orSelf(dirty); &#160; &#160; &#160; &#160;这段代码用来计算屏幕的脏区域。我们首先解释一下屏幕的脏区域是如何计算的。将所有应用程序窗口的脏区域都组合起来，就可以得到屏幕的脏区域，这个脏区域就是需要重新执行渲染操作的。因此，为了得到屏幕的脏区域，我们要知道当前正在处理的应用程序窗口的脏区域，以及之前已经处理了的应用程序窗口脏区域组合。前者使用变量dirty来描述，而后者使用输出参数outDirtyRegion来描述。 &#160; &#160; &#160; &#160;我们首先来看当前正在处理的应用程序窗口的脏区域dirty是如何计算的。我们分两种情况来考虑。&#160; &#160; &#160; &#160;1）首先考虑当前正在处理的应用程序窗口上一次的状态还未来得及处理的情况，即它当前的内容是脏的。在这种情况下，layer的成员变量contentDirty的值就会等于true。这时候我们就需要将该应用程序窗口的上一次可见区域，以及当前的可见区域合并起来，形成一个大的脏区域，这样就可以将两次渲染操作合并成一次来执行。当前正在处理的应用程序窗口的上一次可见区域保存在layer的成员变量visibleRegion中，而它前的可见区域保存在变量visibleRegion中。将这两者相或一下，就可以得到当前正在处理的应用程序窗口的脏区域dirty。&#160; &#160; &#160; &#160;2）接着考虑当前正在处理的应用程序窗口上一次的状态已经处理了的情况，即它当前的内容不是脏的，这意味着它所要显示的内容没有发生变化。在这种情况下，就不需要重新渲染所有的可见区域。那么那些区域是需要重新渲染的呢？第一部分区域是之前是被覆盖的，现在不被覆盖了，第二部分是由于窗口大小变化而引发的新增不被覆盖区域。接下来，我们就来看看这两部分区域是如何计算的。&#160; &#160; &#160; &#160;将一个应用程序窗口的当前可见区域减去被覆盖区域，就可以它的当前不被覆盖的区域newExposed，按照同样的方法，我们可以也可以得到它的上一次不被覆盖的区域oldExposed。注意，一个应用程序窗口的上一次可见区域和被覆盖区域分别保存与它相对应的一个Layer对象的成员变量visibleRegion和coveredRegion中。这样，将一个应用程序窗口的当前不被覆盖的区域newExposed减去它的上一次不被覆盖的区域oldExposed，就可以得到新增的不被覆盖区域，即可以得到第二部分需要重新渲染的区域。另一方面，将一个应用程序窗口的当前可见区域visibleRegion与它的上一次被覆盖区域oldCoveredRegion相交，就可以得到之前是被覆盖的而现在不被覆盖了的区域，即可以得到第一部分需要重新渲染的区域。将第一部分和第二部分需要重新渲染的区域组合起来，就可以得到当前正在处理的应用程序窗口的脏区域dirty。&#160; &#160; &#160; &#160;得到了当前正在处理的应用程序窗口的脏区域dirty，接下来的事情就好办了。首先从该脏区域dirty减去上层的完全不透明区域，因为后者的渲染不需要当前应用程序窗口来参与，接着最将得到的新的脏区域dirty累加到输出参数outDirtyRegion中去，这样就可以得到目前为止，SurfaceFlinger服务需要渲染的脏区域。 &#160; &#160; &#160; &#160;go on:123456789101112 // Update aboveOpaqueLayers for next (lower) layer aboveOpaqueLayers.orSelf(opaqueRegion); // Store the visible region in screen space layer-&gt;setVisibleRegion(visibleRegion); layer-&gt;setCoveredRegion(coveredRegion); layer-&gt;setVisibleNonTransparentRegion( visibleRegion.subtract(transparentRegion)); &#125; outOpaqueRegion = aboveOpaqueLayers;&#125; &#160; &#160; &#160; &#160;这段代码是前面的while循环的几行结束代码，主要用来做三件事情。&#160; &#160; &#160; &#160;第一件事情是计算到目前为止所得到的上层应用程序窗口的完全不透明区域，这是通过组合当前正在处理的应用程序窗口的完全不透明区域与位于它上面的的所有应用程序窗口的完全不透明区域aboveOpaqueLayers来得到的，并且最终结果保存在变量aboveOpaqueLayers中。&#160; &#160; &#160; &#160;第二件事情是调用layer的成员函数setVisibleRegion、setCoveredRegion和setVisibleNonTransparentRegion来保存当前正在处理的应用程序窗口的可见区域、被覆盖区域和没有透明区域的可见区域。&#160; &#160; &#160; &#160;第三件事情是，函数还将前面所有的应用程序窗口组成的完全不透明区域aboveOpaqueLayers保存在输出参数opaqueRegion，以便可以返回给调用者使用。 &#160; &#160; &#160; &#160;这就是计算Layer可视区域的所有流程，只要理解了里面每个变量代表对应含义区域的意义，就不难分析。这里我们rebuildLayerStacks流程就分析完了。我们继续往下走，分析下一步。 setUpHWComposer&#160; &#160; &#160; &#160;合成步骤的下一步就是setUpHWComposer函数调用，配置硬件合成器。 回顾HWC&#160; &#160; &#160; &#160;我们之前在Android SurfaceFlinger 学习之路(二)—-SurfaceFlinger概述讲过，用于合成surface的功能模块可以有2个，OpenGL ES &amp; HWC，它的管理实在HWC里面实现的。&#160; &#160; &#160; &#160;官网也给出了一些解释，https://source.android.google.cn/devices/graphics/arch-sf-hwc ，不过中文翻译的不是很好，最好看看英文：&#160; &#160; &#160; &#160;Hardware Composer HAL (HWC) 是在 Android 3.0 中推出的，并且多年来一直都在不断演进。它主要是用来确定通过可用硬件来合成缓冲区的最有效方法。作为 HAL，其实现是特定于设备的，而且通常由显示设备硬件原始设备制造商 (OEM) 完成。&#160; &#160; &#160; &#160;当考虑叠加平面时，很容易发现这种方法的好处。它的目的是在显示硬件（而不是 GPU）中将多个缓冲区合成在一起。例如，假设有一部处于纵向模式的普通 Android 手机，其状态栏在顶部，导航栏在底部，其他地方为应用内容。每个层的内容都在单独的缓冲区中。您可以使用以下任一方法处理合成： 将应用内容渲染到暂存缓冲区中，然后在其上渲染状态栏，再在其上渲染导航栏，最后将暂存缓冲区传送到显示硬件。 将三个缓冲区全部传送到显示硬件，并告知它从不同的缓冲区读取屏幕不同部分的数据。 &#160; &#160; &#160; &#160;后一种方法可以显著提高效率。&#160; &#160; &#160; &#160;上面两种合成分别叫离线合成和在线合成。 &#160; &#160; &#160; &#160;离线合成：先将所有图层画到一个最终层（FrameBuffer）上，再将FrameBuffer送到LCD显示。由于合成FrameBuffer与送LCD显示一般是异步的（线下生成FrameBuffer，需要时线上的LCD去取），因此叫离线合成。&#160; &#160; &#160; &#160;在线合成：不使用FrameBuffer，在LCD需要显示某一行的像素时，用显示控制器将所有图层与该行相关的数据取出，合成一行像素送过去。只有一个图层时，又叫Overlay技术。&#160; &#160; &#160; &#160;由于省去合成FrameBuffer时读图层，写FrameBuffer的步骤，大幅降低了内存传输量，减少了功耗，但这个需要硬件支持。 &#160; &#160; &#160; &#160;效率对比：大部分情况下，在线合成比起离线合成有很明显的优势，大幅降低了内存带宽的消耗。不过对于多屏显示，静态场景（仅限LCD不带缓存的情况），离线合成会有优势，做下简单的计算不难推得。 &#160; &#160; &#160; &#160;显示处理器功能差异很大。overlay的数量（无论层是否可以旋转或混合）以及对定位和叠加的限制很难通过 API 表达。HWC 会尝试通过一系列决策来适应这种多样性： SurfaceFlinger给HWC提供layer list，询问如何处理这些layer； HWC将每个layer标记为overlay或者GLES composition，然后回馈给SurfaceFlinger； SurfaceFlinger需要去处理那些GLES的合成，而不用去管overlay的合成，最后将overlay的layer和GLES合成后的buffer发送给HWC处理。 &#160; &#160; &#160; &#160;用之前一幅图表示： &#160; &#160; &#160; &#160;屏幕上的内容没有变化时，overlay的效率可能会低于 GL 合成。当overlay内容具有透明像素且overlay混合在一起时，尤其如此。在此类情况下，HWC 可以选择为部分或全部层请求 GLES 合成，并保留合成的缓冲区。如果 SurfaceFlinger 返回要求合成同一组缓冲区，HWC 可以继续显示先前合成的暂存缓冲区。这可以延长闲置设备的电池续航时间。&#160; &#160; &#160; &#160;运行 Android 4.4 或更高版本的设备通常支持 4 个overlay。尝试合成多于overlay的层会导致系统对其中一些层使用 GLES 合成，这意味着应用使用的层数会对功耗和性能产生重大影响。 &#160; &#160; &#160; &#160;关于HWC HAL有更详细的解释，官网在置如此：https://source.android.google.cn/devices/graphics/implement-hwc ，中文翻译的依旧有些别扭： &#160; &#160; &#160; &#160;Hardware Composer HAL (HWC) 由 SurfaceFlinger 用来将 Surface 合成到屏幕。HWC 可以抽象出叠加层和 2D 位块传送器等对象，有助于分载通常使用 OpenGL 完成的一些工作。Android 7.0 包含新版本的 HWC (HWC2)，由 SurfaceFlinger 用来与专门的窗口合成硬件进行通信。SurfaceFlinger 包含使用 3D 图形处理器 (GPU) 执行窗口合成任务的备用路径，但由于以下几个原因，此路径并不理想： 通常，GPU 未针对此用例进行过优化，因此能耗可能要大于执行合成所需的能耗。 每次 SurfaceFlinger 使用 GPU 进行合成时，应用都无法使用处理器进行自我渲染，因此应尽可能使用专门的硬件而不是 GPU 进行合成。 &#160; &#160; &#160; &#160;由于 Hardware Composer 抽象层后的物理显示设备硬件可因设备而异，因此很难就具体功能提供建议。一般来说，请遵循以下准则： HWC 应至少支持 4 个叠加层（状态栏、系统栏、应用和壁纸/背景）。 层可以大于屏幕，因此 HWC 应能处理大于显示屏的层（例如壁纸）。 应同时支持预乘每像素 Alpha 混合和每平面 Alpha 混合。 HWC 应能消耗 GPU、相机和视频解码器生成的相同缓冲区，因此支持以下某些属性很有帮助： RGBA 打包顺序 YUV 格式 平铺、重排和步幅属性 为了支持受保护的内容，必须提供受保护视频播放的硬件路径。 &#160; &#160; &#160; &#160;常规建议是首先实现非运行的 HWC；在结构完成后，实现一个简单的算法，以将合成委托给 HWC（例如，仅将前 3 个或前 4 个 Surface 委托给 HWC 的叠加硬件）。 &#160; &#160; &#160; &#160;专注于优化，例如智能地选择要发送到叠加硬件的 Surface，以最大限度提高从 GPU 移除的负载。另一种优化是检测屏幕是否正在更新；如果不是，则将合成委托给 OpenGL 而不是 HWC，以节省电量。当屏幕再次更新时，继续将合成分载到 HWC。&#160; &#160; &#160; &#160;为常见用例做准备，如： 纵向和横向模式下的全屏游戏 带有字幕和播放控件的全屏视频 主屏幕（合成状态栏、系统栏、应用窗口和动态壁纸） 受保护的视频播放 多显示设备支持 &#160; &#160; &#160; &#160;这些用例应针对常规可预测的用途，而不是很少遇到的边缘用例（否则，优化将收效甚微）。实现必须平衡动画流畅性和交互延迟时间这两个相互矛盾的目标。 &#160; &#160; &#160; &#160;根据这些Google定义的这些准则，我们能够更好地分析配置硬件合成器的步骤。 准备工作&#160; &#160; &#160; &#160;在分析setUpHWComposer函数之前，我们要回顾一下Android SurfaceFlinger 学习之路(五)—-VSync 工作原理中的硬件加载的部分，初始化显示设备和HWComposer设备的内容。 &#160; &#160; &#160; &#160;从上一部分得知，比如使用3D合成，需要大面积的像素混合计算和大量的内存传输（GPU读写GraphicBuffer所需），对GPU和DDR来说是一个巨大的负担。在GPU/DDR重度使用的场景（比如玩游戏），会造成发热、卡顿等。&#160; &#160; &#160; &#160;为了提升性能，减少功耗，可以将合成这个过程交由另一个芯片完成，减轻GPU负担。进一步，直接让这个芯片连LCD，在LCD需要显示某一行时在线合成。&#160; &#160; &#160; &#160;HwComposer便是这一个/多个专用合成芯片的驱动HAL层。&#160; &#160; &#160; &#160;驱动由集成芯片系统的厂商自行设计，但需要遵循一定的标准，这个标准就是Android规定的HwComposer接口。 &#160; &#160; &#160; &#160;hwcomposer的接口定义位于此文件：&#160; &#160; &#160; &#160;hardware/libhardware/include/hardware/hwcomposer.h&#160; &#160; &#160; &#160;其中部分宏定义在：&#160; &#160; &#160; &#160;hardware/libhardware/include/hardware/hwcomposer_defs.h &#160; &#160; &#160; &#160;相关结构解释如下： Layer&#160; &#160; &#160; &#160;在SurfaceFlinger中，Layer对应于window表示一个Buffer循环体系，对HwComposer而言，Layer仅指代当前Buffer，也即SurfaceFlinger中的Layer的当前帧。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687typedef struct hwc_layer_1 &#123; int32_t compositionType;//合成类型，SurfaceFlinger将合成目标Framebuffer的合成类型设为HWC_FRAMEBUFFER_TARGET，其他hwcomposer在prepare时根据实际情况修改 //在(*prepare)()调用前，需要复位HWC_BACKGROUND or HWC_FRAMEBUFFER，需要设置HWC_GEOMETRY_CHANGED的标志符，并在(*prepare)()过程中保持， /* * HWC_BACKGROUND：(*prepare)()调用前设置，表明这是个特殊的"background"的层，backgroundColor是无效的，HWC向HWC_FRAMEBUFFER切换此值，并表示无法使用backgroundColor。 * HWC_FRAMEBUFFER_TARGET：在prepare前设置此值，此值表明此层是framebuffersurface层，作为OpenGLEScomposition的对象，如果HWC设置其他层为HWC_OVERLAY或则为HWC_BACKGROUND，则在set（）过程中opengles则什么都不干。 * 此标志仅仅用在版本至少为HWC_DEVICE_API_VERSION_1_1，在老版本的过程中，OpenGLES target 与（dpy, sur)通信，在HWC实现的过程中此值不能设置。 * 该Layer是3D合成的目标Layer * HWC_FRAMEBUFFER：在(*prepare)()调用前设置，仅仅在HWC_GEOMETRY_CHANGED flag 也在设置的时候设置，并表明此层将使用opengles画进framebuffer中。HWC可以切换此值到HWC_OVERLAY表明其将要管理此层。 * hwcomposer无法处理此Layer，该Layer需要走3D合成流程，用OpenGL绘制 * HWC_OVERLAY：在(*prepare)()过程中，通过HWC设置，表明此层由HWC设置，不能通过OpenGLES 合成。 * 该Layer为硬件合成器所处理，不需要OpenGLES去渲染 * HWC_SIDEBAND：在(*prepare)()前设置，此值表明此层的内容来自于边带视频流，此流在适当的时间（去同步多媒体流），与其他层当前内容进行合成，显示结果图片。此现象依赖于正常的prepare/set周期。prepare/set调用仅仅发生在其他层的改变，或则是边带视频流的位置或则大小的改变。假如h/w composer 无法管理层由于边带视频流的原因（unsupportedscaling/blending/rotation, or too many sideband layers)，其可以在(*prepare)()设置合成类型为HWC_FRAMEBUFFER。但是，这样做显示维实体颜色，因为平台无法使用GPU进行合成边带视频层。这个问题在未来的平台版本中将得到改善。 * 该Layer为视频的边频带，需要硬件合成器作特殊处理，若不支持，OpenGL方式只能以一个色块替代，这个标志是外界（应用/驱动）调用窗口系统的perform方法配置的 * HWC_CURSOR_OVERLAY：在(*prepare)()期间，设置HWC实现，这个值意味着此层的合成将要被HWC的管理，另外，客户端在屏幕上可以异步刷新，这个层的位置可以使用setCursorPositionAsync() api。 * 该Layer可通过setCursorPositionAsync 方法改变坐标 */ uint32_t hints;//hwcomposer设置，通知SurfaceFlinger需要修改的配置 /* HWC_HINT_TRIPLE_BUFFER = 0x00000001：表示需要SurfaceFlinger将此Layer改成3Buffer循环 HWC_HINT_CLEAR_FB = 0x00000002：要求SurfaceFlinger清空该Layer位置的FrameBuffer数据（即置0） */ uint32_t flags;//SurfaceFlinger设置，hwcomposer作处理 /* HWC_SKIP_LAYER = 0x00000001：此Layer不参与合成，应当忽略 HWC_IS_CURSOR_LAYER = 0x00000002：此Layer建议设定为一个CURSOR_LAYER，hwcomposer能处理的话将其合成类型改为HWC_CURSOR_OVERLAY */ /*该Layer的颜色/Buffer信息*/ union &#123; hwc_color_t backgroundColor;//背景颜色，适用于纯色Layer，hwc_color_t 为一个 argb 结构体 struct &#123; union &#123; buffer_handle_t handle;//此即之间提到的GraphicBuffer const native_handle_t* sidebandStream;//HWC_SIDEBAND类型Layer的buffer &#125;; uint32_t transform;//该Layer所需要作的变换，具体为： /* HWC_TRANSFORM_FLIP_H = HAL_TRANSFORM_FLIP_H//水平翻转 HWC_TRANSFORM_FLIP_V = HAL_TRANSFORM_FLIP_V//垂直翻转 HWC_TRANSFORM_ROT_90 = HAL_TRANSFORM_ROT_90,//需要旋转90度 HWC_TRANSFORM_ROT_180 = HAL_TRANSFORM_ROT_180,//需要旋转180度 HWC_TRANSFORM_ROT_270 = HAL_TRANSFORM_ROT_270,//需要旋转270度 */ int32_t blending;//当前Layer绘制时，和底色/目标色的混合方式 //HWC_BLENDING_NONE = 0x100：不混合，直接覆盖 //HWC_BLENDING_PREMULT = 0x105：该Layer的颜色已经做过alpha预乘，因此混合方式为 src + (1-src.a)*dst //HWC_BLENDING_COVERAGE = 0x405：该Layer的颜色未做过预乘，按 src.a * src + (1-src.a) * dst 的方式混合 union &#123; // crop rectangle in integer (pre HWC_DEVICE_API_VERSION_1_3) hwc_rect_t sourceCropi; hwc_rect_t sourceCrop; // just for source compatibility // crop rectangle in floats (as of HWC_DEVICE_API_VERSION_1_3) hwc_frect_t sourceCropf; &#125;;//该Layer取哪一个区域进行合成 hwc_rect_t displayFrame;//该Layer合成的目标区域 hwc_region_t visibleRegionScreen;//该Layer的可见区域，该区域必然是displayFrame的子集。这个区域由SurfaceFlinger计算而得，用于提示hwcomposer不去合成该Layer的不可见区域，hwcomposer中应当以这个为基准，对应计算该Layer相应的sourcecrop。 int acquireFenceFd;//由Buffer生产者创建，SurfaceFlinger传递进来，hwcomposer在使用该Layer的Buffer之前，需要等这个fence int releaseFenceFd;//由hwcomposer创建，生产者在使用该Buffer之前需要等此fence uint8_t planeAlpha;//整个Layer的alpha值，在取Layer的像素作运算之前，需要先乘 planeAlpha/255。 /* Pad to 32 bits */ uint8_t _pad[3];//用于结构体对齐，占位用 hwc_region_t surfaceDamage;//记录相对上一次合成而言，发生了改变的source区域 &#125;; &#125;;//保留位，用于驱动层自行设计#ifdef __LP64__ uint8_t reserved[120 - 112];#else uint8_t reserved[96 - 84];#endif&#125; hwc_layer_1_t; &#160; &#160; &#160; &#160;这里面最难理解和最易出错的是 SourceCrop、DisplayFrame和VisibleRegion，在处理SOC上的显示问题时，这往往是首先考虑的因素： &#160; &#160; &#160; &#160;如图所示，该Layer的显示区域部分被L2完全挡住，source crop 为该Layer参与合成的范围，display frame 为该Layer合成的目标区域，visibleRegion为该Layer被挡住后，剩余的可见区域集。 &#160; &#160; &#160; &#160;这里我用了我的渣渣小米5，在Home页面，然后进入调试模式，输入adb shell dumpsys SurfaceFlinger ： &#160; &#160; &#160; &#160;截取出这段信息，这段信息是SurfaceFlinger告知硬件合成器如何进行合成的。最后一个FramebufferTarget是目标层，不算进去，参与合成的图层是三个，分别是 : com.android.systemui.ImageWallpaper com.miui.home/com.miui.home.launcher.Launcher StatusBar &#160; &#160; &#160; &#160;其他参数对比我们上面的hwc_layer_1_t结构体，就能理解这些参数的意义。 Display1234567891011121314151617181920212223242526typedef struct hwc_display_contents_1 &#123; /* hwcomposer设置，surfaceflinger去等的fence。对于物理屏(实际上是使用在线合成方式的物理屏），对于虚拟屏/离线合成，此fence在离线合成的目标buffer完成全部写入后解除。 */ int retireFenceFd; union &#123; struct &#123; /* HWC_DEVICE_VERSION_1_0 使用，dpy和sur对应于EGLDisplay 和 EGLSurface */ hwc_display_t dpy; hwc_surface_t sur; &#125;;//HWC_DEVICE_VERSION_1_0 struct &#123; /*HWC_DEVICE_VERSION_1_3 之后支持 hwcomposer合成多屏，这里是指虚拟屏的输出buffer和对应的fence*/ buffer_handle_t outbuf; int outbufAcquireFenceFd; &#125;; &#125;; /*合成该显示屏的所有Layer*/ uint32_t flags; size_t numHwLayers; hwc_layer_1_t hwLayers[0];&#125; hwc_display_contents_1_t; &#160; &#160; &#160; &#160;物理显示屏表示连接实际的显示仪器如LCD，目的是产生显示效果，可以使用在线合成。&#160; &#160; &#160; &#160;虚拟显示屏表示目的是合成一个Buffer，不需要理会这个Buffer后续如何产生显示效果，这时需要把所有图层合成到指定的Buffer上。这种情况下必须离线合成。典型场景是手机连WFD/hdmi，手机合成好的Buffer通过wifi/hdmi传输到电视上显示。 Device&#160; &#160; &#160; &#160;最后是device的函数指针定义 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061typedef struct hwc_composer_device_1 &#123; struct hw_device_t common; //这种方式相当于C语言的继承实现，理解为 hwc_composer_device_1 继承于 hw_device_t 就可以了。 //hw_device_t 包含一个基本信息 version（版本号），下面会提到 /*对所有显示屏中所有Layer作合成准备（此时也可以开始发送合成的命令码下去，启动硬件合成，但不需要等待完成），hwcomposer需要正确汇报每个Layer的composetype，以告知SurfaceFlinger是否需要额外处理。 version 为 1.0 的驱动，只支持一个显示屏 version 为 1.1 的驱动，只支持物理显示屏 version 为 1.3 及以上的驱动，支持物理和虚拟显示屏*/ int (*prepare)(struct hwc_composer_device_1 *dev, size_t numDisplays, hwc_display_contents_1_t** displays); /*此方法将完成各个图层的合成与显示，等效于EGL标准里面的eglSwapBuffers，不过eglSwapBuffers是对OpenGL标准/GPU有效，此方法是对硬件合成器有效*/ int (*set)(struct hwc_composer_device_1 *dev, size_t numDisplays, hwc_display_contents_1_t** displays); /*这个接口一般只用于开启hwcomposer的vsync，在线合成触发vsync的机制较离线合成的设计会麻烦一些*/ int (*eventControl)(struct hwc_composer_device_1* dev, int disp, int event, int enabled); //控制屏幕状态的方法 union &#123; /* HWC 1.3及之前版本，采用 blank 方式，开关某个显示屏 */ int (*blank)(struct hwc_composer_device_1* dev, int disp, int blank); /* HWC 1.4及之后，提供更精细的 setPowerMode ，支持显示屏以较低功耗（但不关）的状态显示*/ int (*setPowerMode)(struct hwc_composer_device_1* dev, int disp, int mode); &#125;; /*查询Hwcomposer的信息*/ int (*query)(struct hwc_composer_device_1* dev, int what, int* value); /*注册 *invalidate：屏幕刷新时触发 *vsync：hwcomopser中的vsync事件产生时触发 *hotplug：显示屏连接/断开时触发 * 三个回调函数*/ void (*registerProcs)(struct hwc_composer_device_1* dev, hwc_procs_t const* procs); /*打印信息，调试用*/ void (*dump)(struct hwc_composer_device_1* dev, char *buff, int buff_len); /*获取显示屏配置、属性*/ int (*getDisplayConfigs)(struct hwc_composer_device_1* dev, int disp, uint32_t* configs, size_t* numConfigs); int (*getDisplayAttributes)(struct hwc_composer_device_1* dev, int disp, uint32_t config, const uint32_t* attributes, int32_t* values); int (*getActiveConfig)(struct hwc_composer_device_1* dev, int disp); /*配置属性，index表示有效属性的编号*/ int (*setActiveConfig)(struct hwc_composer_device_1* dev, int disp, int index); /*更改游标层的坐标*/ int (*setCursorPositionAsync)(struct hwc_composer_device_1 *dev, int disp, int x_pos, int y_pos); /*自行添加的保留函数*/ void* reserved_proc[1];&#125; hwc_composer_device_1_t; 实现&#160; &#160; &#160; &#160;然后我们看看setUpHWComposer的实现。这个函数有点长，但是每个步骤都比较清晰，我们分步骤查看：1234567891011121314151617181920212223242526272829303132333435void SurfaceFlinger::setUpHWComposer() &#123; for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; bool dirty = !mDisplays[dpy]-&gt;getDirtyRegion(false).isEmpty(); bool empty = mDisplays[dpy]-&gt;getVisibleLayersSortedByZ().size() == 0; bool wasEmpty = !mDisplays[dpy]-&gt;lastCompositionHadVisibleLayers; // If nothing has changed (!dirty), don't recompose. // If something changed, but we don't currently have any visible layers, // and didn't when we last did a composition, then skip it this time. // The second rule does two things: // - When all layers are removed from a display, we'll emit one black // frame, then nothing more until we get new layers. // - When a display is created with a private layer stack, we won't // emit any black frames until a layer is added to the layer stack. //如果没有改变（！dirty），不重新合成 //如果有变化，但是当前没有可见的layers，并且上次合成过，那么这次跳过 //有两个规则如下： //1.当所有的layers从显示屏移除，我们发射一个黑的frame，在来到新的layer之前都是黑的 //2.当一个显示屏被创建时候带着一个私有的layer栈，我们直到新的layer被添加到这个layer stack时， //才发射黑的frame bool mustRecompose = dirty &amp;&amp; !(empty &amp;&amp; wasEmpty); ALOGV_IF(mDisplays[dpy]-&gt;getDisplayType() == DisplayDevice::DISPLAY_VIRTUAL, "dpy[%zu]: %s composition (%sdirty %sempty %swasEmpty)", dpy, mustRecompose ? "doing" : "skipping", dirty ? "+" : "-", empty ? "+" : "-", wasEmpty ? "+" : "-"); //DisplayDevice的beginFrame函数，调用FrameBufferSurface的beginFrame函数，返回NO_ERROR mDisplays[dpy]-&gt;beginFrame(mustRecompose); if (mustRecompose) &#123; mDisplays[dpy]-&gt;lastCompositionHadVisibleLayers = !empty; &#125; &#125; &#160; &#160; &#160; &#160;这一步检查每个显示的dirty区域是否改变了，如果是就要Recompose。DisplayDevice的beginFrame函数，调用FrameBufferSurface的beginFrame函数，返回NO_ERROR。如果需要重新合成，将DisplayDevice的lastCompositionHadVisibleLayers标志只为true（！empty）。&#160; &#160; &#160; &#160;这一步作用不大，可以忽略。 &#160; &#160; &#160; &#160;go on：1234567891011121314151617181920212223242526272829HWComposer&amp; hwc(getHwComposer()); if (hwc.initCheck() == NO_ERROR) &#123; // build the h/w work list if (CC_UNLIKELY(mHwWorkListDirty)) &#123; mHwWorkListDirty = false; for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123;//遍历mDisplays sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); //以前我们讲过，这个id就是不同display的type，比如0，1，2 const int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;= 0) &#123; const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers(//遍历DisplayDevice所有可见layer hw-&gt;getVisibleLayersSortedByZ()); const size_t count = currentLayers.size(); if (hwc.createWorkList(id, count) == NO_ERROR) &#123;//根据layer数量调用createWorkList创建hwc_layer_list_t列表 HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i=0 ; cur!=end &amp;&amp; i&lt;count ; ++i, ++cur) &#123; const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); //设置HWC每一个帧hwc_layer_1_t的Geometry信息 //如transform/orientation/alpha等 layer-&gt;setGeometry(hw, *cur); if (mDebugDisableHWC || mDebugRegion || mDaltonize || mHasColorMatrix) &#123; cur-&gt;setSkip(true); &#125; &#125; &#125; &#125; &#125; &#125; &#160; &#160; &#160; &#160;这一步主要工作有：&#160; &#160; &#160; &#160;1）遍历mDisplays，在所有显示屏信息中，遍历DisplayDevice所有可见layer；&#160; &#160; &#160; &#160;2）根据layer数量调用createWorkList创建hwc_layer_list_t列表；&#160; &#160; &#160; &#160;3）设置HWC每一个帧hwc_layer_1_t的Geometry信息，如transform/orientation/alpha等。 &#160; &#160; &#160; &#160;我们先来看下HWComposer的createWorkList函数实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263status_t HWComposer::createWorkList(int32_t id, size_t numLayers) &#123; if (uint32_t(id)&gt;31 || !mAllocatedDisplayIDs.hasBit(id)) &#123; return BAD_INDEX; &#125; if (mHwc) &#123; DisplayData&amp; disp(mDisplayData[id]); if (hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &#123; // we need space for the HWC_FRAMEBUFFER_TARGET //当支持Open GL合成时候，会把numLayer数目+1， //这个多余的1就是合成目标HWC_FRAMEBUFFER_TARGET //之前从dumpsys SurfaceFlinger中能看到最后一个是HWC_FRAMEBUFFER_TARGET numLayers++; &#125; ////当DisplayData中的list为空，我们就要malloc if (disp.capacity &lt; numLayers || disp.list == NULL) &#123; //整个申请内存长度，hwc_display_contents_1_t结构体本身的长度加上后面hwc_layer_1_t个数的长度 size_t size = sizeof(hwc_display_contents_1_t) + numLayers * sizeof(hwc_layer_1_t); free(disp.list); disp.list = (hwc_display_contents_1_t*)malloc(size); disp.capacity = numLayers; &#125; if (hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &#123; disp.framebufferTarget = &amp;disp.list-&gt;hwLayers[numLayers - 1]; //DisplayData的framebufferTarget清0 memset(disp.framebufferTarget, 0, sizeof(hwc_layer_1_t)); const DisplayConfig&amp; currentConfig = disp.configs[disp.currentConfig]; const hwc_rect_t r = &#123; 0, 0, (int) currentConfig.width, (int) currentConfig.height &#125;; disp.framebufferTarget-&gt;compositionType = HWC_FRAMEBUFFER_TARGET; disp.framebufferTarget-&gt;hints = 0; disp.framebufferTarget-&gt;flags = 0; disp.framebufferTarget-&gt;handle = disp.fbTargetHandle; disp.framebufferTarget-&gt;transform = 0; disp.framebufferTarget-&gt;blending = HWC_BLENDING_PREMULT; //framebufferTarget的sourceCrop初始化 if (hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_3)) &#123; disp.framebufferTarget-&gt;sourceCropf.left = 0; disp.framebufferTarget-&gt;sourceCropf.top = 0; disp.framebufferTarget-&gt;sourceCropf.right = currentConfig.width; disp.framebufferTarget-&gt;sourceCropf.bottom = currentConfig.height; &#125; else &#123; disp.framebufferTarget-&gt;sourceCrop = r; &#125; //framebufferTarget的displayFrame初始化 disp.framebufferTarget-&gt;displayFrame = r; disp.framebufferTarget-&gt;visibleRegionScreen.numRects = 1; disp.framebufferTarget-&gt;visibleRegionScreen.rects = &amp;disp.framebufferTarget-&gt;displayFrame; disp.framebufferTarget-&gt;acquireFenceFd = -1; disp.framebufferTarget-&gt;releaseFenceFd = -1; disp.framebufferTarget-&gt;planeAlpha = 0xFF; &#125; disp.list-&gt;retireFenceFd = -1; disp.list-&gt;flags = HWC_GEOMETRY_CHANGED; disp.list-&gt;numHwLayers = numLayers; &#125; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;当支持Open GL合成时候，会把numLayer数目+1，这个多余的1就是合成目标HWC_FRAMEBUFFER_TARGET，之前从dumpsys SurfaceFlinger中能看到最后一个是HWC_FRAMEBUFFER_TARGET。&#160; &#160; &#160; &#160;然后就是为DisplayData的list申请内存，这个内存要先hwc_display_contents_1_t结构体本身的长度加上后面hwc_layer_1_t个数的长度。&#160; &#160; &#160; &#160;后面用DisplayData中list中hwLayers最后一个作为DisplayData中的framebufferTarget。&#160; &#160; &#160; &#160;最后就是framebufferTarget的一些初始化操作。 &#160; &#160; &#160; &#160;关于DisplayData结构体，可以回顾Android SurfaceFlinger 学习之路(五)—-VSync 工作原理中的硬件加载的部分，初始化显示设备和HWComposer设备的内容，这里先贴出来结构体内容：123456789101112131415161718192021222324struct DisplayData &#123; DisplayData(); ~DisplayData(); Vector&lt;DisplayConfig&gt; configs; size_t currentConfig; uint32_t format; // pixel format from FB hal, for pre-hwc-1.1 bool connected; bool hasFbComp; bool hasOvComp; size_t capacity; //上面构造Worklist，并且给DisplayData:list 申请空间 hwc_display_contents_1* list; //DisplayData中list中hwLayers最后一个作为DisplayData中的framebufferTarget hwc_layer_1* framebufferTarget; buffer_handle_t fbTargetHandle; sp&lt;Fence&gt; lastRetireFence; // signals when the last set op retires sp&lt;Fence&gt; lastDisplayFence; // signals when the last set op takes // effect on screen buffer_handle_t outbufHandle; sp&lt;Fence&gt; outbufAcquireFence; // protected by mEventControlLock int32_t events; &#125;; &#160; &#160; &#160; &#160;DisplayData 结构体的成员list是一个hwc_display_contents_1指针，这个hwc_display_contents_1结构体我们上面讲过。里面有个联合体union，联合体里面又有两个结构体。 第一个结构体是HWC_DEVICE_VERSION_1_0 使用，dpy和sur对应于EGLDisplay 和 EGLSurface，这是EGL使用的 第二个结构体是HWC_DEVICE_VERSION_1_3 之后支持 hwcomposer合成多屏，这里是指虚拟屏的输出buffer和对应的fence &#160; &#160; &#160; &#160;hwc_display_contents_1后面成员保存了后面还保存了layer的个数和hwc_layer_1_t 数组的起始地址hwLayers[0]。 &#160; &#160; &#160; &#160;DisplayData 结构体的成员framebufferTarget是一个hwc_layer_1指针，这个hwc_layer_1我们上面也贴出来了。相关成员可以参考上面的内容，包括上面提到的 SourceCrop、DisplayFrame和VisibleRegion。其主要数据在buffer_handle_t handle中，buffer_handle_t 其实就是native_handle_t之前分析过，里面有共享内存的fd和地址。关于buffer_handle_t可以回顾之前申请GraphicBuffer的内容。 &#160; &#160; &#160; &#160;然后就是设置HWC每一个帧hwc_layer_1_t的Geometry信息，如transform/orientation/alpha等。不过这之前我们要先看看HWComposer的begin和end函数，还有相关类或者结构体。 &#160; &#160; &#160; &#160;先看下HWComposer中的begin函数：123456789101112131415HWComposer::LayerListIterator HWComposer::begin(int32_t id) &#123; return getLayerIterator(id, 0);&#125;HWComposer::LayerListIterator HWComposer::getLayerIterator(int32_t id, size_t index) &#123; if (uint32_t(id)&gt;31 || !mAllocatedDisplayIDs.hasBit(id)) &#123;//pass return LayerListIterator(); &#125; const DisplayData&amp; disp(mDisplayData[id]); if (!mHwc || !disp.list || index &gt; disp.list-&gt;numHwLayers) &#123;//pass return LayerListIterator(); &#125; //go to return LayerListIterator(new HWCLayerVersion1(mHwc, disp.list-&gt;hwLayers), index);&#125; &#160; &#160; &#160; &#160;这里不走前两个if，最后新建一个LayerListIterator对象，构造函数中传入HWCLayerVersion1对象，和index，为0。我们贴出来LayerListIterator类结构：1234567891011121314151617181920212223242526272829303132333435363738394041424344/* * Iterator through a HWCLayer list. * This behaves more or less like a forward iterator. */class LayerListIterator &#123; friend struct HWComposer; HWCLayer* const mLayerList; size_t mIndex; LayerListIterator() : mLayerList(NULL), mIndex(0) &#123; &#125; LayerListIterator(HWCLayer* layer, size_t index) : mLayerList(layer), mIndex(index) &#123; &#125; // we don't allow assignment, because we don't need it for now LayerListIterator&amp; operator = (const LayerListIterator&amp; rhs);public: // copy operators LayerListIterator(const LayerListIterator&amp; rhs) : mLayerList(HWCLayer::copy(rhs.mLayerList)), mIndex(rhs.mIndex) &#123; &#125; ~LayerListIterator() &#123; delete mLayerList; &#125; // pre-increment LayerListIterator&amp; operator++() &#123; mLayerList-&gt;setLayer(++mIndex); return *this; &#125; // dereference HWCLayerInterface&amp; operator * () &#123; return *mLayerList; &#125; HWCLayerInterface* operator -&gt; () &#123; return mLayerList; &#125; // comparison bool operator == (const LayerListIterator&amp; rhs) const &#123; return mIndex == rhs.mIndex; &#125; bool operator != (const LayerListIterator&amp; rhs) const &#123; return !operator==(rhs); &#125;&#125;; &#160; &#160; &#160; &#160;这里要留意一下后面几个运算符重载，后面会用到。&#160; &#160; &#160; &#160;这里面HWCLayerVersion1就是mLayerList， index就是mIndex。HWCLayerVersion1的结构如下：1234567891011/* * Concrete implementation of HWCLayer for HWC_DEVICE_API_VERSION_1_0. * This implements the HWCLayer side of HWCIterableLayer. */class HWCLayerVersion1 : public Iterable&lt;HWCLayerVersion1, hwc_layer_1_t&gt; &#123; struct hwc_composer_device_1* mHwc;public: HWCLayerVersion1(struct hwc_composer_device_1* hwc, hwc_layer_1_t* layer) : Iterable&lt;HWCLayerVersion1, hwc_layer_1_t&gt;(layer), mHwc(hwc) &#123; &#125;//ignore &#160; &#160; &#160; &#160;构造函数中传入了mHwc和disp.list-&gt;hwLayers，就是HWC Device HAL层指针和上面我们分析的合成该显示屏的所有Layer。 &#160; &#160; &#160; &#160;而我们再来看看下HWComposer中的end函数：123456789101112131415161718HWComposer::LayerListIterator HWComposer::end(int32_t id) &#123; size_t numLayers = 0; if (uint32_t(id) &lt;= 31 &amp;&amp; mAllocatedDisplayIDs.hasBit(id)) &#123; const DisplayData&amp; disp(mDisplayData[id]); if (mHwc &amp;&amp; disp.list) &#123; numLayers = disp.list-&gt;numHwLayers;//获取到list中laye的个数 if (hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &#123; // with HWC 1.1, the last layer is always the HWC_FRAMEBUFFER_TARGET, // which we ignore when iterating through the layer list. ALOGE_IF(!numLayers, "mDisplayData[%d].list-&gt;numHwLayers is 0", id); if (numLayers) &#123; numLayers--;//变成最后一个用于合成的layer，因为本来最后一个是HWC_FRAMEBUFFER_TARGET &#125; &#125; &#125; &#125; return getLayerIterator(id, numLayers);//和上面一样，只是传入的index不一样。&#125; &#160; &#160; &#160; &#160;end函数和begin很像只是在调用getLayerIterator的时候，begin传入0，end传入layer的最后一位，变成最后一个用于合成的layer，因为本来最后一个是HWC_FRAMEBUFFER_TARGET。最后就是mIndex成员变量不一样。 &#160; &#160; &#160; &#160;接着我们回到上面setUpHWComposer函数，第二部第三个步骤，layer-&gt;setGeometry(hw, （星号，解引用，MD语法转义了这个符号）cur);这里layer是Layer对象，hw是DisplayDevice对象，cur是HWComposer::LayerListIterator对象。我们上面强调了一定要注意LayerListIterator类内部的运算符重载，这里用到了（星号，解引用，MD语法转义了这个符号）运算符：1HWCLayerInterface&amp; operator * () &#123; return *mLayerList; &#125; &#160; &#160; &#160; &#160;而上面的mLayerList就是HWCLayerVersion1对象。所以我们继续查看Layer的setGeometry函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475void Layer::setGeometry( const sp&lt;const DisplayDevice&gt;&amp; hw, HWComposer::HWCLayerInterface&amp; layer)&#123; layer.setDefaultState(); // enable this layer layer.setSkip(false); //受安全保护的layer，不可以在进程间传入，所以合成时候跳过 if (isSecure() &amp;&amp; !hw-&gt;isSecure()) &#123; layer.setSkip(true); &#125; // this gives us only the "orientation" component of the transform const State&amp; s(getDrawingState()); //半透明颜色处理 if (!isOpaque(s) || s.alpha != 0xFF) &#123; layer.setBlending(mPremultipliedAlpha ? HWC_BLENDING_PREMULT : HWC_BLENDING_COVERAGE); &#125; // apply the layer's transform, followed by the display's global transform // here we're guaranteed that the layer's transform preserves rects //处理displayFrame，sourceCrop，还有透明度 Rect frame(s.transform.transform(computeBounds())); frame.intersect(hw-&gt;getViewport(), &amp;frame); const Transform&amp; tr(hw-&gt;getTransform()); layer.setFrame(tr.transform(frame)); layer.setCrop(computeCrop(hw)); layer.setPlaneAlpha(s.alpha); /* * Transformations are applied in this order: * 1) buffer orientation/flip/mirror * 2) state transformation (window manager) * 3) layer orientation (screen orientation) * (NOTE: the matrices are multiplied in reverse order) */ const Transform bufferOrientation(mCurrentTransform); Transform transform(tr * s.transform * bufferOrientation); //如果这个图形缓冲区之前曾经被旋转过，例如，被水平翻转或者垂直翻转过，那么在对它进行合并之前，还需要将它的旋转方向恢复回来。 if (mSurfaceFlingerConsumer-&gt;getTransformToDisplayInverse()) &#123; /* * the code below applies the display's inverse transform to the buffer */ uint32_t invTransform = hw-&gt;getOrientationTransform(); uint32_t t_orientation = transform.getOrientation(); // calculate the inverse transform if (invTransform &amp; NATIVE_WINDOW_TRANSFORM_ROT_90) &#123; invTransform ^= NATIVE_WINDOW_TRANSFORM_FLIP_V | NATIVE_WINDOW_TRANSFORM_FLIP_H; // If the transform has been rotated the axis of flip has been swapped // so we need to swap which flip operations we are performing bool is_h_flipped = (t_orientation &amp; NATIVE_WINDOW_TRANSFORM_FLIP_H) != 0; bool is_v_flipped = (t_orientation &amp; NATIVE_WINDOW_TRANSFORM_FLIP_V) != 0; if (is_h_flipped != is_v_flipped) &#123; t_orientation ^= NATIVE_WINDOW_TRANSFORM_FLIP_V | NATIVE_WINDOW_TRANSFORM_FLIP_H; &#125; &#125; // and apply to the current transform transform = Transform(t_orientation) * Transform(invTransform); &#125; // this gives us only the "orientation" component of the transform const uint32_t orientation = transform.getOrientation(); if (orientation &amp; Transform::ROT_INVALID) &#123; // we can only handle simple transformation layer.setSkip(true); &#125; else &#123; layer.setTransform(orientation); &#125;&#125; &#160; &#160; &#160; &#160;上面就是设置Layer的Geometry信息：&#160; &#160; &#160; &#160;1）受安全保护的layer，不可以在进程间传入，所以合成时候跳过。界面受到安全保护的应用程序窗口的内容是不可以在进程间传输的，这个属性主要是应用在屏幕截图中。例如，如果系统中存在一个界面受到安全保护的应用程序窗口，那么我们就不可以请求SurfaceFlinger服务执行截屏功能，因为SurfaceFlinger服务截取下来的屏幕会被传输给请求的进程使用。&#160; &#160; &#160; &#160;2）处理displayFrame，sourceCrop，还有透明度。&#160; &#160; &#160; &#160;3）如果这个图形缓冲区之前曾经被旋转过，例如，被水平翻转或者垂直翻转过，那么在对它进行合并之前，还需要将它的旋转方向恢复回来。 &#160; &#160; &#160; &#160;这样我们setUpHWComposer第二部就完了，go on：12345678910111213141516171819202122// set the per-frame datafor (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;= 0) &#123; const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers( hw-&gt;getVisibleLayersSortedByZ()); const size_t count = currentLayers.size(); HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i=0 ; cur!=end &amp;&amp; i&lt;count ; ++i, ++cur) &#123; /* * update the per-frame h/w composer data for each layer * and build the transparent region of the FB */ //将layer的mActiveBuffer设置到HWComposer中去 const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); layer-&gt;setPerFrameData(hw, *cur); &#125; &#125;&#125; &#160; &#160; &#160; &#160;有了上面的基础，我们先留意上面强调过，注意LayerListIterator类内部的运算符重载，这里需要用到的是++的重载：12345678LayerListIterator&amp; operator++() &#123; mLayerList-&gt;setLayer(++mIndex); return *this; &#125; virtual status_t setLayer(size_t index) &#123; mCurrentLayer = &amp;mLayerList[index]; return NO_ERROR; &#125; &#160; &#160; &#160; &#160;会调用mLayerList-&gt;setLayer函数,setLayer会从mLayerList中设置当前的mCurrentLayer，通过mLayerList, 这个变量就是disp.list-&gt;hwLayers。&#160; &#160; &#160; &#160;这样准备工作就好了，我们再来分析Layer的setPerFrameData函数：1234567891011121314151617181920void Layer::setPerFrameData(const sp&lt;const DisplayDevice&gt;&amp; hw, HWComposer::HWCLayerInterface&amp; layer) &#123; // we have to set the visible region on every frame because // we currently free it during onLayerDisplayed(), which is called // after HWComposer::commit() -- every frame. // Apply this display's projection's viewport to the visible region // before giving it to the HWC HAL. const Transform&amp; tr = hw-&gt;getTransform(); Region visible = tr.transform(visibleRegion.intersect(hw-&gt;getViewport())); layer.setVisibleRegionScreen(visible); if (mSidebandStream.get()) &#123; layer.setSidebandStream(mSidebandStream); &#125; else &#123; // NOTE: buffer can be NULL if the client never drew into this // layer yet, or if we ran out of memory layer.setBuffer(mActiveBuffer); &#125;&#125; &#160; &#160; &#160; &#160;主要就是调用了HWCLayerVersion1的setBuffer函数:12345678910111213141516virtual void setBuffer(const sp&lt;GraphicBuffer&gt;&amp; buffer) &#123; if (buffer == 0 || buffer-&gt;handle == 0) &#123; getLayer()-&gt;compositionType = HWC_FRAMEBUFFER; getLayer()-&gt;flags |= HWC_SKIP_LAYER; getLayer()-&gt;handle = 0; &#125; else &#123; if (getLayer()-&gt;compositionType == HWC_SIDEBAND) &#123; // If this was a sideband layer but the stream was removed, reset // it to FRAMEBUFFER. The HWC can change it to OVERLAY in prepare. getLayer()-&gt;compositionType = HWC_FRAMEBUFFER; &#125; getLayer()-&gt;handle = buffer-&gt;handle; &#125; &#125; // inline HWCTYPE* getLayer() &#123; return mCurrentLayer; &#125; &#160; &#160; &#160; &#160;用getLayer函数设置其handle，而getLayer就是mCurrentLayer。之前mCurrentLayer会一个个遍历各个Layer,这样就把所有的layer都设置其handle，就是hwc_layer_1_t中的handle。这样就把GraphicBuffer和hwc_layer_1_t关联起来了。 &#160; &#160; &#160; &#160;go on：ignore // If possible, attempt to use the cursor overlay on each display.&#160; &#160; &#160; &#160;go on：1234567891011 //将使用哪种合成报告给HWC status_t err = hwc.prepare(); ALOGE_IF(err, "HWComposer::prepare failed (%s)", strerror(-err)); for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); //return NO_ERROR hw-&gt;prepareFrame(hwc); &#125; &#125;&#125; &#160; &#160; &#160; &#160;最后会调用HWComposer的prepare函数，将使用哪种合成报告给HWC，我们再来看下这个函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081status_t HWComposer::prepare() &#123; for (size_t i=0 ; i&lt;mNumDisplays ; i++) &#123; DisplayData&amp; disp(mDisplayData[i]); if (disp.framebufferTarget) &#123;//这里其实就是disp.list中最后一个layer // make sure to reset the type to HWC_FRAMEBUFFER_TARGET // DO NOT reset the handle field to NULL, because it's possible // that we have nothing to redraw (eg: eglSwapBuffers() not called) // in which case, we should continue to use the same buffer. LOG_FATAL_IF(disp.list == NULL); disp.framebufferTarget-&gt;compositionType = HWC_FRAMEBUFFER_TARGET; &#125; if (!disp.connected &amp;&amp; disp.list != NULL) &#123; ALOGW("WARNING: disp %zu: connected, non-null list, layers=%zu", i, disp.list-&gt;numHwLayers); &#125; mLists[i] = disp.list;//DisplayData的list就是mList的一个组员,hwc_display_contents_1* if (mLists[i]) &#123; //HWC_DEVICE_VERSION_1_3 之后支持 hwcomposer合成多屏，这里是指虚拟屏的输出buffer和对应的fence if (hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_3)) &#123; mLists[i]-&gt;outbuf = disp.outbufHandle; mLists[i]-&gt;outbufAcquireFenceFd = -1; &#125; else if (hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &#123;// HWC_DEVICE_VERSION_1_0 使用，dpy和sur对应于EGLDisplay 和 EGLSurface // garbage data to catch improper use mLists[i]-&gt;dpy = (hwc_display_t)0xDEADBEEF; mLists[i]-&gt;sur = (hwc_surface_t)0xDEADBEEF; &#125; else &#123; mLists[i]-&gt;dpy = EGL_NO_DISPLAY; mLists[i]-&gt;sur = EGL_NO_SURFACE; &#125; &#125; &#125; /*对所有显示屏中所有Layer作合成准备（此时也可以开始发送合成的命令码下去，启动硬件合成，但不需要等待完成），hwcomposer需要正确汇报每个Layer的composetype，以告知SurfaceFlinger是否需要额外处理。 version 为 1.0 的驱动，只支持一个显示屏 version 为 1.1 的驱动，只支持物理显示屏 version 为 1.3 及以上的驱动，支持物理和虚拟显示屏*/ int err = mHwc-&gt;prepare(mHwc, mNumDisplays, mLists); ALOGE_IF(err, "HWComposer: prepare failed (%s)", strerror(-err)); if (err == NO_ERROR) &#123; // here we're just making sure that "skip" layers are set // to HWC_FRAMEBUFFER and we're also counting how many layers // we have of each type. // // If there are no window layers, we treat the display has having FB // composition, because SurfaceFlinger will use GLES to draw the // wormhole region. for (size_t i=0 ; i&lt;mNumDisplays ; i++) &#123; DisplayData&amp; disp(mDisplayData[i]); disp.hasFbComp = false; disp.hasOvComp = false; if (disp.list) &#123; //下面就是根据hwc汇报的composetype来设置相关属性 for (size_t i=0 ; i&lt;disp.list-&gt;numHwLayers ; i++) &#123; hwc_layer_1_t&amp; l = disp.list-&gt;hwLayers[i]; //ALOGD("prepare: %d, type=%d, handle=%p", // i, l.compositionType, l.handle); if (l.flags &amp; HWC_SKIP_LAYER) &#123; l.compositionType = HWC_FRAMEBUFFER; &#125; if (l.compositionType == HWC_FRAMEBUFFER) &#123; disp.hasFbComp = true; &#125; if (l.compositionType == HWC_OVERLAY) &#123; disp.hasOvComp = true; &#125; if (l.compositionType == HWC_CURSOR_OVERLAY) &#123; disp.hasOvComp = true; &#125; &#125; if (disp.list-&gt;numHwLayers == (disp.framebufferTarget ? 1 : 0)) &#123; disp.hasFbComp = true; &#125; &#125; else &#123; disp.hasFbComp = true; &#125; &#125; &#125; return (status_t)err;&#125; &#160; &#160; &#160; &#160;1）遍历每一个显示屏，先取出framebufferTarget进行设置；&#160; &#160; &#160; &#160;2）填充mList，DisplayData的list就是mList的一个组员,hwc_display_contents_1*，上面createWorkList讲过；&#160; &#160; &#160; &#160;3）根据hwc版本初始化dpy和surface，或者outbuf；&#160; &#160; &#160; &#160;4）调用hwc_composer_device_1 的set函数，对所有显示屏中所有Layer作合成准备（此时也可以开始发送合成的命令码下去，启动硬件合成，但不需要等待完成），hwcomposer需要正确汇报每个Layer的composetype，以告知SurfaceFlinger是否需要额外处理。&#160; &#160; &#160; &#160;5）下面就是根据hwc汇报的composetype来设置相关属性，关于不同type再次贴一遍：1234567/* * HWC_FRAMEBUFFER_TARGET：该Layer是3D合成的目标Layer * HWC_FRAMEBUFFER：hwcomposer无法处理此Layer，该Layer需要走3D合成流程，用OpenGL绘制 * HWC_OVERLAY：该Layer为硬件合成器所处理，不需要OpenGLES去渲染 * HWC_SIDEBAND：该Layer为视频的边频带，需要硬件合成器作特殊处理，若不支持，OpenGL方式只能以一个色块替代，这个标志是外界（应用/驱动）调用窗口系统的perform方法配置的 * HWC_CURSOR_OVERLAY：该Layer可通过setCursorPositionAsync 方法改变坐标 */ &#160; &#160; &#160; &#160;这就是最后一步，提交hwc，然后获得合成类型的步骤。 小结&#160; &#160; &#160; &#160;这就是本节内容，太晚了，不想写总结了，先睡了~]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(十)----SurfaceFlinger处理Layer更新]]></title>
    <url>%2F2017%2F11%2F09%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E5%8D%81-SurfaceFlinger%E5%A4%84%E7%90%86Layer%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;上次我们分析了SurfaceFlinger的事务处理，对合成前接受上层改变SF和Layer的状态的事务做了统一处理，这节就沿着上次的末尾，继续分析Layer的更新流程。（又断了很久了，实在惭愧，很多事情总是身不由己～） 更新Layer Buffer&#160; &#160; &#160; &#160;前几节分析Vsync信号还没有分析完，这一节顺着处理事务的尾巴，处理Layer中Buffer的更新。顺着代码走的，就到了SurfaceFlinger的handlePageFlip函数。 handlePageFlip函数&#160; &#160; &#160; &#160;page flip是翻页的意思，就是说翻过所有的页面，检查每个Layer的更新。依然位于frameworks/native/serivice/surfaceflinger/SurfaceFlinger.cpp中：1234567891011121314151617181920212223242526272829303132333435363738void SurfaceFlinger::handlePageFlip()&#123; Region dirtyRegion; bool visibleRegions = false; const LayerVector&amp; layers(mDrawingState.layersSortedByZ); // Store the set of layers that need updates. This set must not change as // buffers are being latched, as this could result in a deadlock. // Example: Two producers share the same command stream and: // 1.) Layer 0 is latched // 2.) Layer 0 gets a new frame // 2.) Layer 1 gets a new frame // 3.) Layer 1 is latched. // Display is now waiting on Layer 1's frame, which is behind layer 0's // second frame. But layer 0's second frame could be waiting on display. Vector&lt;Layer*&gt; layersWithQueuedFrames; //检查Layer是否需要更新 for (size_t i = 0, count = layers.size(); i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); //该Layer是否有 QUEUED buffer，上上一篇讲过， //每当有queued buffer，Layer的onFrameAvailable函数会回调，然后将mQueuedFrames加1 if (layer-&gt;hasQueuedFrame()) //将需要更新的Layer存入这个数组 layersWithQueuedFrames.push_back(layer.get()); &#125; //遍历每一个需要更新的Layer， //调用Layer的latchBuffer函数计算Layer的脏区域， //最后修改Layer所在的Display上的脏区域 for (size_t i = 0, count = layersWithQueuedFrames.size() ; i&lt;count ; i++) &#123; Layer* layer = layersWithQueuedFrames[i]; const Region dirty(layer-&gt;latchBuffer(visibleRegions)); const Layer::State&amp; s(layer-&gt;getDrawingState()); invalidateLayerStack(s.layerStack, dirty); &#125; mVisibleRegionsDirty |= visibleRegions;&#125; &#160; &#160; &#160; &#160;只看这个函数还是挺简单的：&#160; &#160; &#160; &#160;1）检查每个Layer，找出需要更新Layer的，并存入数组。我们在Android SurfaceFlinger 学习之路(八)—-Surface管理图形缓冲区中讲过，当把Graphic Buffer 在app侧绘制完后会queue给BufferQueue，此时buffer状态为QUEUED，同时会通知Layer的onFrameAvailable回调去通知SF消费。Layer的onFrameAvailable函数如下，位于rameworks/native/services/surfaceflinger/Layer.cpp：1234void Layer::onFrameAvailable() &#123; android_atomic_inc(&amp;mQueuedFrames);//会将mQueuedFrames加1 mFlinger-&gt;signalLayerUpdate();&#125; &#160; &#160; &#160; &#160;相应的Layer的hasQueuedFrame函数位于frameworks/native/services/surfaceflinger/Layer.h中：1234/* * Returns if a frame is queued. */bool hasQueuedFrame() const &#123; return mQueuedFrames &gt; 0 || mSidebandStreamChanged; &#125; &#160; &#160; &#160; &#160;因此可以找出需要更新的Layer，然后存入layersWithQueuedFrames这个数组中。 &#160; &#160; &#160; &#160;2）遍历每一个需要更新的Layer，用Layer的latchBuffer函数计算Layer的脏区域。这一步是核心，我们接下来会仔细分析。 &#160; &#160; &#160; &#160;3）最后修改Layer所在的Display上的脏区域。这一步调用了invalidateLayerStack函数：123456789void SurfaceFlinger::invalidateLayerStack(uint32_t layerStack, const Region&amp; dirty) &#123; for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; const sp&lt;DisplayDevice&gt;&amp; hw(mDisplays[dpy]); if (hw-&gt;getLayerStack() == layerStack) &#123; hw-&gt;dirtyRegion.orSelf(dirty); &#125; &#125;&#125; &#160; &#160; &#160; &#160;先遍历所有的设备，然后找到和Layer的layerstack一样的设备，然后通过“或“运算，将Layer的更新区域加到DisplayDevice的dirtyRegion区域上。 &#160; &#160; &#160; &#160;所以这个函数的核心就是Layer的latchBuffer函数调用，计算需要更新的苍区域。 计算Layer的脏区域&#160; &#160; &#160; &#160;latchBuffer函数比较长，我们分部查看： Part.1 处理SidebandStream12345678910111213141516171819202122232425262728293031323334353637Region Layer::latchBuffer(bool&amp; recomputeVisibleRegions)&#123; ATRACE_CALL(); //如果sideband surface改变了，表该Layer为视频的边频带，需要硬件合成器作特殊处理， //然后将 mSidebandStreamChanged置为false if (android_atomic_acquire_cas(true, false, &amp;mSidebandStreamChanged) == 0) &#123; // mSidebandStreamChanged was true mSidebandStream = mSurfaceFlingerConsumer-&gt;getSidebandStream(); // sideband的情况，需要重新 计算可视区域 recomputeVisibleRegions = true; const State&amp; s(getDrawingState()); return s.transform.transform(Region(Rect(s.active.w, s.active.h))); &#125; Region outDirtyRegion; //如果Layer中有QUEUED帧 if (mQueuedFrames &gt; 0) &#123; // if we've already called updateTexImage() without going through // a composition step, we have to skip this layer at this point // because we cannot call updateTeximage() without a corresponding // compositionComplete() call. // we'll trigger an update in onPreComposition(). if (mRefreshPending) &#123; return outDirtyRegion; &#125; // Capture the old state of the layer for comparisons later //注意这里使用的是DrawingState const State&amp; s(getDrawingState()); const bool oldOpacity = isOpaque(s); sp&lt;GraphicBuffer&gt; oldActiveBuffer = mActiveBuffer; //。。。。。。&#125; &#160; &#160; &#160; &#160;这一部分处理了如果是sidebandstream的逻辑，该Layer为视频的边频带，需要硬件合成器作特殊处理，若不支持，OpenGL方式只能以一个色块替代，这个标志是外界（应用/驱动）调用窗口系统的perform方法配置的。 Part.2 定义Reject结构体123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899struct Reject : public SurfaceFlingerConsumer::BufferRejecter &#123; Layer::State&amp; front;//drawingState Layer::State&amp; current;//currentState bool&amp; recomputeVisibleRegions; bool stickyTransformSet; Reject(Layer::State&amp; front, Layer::State&amp; current, bool&amp; recomputeVisibleRegions, bool stickySet) : front(front), current(current), recomputeVisibleRegions(recomputeVisibleRegions), stickyTransformSet(stickySet) &#123; &#125; virtual bool reject(const sp&lt;GraphicBuffer&gt;&amp; buf, const IGraphicBufferConsumer::BufferItem&amp; item) &#123; if (buf == NULL) &#123; return false; &#125; uint32_t bufWidth = buf-&gt;getWidth(); uint32_t bufHeight = buf-&gt;getHeight(); // check that we received a buffer of the right size // (Take the buffer's orientation into account) //旋转90度 if (item.mTransform &amp; Transform::ROT_90) &#123; swap(bufWidth, bufHeight); &#125; bool isFixedSize = item.mScalingMode != NATIVE_WINDOW_SCALING_MODE_FREEZE; if (front.active != front.requested) &#123; //当app请求SF分配图形缓冲区时候，会传入requestWidth和requestHeight //如果请求的w和h为0，则isFixedSize为true，此时就要用窗口的大小 //或者请求的w／h和传入的buffer的w／h相等 if (isFixedSize || (bufWidth == front.requested.w &amp;&amp; bufHeight == front.requested.h)) &#123; // Here we pretend the transaction happened by updating the // current and drawing states. Drawing state is only accessed // in this thread, no need to have it locked //我们假设事务已经提交更新了，current已经赋值给drawing， //所以我们不用再去给DrawingState加锁 //将request的Geometry区域赋值给active front.active = front.requested; // We also need to update the current state so that // we don't end-up overwriting the drawing state with // this stale current state during the next transaction // // NOTE: We don't need to hold the transaction lock here // because State::active is only accessed from this thread. //下一次事务提交更新时候，还会讲下一次的current与drawing交换赋值， //所以本次我们这里不终止用对这次drawing state对current的覆盖 current.active = front.active; // recompute visible region //重新计算可视区域 recomputeVisibleRegions = true; &#125; //打印些许log //如果传入了请求的w／h，并且不是粘性的（往后一直遗留的）转换 if (!isFixedSize &amp;&amp; !stickyTransformSet) &#123; //但是请求的w/h和传入buffer的w/hb不一致 //那么就需要拒绝掉这个buffer if (front.active.w != bufWidth || front.active.h != bufHeight) &#123; // reject this buffer ALOGE("rejecting buffer: bufWidth=%d, bufHeight=%d, front.active.&#123;w=%d, h=%d&#125;", bufWidth, bufHeight, front.active.w, front.active.h); return true; &#125; &#125; // if the transparent region has changed (this test is // conservative, but that's fine, worst case we're doing // a bit of extra work), we latch the new one and we // trigger a visible-region recompute. //如果透明区域也改变了，也需要更新current的透明区域region，原因同上 if (!front.activeTransparentRegion.isTriviallyEqual( front.requestedTransparentRegion)) &#123; front.activeTransparentRegion = front.requestedTransparentRegion; // We also need to update the current state so that // we don't end-up overwriting the drawing state with // this stale current state during the next transaction // // NOTE: We don't need to hold the transaction lock here // because State::active is only accessed from this thread. current.activeTransparentRegion = front.activeTransparentRegion; // recompute visible region //重新计算可视区域 recomputeVisibleRegions = true; &#125; return false; &#125; &#125;; &#160; &#160; &#160; &#160;Reject结构体的定义，reject方法判断是否拒绝掉传入的buffer，详细都在注释中写道。&#160; &#160; &#160; &#160;核心就是如果请求的w／h和传入buffer的w／h不一致，就会拒绝掉这个buffer。&#160; &#160; &#160; &#160;如果不拒绝，就更新current和drawing的状态，方便下一次transation处理状态更新，左后将recomputeVisibleRegions置为true，表示重新计算可视区域。 Part.3 更新纹理并处理结果12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485//创建一个Reject对象Reject r(mDrawingState, getCurrentState(), recomputeVisibleRegions, getProducerStickyTransform() != 0); //更新纹理 status_t updateResult = mSurfaceFlingerConsumer-&gt;updateTexImage(&amp;r, mFlinger-&gt;mPrimaryDispSync); //延迟显示 if (updateResult == BufferQueue::PRESENT_LATER) &#123; // Producer doesn't want buffer to be displayed yet. Signal a // layer update so we check again at the next opportunity. //延迟显示，触发下一个VSYNC， 即Buffer显示的时间还没到 mFlinger-&gt;signalLayerUpdate(); return outDirtyRegion; &#125; // Decrement the queued-frames count. Signal another event if we // have more frames pending. //减少mQueuedFrames的值 //如果还有Queued的Buffer，那么通知 SurfaceFlinger在下一个VSYNC时进行更新 if (android_atomic_dec(&amp;mQueuedFrames) &gt; 1) &#123; mFlinger-&gt;signalLayerUpdate(); &#125; //发生了异常 if (updateResult != NO_ERROR) &#123; // something happened! recomputeVisibleRegions = true; return outDirtyRegion; &#125; // update the active buffer //mActiveBuffer表示马上要显示的buffer //更新mActiveBuffer，得到现在需要输出的图像数据 mActiveBuffer = mSurfaceFlingerConsumer-&gt;getCurrentBuffer(); if (mActiveBuffer == NULL) &#123; ／／异常发生 // this can only happen if the very first buffer was rejected. return outDirtyRegion; &#125; //即将进入 refresh的阶段 mRefreshPending = true; mFrameLatencyNeeded = true; //如果是第一次接收到Buffer, 需要重新计算可视区域 if (oldActiveBuffer == NULL) &#123; // the first time we receive a buffer, we need to trigger a // geometry invalidation. recomputeVisibleRegions = true; &#125; Rect crop(mSurfaceFlingerConsumer-&gt;getCurrentCrop()); const uint32_t transform(mSurfaceFlingerConsumer-&gt;getCurrentTransform()); const uint32_t scalingMode(mSurfaceFlingerConsumer-&gt;getCurrentScalingMode()); if ((crop != mCurrentCrop) || (transform != mCurrentTransform) || (scalingMode != mCurrentScalingMode)) &#123; //保存最新的 crop 与transform 这些变量 mCurrentCrop = crop; mCurrentTransform = transform; mCurrentScalingMode = scalingMode; recomputeVisibleRegions = true; &#125; //最新的buffer和上一个渲染的buffer的尺寸不一样的，这时需要重新计算可视化区域 if (oldActiveBuffer != NULL) &#123; uint32_t bufWidth = mActiveBuffer-&gt;getWidth(); uint32_t bufHeight = mActiveBuffer-&gt;getHeight(); if (bufWidth != uint32_t(oldActiveBuffer-&gt;width) || bufHeight != uint32_t(oldActiveBuffer-&gt;height)) &#123; recomputeVisibleRegions = true; &#125; &#125; //透明度相关 mCurrentOpacity = getOpacityForFormat(mActiveBuffer-&gt;format); if (oldOpacity != isOpaque(s)) &#123; recomputeVisibleRegions = true; &#125; // FIXME: postedRegion should be dirty &amp; bounds //计算出脏区域 Region dirtyRegion(Rect(s.active.w, s.active.h)); // transform the dirty region to window-manager space outDirtyRegion = (s.transform.transform(dirtyRegion)); &#125; return outDirtyRegion;&#125; &#160; &#160; &#160; &#160;part3最核心的就是更新纹理，之后就是对于返回结果的处理。所以步骤就分两步：&#160; &#160; &#160; &#160;1）updateTexImage更新纹理。这一步比较重要，我们接下来单独分析。&#160; &#160; &#160; &#160;2）处理更新纹理的结果。&#160; &#160; &#160; &#160;延迟显示，触发下一个VSYNC， 即Buffer显示的时间还没到；减少mQueuedFrames的值，如果还有Queued的Buffer，那么通知 SurfaceFlinger在下一个VSYNC时进行更新。&#160; &#160; &#160; &#160;如果更新纹理发生了异常、第一次接收到buffer、crop和transform区域发生改变、最新的buffer和上一个渲染的buffer的尺寸不一样的、透明度区域发生改变，都需要重新可视化区域。&#160; &#160; &#160; &#160;最后返回计算出的脏区域。 更新纹理&#160; &#160; &#160; &#160;我们单独分析更新纹理，SurfaceFlingerConsumer的updateTexImage函数，位于frameworks/native/services/surfaceflinger/SurfaceFlingerConsumer.cpp:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677status_t SurfaceFlingerConsumer::updateTexImage(BufferRejecter* rejecter, const DispSync&amp; dispSync)&#123; ATRACE_CALL(); ALOGV("updateTexImage"); Mutex::Autolock lock(mMutex); if (mAbandoned) &#123;//EGL没有初始化 ALOGE("updateTexImage: GLConsumer is abandoned!"); return NO_INIT; &#125; // Make sure the EGL state is the same as in previous calls. //检查EGL的状态是否ok status_t err = checkAndUpdateEglStateLocked(); if (err != NO_ERROR) &#123; return err; &#125; BufferQueue::BufferItem item; // Acquire the next buffer. // In asynchronous mode the list is guaranteed to be one buffer // deep, while in synchronous mode we use the oldest buffer. //获得要显示出来的Buffer //a, 如果上层canvas绘图，获取到的fencefd为-1 //b, 上层opengl绘图，获取到的fencefd不为-1 err = acquireBufferLocked(&amp;item, computeExpectedPresent(dispSync)); //处理获取buffer异常的状况 if (err != NO_ERROR) &#123; if (err == BufferQueue::NO_BUFFER_AVAILABLE) &#123; err = NO_ERROR; &#125; else if (err == BufferQueue::PRESENT_LATER) &#123; // return the error, without logging &#125; else &#123; ALOGE("updateTexImage: acquire failed: %s (%d)", strerror(-err), err); &#125; return err; &#125; // We call the rejecter here, in case the caller has a reason to // not accept this buffer. This is used by SurfaceFlinger to // reject buffers which have the wrong size int buf = item.mBuf; // 检查是否需要 reject buffer，如果尺寸大小不对，就拒绝掉这个buffer if (rejecter &amp;&amp; rejecter-&gt;reject(mSlots[buf].mGraphicBuffer, item)) &#123; //如果拒绝了这个buffer，就要释放它 releaseBufferLocked(buf, mSlots[buf].mGraphicBuffer, EGL_NO_SYNC_KHR); return NO_ERROR; &#125; // Release the previous buffer. // 每次只能处理一个graphic buffer，要将上一次对应的buffer先release了，供别人使用 //首先创建一个release fence， //将release fence传递给BufferQueue中的slot对应的mSlots[slot]的mFence // 创建GLConsumer里新的EGLImage err = updateAndReleaseLocked(item); if (err != NO_ERROR) &#123; return err; &#125; //4.4和以后已经不走这个if了，会在Layer::onDraw中去创建纹理 if (!SyncFeatures::getInstance().useNativeFenceSync()) &#123; // Bind the new buffer to the GL texture. // // Older devices require the "implicit" synchronization provided // by glEGLImageTargetTexture2DOES, which this method calls. Newer // devices will either call this in Layer::onDraw, or (if it's not // a GL-composited layer) not at all. //绑定EglImage到GL texture err = bindTextureImageLocked(); &#125; return err;&#125; &#160; &#160; &#160; &#160;在这里面会去acquire buffer，然后acquire到的buffer就会去用来合成。主要做的事情：&#160; &#160; &#160; &#160;1）acquire一个新的buffer；&#160; &#160; &#160; &#160;2）将上一次对应的buffer先release了，并为上次的buffer创建一个release fence，将该release fence传递给BufferQueue中的slot对应的mSlots[slot]的mFence；&#160; &#160; &#160; &#160;3）更新mCurrentTexture和mCurrentTextureBuf为这次acquire到的buffer以及slot。&#160; &#160; &#160; &#160;目前acquire fencefd还没使用，因为还未去合成这个layer，没到用layer中数据的时候。 &#160; &#160; &#160; &#160;这里有几个重要的步骤，我们分模块分析。 获取buffer&#160; &#160; &#160; &#160;之前我们讲过buffer的状态迁移，又如下一幅图： &#160; &#160; &#160; &#160;之前讲buffer queued进入BufferQueue，然后等等待SF消费。所以SF本次acquire buffer，buffer状态迁移为ACQUIRED。 &#160; &#160; &#160; &#160;顺着上面的步骤，查看acquireBufferLocked函数。不过之前我们先看看参数中computeExpectedPresent函数计算下一次vsync信号到来的时间：123456789101112131415161718192021222324nsecs_t SurfaceFlingerConsumer::computeExpectedPresent(const DispSync&amp; dispSync)&#123; // The HWC doesn't currently have a way to report additional latency. // Assume that whatever we submit now will appear right after the flip. // For a smart panel this might be 1. This is expressed in frames, // rather than time, because we expect to have a constant frame delay // regardless of the refresh rate. const uint32_t hwcLatency = 0; // Ask DispSync when the next refresh will be (CLOCK_MONOTONIC). //调用DispSync的dispSync.computeNextRefresh计算下一次刷新时间 const nsecs_t nextRefresh = dispSync.computeNextRefresh(hwcLatency); // The DispSync time is already adjusted for the difference between // vsync and reported-vsync (PRESENT_TIME_OFFSET_FROM_VSYNC_NS), so // we don't need to factor that in here. Pad a little to avoid // weird effects if apps might be requesting times right on the edge. nsecs_t extraPadding = 0; if (VSYNC_EVENT_PHASE_OFFSET_NS == 0) &#123; extraPadding = 1000000; // 1ms (6% of 60Hz) &#125; return nextRefresh + extraPadding;&#125; &#160; &#160; &#160; &#160;计算下一次vsync_sf时间，位于frameworks/native/services/surfaceflinger/DispSync.cpp中：12345nsecs_t DispSync::computeNextRefresh(int periodOffset) const &#123; Mutex::Autolock lock(mMutex); nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); return (((now - mPhase) / mPeriod) + periodOffset + 1) * mPeriod + mPhase;&#125; &#160; &#160; &#160; &#160;就是一个屏幕刷新周期之后的时间。 &#160; &#160; &#160; &#160;获得了这个实参，然后就查看获取buffer的函数acquireBufferLocked：12345678status_t SurfaceFlingerConsumer::acquireBufferLocked( BufferQueue::BufferItem *item, nsecs_t presentWhen) &#123; status_t result = GLConsumer::acquireBufferLocked(item, presentWhen); if (result == NO_ERROR) &#123; mTransformToDisplayInverse = item-&gt;mTransformToDisplayInverse; &#125; return result;&#125; &#160; &#160; &#160; &#160;这里调用了GLConsumer的acquireBufferLocked函数，位于frameworks/native/libs/gui/GLConsumer.cpp：123456789101112131415161718192021status_t GLConsumer::acquireBufferLocked(BufferQueue::BufferItem *item, nsecs_t presentWhen) &#123; //调用ConsumerBase的acquireBufferLocked函数 status_t err = ConsumerBase::acquireBufferLocked(item, presentWhen); if (err != NO_ERROR) &#123; return err; &#125; // If item-&gt;mGraphicBuffer is not null, this buffer has not been acquired // before, so any prior EglImage created is using a stale buffer. This // replaces any old EglImage with a new one (using the new buffer). //如果这个buffer已经被acquired，它的图形缓冲区buffer不为空， //那么一些以前的EglImage就会使用过时的buffer //因此创建GLConsumer里新的EGLImage if (item-&gt;mGraphicBuffer != NULL) &#123; int slot = item-&gt;mBuf; mEglSlots[slot].mEglImage = new EglImage(item-&gt;mGraphicBuffer); &#125; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;这里又调用ConsumerBase的acquireBufferLocked函数去获取buffer。之后，如果这个buffer已经被acquired，它的图形缓冲区buffer不为空，那么一些以前的EglImage就会使用过时的buffer，因此创建GLConsumer里新的EGLImage。&#160; &#160; &#160; &#160;我们继续查看ConsumerBase的acquireBufferLocked函数，位于rameworks/native/libs/gui/ConsumerBase.cpp：1234567891011121314151617181920status_t ConsumerBase::acquireBufferLocked(BufferQueue::BufferItem *item, nsecs_t presentWhen) &#123; //又通过binder IPC调用了BufferQueueConsumer的acquireBuffer函数 status_t err = mConsumer-&gt;acquireBuffer(item, presentWhen); if (err != NO_ERROR) &#123; return err; &#125; //获取到buffer之后，然后赋值给对应mSlots数组中index的BufferSlot if (item-&gt;mGraphicBuffer != NULL) &#123; mSlots[item-&gt;mBuf].mGraphicBuffer = item-&gt;mGraphicBuffer; &#125; mSlots[item-&gt;mBuf].mFrameNumber = item-&gt;mFrameNumber; mSlots[item-&gt;mBuf].mFence = item-&gt;mFence; CB_LOGV("acquireBufferLocked: -&gt; slot=%d/%" PRIu64, item-&gt;mBuf, item-&gt;mFrameNumber); return OK;&#125; &#160; &#160; &#160; &#160;这里又通过binder IPC调用了BufferQueueConsumer的acquireBuffer函数，位于frameworks/native/libs/gui/BufferQueueConsumer.cpp:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163status_t BufferQueueConsumer::acquireBuffer(BufferItem* outBuffer, nsecs_t expectedPresent) &#123; ATRACE_CALL(); Mutex::Autolock lock(mCore-&gt;mMutex); // Check that the consumer doesn't currently have the maximum number of // buffers acquired. We allow the max buffer count to be exceeded by one // buffer so that the consumer can successfully set up the newly acquired // buffer before releasing the old one. //统计mActiveBuffers中已经是 ACQUIRED 的Frame 个数 int numAcquiredBuffers = 0; for (int s = 0; s &lt; BufferQueueDefs::NUM_BUFFER_SLOTS; ++s) &#123; //是否已经是 ACQUIRED 的Buffer if (mSlots[s].mBufferState == BufferSlot::ACQUIRED) &#123; ++numAcquiredBuffers; &#125; &#125; //异常检测 if (numAcquiredBuffers &gt;= mCore-&gt;mMaxAcquiredBufferCount + 1) &#123; BQ_LOGE("acquireBuffer: max acquired buffer count reached: %d (max %d)", numAcquiredBuffers, mCore-&gt;mMaxAcquiredBufferCount); return INVALID_OPERATION; &#125; // Check if the queue is empty. // In asynchronous mode the list is guaranteed to be one buffer deep, // while in synchronous mode we use the oldest buffer. if (mCore-&gt;mQueue.empty()) &#123; return NO_BUFFER_AVAILABLE; &#125; //queue中的第一个 buffer BufferQueueCore::Fifo::iterator front(mCore-&gt;mQueue.begin()); // If expectedPresent is specified, we may not want to return a buffer yet. // If it's specified and there's more than one buffer queued, we may want // to drop a buffer. if (expectedPresent != 0) &#123; //一秒作为选择显示的buffer的范围条件 const int MAX_REASONABLE_NSEC = 1000000000ULL; // 1 second // The 'expectedPresent' argument indicates when the buffer is expected // to be presented on-screen. If the buffer's desired present time is // earlier (less) than expectedPresent -- meaning it will be displayed // on time or possibly late if we show it as soon as possible -- we // acquire and return it. If we don't want to display it until after the // expectedPresent time, we return PRESENT_LATER without acquiring it. //expectedPresent参数表示的是buffer被我们期望的显示时间。 //如果buffer自己的渴望显示时间早于被期望的时间，意味着它可以按自己渴望时间显示， //但是也会迟到，比如我们获取了它去迅速返回了，这时候它显示就要迟到了， //此时我们就要返回一个PRESENT_LATER标志让它在下一次vsync信号去处理。 // // To be safe, we don't defer acquisition if expectedPresent is more // than one second in the future beyond the desired present time // (i.e., we'd be holding the buffer for a long time). //安全起见，我们就不推迟采集被期望的时间大于buffer渴望显示时间超过1秒的buffer了， //例如，我们长时间持有一个buffer不释放，就会让expectedPresent很大 // // NOTE: Code assumes monotonic time values from the system clock // are positive. // Start by checking to see if we can drop frames. We skip this check if // the timestamps are being auto-generated by Surface. If the app isn't // generating timestamps explicitly, it probably doesn't want frames to // be discarded based on them. // 检查后面的 Queue, 找到最近需要显示的Buffer while (mCore-&gt;mQueue.size() &gt; 1 &amp;&amp; !mCore-&gt;mQueue[0].mIsAutoTimestamp) &#123; // If entry[1] is timely, drop entry[0] (and repeat). We apply an // additional criterion here: we only drop the earlier buffer if our // desiredPresent falls within +/- 1 second of the expected present. // Otherwise, bogus desiredPresent times (e.g., 0 or a small // relative timestamp), which normally mean "ignore the timestamp // and acquire immediately", would cause us to drop frames. //如果entry[1]是及时的，就出drop掉entry[0]的buffer，然后继续往后面选。 //我们额外应用一个准则在这里：如果entry[1]的buffer的 desired time落在期望显示时间误差1秒内，那么我们drop掉entry[0] // // We may want to add an additional criterion: don't drop the // earlier buffer if entry[1]'s fence hasn't signaled yet. //还有一个准则：如果entry[1]的fence还没有到来，不drop entry[1]的buffer const BufferItem&amp; bufferItem(mCore-&gt;mQueue[1]); nsecs_t desiredPresent = bufferItem.mTimestamp; //如果这个buffer显示的时间在expected之后（garbage），或者将来的1s内会，那么不drop掉上一个. // break掉直接显示上一个buffer if (desiredPresent &lt; expectedPresent - MAX_REASONABLE_NSEC || desiredPresent &gt; expectedPresent) &#123; // This buffer is set to display in the near future, or // desiredPresent is garbage. Either way we don't want to drop // the previous buffer just to get this on the screen sooner. BQ_LOGV("acquireBuffer: nodrop desire=%" PRId64 " expect=%" PRId64 " (%" PRId64 ") now=%" PRId64, desiredPresent, expectedPresent, desiredPresent - expectedPresent, systemTime(CLOCK_MONOTONIC)); break; &#125; BQ_LOGV("acquireBuffer: drop desire=%" PRId64 " expect=%" PRId64 " size=%zu", desiredPresent, expectedPresent, mCore-&gt;mQueue.size()); //如果上一个需要drop的buffer仍然在slots中，那么标记他为FREE状态 if (mCore-&gt;stillTracking(front)) &#123; // Front buffer is still in mSlots, so mark the slot as free mSlots[front-&gt;mSlot].mBufferState = BufferSlot::FREE; &#125; //上一个已经没有必要显示了, 重新找一个显示的 mCore-&gt;mQueue.erase(front); front = mCore-&gt;mQueue.begin(); &#125; // See if the front buffer is due buffer要显示的时间还没到，那么标记为PRESENT_LATER，下一帧处理 nsecs_t desiredPresent = front-&gt;mTimestamp; if (desiredPresent &gt; expectedPresent &amp;&amp; desiredPresent &lt; expectedPresent + MAX_REASONABLE_NSEC) &#123; BQ_LOGV("acquireBuffer: defer desire=%" PRId64 " expect=%" PRId64 " (%" PRId64 ") now=%" PRId64, desiredPresent, expectedPresent, desiredPresent - expectedPresent, systemTime(CLOCK_MONOTONIC)); return PRESENT_LATER; &#125; BQ_LOGV("acquireBuffer: accept desire=%" PRId64 " expect=%" PRId64 " " "(%" PRId64 ") now=%" PRId64, desiredPresent, expectedPresent, desiredPresent - expectedPresent, systemTime(CLOCK_MONOTONIC)); &#125; int slot = front-&gt;mSlot; *outBuffer = *front; ATRACE_BUFFER_INDEX(slot); BQ_LOGV("acquireBuffer: acquiring &#123; slot=%d/%" PRIu64 " buffer=%p &#125;", slot, front-&gt;mFrameNumber, front-&gt;mGraphicBuffer-&gt;handle); // If the front buffer is still being tracked, update its slot state //即将要显示的Buffer没有过期，就要更新状态变为 ACQUIRED if (mCore-&gt;stillTracking(front)) &#123; mSlots[slot].mAcquireCalled = true;//BUFFER的状态已经是 ACQUIRED 的了 mSlots[slot].mNeedsCleanupOnRelease = false; mSlots[slot].mBufferState = BufferSlot::ACQUIRED; mSlots[slot].mFence = Fence::NO_FENCE; &#125; // If the buffer has previously been acquired by the consumer, set // mGraphicBuffer to NULL to avoid unnecessarily remapping this buffer // on the consumer side //如果buffer之前已经被获取了，设置mGraphicBuffer变量为NULL，避免consumer不必要的二次映射 if (outBuffer-&gt;mAcquireCalled) &#123; outBuffer-&gt;mGraphicBuffer = NULL; &#125; //将Buffer从 BufferQueueCore列队里移除 mCore-&gt;mQueue.erase(front); // We might have freed a slot while dropping old buffers, or the producer // may be blocked waiting for the number of buffers in the queue to // decrease. //我们在第七节讲过，当dequeue buffer时候，slots有可能没有free的buffer可用， //因此会在producer那儿等待，所以我们这里释放一个slot时候就要去唤醒那儿的lock mCore-&gt;mDequeueCondition.broadcast(); ATRACE_INT(mCore-&gt;mConsumerName.string(), mCore-&gt;mQueue.size()); return NO_ERROR;&#125; &#160; &#160; &#160; &#160;这也就是我们Buffer迁移状态中ACQUIRED状态的流程，Acquire一个需要处理的Buffer。根据前面对Buffer状态迁移的分析，当消费者想处理一块buffer时，它首先要向BufferQueue做acquire申请。那么BufferQueue怎么知道当前要处理哪一个Buffer呢？这是因为其内部维护有一个Fifo先入先出队列。一旦有buffer被enqueue后，就会压入队尾;每次acquire就从队头取最前面的元素进行处理，完成之后就将其从队列移除。&#160; &#160; &#160; &#160;代码流程如下：&#160; &#160; &#160; &#160;1）统计mActiveBuffers中已经是 ACQUIRED 的Frame 个数，并且检测异常；&#160; &#160; &#160; &#160;2）queue中的第一个 buffer，一秒作为选择显示的buffer的范围条件。如果这个entry[1]显示的时间在expected之后（garbage），或者将来的1s内会，那么不drop掉上一个，直接break掉，显示entry[0]；&#160; &#160; &#160; &#160;3）如果entry[1]是及时的，就出drop掉entry[0]的buffer，然后继续往后面选；&#160; &#160; &#160; &#160;4）buffer要显示的时间还没到，那么标记为PRESENT_LATER，下一帧处理；&#160; &#160; &#160; &#160;5）更新buffer状态，并从队列中移除。 释放buffer并更新状态&#160; &#160; &#160; &#160;Acquire到的buffer封装在BufferItem中，item.mBuf代表它在BufferSlot中的序号。正常情况下item.mGraphicBuffer都不为空，我们将它记录到mEGLSlots[buf].mGraphicBuffer中，以便后续操作。&#160; &#160; &#160; &#160;消费者一旦处理完Buffer后，就可以将其release了。此后这个buffer就又恢复FREE状态，以供生产者再次dequeue使用。&#160; &#160; &#160; &#160;那么我们就看看这个释放过程，更新纹理中释放过程，updateAndReleaseLocked函数，实现位于GLConsumer中：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091status_t GLConsumer::updateAndReleaseLocked(const BufferQueue::BufferItem&amp; item)&#123; status_t err = NO_ERROR; //招呼slot int buf = item.mBuf; if (!mAttached) &#123;//Open GL ES 是否attach ST_LOGE("updateAndRelease: GLConsumer is not attached to an OpenGL " "ES context"); releaseBufferLocked(buf, mSlots[buf].mGraphicBuffer, mEglDisplay, EGL_NO_SYNC_KHR); return INVALID_OPERATION; &#125; // Confirm state. //检测EGL状态 err = checkAndUpdateEglStateLocked(); if (err != NO_ERROR) &#123; releaseBufferLocked(buf, mSlots[buf].mGraphicBuffer, mEglDisplay, EGL_NO_SYNC_KHR); return err; &#125; // Ensure we have a valid EglImageKHR for the slot, creating an EglImage // if nessessary, for the gralloc buffer currently in the slot in // ConsumerBase. // We may have to do this even when item.mGraphicBuffer == NULL (which // means the buffer was previously acquired). //创建 EGLImage ，此时 mEglImage 已经有了 //acquireBufferLocked中找到的即将要显示的 BufferItem 的 GraphicBuffer err = mEglSlots[buf].mEglImage-&gt;createIfNeeded(mEglDisplay, item.mCrop); if (err != NO_ERROR) &#123; ST_LOGW("updateAndRelease: unable to createImage on display=%p slot=%d", mEglDisplay, buf); releaseBufferLocked(buf, mSlots[buf].mGraphicBuffer, mEglDisplay, EGL_NO_SYNC_KHR); return UNKNOWN_ERROR; &#125; // Do whatever sync ops we need to do before releasing the old slot. // 在释放老的buffer前，先给添加一个release fence，有可能还在使用 err = syncForReleaseLocked(mEglDisplay); if (err != NO_ERROR) &#123; // Release the buffer we just acquired. It's not safe to // release the old buffer, so instead we just drop the new frame. // As we are still under lock since acquireBuffer, it is safe to // release by slot. releaseBufferLocked(buf, mSlots[buf].mGraphicBuffer, mEglDisplay, EGL_NO_SYNC_KHR); return err; &#125; ST_LOGV("updateAndRelease: (slot=%d buf=%p) -&gt; (slot=%d buf=%p)", mCurrentTexture, mCurrentTextureImage != NULL ? mCurrentTextureImage-&gt;graphicBufferHandle() : 0, buf, mSlots[buf].mGraphicBuffer-&gt;handle); // release old buffer // 先把老的buffer，release了 // 如果是第一次为mCurrentTexture为BufferQueue::INVALID_BUFFER_SLOT，-1 // 将release fence传递给BufferQueue中的slot对应的mSlots[slot]的mFence if (mCurrentTexture != BufferQueue::INVALID_BUFFER_SLOT) &#123; //释放buffer status_t status = releaseBufferLocked( mCurrentTexture, mCurrentTextureImage-&gt;graphicBuffer(), mEglDisplay, mEglSlots[mCurrentTexture].mEglFence); if (status &lt; NO_ERROR) &#123; ST_LOGE("updateAndRelease: failed to release buffer: %s (%d)", strerror(-status), status); err = status; // keep going, with error raised [?] &#125; &#125; // Update the GLConsumer state. // 更新 GLConsumer 状态 //更新这次acquire到的buffer到mCurrentTexture和mCurrentTextureImage mCurrentTexture = buf; mCurrentTextureImage = mEglSlots[buf].mEglImage; mCurrentCrop = item.mCrop; mCurrentTransform = item.mTransform; mCurrentScalingMode = item.mScalingMode; mCurrentTimestamp = item.mTimestamp; mCurrentFence = item.mFence; mCurrentFrameNumber = item.mFrameNumber; //对当前的buffer进行矩阵变换处理 computeCurrentTransformMatrixLocked(); return err;&#125; &#160; &#160; &#160; &#160;updateAndReleaseLocked这个函数释放了buffer，然后更新了mCurrentTexture mCurrentTextureImage等。步骤如下：&#160; &#160; &#160; &#160;1）检测Open GL是否连接，EGL状态是否OK；&#160; &#160; &#160; &#160;2）创建 EGLImage （此时 mEglImage 已经有了，acquireBufferLocked中找到的即将要显示的 BufferItem 的 GraphicBuffer）；&#160; &#160; &#160; &#160;3）释放老的buffer前，先给添加一个release fence，有可能还在使用（我们本届不讨论Fence相关内容，以后有机会会仔细研究）；&#160; &#160; &#160; &#160;4）释放buffer（这个接下来仔细分析）；&#160; &#160; &#160; &#160;5）更新 GLConsumer 状态，更新这次acquire到的buffer到mCurrentTexture和mCurrentTextureImage。 &#160; &#160; &#160; &#160;核心是释放buffer，因此我们看看releaseBufferLocked函数：123456789101112status_t GLConsumer::releaseBufferLocked(int buf, sp&lt;GraphicBuffer&gt; graphicBuffer, EGLDisplay display, EGLSyncKHR eglFence) &#123; // release the buffer if it hasn't already been discarded by the // BufferQueue. This can happen, for example, when the producer of this // buffer has reallocated the original buffer slot after this buffer // was acquired. status_t err = ConsumerBase::releaseBufferLocked( buf, graphicBuffer, display, eglFence); mEglSlots[buf].mEglFence = EGL_NO_SYNC_KHR; return err;&#125; &#160; &#160; &#160; &#160;这里又调用了ConsumerBase的releaseBufferLocked函数：1234567891011121314151617181920212223242526272829303132status_t ConsumerBase::releaseBufferLocked( int slot, const sp&lt;GraphicBuffer&gt; graphicBuffer, EGLDisplay display, EGLSyncKHR eglFence) &#123; // If consumer no longer tracks this graphicBuffer (we received a new // buffer on the same slot), the buffer producer is definitely no longer // tracking it. if (!stillTracking(slot, graphicBuffer)) &#123; return OK; &#125; CB_LOGV("releaseBufferLocked: slot=%d/%" PRIu64, slot, mSlots[slot].mFrameNumber); //release 老buffer的时候，会传入一个mSlots[slot].mFence,即release fence ／／调用BufferQueueConsumer的releaseBuffer函数 status_t err = mConsumer-&gt;releaseBuffer(slot, mSlots[slot].mFrameNumber, display, eglFence, mSlots[slot].mFence); //如果buffer已经过时了但还位于slot中，那么释放他 if (err == IGraphicBufferConsumer::STALE_BUFFER_SLOT) &#123; freeBufferLocked(slot); &#125; mSlots[slot].mFence = Fence::NO_FENCE; return err;&#125;void ConsumerBase::freeBufferLocked(int slotIndex) &#123; CB_LOGV("freeBufferLocked: slotIndex=%d", slotIndex); mSlots[slotIndex].mGraphicBuffer = 0; mSlots[slotIndex].mFence = Fence::NO_FENCE; mSlots[slotIndex].mFrameNumber = 0;&#125; &#160; &#160; &#160; &#160;继而又调用了BufferQueueConsumer的releaseBuffer函数：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364status_t BufferQueueConsumer::releaseBuffer(int slot, uint64_t frameNumber, const sp&lt;Fence&gt;&amp; releaseFence, EGLDisplay eglDisplay, EGLSyncKHR eglFence) &#123; ATRACE_CALL(); ATRACE_BUFFER_INDEX(slot); if (slot &lt; 0 || slot &gt;= BufferQueueDefs::NUM_BUFFER_SLOTS || releaseFence == NULL) &#123; return BAD_VALUE; &#125; sp&lt;IProducerListener&gt; listener; &#123; // Autolock scope Mutex::Autolock lock(mCore-&gt;mMutex); // If the frame number has changed because the buffer has been reallocated, // we can ignore this releaseBuffer for the old buffer //如果frameNumber改变了，意味着buffer被重新申请了， //所以我们要要标记这个老的buffer为STABLE，然后释放掉老的buffer if (frameNumber != mSlots[slot].mFrameNumber) &#123; return STALE_BUFFER_SLOT; &#125; // Make sure this buffer hasn't been queued while acquired by the consumer //当consumer去acquire buffer时候，确保这个buffer不在BufferQueue当中 BufferQueueCore::Fifo::iterator current(mCore-&gt;mQueue.begin()); while (current != mCore-&gt;mQueue.end()) &#123; if (current-&gt;mSlot == slot) &#123; BQ_LOGE("releaseBuffer: buffer slot %d pending release is " "currently queued", slot); return BAD_VALUE; &#125; ++current; &#125; //如果这个buffer被Acquired过了，那么释放他为FREE if (mSlots[slot].mBufferState == BufferSlot::ACQUIRED) &#123; mSlots[slot].mEglDisplay = eglDisplay; mSlots[slot].mEglFence = eglFence; mSlots[slot].mFence = releaseFence; mSlots[slot].mBufferState = BufferSlot::FREE; listener = mCore-&gt;mConnectedProducerListener; BQ_LOGV("releaseBuffer: releasing slot %d", slot); &#125; else if (mSlots[slot].mNeedsCleanupOnRelease) &#123;//如果还存于slot中，是旧的buffer，那么释放他 BQ_LOGV("releaseBuffer: releasing a stale buffer slot %d " "(state = %d)", slot, mSlots[slot].mBufferState); mSlots[slot].mNeedsCleanupOnRelease = false; return STALE_BUFFER_SLOT; &#125; else &#123;//异常 BQ_LOGV("releaseBuffer: attempted to release buffer slot %d " "but its state was %d", slot, mSlots[slot].mBufferState); return BAD_VALUE; &#125; //同上面逻辑，dequeue buffer时候可能会wait，这里唤醒wait锁 mCore-&gt;mDequeueCondition.broadcast(); &#125; // Autolock scope // Call back without lock held //回调onBufferReleased函数，调用ConsumerBase::onBuffersReleased if (listener != NULL) &#123; listener-&gt;onBufferReleased(); &#125; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;释放buffer的逻辑如下：&#160; &#160; &#160; &#160;1）异常检测。slot 的index是否合法，是否因为reallocate buffer导致frameNumber改变，当consumer去acquire buffer时候它是否在不在BufferQueue当中；&#160; &#160; &#160; &#160;2）释放buffer。如果这个buffer被Acquired过了，那么释放他为FREE；如果还存于slot中，是旧的buffer，那么释放他；异常或者状态异常；&#160; &#160; &#160; &#160;3）唤醒dequeue buffer时候的wait lock，回调ConsumerBase::onBuffersReleased，通知buffer释放。 &#160; &#160; &#160; &#160;以上就释放buffer的流程。 &#160; &#160; &#160; &#160;结合上一节内容，经过handleTransaction handlePageFlip两个函数处理，SurfaceFlinger中无论是Layer属性的变化还是图像的变化都处理好了，只等VSync信号到来就可以输出了。&#160; &#160; &#160; &#160;Layer更新中比较重要的就是Buffer状态的迁移，本节主要涉及了Buffer的ACQUIRED和RELEASED状态，这也正好完善了下图的剩下两个流程。 &#160; &#160; &#160; &#160;本节内容画一幅时序图如下： 小结&#160; &#160; &#160; &#160;本节主要处理Layer的更新逻辑，先计算layer的脏区域，再更新纹理。更新纹理又包含获取了buffer状态迁移的剩下两环：ACQUIRED、RELEADED。&#160; &#160; &#160; &#160;至此，处理Vsync信号到来时候的前两环已经处理完结了，即SurfaceFlinger中onMessageReceived回调函数的INVALIDATE CASE。下一节开始就是真正的合成阶段——REFRESH，这是个很复杂的流程，我们仍然会一一分析。 &#160; &#160; &#160; &#160;（不如意事常八九，可与语人无二三。）]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(九)----SurfaceFlinger事务处理]]></title>
    <url>%2F2017%2F10%2F24%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E4%B9%9D-SurfaceFlinger%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;荒废三个月，终于更新了~这期间事情巨多，一言难尽~不过对SurfaceFlinger的研究不能停啊，还是要一点一点钻研。&#160; &#160; &#160; &#160;这次继续顺着上次末尾，分析一下SurfaceFlinger事务处理。 事务概述&#160; &#160; &#160; &#160;Transaction是“事务”的意思。在我脑海中，关于事务的知识来自于数据库。在数据库操作中，事务意味着一次可以提交多个SQL语句，然后一个commit就可让它们集中执行，而且数据库中的事务还可以回滚，即恢复到事务提交前的状态。&#160; &#160; &#160; &#160;SurfaceFlinger为什么需要事务呢？从上面对数据库事务的描述来看，是不是意味着一次执行多个请求呢？我们从数据库事务概念大致可以看出一点端倪, 它的其中一个很重要的信息就是说”将一组相关操作组合成一个单元去处理“。这个思路也运用在了SurfaceFlinger中。我们先举个没有事务的例子，再一个Vsync周期里，Layer的属性改变了很多，比如有大小、位置、透明度、z-order都改变了，并且还不是一个Layer变了，如果每个都改变了，那么如果我们没改变一个属性就去触发相关操作，让SurfaceFlinger去重新合成，这样他岂不是累死了？所以正确的做法是，每当Layer属性改变，先记下来，将所有改变何如一条事务当中，最后在合成前handleTransaction 一次处理掉，这样就省事多了。 事务标志&#160; &#160; &#160; &#160;我们之前分析java层Surface创建的时候，使用过SurfaceControl这个类，它里面有两个方法，openTransaction和closeTransaction，这就是对Surface操作开事务的方法。不过我们这里先不分析这个，我们先看看C++层这一侧的事务。&#160; &#160; &#160; &#160;事务的标志有以下几种，位于frameworks/native/services/surfaceflinger/SurfaceFlinger.h中：123456enum &#123; eTransactionNeeded = 0x01,//Layer的属性发生变化了，表示需要处理事务 eTraversalNeeded = 0x02,//遍历的是SurfaceFlinger中所有的Layer eDisplayTransactionNeeded = 0x04,//这个是显示器相关的事务,如显示器hotplug eTransactionMask = 0x07//掩码&#125;; &#160; &#160; &#160; &#160;事务标志也可以是多种组合，也很好理解，十六进制位，每种标志占一位，掩码占全位。&#160; &#160; &#160; &#160;每种的意义在注释写的比较清楚。 设置和处理flags&#160; &#160; &#160; &#160;mTransactionFlags是surface flinger中的一个成员，注意在layer中也存在一个同名的mTransactionFlags，通过下面的函数设置该flag，位于frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp中：123456789uint32_t SurfaceFlinger::setTransactionFlags(uint32_t flags) &#123; //或返回的是mTransactionFlags旧的值 uint32_t old = android_atomic_or(flags, &amp;mTransactionFlags); //如果旧值和flags没有bit是相同的? if ((old &amp; flags)==0) &#123; // wake the server up,触发一次invalidate signalTransaction(); &#125; return old;&#125; &#160; &#160; &#160; &#160;在surfaceflinger中， createLayer()，removeLayer()，setClientStateLocked都会去设置事务flag为eTransactionNeeded； setClientStateLocked()会去设置eTraversalNeeded； createDisplay()，destroyDisplay()，onHotplugReceived()，setDisplayStateLocked()会去设置eDisplayTransactionNeeded。 &#160; &#160; &#160; &#160;然后就是调用SurfaceFlinger的signalTransaction，我们看看它：123void SurfaceFlinger::signalTransaction() &#123; mEventQueue.invalidate();&#125; &#160; &#160; &#160; &#160;这个我们之前分析过，调用MessageQueue的invalidate函数，位于frameworks/native/services/surfaceflinger/MessageQueue.cpp中：12345678910#define INVALIDATE_ON_VSYNC 1void MessageQueue::invalidate() &#123;// INVALIDATE_ON_VSYNC宏默认为true#if INVALIDATE_ON_VSYNC mEvents-&gt;requestNextVsync();#else mHandler-&gt;dispatchInvalidate();#endif&#125; &#160; &#160; &#160; &#160;mEvents就是EventThread类中的Connection类，第五篇分析vsync的时候介绍过，位于frameworks/native/services/surfaceflinger/EventThread.cpp中：123456789101112131415void EventThread::Connection::requestNextVsync() &#123; mEventThread-&gt;requestNextVsync(this);&#125;// 关于Connection中的count，当为0时为一次性的事件，即触发一次sync信号// count &gt;= 1 : continuous event. count is the vsync rate// count == 0 : one-shot event that has not fired// count ==-1 : one-shot event that fired this round / disabledvoid EventThread::requestNextVsync( const sp&lt;EventThread::Connection&gt;&amp; connection) &#123; Mutex::Autolock _l(mLock); if (connection-&gt;count &lt; 0) &#123; connection-&gt;count = 0; mCondition.broadcast(); &#125;&#125; &#160; &#160; &#160; &#160;这里将mCondition wait的地方释放，在Android SurfaceFlinger 学习之路(五)—-VSync 工作原理我们分析过，各种事务都会去触发一次vsync，前面文章分析过，在每次vsync信号到来时，处理这个vsync事件会去调用SurfaceFlinger的onMessageReceived函数，我们看看它的实现:1234567891011121314151617void SurfaceFlinger::onMessageReceived(int32_t what) &#123; ATRACE_CALL(); switch (what) &#123; case MessageQueue::TRANSACTION: handleMessageTransaction(); break; //我们走的是这个case case MessageQueue::INVALIDATE: handleMessageTransaction(); handleMessageInvalidate(); signalRefresh(); break; case MessageQueue::REFRESH: handleMessageRefresh(); break; &#125;&#125; &#160; &#160; &#160; &#160;我们走的是第二个case。这里先分析这个case第一个函数调用，后面的部分是SurfaceFlinger合成步骤，我们后续在分析。进而调用handleMessageTransaction，根据mTransactionFlags中设置的bit值做相应的处理。123456void SurfaceFlinger::handleMessageTransaction() &#123; uint32_t transactionFlags = peekTransactionFlags(eTransactionMask); if (transactionFlags) &#123; handleTransaction(transactionFlags); &#125;&#125; &#160; &#160; &#160; &#160;这样就回到我们今天的主题了，对事务的处理，我们接下来会逐步分析。 SurfaceFlinger处理事务&#160; &#160; &#160; &#160;我们首先要看SurfaceFlinger几个重要的变量： State mCurrentStateSurfaceFlinger下一帧的状态，即表示即将绘制的下一帧的状态。 State mDrawingState当前正在绘制的状态，即表示上一帧处理完事务后更新出来的状态，是最终的状态。 mVisibleRegionsDirty表示当前可见区域是否脏了，如果脏了的话，比如(layer added/removed, Display added/remove相关)在最后合成的时候会对每个屏幕重建layer stack, 但是一般都为false。 &#160; &#160; &#160; &#160;这两个状态是SurfaceFlinger中的State，在Layer当中也有一个State，我们后面会讲到，我们看看这个State的定义，位于frameworks/native/services/surfaceflinger/SurfaceFlinger.h：1234struct State &#123; LayerVector layersSortedByZ;//SurfaceFlinger中所有的按照Z order排序的 Layer DefaultKeyedVector&lt; wp&lt;IBinder&gt;, DisplayDeviceState&gt; displays;//外接的显示屏&#125;; &#160; &#160; &#160; &#160;前者是上一次“drawing”时的状态，而后者则是当前状态。这样我们只要通过对比这两个State，就知道系统中发生了什么变化，然后采取相应的措施。它们内部都包含了一个LayerVector类型的layersSortedByZ成员变量，从变量名可以看出是所有layers按照Z-order顺序排列的Vector。&#160; &#160; &#160; &#160;所以我们可以从mCurrentState.layersSortedByZ来访问到所有layer，然后对有需要执行transaction的图层再调用内部的doTransaction()。显然，并不是每个layer在每次handleTransactionLocked中都需要调用doTransaction，判断的标准就是Layer::getTransactionFlags返回的标志中是否要求了eTransactionNeeded。大家要注意SurfaceFlinger和各Layer都有一个mTransactionFlags变量，不过含义不同。另外，Layer中也同样有mCurrentState和mDrawingState，虽然它们也分别表示上一次和当前的状态，但所属的State结构体是完全不同的。 handleTransaction&#160; &#160; &#160; &#160;我们顺着上一节末位，处理vsync信号之后要去合成，之前先处理事务。因此走到了SurfaceFlinger的handleTransaction函数：123456789101112131415161718192021222324252627282930void SurfaceFlinger::handleTransaction(uint32_t transactionFlags)&#123; ATRACE_CALL(); // here we keep a copy of the drawing state (that is the state that's // going to be overwritten by handleTransactionLocked()) outside of // mStateLock so that the side-effects of the State assignment // don't happen with mStateLock held (which can cause deadlocks). //保存mDrawingState，但是却没有使用，are you kidding me？ State drawingState(mDrawingState); Mutex::Autolock _l(mStateLock); const nsecs_t now = systemTime(); mDebugInTransaction = now; // Here we're guaranteed that some transaction flags are set // so we can call handleTransactionLocked() unconditionally. // We call getTransactionFlags(), which will also clear the flags, // with mStateLock held to guarantee that mCurrentState won't change // until the transaction is committed. ////获得SurfaceFlinger中的Transaction标志位 transactionFlags = getTransactionFlags(eTransactionMask); //在这里处理Transaction handleTransactionLocked(transactionFlags); mLastTransactionTime = systemTime() - now; mDebugInTransaction = 0; invalidateHwcGeometry(); // here the transaction has been committed&#125; &#160; &#160; &#160; &#160;先获取了SurfaceFlinger的transactionFlags ，然后调用handleTransactionLocked处理这些事务。这个函数很长，我们需要分步查看。 Part.1 遍历所有的Layer, 让Layer去执行自己的事务12345678910111213141516171819202122232425262728void SurfaceFlinger::handleTransactionLocked(uint32_t transactionFlags)&#123; //获取当前所有Layer const LayerVector&amp; currentLayers(mCurrentState.layersSortedByZ); const size_t count = currentLayers.size(); /* * Traversal of the children * (perform the transaction for each of them if needed) */ //先判断有没有eTraversalNeeded标志位 if (transactionFlags &amp; eTraversalNeeded) &#123; //然后遍历所有Layer,让Layer去执行自己的事务 for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); //获取Layer的transaction flags uint32_t trFlags = layer-&gt;getTransactionFlags(eTransactionNeeded); if (!trFlags) continue; // Layer处理自己的事务 const uint32_t flags = layer-&gt;doTransaction(0); //如果Layer的可见区域改变了，则SurfaceFlinger就标注出当前可视区域改变了 if (flags &amp; Layer::eVisibleRegion) mVisibleRegionsDirty = true; &#125; &#125; ......&#125; &#160; &#160; &#160; &#160;先获取下一帧状态State所有的Layer,然后判断SF有没有eTraversalNeeded标志位，如果有，则遍历所有的Layer，让Layer去执行自己的事务，如果Layer的可见区域改变了，则SurfaceFlinger就标注出当前可视区域改变了，将mVisibleRegionsDirty 置为true。 Part.2 处理显示屏相关事务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183void SurfaceFlinger::handleTransactionLocked(uint32_t transactionFlags)&#123; ...Part.1... /* * Perform display own transactions if needed */ //如果有eDisplayTransactionNeeded标志位 if (transactionFlags &amp; eDisplayTransactionNeeded) &#123; // here we take advantage of Vector's copy-on-write semantics to // improve performance by skipping the transaction entirely when // know that the lists are identical //当我们知道上一次绘制状态和当前需要去绘制状态的显示屏信息相同时， //通过完全跳过事务，我们利用Vector的copy-on-write的特点去提高性能 //下一帧State的显示屏信息 const KeyedVector&lt; wp&lt;IBinder&gt;, DisplayDeviceState&gt;&amp; curr(mCurrentState.displays); //上一次绘制状态State的显示屏信息 const KeyedVector&lt; wp&lt;IBinder&gt;, DisplayDeviceState&gt;&amp; draw(mDrawingState.displays); //如果下一帧State的显示屏信息和上一次绘制状态State的显示屏信息不同 if (!curr.isIdenticalTo(draw)) &#123; //表示当前可见区域脏了 mVisibleRegionsDirty = true; //下一帧需要显示状态 const size_t cc = curr.size(); //上一帧绘制状态 size_t dc = draw.size(); // find the displays that were removed // (ie: in drawing state but not in current state) // also handle displays that changed // (ie: displays that are in both lists) //对比上一帧绘制状态，找到我们移除的显示屏 for (size_t i=0 ; i&lt;dc ; i++) &#123; const ssize_t j = curr.indexOfKey(draw.keyAt(i)); //移除显示屏的index if (j &lt; 0) &#123; // in drawing state but not in current state //移除的不是主显示屏 if (!draw[i].isMainDisplay()) &#123; // Call makeCurrent() on the primary display so we can // be sure that nothing associated with this display // is current. //主显示屏调用makeCurrent函数确保移除的显示屏和它没有联系了 const sp&lt;const DisplayDevice&gt; defaultDisplay(getDefaultDisplayDevice()); defaultDisplay-&gt;makeCurrent(mEGLDisplay, mEGLContext); sp&lt;DisplayDevice&gt; hw(getDisplayDevice(draw.keyAt(i))); //断开连接 if (hw != NULL) hw-&gt;disconnect(getHwComposer()); if (draw[i].type &lt; DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES) //并回调热插拔接口onHotplugReceived，用于触发下一次vsync信号相应事件 mEventThread-&gt;onHotplugReceived(draw[i].type, false); mDisplays.removeItem(draw.keyAt(i)); &#125; else &#123; ALOGW("trying to remove the main display"); &#125; &#125; else &#123; //如果这个显示屏没有被移除，看看它是否改变了 // this display is in both lists. see if something changed. const DisplayDeviceState&amp; state(curr[j]); const wp&lt;IBinder&gt;&amp; display(curr.keyAt(j)); if (state.surface-&gt;asBinder() != draw[i].surface-&gt;asBinder()) &#123; // changing the surface is like destroying and // recreating the DisplayDevice, so we just remove it // from the drawing state, so that it get re-added // below. //如果显示屏是销毁又重建的，所以没有被移除， //那么我们要移除并重新添加他 sp&lt;DisplayDevice&gt; hw(getDisplayDevice(display)); if (hw != NULL) hw-&gt;disconnect(getHwComposer()); mDisplays.removeItem(display); mDrawingState.displays.removeItemsAt(i); dc--; i--; // at this point we must loop to the next item continue; &#125; const sp&lt;DisplayDevice&gt; disp(getDisplayDevice(display)); //查看显示屏信息是否改变了，如果变了， //就要重新将drawing信息赋值为current if (disp != NULL) &#123; //z-order排布的layer层改变了 if (state.layerStack != draw[i].layerStack) &#123; disp-&gt;setLayerStack(state.layerStack); &#125; //方向、视角、帧结构改变了 if ((state.orientation != draw[i].orientation) || (state.viewport != draw[i].viewport) || (state.frame != draw[i].frame)) &#123; disp-&gt;setProjection(state.orientation, state.viewport, state.frame); &#125; //大小改变了 if (state.width != draw[i].width || state.height != draw[i].height) &#123; disp-&gt;setDisplaySize(state.width, state.height); &#125; &#125; &#125; &#125; // find displays that were added // (ie: in current state but not in drawing state) //找到我们新增加的显示屏信息 for (size_t i=0 ; i&lt;cc ; i++) &#123; if (draw.indexOfKey(curr.keyAt(i)) &lt; 0) &#123; const DisplayDeviceState&amp; state(curr[i]); //新增显示屏，需要一些变量属性： //FrameBufferSurface、BufferQueueProducer、 //BufferQueueConsumer等 sp&lt;DisplaySurface&gt; dispSurface; sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferProducer&gt; bqProducer; sp&lt;IGraphicBufferConsumer&gt; bqConsumer; BufferQueue::createBufferQueue(&amp;bqProducer, &amp;bqConsumer, new GraphicBufferAlloc()); int32_t hwcDisplayId = -1; //如果是虚拟设备 if (state.isVirtualDisplay()) &#123; // Virtual displays without a surface are dormant: // they have external state (layer stack, projection, // etc.) but no internal state (i.e. a DisplayDevice). if (state.surface != NULL) &#123; hwcDisplayId = allocateHwcDisplayId(state.type); sp&lt;VirtualDisplaySurface&gt; vds = new VirtualDisplaySurface( *mHwc, hwcDisplayId, state.surface, bqProducer, bqConsumer, state.displayName); dispSurface = vds; producer = vds; &#125; &#125; else &#123; //如果不是虚拟设备 ALOGE_IF(state.surface!=NULL, "adding a supported display, but rendering " "surface is provided (%p), ignoring it", state.surface.get()); //申请设备id hwcDisplayId = allocateHwcDisplayId(state.type); // for supported (by hwc) displays we provide our // own rendering surface //创建FrameBufferSurface用于渲染显示 dispSurface = new FramebufferSurface(*mHwc, state.type, bqConsumer); producer = bqProducer; &#125; const wp&lt;IBinder&gt;&amp; display(curr.keyAt(i)); //设置显示屏相关属性 if (dispSurface != NULL) &#123; //创建显示屏信息 sp&lt;DisplayDevice&gt; hw = new DisplayDevice(this, state.type, hwcDisplayId, mHwc-&gt;getFormat(hwcDisplayId), state.isSecure, display, dispSurface, producer, mRenderEngine-&gt;getEGLConfig()); hw-&gt;setLayerStack(state.layerStack); hw-&gt;setProjection(state.orientation, state.viewport, state.frame); hw-&gt;setDisplayName(state.displayName); mDisplays.add(display, hw); if (state.isVirtualDisplay()) &#123; if (hwcDisplayId &gt;= 0) &#123; mHwc-&gt;setVirtualDisplayProperties(hwcDisplayId, hw-&gt;getWidth(), hw-&gt;getHeight(), hw-&gt;getFormat()); &#125; &#125; else &#123; mEventThread-&gt;onHotplugReceived(state.type, true); &#125; &#125; &#125; &#125; &#125; &#125; ......&#125; &#160; &#160; &#160; &#160;代码虽然多，但是逻辑很清晰，做的事情主要如下：&#160; &#160; &#160; &#160;1）对比上一帧绘制状态，找到我们移除的显示屏。如果找到移除的就要断开连接，并且remove；如果没有移除，则判断是否为destroy-recreate的，如果是，则要移除重新add进来，如果不是，则看看是否状态发生改变，然后设置current状态；&#160; &#160; &#160; &#160;2）找到我们新增加的显示屏信息。新增显示屏，则要创建DisplayDevice描述对象，构建这个对象又需要FrameBufferSurface用于渲染显示，构建FrameBufferSurface又需要BufferQueue一套对象，因此一一创建。 Part.3 找到Layer所在显示屏并更新旋转方向12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970void SurfaceFlinger::handleTransactionLocked(uint32_t transactionFlags)&#123; ...Part.1... ...Part.2... if (transactionFlags &amp; (eTraversalNeeded|eDisplayTransactionNeeded)) &#123; // The transform hint might have changed for some layers // (either because a display has changed, or because a layer // as changed). // // Walk through all the layers in currentLayers, // and update their transform hint. // // If a layer is visible only on a single display, then that // display is used to calculate the hint, otherwise we use the // default display. // // NOTE: we do this here, rather than in rebuildLayerStacks() so that // the hint is set before we acquire a buffer from the surface texture. // // NOTE: layer transactions have taken place already, so we use their // drawing state. However, SurfaceFlinger's own transaction has not // happened yet, so we must use the current state layer list // (soon to become the drawing state list). // sp&lt;const DisplayDevice&gt; disp; uint32_t currentlayerStack = 0; for (size_t i=0; i&lt;count; i++) &#123; // NOTE: we rely on the fact that layers are sorted by // layerStack first (so we don't have to traverse the list // of displays for every layer). const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); uint32_t layerStack = layer-&gt;getDrawingState().layerStack; //通过遍历所有的Display来找到Layer所在的显示屏 if (i==0 || currentlayerStack != layerStack) &#123; currentlayerStack = layerStack; // figure out if this layerstack is mirrored // (more than one display) if so, pick the default display, // if not, pick the only display it's on. disp.clear(); for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); if (hw-&gt;getLayerStack() == currentlayerStack) &#123; if (disp == NULL) &#123; disp = hw; &#125; else &#123; disp = NULL; break; &#125; &#125; &#125; &#125; if (disp == NULL) &#123; // NOTE: TEMPORARY FIX ONLY. Real fix should cause layers to // redraw after transform hint changes. See bug 8508397. // could be null when this layer is using a layerStack // that is not visible on any display. Also can occur at // screen off/on times. disp = getDefaultDisplayDevice(); &#125; //更新Layer的旋转方向，最终会体现在 BufferQueueCore中的mTransformHint变量 layer-&gt;updateTransformHint(disp); &#125; &#125; ......&#125; &#160; &#160; &#160; &#160;Part3做的事情主要是找到Layer所在显示屏并更新旋转方向，也是layer的事务。 Part.4 处理Layer移除逻辑12345678910111213141516171819202122232425262728293031323334353637383940414243444546void SurfaceFlinger::handleTransactionLocked(uint32_t transactionFlags)&#123; ...Part.1... ...Part.2... ...Part.3... /* * Perform our own transaction if needed */ //如果需要合成的layer小于上一次绘制状态的layer个数， //则有Layer需要去移除 const LayerVector&amp; layers(mDrawingState.layersSortedByZ); if (currentLayers.size() &gt; layers.size()) &#123; // layers have been added mVisibleRegionsDirty = true; &#125; // some layers might have been removed, so // we need to update the regions they're exposing. //下面有layer移除的情况 if (mLayersRemoved) &#123; mLayersRemoved = false; mVisibleRegionsDirty = true; const size_t count = layers.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); //找到移除的layer if (currentLayers.indexOf(layer) &lt; 0) &#123; // this layer is not visible anymore // TODO: we could traverse the tree from front to back and // compute the actual visible region // TODO: we could cache the transformed region const Layer::State&amp; s(layer-&gt;getDrawingState()); //获得移除的Layer的可见区域, 这块可见区域就是dirty的 Region visibleReg = s.transform.transform( Region(Rect(s.active.w, s.active.h))); //找到被移除掉的Layer所在的Display, 然后更新Diplay的dirty 区域，也就是对region做或运算 invalidateLayerStack(s.layerStack, visibleReg); &#125; &#125; &#125; //提交事务 commitTransaction(); //更新显示屏中的光标 updateCursorAsync();&#125; &#160; &#160; &#160; &#160;这一段主要处理Layer的移除逻辑。如果需要合成的layer小于上一次绘制状态的layer个数，则有layer需要去移除。找到需要移除的Layer，获得可见区域，然后更新Display的dirty区域，也就是对region做或运算。 提交事务&#160; &#160; &#160; &#160;剩下就是提交事务和更新显示屏的光标了，我们看看提交事务。123456789101112131415161718192021222324void SurfaceFlinger::commitTransaction()&#123; //mLayersPendingRemoval是保存的是pending 着需要移除的Layer. //比如APP调用destroySurface if (!mLayersPendingRemoval.isEmpty()) &#123; // Notify removed layers now that they can't be drawn from for (size_t i = 0; i &lt; mLayersPendingRemoval.size(); i++) &#123; mLayersPendingRemoval[i]-&gt;onRemoved(); &#125; mLayersPendingRemoval.clear(); &#125; // If this transaction is part of a window animation then the next frame // we composite should be considered an animation as well. mAnimCompositionPending = mAnimTransactionPending; //更新 mDrawingState mDrawingState = mCurrentState; mTransactionPending = false; mAnimTransactionPending = false; //释放mTransactionCV, 如果SurfaceFlinger正在处理事务， //而这时如果调用setTransactionState就可能会一直等着mTransactionCV, //因为setTransactionState可能会改变SurfaceFlinger的Transaction标志位，导致前后不一致 mTransactionCV.broadcast();&#125; &#160; &#160; &#160; &#160;这就是提交事务的操作，先处理pendingRemove的layer，回调onRemove接口；然后更新mDrawingState状态；最后释放mTransactionCV条件锁。 &#160; &#160; &#160; &#160;以上就是SurfaceFlinger处理事务的流程，接下来我们分析Layer怎么处理事务。 Layer处理事务&#160; &#160; &#160; &#160;在handleTransactionLocked函数中的Part1中，我们我们简要分析了layer处理自己事务的流程，接下来我们详细分析这个过程。 Layer的重要变量&#160; &#160; &#160; &#160;我们在Android SurfaceFlinger 学习之路(六)—-SurfaceFlinger创建Surface中提到过，每一个Layer都有很多属性。 &#160; &#160; &#160; &#160;Each Layer has： Z-order Alpha value from 0 to 255 visibleRegion crop region transformation: rotate 0, 90, 180, 270: flip H, V: scale &#160; &#160; &#160; &#160;当多个Layer进行合成的时候，并不是整个Layer的空间都会被完全显示，根据这个Layer最终的显示效果，一个Layer可以被划分成很多的Region, Android SurfaceFlinger 定义了以下一些Region类型： TransparantRegion： 完全透明的区域，在它之下的区域将被显示出来。 OpaqueRegion: 完全不透明的区域，是否显示取决于它上面是否有遮挡或是否透明。 VisibleRegion: 可见区域，包括完全不透明无遮挡区域或半透明区域。 visibleRegion = Region - above OpaqueRegion. CoveredRegion: 被遮挡区域，在它之上，有不透明或半透明区域。 DirtyRegion: 可见部分改变区域，包括新的被遮挡区域，和新的露出区域。 &#160; &#160; &#160; &#160;我们这里先看看以下几个重要属性： State mCurrentState;表示Layer下一帧的属性状态，当某个属性变化时，直接操作该变量 State mDrawingState;表示当前正在绘制的帧的属性状态。Layer处理完事务后，最终的用于绘制的状态 &#160; &#160; &#160; &#160;我们再次看看Layer::State结构体的定义：123456789101112131415161718struct State &#123; Geometry active;//当前Layer的可见区域 Geometry requested;//请求的Layer的可见区域, 在Layer做doTransaction时会将 requested赋值给active. setSize/setMatrix/setPosition uint32_t z;//z-order uint32_t layerStack;//layerStack指明当前Layer属于哪个Display，Display的layer stack可以用 hw-&gt;getLayerStack获得 uint8_t alpha; uint8_t flags; uint8_t reserved[2]; ////当Layer的属性变化时， sequence就会加1 int32_t sequence; // changes when visible regions can change //更新region的传输矩阵 Transform transform; // the transparentRegion hint is a bit special, it's latched only // when we receive a buffer -- this is because it's "content" // dependent. Region activeTransparentRegion; Region requestedTransparentRegion;&#125;; doTransaction&#160; &#160; &#160; &#160;我们继续分析Layer处理事务的逻辑，看看Layer的doTransaction函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576uint32_t Layer::doTransaction(uint32_t flags) &#123;// 0 ATRACE_CALL(); //上一次绘制状态 const Layer::State&amp; s(getDrawingState()); //下一帧状态 const Layer::State&amp; c(getCurrentState()); //如果requested的可见区域与旧的可见区域不同了，则size changed const bool sizeChanged = (c.requested.w != s.requested.w) || (c.requested.h != s.requested.h); if (sizeChanged) &#123; // the size changed, we need to ask our client to request a new buffer //省略一些log信息 // record the new size, form this point on, when the client request // a buffer, it'll get the new size. // 如果size changed, 把新的 w|h 设置到 BufferQueueCore中的 mDefaultWidth|mDefaultHeight中去 mSurfaceFlingerConsumer-&gt;setDefaultBufferSize( c.requested.w, c.requested.h); &#125; //如果新的请求的 w|h 与新的原本要显示的区域不同，表明是 resize了 if (!isFixedSize()) &#123; const bool resizePending = (c.requested.w != c.active.w) || (c.requested.h != c.active.h); if (resizePending) &#123; //resize只发生在非固定模式下 // don't let Layer::doTransaction update the drawing state // if we have a pending resize, unless we are in fixed-size mode. // the drawing state will be updated only once we receive a buffer // with the correct size. // // in particular, we want to make sure the clip (which is part // of the geometry state) is latched together with the size but is // latched immediately when no resizing is involved. //如果resize了，则打上eDontUpdateGeometryState的flag flags |= eDontUpdateGeometryState; &#125; &#125; // always set active to requested, unless we're asked not to // this is used by Layer, which special cases resizes. if (flags &amp; eDontUpdateGeometryState) &#123;//skip &#125; else &#123; //这里是允许更新 可见区域 Layer::State&amp; editCurrentState(getCurrentState()); editCurrentState.active = c.requested; &#125; //如果可见区域发生改变，刷新并重新计算可见区域 if (s.active != c.active) &#123; // invalidate and recompute the visible regions if needed flags |= Layer::eVisibleRegion; &#125; // 只要Layer有属性发生变化了，sequence就会加1, //这样可以很直观判断是否当前的state和旧的state是否发生变化了 // 但是这样并不能保证sequence相同，但是属性变化的这种情况 if (c.sequence != s.sequence) &#123; // invalidate and recompute the visible regions if needed flags |= eVisibleRegion; this-&gt;contentDirty = true; // we may use linear filtering, if the matrix scales us const uint8_t type = c.transform.getType(); mNeedsFiltering = (!c.transform.preserveRects() || (type &gt;= Transform::SCALE)); &#125; // Commit the transaction //提交事务 commitTransaction(); return flags;&#125; &#160; &#160; &#160; &#160;这个函数长度还行，看着也不负责，流程如下：&#160; &#160; &#160; &#160;1）如果requested的可见区域与旧的可见区域不同了，则size changed。如果size changed, 把新的 w|h 设置到 BufferQueueCore中的 mDefaultWidth|mDefaultHeight中去。&#160; &#160; &#160; &#160;这里mSurfaceFlingerConsumer-&gt;setDefaultBufferSize去设置变化后的大小，我们在Android SurfaceFlinger 学习之路(六)—-SurfaceFlinger创建Surface分析过mSurfaceFlingerConsumer的创建，位于Layer的onFirstRef函数中。&#160; &#160; &#160; &#160;2）如果可见区域发生改变，刷新并重新计算可见区域。&#160; &#160; &#160; &#160;3）sequence发生改变，重新获取大小。Sequence是个什么东西？当Layer的position，Zorder，alpha,matrix,transparent region,flags,crops.等发生变化的时候，sequence就会自增。也就是，当这些属性发生变化是，页面在Vsync信号触发的时候，根据sequence来判断是否需要属性页面。&#160; &#160; &#160; &#160;4）提交事务。主要就是同步2个state，将current state赋值给drawing state。看看代码如下：123void Layer::commitTransaction() &#123; mDrawingState = mCurrentState;&#125; &#160; &#160; &#160; &#160;以上就是SurfaceFlinger和Layer处理事务的逻辑。通俗的奖就是三句话： 新增layer，对比2个state中的layer队列，就可以知道新增的layer。 移除layer，也是比较2个layer队列，就可以找到移除的layer。 提交transaction，主要就是同步2个state。然后currentstate继续跟踪layer变化，如此往复。 &#160; &#160; &#160; &#160;贴个时序图就是下面的逻辑。 小结&#160; &#160; &#160; &#160;以上就是SurfaceFlinger处理事务的简要流程。回顾开始的内容，我们说过SurfaceControl的openTransation和closeTransaction底层实现也是SurfaceFlinger处理事务的操作，不过这个我们以后在分析。下一篇就应该去讲Layer的合成了，也是个十分复杂的过程。&#160; &#160; &#160; &#160;断更三个月，期间也是尝尽辛酸苦辣，痛苦和悔恨只能自己默默下咽。只希望以后能够少点折腾，多一些安逸。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(八)----Surface管理图形缓冲区]]></title>
    <url>%2F2017%2F07%2F03%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E5%85%AB-Surface%E7%AE%A1%E7%90%86%E5%9B%BE%E5%BD%A2%E7%BC%93%E5%86%B2%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;本节我们学习一下上层app创建Surface到底层Surface管理图形缓冲区的简要过程，详细的从Activity到WMS最后至底层SurfaceFlinger消费surface我们以后再分析。 上层概述Surface概述&#160; &#160; &#160; &#160;Surface也是个很复杂的系统，通过之前的学习，我们知道Surface相关的内容主要有两点： 应用程序和Surface的关系 Surface和SurfaceFlinger之间的关系 &#160; &#160; &#160; &#160;通过之前对SurfaceFlinger系统的概述，大概用如下图表示： &#160; &#160; &#160; &#160;(右图合成不止有OpenGL，还有HWC硬件，这个我们图中没有画出)&#160; &#160; &#160; &#160;先看左图。可以发现，不论是使用Skia绘制二维图像，还是用OpenGL绘制三维图像，最终Application都要和Surface交互。Surface就像是UI的画布，而App则像是在Surface上作画。&#160; &#160; &#160; &#160;再看右图。Surface和SurfaceFlinger的关系，Surface向SurfaceFlinger提供数据，而SurfaceFlinger则混合数据。其实就是调用OpenGL ES库中的EGL去合成（或者HWC硬件），然后在FrameBuffer上渲染。 简述Activity显示&#160; &#160; &#160; &#160;(这一部分我们以后会详细分析，这里因为后面分析surface需要用到，所以简单提一下) &#160; &#160; &#160; &#160;一般来说，应用程序的外表是通过Activity来展示的。那么，Activity是如何完成界面绘制工作的呢？根据前面所讲的知识，应用程序的显示和Surface有关，那么具体到Activity上，它和Surface又是什么关系呢？因为我们只是提一下，所以用两幅巨图来表示： &#160; &#160; &#160; &#160;Android 应用程序是怎么创建出来的，大概的流程是 ActivityManagerService -&gt; Zygote -&gt; Fork App,。一个新的应用被fork完后，第一个调用的方法就是 ActivityThread的main()，这个函数主要做的事情就是创建一个ActivityThread线程，然后调用loop()开始等待。当收到来自 ActivityManager 的 LAUNCH_ACTIVITY 消息后，Activity开始了他的显示之旅。下图描绘的是Activity在显示前的准备流程： &#160; &#160; &#160; &#160;我们可以说，一个应用可以有多个Activity，每个 Activity 一个Window(PhoneWindow)， 每个Window 有一个DecorView, 一个ViewRootImpl, 对应在WindowManagerService 里有一个Window(WindowState)。我们再看另一幅巨图： &#160; &#160; &#160; &#160;ViewRootImpl 在整个Android的GUI系统中占据非常重要的位置，如果把Activity和View 看作 ‘MVC’ 中的V, 把各种后台服务看作Modal，ViewRootImpl 则是’MVC’ 中的’C’ - Controller. Controller在MVC架构里承担着承上启下的作用，一般来说它的逻辑最为复杂。从下图可以看到，ViewRootImpl 与 用户输入系统(接收用户按键，触摸屏输入), 窗口系统（复杂窗口的布局，刷新，动画），显示合成系统（包括定时器Choreograph, SurfaceFlinger), 乃至Audio系统（音效输出）等均有密切的关联。 &#160; &#160; &#160; &#160;关于创建一Surface的简要时序图可以如下表示： &#160; &#160; &#160; &#160;应用层创建Surface上面时序图已经列出了，即：ViewRootImpl通过Session向WindowMangerService简历通信回话，然后请求WindowManagerService调用relayoutWindow方法，在方法内部通过WindowStateAnimator创建一个SurfaceControl，然后将ViewRootImpl中的无参构造的Surface通过调用copyFrom在JNI层创建一个Surface，并返回句柄给java层。（只是简述，后面章节在分析）相关方法如下：123456789101112131415161718192021222324252627282930313233343536/*WindowManagerService的relayoutWindow方法*/public int relayoutWindow(Session session, IWindow client, int seq, WindowManager.LayoutParams attrs, int requestedWidth, int requestedHeight, int viewVisibility, int flags, Rect outFrame, Rect outOverscanInsets, Rect outContentInsets, Rect outVisibleInsets, Rect outStableInsets, Configuration outConfig, Surface outSurface) &#123; //这个最后的参数outSurface就是ViewRootImpl的无参构造方法创建的对象 //private final Surface mSurface = new Surface(); ...... //调用WindowStateAnimator的createSurfaceLocked方法创建一个SurfaceControlSurfaceControl surfaceControl = winAnimator.createSurfaceLocked(); if (surfaceControl != null) &#123; //调用Surface的copyFrom方法，在JNI层构造一个Surface outSurface.copyFrom(surfaceControl); if (SHOW_TRANSACTIONS) Slog.i(TAG, " OUT SURFACE " + outSurface + ": copied"); &#125; else &#123; // For some reason there isn't a surface. Clear the // caller's object so they see the same state. outSurface.release(); &#125; ...... &#125;SurfaceControl createSurfaceLocked() &#123; ...省略其他步骤... //这个构造方法第一个参数SurfaceSession就是SurfaceComposerClient在java层的代表 mSurfaceControl = new SurfaceControl( mSession.mSurfaceSession, attrs.getTitle().toString(), width, height, format, flags); ...省略其他步骤... return mSurfaceControl;&#125; &#160; &#160; &#160; &#160;我们从Android SurfaceFlinger 学习之路(六)—-SurfaceFlinger创建Surface得知应用层创建Surface对应在SurfaceFlinger这边就是一个Layer，而这个Layer是由SurfaceFlinger创建的。所以我们上面创建java层Surface也需要拿到SurfaceFlinger服务的客户端代理才行。这个代理就是SurfaceSession类对象，mSession.mSurfaceSession。&#160; &#160; &#160; &#160;它的创建是构造ViewRootImpl时候，调用WindowManagerGlobal的getWindowSession方法，通过WMS的代理创建openSession创建一个Session回话。然后ViewRootImpl的setView方法，方法内通过WMS代理调用WMS的addWindow方法，里面利用Session对象调用windowAddedLocked方法创建一个SurfaceSession。这就是它的流程，我们仅仅简单描述。 &#160; &#160; &#160; &#160;上面就是Activity创建Surface的java层简要过程。&#160; &#160; &#160; &#160;关于绘制这个流程很复杂，我们后续章节在分析。这里我们因为要分析Surface机制，所以只分析ViewRootImpl的draw流程。（如果开启了硬件加速功能，则会使用hwui硬件绘制功能，这里我们忽略这个，使用默认的软件绘制流程drawSoftware）位于frameworks/base/core/java/android/view/ViewRootImpl.java：123456789101112131415161718192021222324private final Surface mSurface = new Surface();private boolean drawSoftware(Surface surface, AttachInfo attachInfo, int xoff, int yoff, boolean scalingRequired, Rect dirty) &#123; // Draw with software renderer. final Canvas canvas; try &#123; final int left = dirty.left; final int top = dirty.top; final int right = dirty.right; final int bottom = dirty.bottom; canvas = mSurface.lockCanvas(dirty); ...... mView.draw(canvas); ...... surface.unlockCanvasAndPost(canvas); &#125; return true;&#125; &#160; &#160; &#160; &#160;我们核心就是这三行代码，这里中间的mView.draw(canvas);这个是调用skia图形库的绘制流程。我们暂时不分析，因为我们不关注这个绘制流程。 &#160; &#160; &#160; &#160;我们主要分析canvas = mSurface.lockCanvas(dirty);和surface.unlockCanvasAndPost(canvas);的流程。这两个关系到我们下面的图形缓冲区的管理。 Surface的JNI调用&#160; &#160; &#160; &#160;我们分别分析Surface的构建还有lockCanvas还有unlockCanvasAndPost方法。 Surface的构造&#160; &#160; &#160; &#160;其实看到上面代码，让我们想起来java层构建Surface和在Android SurfaceFlinger 学习之路(六)—-SurfaceFlinger创建Surface分析的native层创建Surface流程基本一样，也是先创建一个SurfaceControl，然后利用SurfaceControl创建Surface。我们分部查看这个过程。 &#160; &#160; &#160; &#160;1）因为创建SurfaceControl，在native层需要SurfaceFlinger服务，所以我们看看java层如何获取SurfaceFlinger服务。所以我们看看上面讲到的创建SurfaceSession，位于frameworks/base/core/java/android/view/SurfaceSession.java：12345678// Note: This field is accessed by native code.private long mNativeClient; // SurfaceComposerClient*private static native long nativeCreate();/** Create a new connection with the surface flinger. */public SurfaceSession() &#123; mNativeClient = nativeCreate();&#125; &#160; &#160; &#160; &#160;对于JNI层位于frameworks/base/core/jni/android_view_SurfaceSession.cpp：123456static jlong nativeCreate(JNIEnv* env, jclass clazz) &#123; //SurfaceComposerClient专门用来和surfaceflinger建立connection（ISurfaceComposerClient） SurfaceComposerClient* client = new SurfaceComposerClient(); client-&gt;incStrong((void*)nativeCreate); return reinterpret_cast&lt;jlong&gt;(client);&#125; &#160; &#160; &#160; &#160;SurfaceComposerClient专门用来和surfaceflinger建立connection（ISurfaceComposerClient） ，SurfaceComposerClient是client，而surface flinger中的server为Client，SurfaceSession就是SurfaceComposerClient在java层的代表，其mNativeClient 成员就是native层的SurfaceComposerClient对象的指针。这一部分可以查看Android SurfaceFlinger 学习之路(四)—-SurfaceFlinger服务的启动与连接过程。 &#160; &#160; &#160; &#160;2）然后我们该看看java层SurfaceControl的创建了，位于frameworks/native/libs/gui/SurfaceControl.java：1234567891011121314public SurfaceControl(SurfaceSession session, String name, int w, int h, int format, int flags) throws OutOfResourcesException &#123; ...... mName = name; //调用nativeCreate本地方法 mNativeObject = nativeCreate(session, name, w, h, format, flags); ......&#125;private final String mName;long mNativeObject; // package visibility only for Surface.java accessprivate static native long nativeCreate(SurfaceSession session, String name, int w, int h, int format, int flags) throws OutOfResourcesException; &#160; &#160; &#160; &#160;nativeCreate方法对应于JNI层位于frameworks/base/core/jni/android_view_SurfaceControl.cpp：12345678910111213static jlong nativeCreate(JNIEnv* env, jclass clazz, jobject sessionObj, jstring nameStr, jint w, jint h, jint format, jint flags) &#123; ScopedUtfChars name(env, nameStr); sp&lt;SurfaceComposerClient&gt; client(android_view_SurfaceSession_getClient(env, sessionObj)); sp&lt;SurfaceControl&gt; surface = client-&gt;createSurface( String8(name.c_str()), w, h, format, flags); if (surface == NULL) &#123; jniThrowException(env, OutOfResourcesException, NULL); return 0; &#125; surface-&gt;incStrong((void *)nativeCreate); return reinterpret_cast&lt;jlong&gt;(surface.get());&#125; &#160; &#160; &#160; &#160;这里在JNI层调用上面创建的SurfaceComposerClient对象的createSurface函数，请求SurfaceFlinger去创建一个Layer，然后将这些信息封装成一个SurfaceControl。这一部分我们之前讲过，可以参考Android SurfaceFlinger 学习之路(六)—-SurfaceFlinger创建Surface。 &#160; &#160; &#160; &#160;3）最后就是Surface的copyFrom方法，从SurfaceControl获取Surface信息。位于frameworks/base/core/java/android/view/Surface.java：12345678910111213141516171819public void copyFrom(SurfaceControl other) &#123; if (other == null) &#123; throw new IllegalArgumentException("other must not be null"); &#125; long surfaceControlPtr = other.mNativeObject; if (surfaceControlPtr == 0) &#123; throw new NullPointerException( "SurfaceControl native object is null. Are you using a released SurfaceControl?"); &#125; long newNativeObject = nativeCreateFromSurfaceControl(surfaceControlPtr); synchronized (mLock) &#123; if (mNativeObject != 0) &#123; nativeRelease(mNativeObject); &#125; setNativeObjectLocked(newNativeObject); &#125;&#125; &#160; &#160; &#160; &#160;对应的JNI层调用位于frameworks/base/core/jni/android_view_Surface.cpp：12345678910111213141516static jlong nativeCreateFromSurfaceControl(JNIEnv* env, jclass clazz, jlong surfaceControlNativeObj) &#123; /* * This is used by the WindowManagerService just after constructing * a Surface and is necessary for returning the Surface reference to * the caller. At this point, we should only have a SurfaceControl. */ sp&lt;SurfaceControl&gt; ctrl(reinterpret_cast&lt;SurfaceControl *&gt;(surfaceControlNativeObj)); //调用SurfaceControl的getSurface函数 sp&lt;Surface&gt; surface(ctrl-&gt;getSurface()); if (surface != NULL) &#123; surface-&gt;incStrong(&amp;sRefBaseOwner); &#125; return reinterpret_cast&lt;jlong&gt;(surface.get());&#125; &#160; &#160; &#160; &#160;先从java层保存c++层SurfaceControl的long型句柄，获取C++层SurfaceControl，然后调用其getSurface函数创建一个Surface，最后将这个Surface的句柄通过JNI回传给java层保存。这个getSurface函数我们以前讲过，可以查看Android SurfaceFlinger 学习之路(六)—-SurfaceFlinger创建Surface。 &#160; &#160; &#160; &#160;至此我们的Surface的创建就完成了。 lockCanvas&#160; &#160; &#160; &#160;我们接着回到ViewRootImpl的drawSoftware方法，按步骤来，先卡看Surface的lockCanvas方法，位于frameworks/base/core/java/android/view/Surface.java：123456789101112131415161718//mCanvas 变量直接赋值private final Canvas mCanvas = new CompatibleCanvas();public Canvas lockCanvas(Rect inOutDirty) throws Surface.OutOfResourcesException, IllegalArgumentException &#123; synchronized (mLock) &#123; checkNotReleasedLocked(); if (mLockedObject != 0) &#123; // Ideally, nativeLockCanvas() would throw in this situation and prevent the // double-lock, but that won't happen if mNativeObject was updated. We can't // abandon the old mLockedObject because it might still be in use, so instead // we just refuse to re-lock the Surface. throw new IllegalArgumentException("Surface was already locked"); &#125; mLockedObject = nativeLockCanvas(mNativeObject, mCanvas, inOutDirty); return mCanvas; &#125;&#125; &#160; &#160; &#160; &#160;老路子，继续查看JNI代码，位于frameworks/base/core/jni/android_view_Surface.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081static jlong nativeLockCanvas(JNIEnv* env, jclass clazz, jlong nativeObject, jobject canvasObj, jobject dirtyRectObj) &#123; //获取java层的Surface保存的long型句柄 sp&lt;Surface&gt; surface(reinterpret_cast&lt;Surface *&gt;(nativeObject)); if (!isSurfaceValid(surface)) &#123; doThrowIAE(env); return 0; &#125; Rect dirtyRect; Rect* dirtyRectPtr = NULL; //获取java层dirty Rect的位置大小信息 if (dirtyRectObj) &#123; dirtyRect.left = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.left); dirtyRect.top = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.top); dirtyRect.right = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.right); dirtyRect.bottom = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.bottom); dirtyRectPtr = &amp;dirtyRect; &#125; ANativeWindow_Buffer outBuffer; //调用Surface的lock方法,将申请的图形缓冲区赋给outBuffer status_t err = surface-&gt;lock(&amp;outBuffer, dirtyRectPtr); if (err &lt; 0) &#123; const char* const exception = (err == NO_MEMORY) ? OutOfResourcesException : "java/lang/IllegalArgumentException"; jniThrowException(env, exception, NULL); return 0; &#125; // Associate a SkCanvas object to this surface //给java层的Canvas对象的mSurfaceFormat变量赋值。 //Java的Surface对象构造的时候会创建一个CompatibleCanvas env-&gt;SetIntField(canvasObj, gCanvasClassInfo.mSurfaceFormat, outBuffer.format); //创建一个SkImageInfo ，用于下面创建SkBitmap SkImageInfo info = SkImageInfo::Make(outBuffer.width, outBuffer.height, convertPixelFormat(outBuffer.format), kPremul_SkAlphaType); if (outBuffer.format == PIXEL_FORMAT_RGBX_8888) &#123; info.fAlphaType = kOpaque_SkAlphaType; &#125; SkBitmap bitmap;//创建一个SkBitmap //图形缓冲区每一行像素大小 ssize_t bpr = outBuffer.stride * bytesPerPixel(outBuffer.format); 然后填充SkBitmap相关变量 bitmap.setInfo(info, bpr); if (outBuffer.width &gt; 0 &amp;&amp; outBuffer.height &gt; 0) &#123; //outBuffer.bits指向一块存储区域 bitmap.setPixels(outBuffer.bits); &#125; else &#123; // be safe with an empty bitmap. bitmap.setPixels(NULL); &#125; //调用Java层Canvas的setNativeBitmap方法，保存JNI层构建的SkBitmap env-&gt;CallVoidMethod(canvasObj, gCanvasClassInfo.setNativeBitmap, reinterpret_cast&lt;jlong&gt;(&amp;bitmap)); if (dirtyRectPtr) &#123; //剪裁出dirty区域大小 SkCanvas* nativeCanvas = GraphicsJNI::getNativeCanvas(env, canvasObj); nativeCanvas-&gt;clipRect( SkRect::Make(reinterpret_cast&lt;const SkIRect&amp;&gt;(dirtyRect)) ); &#125; if (dirtyRectObj) &#123; //将剪裁位置大小信息赋给java层Canvas对象 env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.left, dirtyRect.left); env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.top, dirtyRect.top); env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.right, dirtyRect.right); env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.bottom, dirtyRect.bottom); &#125; // Create another reference to the surface and return it. This reference // should be passed to nativeUnlockCanvasAndPost in place of mNativeObject, // because the latter could be replaced while the surface is locked. sp&lt;Surface&gt; lockedSurface(surface); lockedSurface-&gt;incStrong(&amp;sRefBaseOwner); return (jlong) lockedSurface.get();&#125; &#160; &#160; &#160; &#160;这段代码逻辑主要如下：&#160; &#160; &#160; &#160;1）获取java层dirty 的Rect大小和位置信息；&#160; &#160; &#160; &#160;2）调用Surface的lock方法,将申请的图形缓冲区赋给outBuffer；&#160; &#160; &#160; &#160;3）创建一个Skbitmap，填充它用来保存申请的图形缓冲区，并赋值给Java层的Canvas对象；&#160; &#160; &#160; &#160;4）将剪裁位置大小信息赋给java层Canvas对象。 &#160; &#160; &#160; &#160;我们关注的核心就是在第二步申请图形缓冲区，这个我们下面会分析到。 unlockCanvasAndPost&#160; &#160; &#160; &#160;我们跳过draw过程，不分析这个，所以到了最后一步，Surface绘制完毕后，unlockCanvasAndPost操作。123456789101112131415161718192021222324public void unlockCanvasAndPost(Canvas canvas) &#123; if (canvas != mCanvas) &#123; throw new IllegalArgumentException("canvas object must be the same instance that " + "was previously returned by lockCanvas"); &#125; synchronized (mLock) &#123; checkNotReleasedLocked(); if (mNativeObject != mLockedObject) &#123; Log.w(TAG, "WARNING: Surface's mNativeObject (0x" + Long.toHexString(mNativeObject) + ") != mLockedObject (0x" + Long.toHexString(mLockedObject) +")"); &#125; if (mLockedObject == 0) &#123; throw new IllegalStateException("Surface was not locked"); &#125; try &#123; nativeUnlockCanvasAndPost(mLockedObject, canvas); &#125; finally &#123; nativeRelease(mLockedObject); mLockedObject = 0; &#125; &#125;&#125; &#160; &#160; &#160; &#160;依然查看JNI层代码，位于frameworks/base/core/jni/android_view_Surface.cpp：12345678910111213141516171819static void nativeUnlockCanvasAndPost(JNIEnv* env, jclass clazz, jlong nativeObject, jobject canvasObj) &#123; //获取java层的Surface保存的long型句柄 sp&lt;Surface&gt; surface(reinterpret_cast&lt;Surface *&gt;(nativeObject)); if (!isSurfaceValid(surface)) &#123; return; &#125; // detach the canvas from the surface //接触java层对native层SkBitmap的引用 env-&gt;CallVoidMethod(canvasObj, gCanvasClassInfo.setNativeBitmap, (jlong)0); // unlock surface //调用native层Surface的unlockAndPost函数 status_t err = surface-&gt;unlockAndPost(); if (err &lt; 0) &#123; doThrowIAE(env); &#125;&#125; &#160; &#160; &#160; &#160;这个代码依然很简单，核心就是调用native层Surface的unlockAndPost函数。我们接下来会分析。 &#160; &#160; &#160; &#160;综上这个流程大概是如下图： Surface管理图形缓冲区Surface申请图形缓冲区&#160; &#160; &#160; &#160;我们上边分析到了申请图形缓冲区，用到了Surface的lock函数，我们继续查看，位于frameworks/native/libs/gui/Surface.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115status_t Surface::lock( ANativeWindow_Buffer* outBuffer, ARect* inOutDirtyBounds)&#123; if (mLockedBuffer != 0) &#123;//还没有lock ALOGE("Surface::lock failed, already locked"); return INVALID_OPERATION; &#125; if (!mConnectedToCpu) &#123;//false //调用connect函数完成一些初始化 int err = Surface::connect(NATIVE_WINDOW_API_CPU); if (err) &#123; return err; &#125; // we're intending to do software rendering from this point //设置GRALLOC_USAGE_SW_READ_OFTEN 和GRALLOC_USAGE_SW_WRITE_OFTEN标志位， //表示打算使用软件渲染 setUsage(GRALLOC_USAGE_SW_READ_OFTEN | GRALLOC_USAGE_SW_WRITE_OFTEN); &#125; ANativeWindowBuffer* out; int fenceFd = -1; //调用dequeueBuffer函数，申请图形缓冲区 status_t err = dequeueBuffer(&amp;out, &amp;fenceFd); ALOGE_IF(err, "dequeueBuffer failed (%s)", strerror(-err)); if (err == NO_ERROR) &#123; //获取图形缓冲区区域大小,赋给后备缓冲区变量backBuffer sp&lt;GraphicBuffer&gt; backBuffer(GraphicBuffer::getSelf(out)); const Rect bounds(backBuffer-&gt;width, backBuffer-&gt;height); Region newDirtyRegion; if (inOutDirtyBounds) &#123; //如果上层指定乐刷新脏矩形区域，则用这个区域和缓冲区区域求交集， //然后将交集的结果设给需要去刷新的新区域 newDirtyRegion.set(static_cast&lt;Rect const&amp;&gt;(*inOutDirtyBounds)); //与运算，求交集 newDirtyRegion.andSelf(bounds); &#125; else &#123; //如果上层没有指定脏矩形区域，所以刷新整个图形缓冲区 newDirtyRegion.set(bounds); &#125; // figure out if we can copy the frontbuffer back //上一次绘制的信息保存在mPostedBuffer中，而这个mPostedBuffer则要在unLockAndPost函数中设置 const sp&lt;GraphicBuffer&gt;&amp; frontBuffer(mPostedBuffer); const bool canCopyBack = (frontBuffer != 0 &amp;&amp; backBuffer-&gt;width == frontBuffer-&gt;width &amp;&amp; backBuffer-&gt;height == frontBuffer-&gt;height &amp;&amp; backBuffer-&gt;format == frontBuffer-&gt;format); if (canCopyBack) &#123;//如果后备缓冲区大小和上次绘制缓冲区大小一样，则不用更新整个buffer // copy the area that is invalid and not repainted this round //将需要更新的dirty区域剪裁出来，剩余的不需要更新的区域我们不用重绘 //附注：mDirtyRegion变量命名在老版本Android是mOldDirtyRegion，指的是上一次绘制的dirty区域 const Region copyback(mDirtyRegion.subtract(newDirtyRegion)); if (!copyback.isEmpty()) //这里把mPostedBuffer中的旧数据拷贝到BackBuffer中。 //后续的绘画只要更新脏区域就可以了，这会节约不少资源 copyBlt(backBuffer, frontBuffer, copyback); &#125; else &#123; // if we can't copy-back anything, modify the user's dirty // region to make sure they redraw the whole buffer //如果两次图形缓冲区大小不一致，我们就要修改用户指定的dirty区域大小为整个缓冲区大小， //然后去更新整个缓冲区 newDirtyRegion.set(bounds); mDirtyRegion.clear(); Mutex::Autolock lock(mMutex); for (size_t i=0 ; i&lt;NUM_BUFFER_SLOTS ; i++) &#123; mSlots[i].dirtyRegion.clear(); &#125; &#125; &#123; // scope for the lock Mutex::Autolock lock(mMutex); //得到此次后备缓冲器的bufferSlot在mSlots里面的index int backBufferSlot(getSlotFromBufferLocked(backBuffer.get())); if (backBufferSlot &gt;= 0) &#123; //将新的dirty赋给这个bufferslot Region&amp; dirtyRegion(mSlots[backBufferSlot].dirtyRegion); mDirtyRegion.subtract(dirtyRegion); dirtyRegion = newDirtyRegion; &#125; &#125; //然后再加上新的dirty区域，或运算，相叠加 mDirtyRegion.orSelf(newDirtyRegion); if (inOutDirtyBounds) &#123; *inOutDirtyBounds = newDirtyRegion.getBounds(); &#125; void* vaddr; //lock和unlock分别用来锁定和解锁一个指定的图形缓冲区，在访问一块图形缓冲区的时候， //例如，向一块图形缓冲写入内容的时候，需要将该图形缓冲区锁定，用来避免访问冲突, //锁定之后，就可以获得由参数参数l、t、w和h所圈定的一块缓冲区的起始地址，保存在输出参数vaddr中 status_t res = backBuffer-&gt;lockAsync( GRALLOC_USAGE_SW_READ_OFTEN | GRALLOC_USAGE_SW_WRITE_OFTEN, newDirtyRegion.bounds(), &amp;vaddr, fenceFd); ALOGW_IF(res, "failed locking buffer (handle = %p)", backBuffer-&gt;handle); if (res != 0) &#123; err = INVALID_OPERATION; &#125; else &#123; //相关赋值 mLockedBuffer = backBuffer; outBuffer-&gt;width = backBuffer-&gt;width; outBuffer-&gt;height = backBuffer-&gt;height; outBuffer-&gt;stride = backBuffer-&gt;stride; outBuffer-&gt;format = backBuffer-&gt;format; outBuffer-&gt;bits = vaddr; &#125; &#125; return err;&#125; &#160; &#160; &#160; &#160;Surface的lock函数用来申请图形缓冲区和一些操作，方法不长，大概工作有：&#160; &#160; &#160; &#160;1）调用connect函数完成一些初始化；&#160; &#160; &#160; &#160;2）调用dequeueBuffer函数，申请图形缓冲区；&#160; &#160; &#160; &#160;3）计算需要绘制的新的dirty区域，旧的区域原样copy数据。&#160; &#160; &#160; &#160;在大部分情况下，UI只有一小部分会发生变化（例如一个按钮被按下去，导致颜色发生变化），这一小部分UI只对应整个GraphicBuffer中的一小块存储（就是在前面代码中见到的dirtyRegion），如果整块存储都更新，则会极大地浪费资源。怎么办？&#160; &#160; &#160; &#160;这就需要将变化的图像和没有发生变化的图像进行叠加。上一次绘制的信息保存在mPostedBuffer中，而这个mPostedBuffer则要在unLockAndPost函数中设置。这里将根据需要，把mPostedBuffer中的旧数据拷贝到BackBuffer中。后续的绘画只要更新脏区域就可以了，这会节约不少资源。&#160; &#160; &#160; &#160;4）lock和unlock分别用来锁定和解锁一个指定的图形缓冲区，在访问一块图形缓冲区的时候，例如，向一块图形缓冲写入内容的时候，需要将该图形缓冲区锁定，用来避免访问冲突,锁定之后，就可以获得由参数参数l、t、w和h所圈定的一块缓冲区的起始地址，保存在输出参数vaddr中。另一方面，在访问完成一块图形缓冲区之后，需要解除这块图形缓冲区的锁定。 &#160; &#160; &#160; &#160;其他步骤都已经在注释里说明了，我们主要看看那第二步，dequeueBuffer函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172int Surface::dequeueBuffer(android_native_buffer_t** buffer, int* fenceFd) &#123; ATRACE_CALL(); ALOGV("Surface::dequeueBuffer"); int reqW; int reqH; bool swapIntervalZero; uint32_t reqFormat; uint32_t reqUsage; &#123; Mutex::Autolock lock(mMutex); reqW = mReqWidth ? mReqWidth : mUserWidth; reqH = mReqHeight ? mReqHeight : mUserHeight; swapIntervalZero = mSwapIntervalZero; reqFormat = mReqFormat; reqUsage = mReqUsage; &#125; // Drop the lock so that we can still touch the Surface while blocking in IGBP::dequeueBuffer int buf = -1; sp&lt;Fence&gt; fence; //申请图形缓冲区，我们上一节分析过这个过程 status_t result = mGraphicBufferProducer-&gt;dequeueBuffer(&amp;buf, &amp;fence, swapIntervalZero, reqW, reqH, reqFormat, reqUsage); if (result &lt; 0) &#123; ALOGV("dequeueBuffer: IGraphicBufferProducer::dequeueBuffer(%d, %d, %d, %d, %d)" "failed: %d", swapIntervalZero, reqW, reqH, reqFormat, reqUsage, result); return result; &#125; Mutex::Autolock lock(mMutex); //根据index获取缓冲区 sp&lt;GraphicBuffer&gt;&amp; gbuf(mSlots[buf].buffer); // this should never happen ALOGE_IF(fence == NULL, "Surface::dequeueBuffer: received null Fence! buf=%d", buf); //上一篇分析过，如果所有状态buffer的个数大于64，就要释放所有超过64的buffer，并贴上RELEASE_ALL_BUFFERS标志位 if (result &amp; IGraphicBufferProducer::RELEASE_ALL_BUFFERS) &#123; freeAllBuffers(); &#125; //增加BUFFER_NEEDS_REALLOCATION标志，已重新分配GraphicBuffer if ((result &amp; IGraphicBufferProducer::BUFFER_NEEDS_REALLOCATION) || gbuf == 0) &#123; //由于申请的内存是在surfaceflinger进程中， //BufferQueue中的图形缓冲区也是通过匿名共享内存和binder传递描述符映射过去的， //Surface通过调用requestBuffer将图形缓冲区映射到Surface所在进程 result = mGraphicBufferProducer-&gt;requestBuffer(buf, &amp;gbuf); if (result != NO_ERROR) &#123; ALOGE("dequeueBuffer: IGraphicBufferProducer::requestBuffer failed: %d", result); mGraphicBufferProducer-&gt;cancelBuffer(buf, fence); return result; &#125; &#125; if (fence-&gt;isValid()) &#123; *fenceFd = fence-&gt;dup(); if (*fenceFd == -1) &#123; ALOGE("dequeueBuffer: error duping fence: %d", errno); // dup() should never fail; something is badly wrong. Soldier on // and hope for the best; the worst that should happen is some // visible corruption that lasts until the next frame. &#125; &#125; else &#123; *fenceFd = -1; &#125; //获取这个这个buffer对象的指针内容 *buffer = gbuf.get(); return OK;&#125; &#160; &#160; &#160; &#160;首先调用BufferQueueProducer的dequeueBuffer函数申请一块图形缓冲区，这个我们上一节讲过，Android SurfaceFlinger 学习之路(七)—-创建图形缓冲区GraphicBuffer。&#160; &#160; &#160; &#160;先在mSlot数组中查找FREE状态的slot，如果找到了就返回这个slot中的index。这个操作是调用了waitForFreeSlotThenRelock函数。如果没有找到，就要从匿名共享内存中重新分配。重新分配返回的flag会加上BUFFER_NEEDS_REALLOCATION标志位。&#160; &#160; &#160; &#160;如果是在匿名共享内存中重新分配的，就要根据index调用BufferQueueProducer的requestBuffer去获取mSlots数组中的bufferSlot，然后取出slot中的buffer。&#160; &#160; &#160; &#160;最后就是将取到的Buffer返回。 &#160; &#160; &#160; &#160;我们上一篇没有分析BufferQueueProducer的requestBuffer，也是通过IPC Binder调用远程接口，省去这个过程，直接看实现，位于frameworks/native/libs/gui/BufferQueueProducer.cpp：123456789101112131415161718192021222324status_t BufferQueueProducer::requestBuffer(int slot, sp&lt;GraphicBuffer&gt;* buf) &#123; ATRACE_CALL(); BQ_LOGV("requestBuffer: slot %d", slot); Mutex::Autolock lock(mCore-&gt;mMutex); if (mCore-&gt;mIsAbandoned) &#123; BQ_LOGE("requestBuffer: BufferQueue has been abandoned"); return NO_INIT; &#125; //check index and state if (slot &lt; 0 || slot &gt;= BufferQueueDefs::NUM_BUFFER_SLOTS) &#123; BQ_LOGE("requestBuffer: slot index %d out of range [0, %d)", slot, BufferQueueDefs::NUM_BUFFER_SLOTS); return BAD_VALUE; &#125; else if (mSlots[slot].mBufferState != BufferSlot::DEQUEUED) &#123; BQ_LOGE("requestBuffer: slot %d is not owned by the producer " "(state = %d)", slot, mSlots[slot].mBufferState); return BAD_VALUE; &#125; //将slot的mRequestBufferCalled 标志位置为true，并返回指定index的slot的Graphicbuffer mSlots[slot].mRequestBufferCalled = true; *buf = mSlots[slot].mGraphicBuffer; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;这个比较简单，还是很好理解的额，就是根据指定index取出mSlots中的slot中的buffer。 匿名共享内存 OR 帧缓冲区&#160; &#160; &#160; &#160;我们上边在注释中说了，申请的图形缓冲区是在匿名共享内存中申请的，不是直接在帧缓冲区。这个是有原因，我们下面分析一下。 &#160; &#160; &#160; &#160;之前在Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现中我们讲过： &#160; &#160; &#160; &#160;图形缓冲区可以从系统帧缓冲区分配也可以从内存中分配，分配一个图形缓冲区后还需要将该图形缓冲区映射到分配该buffer的进程地址空间来，在Android系统中，图形缓冲区的管理由SurfaceFlinger服务来负责。在系统帧缓冲区中分配的图形缓冲区是在SurfaceFlinger服务中使用，而在内存中分配的图形缓冲区既可以在SurfaceFlinger服务中使用，也可以在其它的应用程序中使用。当其它的应用程序需要使用图形缓冲区的时候，它们就会请求SurfaceFlinger服务为它们分配并将SurfaceFlinger服务返回来的图形缓冲区映射到应用程序进程地址空间。在从内存中分配buffer时，已经将分配的buffer映射到了SurfaceFlinger服务进程地址空间，如果该buffer是应用程序请求SurfaceFlinger服务为它们分配的，那么还需要将SurfaceFlinger服务返回来的图形缓冲区映射到应用程序进程地址空间。 &#160; &#160; &#160; &#160;我们这里是应用请求SurfaceFlinger去分配图形缓冲，在匿名共享内存当中。我们要去验证一下。&#160; &#160; &#160; &#160;首先在Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现中讲到，用户空间的应用程序用到的图形缓冲区是由Gralloc模块中的函数gralloc_alloc来分配的，这个函数实现在文件hardware/libhardware/modules/gralloc/gralloc.cpp中：123456789101112131415161718192021222324252627282930313233343536373839404142434445static int gralloc_alloc(alloc_device_t* dev, int w, int h, int format, int usage, buffer_handle_t* pHandle, int* pStride)&#123; if (!pHandle || !pStride) return -EINVAL; size_t size, stride; int align = 4; int bpp = 0; switch (format) &#123; case HAL_PIXEL_FORMAT_RGBA_8888: case HAL_PIXEL_FORMAT_RGBX_8888: case HAL_PIXEL_FORMAT_BGRA_8888: bpp = 4; break; case HAL_PIXEL_FORMAT_RGB_888: bpp = 3; break; case HAL_PIXEL_FORMAT_RGB_565: case HAL_PIXEL_FORMAT_RAW_SENSOR: bpp = 2; break; default: return -EINVAL; &#125; size_t bpr = (w*bpp + (align-1)) &amp; ~(align-1); size = bpr * h; stride = bpr / bpp; int err; if (usage &amp; GRALLOC_USAGE_HW_FB) &#123; err = gralloc_alloc_framebuffer(dev, size, usage, pHandle); &#125; else &#123; err = gralloc_alloc_buffer(dev, size, usage, pHandle); &#125; if (err &lt; 0) &#123; return err; &#125; *pStride = stride; return 0;&#125; &#160; &#160; &#160; &#160;参数usage用来描述要分配的图形缓冲区的用途。如果是用来在系统帧缓冲区中渲染的，即参数usage的GRALLOC_USAGE_HW_FB位等于1，那么就必须要系统帧缓冲区中分配，否则的话，就在内存中分配。注意，在内存中分配的图形缓冲区，最终是需要拷贝到系统帧缓冲区去的，以便可以将它所描述的图形渲染出来。 &#160; &#160; &#160; &#160;而这个usage是由上层app然后到SurfaceFlinger一系列过程增减标志位的，所以我们从下往上，看看标志是怎么设置的。&#160; &#160; &#160; &#160;同事我们回顾Android SurfaceFlinger 学习之路(七)—-创建图形缓冲区GraphicBuffer的内容。标志位设置在一下几个地方：&#160; &#160; &#160; &#160;1）创建Layer时候，在onFirstRef函数中：12345void Layer::onFirstRef() &#123; //这里getEffectiveUsage获取一些标志位 mSurfaceFlingerConsumer-&gt;setConsumerUsageBits(getEffectiveUsage(0));&#125; &#160; &#160; &#160; &#160;这里getEffectiveUsage获取一些标志位，我们继续查看：12345678910111213uint32_t Layer::getEffectiveUsage(uint32_t usage) const&#123; // TODO: should we do something special if mSecure is set? if (mProtectedByApp) &#123;//// application requires protected path to external sink // need a hardware-protected path to external video sink usage |= GraphicBuffer::USAGE_PROTECTED; &#125; if (mPotentialCursor) &#123;//// This layer can be a cursor on some displays. usage |= GraphicBuffer::USAGE_CURSOR; &#125; usage |= GraphicBuffer::USAGE_HW_COMPOSER; return usage;&#125; &#160; &#160; &#160; &#160;如果mProtectedByApp为true，则设置GraphicBuffer::USAGE_PROTECTED标志位，默认为false；如果mPotentialCursor为true，设置USAGE_CURSOR标志位，默认为false。这两个注释已经说明了。&#160; &#160; &#160; &#160;最后设置GraphicBuffer::USAGE_HW_COMPOSER，表示使用HWC硬件合成。 &#160; &#160; &#160; &#160;2）在上面Surface的lock方法中，设置了setUsage(GRALLOC_USAGE_SW_READ_OFTEN | GRALLOC_USAGE_SW_WRITE_OFTEN);we’re intending to do software rendering from this point，打算使用软件绘制。 &#160; &#160; &#160; &#160;3）Surface的lock方法中，锁定图形缓冲区的函数调用处：status_t res = backBuffer-&gt;lockAsync( GRALLOC_USAGE_SW_READ_OFTEN | GRALLOC_USAGE_SW_WRITE_OFTEN, newDirtyRegion.bounds(), &amp;vaddr, fenceFd);函数就会将参数usage的值修改为(GraphicBuffer::USAGE_SW_READ_OFTEN | GraphicBuffer::USAGE_SW_WRITE_OFTEN)，目的是防止该Surface的图形缓冲区直接在硬件帧缓冲区上分配。&#160; &#160; &#160; &#160;4）上层app在WindowStateAnimator的createSurfaceLocked构建java层SurfaceControl时也仅仅设置了几个变量：1234567891011121314151617181920212223242526272829SurfaceControl createSurfaceLocked() &#123; ...... int flags = SurfaceControl.HIDDEN; final WindowManager.LayoutParams attrs = w.mAttrs; if ((attrs.flags&amp;WindowManager.LayoutParams.FLAG_SECURE) != 0) &#123; flags |= SurfaceControl.SECURE; &#125; if (mService.isScreenCaptureDisabledLocked(UserHandle.getUserId(mWin.mOwnerUid))) &#123; flags |= SurfaceControl.SECURE; &#125; ...... if (!PixelFormat.formatHasAlpha(attrs.format) &amp;&amp; attrs.surfaceInsets.left == 0 &amp;&amp; attrs.surfaceInsets.top == 0 &amp;&amp; attrs.surfaceInsets.right == 0 &amp;&amp; attrs.surfaceInsets.bottom == 0) &#123; flags |= SurfaceControl.OPAQUE; &#125; ...... mSurfaceControl = new SurfaceControl( mSession.mSurfaceSession, attrs.getTitle().toString(), width, height, format, flags); ......&#125; &#160; &#160; &#160; &#160;也是简单的flag，影响不大。 &#160; &#160; &#160; &#160;我们找了这么多，也没有GRALLOC_USAGE_HW_FB标志位。我们再看看这些标志位的定义，位于hardware/libhardware/include/hardware/Gralloc.h：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768enum &#123; /* buffer is never read in software */ GRALLOC_USAGE_SW_READ_NEVER = 0x00000000, /* buffer is rarely read in software */ GRALLOC_USAGE_SW_READ_RARELY = 0x00000002, /* buffer is often read in software */ GRALLOC_USAGE_SW_READ_OFTEN = 0x00000003, /* mask for the software read values */ GRALLOC_USAGE_SW_READ_MASK = 0x0000000F, /* buffer is never written in software */ GRALLOC_USAGE_SW_WRITE_NEVER = 0x00000000, /* buffer is rarely written in software */ GRALLOC_USAGE_SW_WRITE_RARELY = 0x00000020, /* buffer is often written in software */ GRALLOC_USAGE_SW_WRITE_OFTEN = 0x00000030, /* mask for the software write values */ GRALLOC_USAGE_SW_WRITE_MASK = 0x000000F0, /* buffer will be used as an OpenGL ES texture */ GRALLOC_USAGE_HW_TEXTURE = 0x00000100, /* buffer will be used as an OpenGL ES render target */ GRALLOC_USAGE_HW_RENDER = 0x00000200, /* buffer will be used by the 2D hardware blitter */ GRALLOC_USAGE_HW_2D = 0x00000400, /* buffer will be used by the HWComposer HAL module */ GRALLOC_USAGE_HW_COMPOSER = 0x00000800, /* buffer will be used with the framebuffer device */ GRALLOC_USAGE_HW_FB = 0x00001000, /* buffer will be used with the HW video encoder */ GRALLOC_USAGE_HW_VIDEO_ENCODER = 0x00010000, /* buffer will be written by the HW camera pipeline */ GRALLOC_USAGE_HW_CAMERA_WRITE = 0x00020000, /* buffer will be read by the HW camera pipeline */ GRALLOC_USAGE_HW_CAMERA_READ = 0x00040000, /* buffer will be used as part of zero-shutter-lag queue */ GRALLOC_USAGE_HW_CAMERA_ZSL = 0x00060000, /* mask for the camera access values */ GRALLOC_USAGE_HW_CAMERA_MASK = 0x00060000, /* mask for the software usage bit-mask */ GRALLOC_USAGE_HW_MASK = 0x00071F00, /* buffer will be used as a RenderScript Allocation */ GRALLOC_USAGE_RENDERSCRIPT = 0x00100000, /* buffer should be displayed full-screen on an external display when * possible */ GRALLOC_USAGE_EXTERNAL_DISP = 0x00002000, /* Must have a hardware-protected path to external display sink for * this buffer. If a hardware-protected path is not available, then * either don't composite only this buffer (preferred) to the * external sink, or (less desirable) do not route the entire * composition to the external sink. */ GRALLOC_USAGE_PROTECTED = 0x00004000, /* buffer may be used as a cursor */ GRALLOC_USAGE_CURSOR = 0x00008000, /* implementation-specific private usage flags */ GRALLOC_USAGE_PRIVATE_0 = 0x10000000, GRALLOC_USAGE_PRIVATE_1 = 0x20000000, GRALLOC_USAGE_PRIVATE_2 = 0x40000000, GRALLOC_USAGE_PRIVATE_3 = 0x80000000, GRALLOC_USAGE_PRIVATE_MASK = 0xF0000000,&#125;; &#160; &#160; &#160; &#160;由于整个系统在硬件上就只有一个帧缓冲区，它是由SurfaceFlinger服务来统一管理的，即只有SurfaceFlinger服务使用的图形缓冲区才可以在上面分配，否则的话，随便一个应用程序进程都可以在上面分配图形缓冲区来使用，这个帧缓冲区的管理就乱套了。应用程序进程使用的图形缓冲区一般都是在匿名共享内存里面分配的，这个图形缓冲区填好数据之后，就会再交给SurfaceFlinger服务来合成到硬件帧缓冲区上去渲染。因此，从前面传过来给函数gralloc_alloc的参数usage的GRALLOC_USAGE_HW_FB位会被设置为0，以便可以在匿名共享内存中分配一个图形缓冲区。 图形缓冲区入队&#160; &#160; &#160; &#160;我们前面讲了，省略了第二步绘制流程，因此我们这里分析第三部，绘制完毕后再queueBuffer。 &#160; &#160; &#160; &#160;同样，调用了Surface的unlockCanvasAndPost函数，我们查看它的实现：123456789101112131415161718192021status_t Surface::unlockAndPost()&#123; if (mLockedBuffer == 0) &#123; ALOGE("Surface::unlockAndPost failed, no locked buffer"); return INVALID_OPERATION; &#125; int fd = -1; //解锁图形缓冲区，和前面的lockAsync成对出现 status_t err = mLockedBuffer-&gt;unlockAsync(&amp;fd); ALOGE_IF(err, "failed unlocking buffer (%p)", mLockedBuffer-&gt;handle); //queueBuffer去归还图形缓冲区 err = queueBuffer(mLockedBuffer.get(), fd); ALOGE_IF(err, "queueBuffer (handle=%p) failed (%s)", mLockedBuffer-&gt;handle, strerror(-err)); //将绘制buffer赋值给mPostedBuffer ，在lock函数里提到了 mPostedBuffer = mLockedBuffer; //最后将mLockedBuffer 置为0 mLockedBuffer = 0; return err;&#125; &#160; &#160; &#160; &#160;这里也比较简单，核心也是分两步：&#160; &#160; &#160; &#160;1）解锁图形缓冲区，和前面的lockAsync成对出现；&#160; &#160; &#160; &#160;2）queueBuffer去归还图形缓冲区； &#160; &#160; &#160; &#160;所以我们还是重点分析第二步，查看queueBuffer的实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152int Surface::queueBuffer(android_native_buffer_t* buffer, int fenceFd) &#123; ATRACE_CALL(); ALOGV("Surface::queueBuffer"); Mutex::Autolock lock(mMutex); int64_t timestamp; bool isAutoTimestamp = false; if (mTimestamp == NATIVE_WINDOW_TIMESTAMP_AUTO) &#123; timestamp = systemTime(SYSTEM_TIME_MONOTONIC); isAutoTimestamp = true; ALOGV("Surface::queueBuffer making up timestamp: %.2f ms", timestamp / 1000000.f); &#125; else &#123; timestamp = mTimestamp; &#125; //获取图形缓冲区的slot在mSlots中的index int i = getSlotFromBufferLocked(buffer); if (i &lt; 0) &#123; return i; &#125; // Make sure the crop rectangle is entirely inside the buffer. 确保剪裁的区域完全位于图形缓冲区内部 Rect crop; mCrop.intersect(Rect(buffer-&gt;width, buffer-&gt;height), &amp;crop); sp&lt;Fence&gt; fence(fenceFd &gt;= 0 ? new Fence(fenceFd) : Fence::NO_FENCE); //创建in、out两个栈对象，用于填充数据 IGraphicBufferProducer::QueueBufferOutput output; IGraphicBufferProducer::QueueBufferInput input(timestamp, isAutoTimestamp, crop, mScalingMode, mTransform ^ mStickyTransform, mSwapIntervalZero, fence, mStickyTransform); //调用BufferQueueProducer的queueBuffer归还缓冲区 status_t err = mGraphicBufferProducer-&gt;queueBuffer(i, input, &amp;output); if (err != OK) &#123; ALOGE("queueBuffer: error queuing buffer to SurfaceTexture, %d", err); &#125; uint32_t numPendingBuffers = 0; uint32_t hint = 0; //解析赋值给全局变量 output.deflate(&amp;mDefaultWidth, &amp;mDefaultHeight, &amp;hint, &amp;numPendingBuffers); // Disable transform hint if sticky transform is set. if (mStickyTransform == 0) &#123; mTransformHint = hint; &#125; mConsumerRunningBehind = (numPendingBuffers &gt;= 2); return err;&#125; &#160; &#160; &#160; &#160;这里创建了QueueBufferOutput、QueueBufferInput 两个栈对象用来传输数据，这两个我们在下面的附件内容会讲到，有一些关于结构体的知识。 &#160; &#160; &#160; &#160;上述的核心就是调用BufferQueueProducer的queueBuffer归还缓冲区，j将绘制后的图形缓冲区queue回去。我们依然省略IPC Binder过程，直接看实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110status_t BufferQueueProducer::queueBuffer(int slot, const QueueBufferInput &amp;input, QueueBufferOutput *output) &#123; ATRACE_CALL(); ATRACE_BUFFER_INDEX(slot); int64_t timestamp; bool isAutoTimestamp; Rect crop; int scalingMode; uint32_t transform; uint32_t stickyTransform; bool async; sp&lt;Fence&gt; fence; //从传入的QueueBufferInput ，解析填充一些变量 input.deflate(&amp;timestamp, &amp;isAutoTimestamp, &amp;crop, &amp;scalingMode, &amp;transform, &amp;async, &amp;fence, &amp;stickyTransform); ...... sp&lt;IConsumerListener&gt; listener; &#123; // Autolock scope Mutex::Autolock lock(mCore-&gt;mMutex); ...省略一些错误判断逻辑... //获取入队的GraphicBuffer的区域大小 const sp&lt;GraphicBuffer&gt;&amp; graphicBuffer(mSlots[slot].mGraphicBuffer); Rect bufferRect(graphicBuffer-&gt;getWidth(), graphicBuffer-&gt;getHeight()); Rect croppedRect; crop.intersect(bufferRect, &amp;croppedRect); if (croppedRect != crop) &#123; BQ_LOGE("queueBuffer: crop rect is not contained within the " "buffer in slot %d", slot); return BAD_VALUE; &#125; //改变入队的slot的状态为QUEUED mSlots[slot].mFence = fence; mSlots[slot].mBufferState = BufferSlot::QUEUED; //每次推进来，mFrameCounter都加1， //上一篇讲分配缓冲区返回最老的FREE状态buffer，就是用这个mFrameCounter最小值判断 ++mCore-&gt;mFrameCounter; mSlots[slot].mFrameNumber = mCore-&gt;mFrameCounter; //创建一个BufferItem来描述GraphicBuffer BufferItem item; item.mAcquireCalled = mSlots[slot].mAcquireCalled; item.mGraphicBuffer = mSlots[slot].mGraphicBuffer; item.mCrop = crop; item.mTransform = transform &amp; ~NATIVE_WINDOW_TRANSFORM_INVERSE_DISPLAY; item.mTransformToDisplayInverse = bool(transform &amp; NATIVE_WINDOW_TRANSFORM_INVERSE_DISPLAY); item.mScalingMode = scalingMode; item.mTimestamp = timestamp; item.mIsAutoTimestamp = isAutoTimestamp; item.mFrameNumber = mCore-&gt;mFrameCounter;//每次加1，最小的肯定是最早推进去的 item.mSlot = slot; item.mFence = fence; item.mIsDroppable = mCore-&gt;mDequeueBufferCannotBlock || async; mStickyTransform = stickyTransform; if (mCore-&gt;mQueue.empty()) &#123;//如果mQueue队列为空，则直接push进入这个mQueue，不用考虑阻塞 // When the queue is empty, we can ignore mDequeueBufferCannotBlock // and simply queue this buffer mCore-&gt;mQueue.push_back(item); listener = mCore-&gt;mConsumerListener; &#125; else &#123; // When the queue is not empty, we need to look at the front buffer // state to see if we need to replace it //如果这个mQueue队列不为空，我们要看看它的队头的状态，然后判断是否替换它 BufferQueueCore::Fifo::iterator front(mCore-&gt;mQueue.begin()); if (front-&gt;mIsDroppable) &#123;//如果队头可以丢弃 // If the front queued buffer is still being tracked, we first // mark it as freed //如果已经出队的buffer对应的slot依然保留在mSlots数组中，就要释放他，变为FREE状态 if (mCore-&gt;stillTracking(front)) &#123; mSlots[front-&gt;mSlot].mBufferState = BufferSlot::FREE; // Reset the frame number of the freed buffer so that it is // the first in line to be dequeued again mSlots[front-&gt;mSlot].mFrameNumber = 0; &#125; // Overwrite the droppable buffer with the incoming one //然后队头插入新入队的BufferItem *front = item; &#125; else &#123; //如果队头不能丢弃，则直接将新的Item入队 mCore-&gt;mQueue.push_back(item); //将BufferQueueCore的mConsumerListener赋给listener，这个我们上一节讲过 listener = mCore-&gt;mConsumerListener; &#125; &#125; mCore-&gt;mBufferHasBeenQueued = true;//入队后mBufferHasBeenQueued 置为true //上一篇讲了，没有空闲buffer去dequeue，则要等待，这里解除等待 mCore-&gt;mDequeueCondition.broadcast(); output-&gt;inflate(mCore-&gt;mDefaultWidth, mCore-&gt;mDefaultHeight, mCore-&gt;mTransformHint, mCore-&gt;mQueue.size()); ATRACE_INT(mCore-&gt;mConsumerName.string(), mCore-&gt;mQueue.size()); &#125; // Autolock scope // Call back without lock held //然后通知SurfaceFlinger去消费，这个我们上一篇讲过，下面会在分析 if (listener != NULL) &#123; listener-&gt;onFrameAvailable(); &#125; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;queueBuffer方法也挺长的，但是我们分部查看，每一步也不难：&#160; &#160; &#160; &#160;1）从传入的QueueBufferInput ，解析填充一些变量；&#160; &#160; &#160; &#160;2）改变入队Slot的状态为QUEUED，每次推进来，mFrameCounter都加1。这里的slot，上一篇讲分配缓冲区返回最老的FREE状态buffer，就是用这个mFrameCounter最小值判断，就是上一篇LRU算法的判断；&#160; &#160; &#160; &#160;3）创建一个BufferItem来描述GraphicBuffer，用mSlots[slot]中的slot填充BufferItem；&#160; &#160; &#160; &#160;4）将BufferItem塞进mCore的mQueue队列，依照指定规则；&#160; &#160; &#160; &#160;5）然后通知SurfaceFlinger去消费，这个我们上一篇讲过，这个我们下面会讲到。 &#160; &#160; &#160; &#160;上述lockCanvas和unlockCanvasAndPost可以用下图来总结一下： 通知SF消费合成&#160; &#160; &#160; &#160;上面讲到，当绘制完毕的GraphicBuffer入队之后，会通知SurfaceFlinger去消费，就是BufferQueueProducer的queueBuffer函数的最后几行，listener-&gt;onFrameAvailable()。&#160; &#160; &#160; &#160;这个listener赋值为mCore-&gt;mConsumerListener，我们上一篇Android SurfaceFlinger 学习之路(七)—-创建图形缓冲区GraphicBuffer在讲到Consumer时候，得知这个listener最终通过回调，会回到Layer当中，所以最终调用Layer的onFrameAvailable接口，我们看看它的实现：1234void Layer::onFrameAvailable() &#123; android_atomic_inc(&amp;mQueuedFrames); mFlinger-&gt;signalLayerUpdate();&#125; &#160; &#160; &#160; &#160;这里又调用SurfaceFlinger的signalLayerUpdate函数，继续查看：123void SurfaceFlinger::signalLayerUpdate() &#123; mEventQueue.invalidate();&#125; &#160; &#160; &#160; &#160;这里又调用MessageQueue的invalidate函数：1234567void MessageQueue::invalidate() &#123;#if INVALIDATE_ON_VSYNC //1 mEvents-&gt;requestNextVsync();#else mHandler-&gt;dispatchInvalidate();#endif&#125; &#160; &#160; &#160; &#160;到这里就比较熟悉了，就是们讲Vsync信号的时候内容了，可以查看Android SurfaceFlinger 学习之路(五)—-VSync 工作原理。&#160; &#160; &#160; &#160;其实上面和下面执行的逻辑都一样，最种结果都会走到SurfaceFlinger的vsync信号接收逻辑，即SurfaceFlinger的onMessageReceived函数，走的case是INVALIDATE：12345678910111213141516void SurfaceFlinger::onMessageReceived(int32_t what) &#123; ATRACE_CALL(); switch (what) &#123; case MessageQueue::TRANSACTION: handleMessageTransaction(); break; case MessageQueue::INVALIDATE: handleMessageTransaction(); handleMessageInvalidate(); signalRefresh(); break; case MessageQueue::REFRESH: handleMessageRefresh(); break; &#125;&#125; &#160; &#160; &#160; &#160;SurfaceFlinger就去消费这块图形缓冲，然后合成图像，送给FrameBuffer，显示设备读取FrameBuffer内容去显示。这个我们以后再讲。 附：结构体对齐&#160; &#160; &#160; &#160;我们上面讲Surface的queueBuffer时候，会创建QueueBufferOutput、QueueBufferInput 两个栈对象用来传输数据。他们定义位于frameworks/native/include/gui/IGraphicBufferProducer.h：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182struct QueueBufferInput : public Flattenable&lt;QueueBufferInput&gt; &#123; friend class Flattenable&lt;QueueBufferInput&gt;; inline QueueBufferInput(const Parcel&amp; parcel); // timestamp - a monotonically increasing value in nanoseconds // isAutoTimestamp - if the timestamp was synthesized at queue time // crop - a crop rectangle that's used as a hint to the consumer // scalingMode - a set of flags from NATIVE_WINDOW_SCALING_* in &lt;window.h&gt; // transform - a set of flags from NATIVE_WINDOW_TRANSFORM_* in &lt;window.h&gt; // async - if the buffer is queued in asynchronous mode // fence - a fence that the consumer must wait on before reading the buffer, // set this to Fence::NO_FENCE if the buffer is ready immediately // sticky - the sticky transform set in Surface (only used by the LEGACY // camera mode). inline QueueBufferInput(int64_t timestamp, bool isAutoTimestamp, const Rect&amp; crop, int scalingMode, uint32_t transform, bool async, const sp&lt;Fence&gt;&amp; fence, uint32_t sticky = 0) : timestamp(timestamp), isAutoTimestamp(isAutoTimestamp), crop(crop), scalingMode(scalingMode), transform(transform), stickyTransform(sticky), async(async), fence(fence) &#123; &#125; inline void deflate(int64_t* outTimestamp, bool* outIsAutoTimestamp, Rect* outCrop, int* outScalingMode, uint32_t* outTransform, bool* outAsync, sp&lt;Fence&gt;* outFence, uint32_t* outStickyTransform = NULL) const &#123; *outTimestamp = timestamp; *outIsAutoTimestamp = bool(isAutoTimestamp); *outCrop = crop; *outScalingMode = scalingMode; *outTransform = transform; *outAsync = bool(async); *outFence = fence; if (outStickyTransform != NULL) &#123; *outStickyTransform = stickyTransform; &#125; &#125; // Flattenable protocol size_t getFlattenedSize() const; size_t getFdCount() const; status_t flatten(void*&amp; buffer, size_t&amp; size, int*&amp; fds, size_t&amp; count) const; status_t unflatten(void const*&amp; buffer, size_t&amp; size, int const*&amp; fds, size_t&amp; count);private: int64_t timestamp; int isAutoTimestamp; Rect crop; int scalingMode; uint32_t transform; uint32_t stickyTransform; int async; sp&lt;Fence&gt; fence;&#125;;// QueueBufferOutput must be a POD structurestruct __attribute__ ((__packed__)) QueueBufferOutput &#123; inline QueueBufferOutput() &#123; &#125; // outWidth - filled with default width applied to the buffer // outHeight - filled with default height applied to the buffer // outTransformHint - filled with default transform applied to the buffer // outNumPendingBuffers - num buffers queued that haven't yet been acquired // (counting the currently queued buffer) inline void deflate(uint32_t* outWidth, uint32_t* outHeight, uint32_t* outTransformHint, uint32_t* outNumPendingBuffers) const &#123; *outWidth = width; *outHeight = height; *outTransformHint = transformHint; *outNumPendingBuffers = numPendingBuffers; &#125; inline void inflate(uint32_t inWidth, uint32_t inHeight, uint32_t inTransformHint, uint32_t inNumPendingBuffers) &#123; width = inWidth; height = inHeight; transformHint = inTransformHint; numPendingBuffers = inNumPendingBuffers; &#125;private: uint32_t width; uint32_t height; uint32_t transformHint; uint32_t numPendingBuffers;&#125;; &#160; &#160; &#160; &#160;这两个结构体，一个用来填充数据，一个用来解析数据。 &#160; &#160; &#160; &#160;我们先看看结构字节对齐。 结构体字节对齐&#160; &#160; &#160; &#160;在用sizeof运算符求算某结构体所占空间时，并不是简单地将结构体中所有元素各自占的空间相加，这里涉及到内存字节对齐的问题。从理论上讲，对于任何 变量的访问都可以从任何地址开始访问，但是事实上不是如此，实际上访问特定类型的变量只能在特定的地址访问，这就需要各个变量在空间上按一定的规则排列， 而不是简单地顺序排列，这就是内存对齐。 &#160; &#160; &#160; &#160;内存对齐的原因： 某些平台只能在特定的地址处访问特定类型的数据； 提高存取数据的速度。比如有的平台每次都是从偶地址处读取数据，对于一个int型的变量，若从偶地址单元处存放，则只需一个读取周期即可读取该变量；但是若从奇地址单元处存放，则需要2个读取周期读取该变量。 &#160; &#160; &#160; &#160;对齐策略： 结构体变量的首地址能够被其最宽数据类型成员的大小整除。编译器在为结构体变量开辟空间时，首先找到结构体中最宽的数据类型，然后寻找内存地址能被该数据类型大小整除的位置，这个位置作为结构体变量的首地址。而将最宽数据类型的大小作为对齐标准。 结构体每个成员相对结构体首地址的偏移量(offset)都是每个成员本身大小的整数倍，如有需要会在成员之间填充字节。编译器在为结构体成员开辟空 间时，首先检查预开辟空间的地址相对于结构体首地址的偏移量是否为该成员大小的整数倍，若是，则存放该成员；若不是，则填充若干字节，以达到整数倍的要求。 结构体变量所占空间的大小必定是最宽数据类型大小的整数倍。如有需要会在最后一个成员末尾填充若干字节使得所占空间大小是最宽数据类型大小的整数倍。 &#160; &#160; &#160; &#160;下面看一下sizeof在计算结构体大小的时候具体是怎样计算的：&#160; &#160; &#160; &#160;1）test1 空结构体12345typedef struct node&#123; &#125;S; &#160; &#160; &#160; &#160;则sizeof(S)=1;或sizeof(S)=0;&#160; &#160; &#160; &#160;在C++中占1字节，而在C中占0字节。 &#160; &#160; &#160; &#160;2）test2123456typedef struct node1&#123; int a; char b; short c;&#125;S1; &#160; &#160; &#160; &#160;则sizeof(S1)=8。这是因为结构体node1中最长的数据类型是int，占4个字节，因此以4字节对齐，则该结构体在内存中存放方式为:|--------int--------| 4字节|char|----|--short--| 4字节&#160; &#160; &#160; &#160;总共占8字节 &#160; &#160; &#160; &#160;3）test31234567typedef struct node2&#123; char a; int b; short c;&#125;S2; &#160; &#160; &#160; &#160;则siezof(S3)=12.最长数据类型为int，占4个字节。因此以4字节对齐，其在内存空间存放方式如下:|char|----|----|----| 4字节|--------int--------| 4字节|--short--|----|----| 4字节&#160; &#160; &#160; &#160;总共占12个字节 &#160; &#160; &#160; &#160;4）test4 含有静态数据成员123456typedef struct node3&#123; int a; short b; static int c;&#125;S3; &#160; &#160; &#160; &#160;则sizeof(S3)=8。这里结构体中包含静态数据成员，而静态数据成员的存放位置与结构体实例的存储地址无关(注意只有在C++中结构体中才能含有静态数据成员，而C中结构体中是不允许含有静态数据成员的)。其在内存中存储方式如下：|--------int--------| 4字节|--short--|----|----| 4字节&#160; &#160; &#160; &#160;而变量c是单独存放在静态数据区的，因此用siezof计算其大小时没有将c所占的空间计算进来。 &#160; &#160; &#160; &#160;5）test5 结构体中含有结构体123456typedef struct node4&#123; bool a; S1 s1; short b;&#125;S4; &#160; &#160; &#160; &#160;数据成员包含另一个结构体变量s1的话，则取s1中最 长数据类型与其他数据成员比较，取最长的作为对齐标准，但是s1存放时看做一个单位存放，只需看其他成员即可。&#160; &#160; &#160; &#160;则sizeof(S4)=16。是因为s1占8字节，而s1中最长数据类型为int，占4个字节，bool类型1个字节，short占2字节，因此以4字节对齐，则存储方式为：|-------bool--------| 4字节|-------s1----------| 8字节|-------short-------| 4字节 &#160; &#160; &#160; &#160;6）test61234567typedef struct node5&#123; bool a; S1 s1; double b; int c;&#125;S5; &#160; &#160; &#160; &#160;则sizeof(S5)=32。是因为s1占8字节，而s1中最长数据类型为int，占4字节，而double占8字节，因此以8字节对齐，则存放方式为：|--------bool--------| 8字节|---------s1---------| 8字节|--------double------| 8字节|----int----|--------| 8字节 &#160; &#160; &#160; &#160;总结一下，在计算sizeof时主要注意一下几点： 若为空结构体，则只占1个字节的单元; 若结构体中所有数据类型都相同，则其所占空间为 成员数据类型长度×成员个数；若结构体中数据类型不同，则取最长数据类型成员所占的空间为对齐标准，数据成员包含另一个结构体变量t的话，则取t中最 长数据类型与其他数据成员比较，取最长的作为对齐标准，但是t存放时看做一个单位存放，只需看其他成员即可。 attribute 关键字(双下划线相连并紧贴attribute)&#160; &#160; &#160; &#160;上面的结构体对齐只是普通情况，但是如果对于指定了 attribute (由于MarkDown文本原因，打出来双下划线相连并紧贴attribute有问题，因此先这么表示)关键字，就不痛了。例如我们上面定义 QueueBufferOutput结构体也用了（ attribute (( packed ))）。 &#160; &#160; &#160; &#160;1） attribute 关键字主要是用来在函数或数据声明中设置其属性。给函数赋给属性的主要目的在于让编译器进行优化。函数声明中的 attribute ((noreturn))，就是告诉编译器这个函数不会返回给调用者，以便编译器在优化时去掉不必要的函数返回代码。 &#160; &#160; &#160; &#160;GNU C的一大特色就是 attribute 机制。 attribute 可以设置函数属性（Function Attribute）、变量属性（Variable Attribute）和类型属性（Type Attribute）。 attribute 书写特征是： attribute 前后都有两个下划线，并且后面会紧跟一对括弧，括弧里面是相应的 attribute 参数。 attribute 语法格式为： attribute ((attribute-list)) 其位置约束：放于声明的尾部“；”之前。 函数属性（Function Attribute）：函数属性可以帮助开发者把一些特性添加到函数声明中，从而可以使编译器在错误检查方面的功能更强大。 attribute 机制也很容易同非GNU应用程序做到兼容之功效。 &#160; &#160; &#160; &#160;GNU CC需要使用 –Wall编译器来击活该功能，这是控制警告信息的一个很好的方式。 &#160; &#160; &#160; &#160;2） attribute ((packed)) 的作用就是告诉编译器取消结构在编译过程中的优化对齐,按照实际占用字节数进行对齐，是GCC特有的语法。这个功能是跟操作系统没关系，跟编译器有关，gcc编译器不是紧凑模式的，我在windows下，用vc的编译器也不是紧凑的，用tc的编译器就是紧凑的。例如： 在TC下：struct my{ char ch; int a;} sizeof(int)=2;sizeof(my)=3;（紧凑模式） 在GCC下：struct my{ char ch; int a;} sizeof(int)=4;sizeof(my)=8;（非紧凑模式） 在GCC下：struct my{ char ch; int a;} attrubte ((packed)) sizeof(int)=4;sizeof(my)=5 &#160; &#160; &#160; &#160;packed属性：使用该属性可以使得变量或者结构体成员使用最小的对齐方式，即对变量是一字节对齐，对域（field）是位对齐。 &#160; &#160; &#160; &#160;3）若使用了 attribute ((aligned(n)))命令强制对齐标准，则取n和结构体中最长数据类型占的字节数两者之中的小者作为对齐标准。内存对齐，往往是由编译器来做的，如果你使用的是gcc，可以在定义变量时，添加 attribute ，来决定是否使用内存对齐，或是内存对齐到几个字节，以上面面的结构体为例：12345678typedef struct __attribute__ ((aligned(4))) node5&#123; bool a; S1 s1; double b; int c;&#125;S5; &#160; &#160; &#160; &#160;则sizeof(S5)=24。因为强制以4字节对齐，而S5中最长数据类型为double，占8字节，因此以4字节对齐。在内存中存放方式为：|-----------a--------| 4字节|--------s1----------| 4字节|--------s1----------| 4字节|--------b-----------| 4字节|--------b-----------| 4字节|---------c----------| 4字节 参考http://www.unixwiz.net/techtips/gnu-c-attributes.htmlhttp://www.cnblogs.com/dolphin0520/archive/2011/09/17/2179466.html 小结&#160; &#160; &#160; &#160;本篇我们讲了Surface对图形缓冲区的管理，前面从Activity到Surface创建过程我们只大概提了下，这个放在以后有空会分析的。下一篇我们讲讲SurfaceFlinger消费图形缓冲区，还有合成等等操作。 &#160; &#160; &#160; &#160;唉，有一件特别悲催的事情，就是我失业了ToT，所以估计有一段时间不能更新了，望大家见谅。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(七)----创建图形缓冲区GraphicBuffer]]></title>
    <url>%2F2017%2F06%2F22%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E4%B8%83-%E5%88%9B%E5%BB%BA%E5%9B%BE%E5%BD%A2%E7%BC%93%E5%86%B2%E5%8C%BAGraphicBuffer%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;在上一篇SurfaceFlinger创建Surface流程中，我们会关联到BufferQueue对GraphicBuffer的管理机制，我们这次就来分析一下它的原理。 角色扮演&#160; &#160; &#160; &#160;Android应用的UI显示到Display的过程中，SurfaceFlinger扮演的角色只是“Flinger”，就是定于检查Layer更新，然后计算DirtyRegion，然后将结果推送给底层显示驱动进行显示。但是在App端，Surface的内容绘制却由应用层来承担。应用层绘制UI内容都需要一个GraphicBuffer，那么Surface对GraphicBuffer的申请、绘制完毕交给SurfaceFlinger去合成然后显示，这个工作也需要一套机制去管理。所以对于GraphicBuffer的管理，Android也设计了一套机制：BufferQueue，作为SurfaceFlinger管理和消费surface的中介。 生产-消费 模型&#160; &#160; &#160; &#160;回顾Android SurfaceFlinger 学习之路(二)—-SurfaceFlinger概述，我们可以用这副简图表示App、BufferQueue和SurfaceFlinger的关系。 &#160; &#160; &#160; &#160;虽说是三者的关系，但是他们所属的层却只有两个，app属于Java层，BufferQueue/SurfaceFlinger属于native层。也就是说BufferQueue也是隶属SurfaceFlinger，所有工作围绕SurfaceFlinger展开。 生产者模型&#160; &#160; &#160; &#160; 这里IGraphicBufferProducer就是app和BufferQueue重要桥梁，GraphicBufferProducer承担着单个应用进程中的UI显示需求，与BufferQueue打交道的就是它。&#160; &#160; &#160; &#160; 根据上一篇Surface的创建流程，我们从Surface类图分析了GraphicBufferProducer在Surface中的位置，下图也可以表示： &#160; &#160; &#160; &#160;BpGraphicBufferProducer是GraphicBufferProducer在客户端这边的代理对象，负责和SF交互，GraphicBufferProducer通过gbp（IGraphicBufferProducer类对象）向BufferQueue获取buffer，然后进行填充UI信息，当填充完毕会通知SF，SF知道后就对该Buffer进行下一步操作。典型的生产-消费者模式。 消费者模型&#160; &#160; &#160; &#160;BufferQueue和SurfaceFlinger之间的通信模式如下： &#160; &#160; &#160; &#160;也是有一对BpGraphicBufferConsumer/BnGraphicBufferConsumer支持他们之间的信息传输。 模型实现&#160; &#160; &#160; &#160;上一篇创建Surface流程中，当创建Layer时候，在onFirstRef里面创建了一个生产者和消费者，并对两者进行了包装：1234567891011121314void Layer::onFirstRef() &#123; // Creates a custom BufferQueue for SurfaceFlingerConsumer to use sp&lt;IGraphicBufferProducer&gt; producer;//生产者 sp&lt;IGraphicBufferConsumer&gt; consumer;//消费者 BufferQueue::createBufferQueue(&amp;producer, &amp;consumer);//创建生产者和消费者 mProducer = new MonitoredProducer(producer, mFlinger);//包装生产者 mSurfaceFlingerConsumer = new SurfaceFlingerConsumer(consumer, mTextureName);//包装消费者 //设置消费者相关回调 mSurfaceFlingerConsumer-&gt;setConsumerUsageBits(getEffectiveUsage(0)); mSurfaceFlingerConsumer-&gt;setContentsChangedListener(this); mSurfaceFlingerConsumer-&gt;setName(mName); ......&#125; &#160; &#160; &#160; &#160;MonitoredProducer对象，这个对象只是一个代理，真正实是BufferQueueProducer类，这个对象和BufferQueueCore有关联，可以管理最多达64块的缓冲区。&#160; &#160; &#160; &#160;SurfaceFlingerConsumer构造函数将BufferQueueConsumer传入，并设置相关回调，以便生产者填充完GraphicBuffer然后通知Consumer消费。 Core&#160; &#160; &#160; &#160;所以创建生产者和消费者就是由BufferQueue来创建，我们可以看看BufferQueue的createBufferQueue函数，位于frameworks/native/libs/gui/BufferQueue.cpp：1234567891011121314151617void BufferQueue::createBufferQueue(sp&lt;IGraphicBufferProducer&gt;* outProducer, sp&lt;IGraphicBufferConsumer&gt;* outConsumer, const sp&lt;IGraphicBufferAlloc&gt;&amp; allocator) &#123;//allocator == NULL ...... //创建一个BufferQueueCore，她是核心 sp&lt;BufferQueueCore&gt; core(new BufferQueueCore(allocator)); ...... //用BufferQueueCore创建生产者 sp&lt;IGraphicBufferProducer&gt; producer(new BufferQueueProducer(core)); ...... //用core创建消费者 sp&lt;IGraphicBufferConsumer&gt; consumer(new BufferQueueConsumer(core)); ...... //向外面传入指针赋值 *outProducer = producer; *outConsumer = consumer;&#125; &#160; &#160; &#160; &#160;所以核心都是这个BufferQueueCore，他是管理图形缓冲区的中枢。这里举一个SurfaceTexture的例子，来看看他们之间的关系： &#160; &#160; &#160; &#160;可以认为BufferQueueCore是一个服务中心，生产者、消费者都要通过它来管理buffer。 里面有一个重要的成员数组：BufferQueueDefs::SlotsType mSlots; 这个BufferSlot中有一个成员变量：sp&lt; GraphicBuffer &gt;mGraphicBuffer;记录这个slot所涉及的缓冲区； 另一个变量BufferState mBufferState;用于跟踪这个缓冲区的状态。 &#160; &#160; &#160; &#160;1）在BufferQueueCore类中定义了一个64项的数据mSlots，framework/native/libs/gui/BufferQueueCore.h：123456// mSlots is an array of buffer slots that must be mirrored on the producer// side. This allows buffer ownership to be transferred between the producer// and consumer without sending a GraphicBuffer over Binder. The entire// array is initialized to NULL at construction time, and buffers are// allocated for a slot when requestBuffer is called with that slot's index.BufferQueueDefs::SlotsType mSlots; &#160; &#160; &#160; &#160;我们看看这个mSlot定义所在的位置，位于frameworks/native/include/gui/BufferQueueDefs.h中：12345678namespace BufferQueueDefs &#123; // BufferQueue will keep track of at most this value of buffers. // Attempts at runtime to increase the number of buffers past this // will fail. enum &#123; NUM_BUFFER_SLOTS = 64 &#125;; typedef BufferSlot SlotsType[NUM_BUFFER_SLOTS]; &#125; // namespace BufferQueueDefs &#160; &#160; &#160; &#160;我们可以看到这是一个容量大小为64的数组，因此BufferQueueCore可以管理最多64块的GraphicBuffer。 &#160; &#160; &#160; &#160;我们可以帖一幅图表示Slot的角色： &#160; &#160; &#160; &#160;2）BufferSlot的定义位于frameworks/native/include/gui/BufferSlot.h中：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859struct BufferSlot &#123; BufferSlot() : mEglDisplay(EGL_NO_DISPLAY), mBufferState(BufferSlot::FREE), mRequestBufferCalled(false), mFrameNumber(0), mEglFence(EGL_NO_SYNC_KHR), mAcquireCalled(false), mNeedsCleanupOnRelease(false), mAttachedByConsumer(false) &#123; &#125; // mGraphicBuffer points to the buffer allocated for this slot or is NULL // if no buffer has been allocated. sp&lt;GraphicBuffer&gt; mGraphicBuffer; // BufferState represents the different states in which a buffer slot // can be. All slots are initially FREE. enum BufferState &#123; // FREE indicates that the buffer is available to be dequeued // by the producer. The buffer may be in use by the consumer for // a finite time, so the buffer must not be modified until the // associated fence is signaled. // // The slot is "owned" by BufferQueue. It transitions to DEQUEUED // when dequeueBuffer is called. FREE = 0, // DEQUEUED indicates that the buffer has been dequeued by the // producer, but has not yet been queued or canceled. The // producer may modify the buffer's contents as soon as the // associated ready fence is signaled. // // The slot is "owned" by the producer. It can transition to // QUEUED (via queueBuffer) or back to FREE (via cancelBuffer). DEQUEUED = 1, // QUEUED indicates that the buffer has been filled by the // producer and queued for use by the consumer. The buffer // contents may continue to be modified for a finite time, so // the contents must not be accessed until the associated fence // is signaled. // // The slot is "owned" by BufferQueue. It can transition to // ACQUIRED (via acquireBuffer) or to FREE (if another buffer is // queued in asynchronous mode). QUEUED = 2, // ACQUIRED indicates that the buffer has been acquired by the // consumer. As with QUEUED, the contents must not be accessed // by the consumer until the fence is signaled. // // The slot is "owned" by the consumer. It transitions to FREE // when releaseBuffer is called. ACQUIRED = 3 &#125;; // mBufferState is the current state of this buffer slot. BufferState mBufferState;&#125; &#160; &#160; &#160; &#160;mGraphicBuffer代表一块图形缓冲区GraphicBuffer，用于应用绘制UI。&#160; &#160; &#160; &#160;mBufferState类型为BufferState ，BufferState 的定义也位于其中，代表图形缓冲区的几种状态： FREE：buffer当前可用，可以被生产者dequeued，此时owner是BufferQueueCore，当dequeuebuffer调用时，状态可以转为dequeued。 DEQUEUED：buffer已经被dequeued，还没被queue或canceld，此时owner是producer。 QUEUED：buffer已经被生产者填充，并被queued，此时的owner是bufferQueueCore。 ACQUIRED：buffer已经被消费者获得，此时的owner是consumer。 &#160; &#160; &#160; &#160;一般的buffer大致会经过FREE-&gt;DEQUEUED-&gt;QUEUED-&gt;ACQUIRED-&gt;FREE这个流程，我们下面会讲到它。 &#160; &#160; &#160; &#160;3）在BufferQueueCore创建时候，还会创建一个GraphicBufferAlloc，并将它赋值给BufferQueueCore的mAllocator成员变量。我们可以看看BufferQueueCore的构造函数，位于framework/native/libs/gui/BufferQueueCore.cpp：123456789101112131415161718192021222324252627282930313233343536BufferQueueCore::BufferQueueCore(const sp&lt;IGraphicBufferAlloc&gt;&amp; allocator) ://allocator为NULL mAllocator(allocator), mMutex(), mIsAbandoned(false), mConsumerControlledByApp(false), mConsumerName(getUniqueName()), mConsumerListener(), mConsumerUsageBits(0), mConnectedApi(NO_CONNECTED_API), mConnectedProducerListener(), mSlots(), mQueue(), mOverrideMaxBufferCount(0), mDequeueCondition(), mUseAsyncBuffer(true), mDequeueBufferCannotBlock(false), mDefaultBufferFormat(PIXEL_FORMAT_RGBA_8888), mDefaultWidth(1), mDefaultHeight(1), mDefaultMaxBufferCount(2), mMaxAcquiredBufferCount(1), mBufferHasBeenQueued(false), mFrameCounter(0), mTransformHint(0), mIsAllocating(false), mIsAllocatingCondition()&#123; if (allocator == NULL) &#123;//NULL sp&lt;ISurfaceComposer&gt; composer(ComposerService::getComposerService()); //请求SurfaceFlinger创建GraphicBufferAlloc mAllocator = composer-&gt;createGraphicBufferAlloc(); if (mAllocator == NULL) &#123; BQ_LOGE("createGraphicBufferAlloc failed"); &#125; &#125;&#125; &#160; &#160; &#160; &#160;然后回去请求SurfaceFlinger创建一个GraphicBufferAlloc，位于frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp中：123456sp&lt;IGraphicBufferAlloc&gt; SurfaceFlinger::createGraphicBufferAlloc()&#123; //创建一个GraphicBufferAlloc对象，并赋给强指针 sp&lt;GraphicBufferAlloc&gt; gba(new GraphicBufferAlloc()); return gba;&#125; &#160; &#160; &#160; &#160;因为BufferQueueCore的mAllocator 类型为IGraphicBufferAlloc，继承于IInterface，所以又是跨进程调用创建。我们下面会分析到它的。 Producer&#160; &#160; &#160; &#160;BufferQueueProducer是生产者的实现，需要实现IGraphicBufferProducer接口，相关方法如下：1234567891011121314151617181920212223242526/* * This class defines the Binder IPC interface for the producer side of * a queue of graphics buffers. It's used to send graphics data from one * component to another. For example, a class that decodes video for * playback might use this to provide frames. This is typically done * indirectly, through Surface. * * The underlying mechanism is a BufferQueue, which implements * BnGraphicBufferProducer. In normal operation, the producer calls * dequeueBuffer() to get an empty buffer, fills it with data, then * calls queueBuffer() to make it available to the consumer. * * This class was previously called ISurfaceTexture. */class IGraphicBufferProducer : public IInterface&#123;public: ...... virtual status_t requestBuffer(int slot, sp&lt;GraphicBuffer&gt;* buf) = 0; virtual status_t dequeueBuffer(int* slot, sp&lt;Fence&gt;* fence, bool async, uint32_t w, uint32_t h, uint32_t format, uint32_t usage) = 0; virtual status_t queueBuffer(int slot, const QueueBufferInput&amp; input, QueueBufferOutput* output) = 0; virtual void cancelBuffer(int slot, const sp&lt;Fence&gt;&amp; fence) = 0; ...... &#125; &#160; &#160; &#160; &#160;这几个函数的注释写的很清楚，但是限于篇幅，没有贴出来。 &#160; &#160; &#160; &#160;BufferQueueProducer的dequeueBuffer函数用来向BufferQueueCore申请一个空闲的slot，这个slot可能已经有缓冲区，也可能没有，如果没有缓冲区，dequeueBuffer函数会分配一块新的缓冲区。得到空闲的slot后，还需要调用requestBuffer函数来取出一块缓冲区，也是从BufferQueueCore中的mSlots数组根据index获取这个位置的slot。得到缓冲区，如果不需要了，可以使用cancelBuffer函数来释放这个slot。调用dequeueBuffer函数之后，缓冲区的拥有者是生产者，缓冲区处于DEQUEUED状态。一旦缓冲区复制数据完成，通过queueBuffer函数把缓冲区的控制权交还给BufferQueueCore，这时候缓冲区将处于QUEUED状态。这些函数的使用之处我们下面会讲到。&#160; &#160; &#160; &#160;BufferQueueProducer类图结构如下： Consumer&#160; &#160; &#160; &#160;同样，BufferQueueConsumer是消费者的实现，需要实现IGraphicBufferConsumer接口，相关方法如下：123456789class IGraphicBufferConsumer : public IInterface &#123;public: ...... virtual status_t acquireBuffer(BufferItem* buffer, nsecs_t presentWhen) = 0; virtual status_t releaseBuffer(int buf, uint64_t frameNumber, EGLDisplay display, EGLSyncKHR fence, const sp&lt;Fence&gt;&amp; releaseFence) = 0; ...... &#125; &#160; &#160; &#160; &#160;注释写的很清晰，但是限于篇幅。。。 &#160; &#160; &#160; &#160;BufferQueueConsumer类是接口IGraphicBufferComsumer的实现，是一个回调接口，如果BufferQueue中有数据准备好了通知消费者取走数据。取走数据的时候，需要调用acquireBuffer函数，将缓冲区状态变成ACQUIRED，使用完之后调用releaseBuffer函数可以吧缓冲区数据归还给BufferQueueCore，这样缓冲区就变成FREE。 &#160; &#160; &#160; &#160;当一块buffer就绪后，消费者就开始工作了，consumer对buffer的处理是被动的，它必须要等到一块buffer填充好才能工作，那consumer怎么知道一块buffer已经填充好了？ &#160; &#160; &#160; &#160;所以我们顺着Layer的onFirstRef函数，里面还有一个对于消费者的封装：SurfaceFlingerConsumer。为了方便，我再贴一遍Layer的onFirstRef函数：1234567891011121314void Layer::onFirstRef() &#123; // Creates a custom BufferQueue for SurfaceFlingerConsumer to use sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferConsumer&gt; consumer; BufferQueue::createBufferQueue(&amp;producer, &amp;consumer); mProducer = new MonitoredProducer(producer, mFlinger); //创建一个SurfaceFlingerConsumer用于通知消费者消费 mSurfaceFlingerConsumer = new SurfaceFlingerConsumer(consumer, mTextureName); mSurfaceFlingerConsumer-&gt;setConsumerUsageBits(getEffectiveUsage(0)); //给消费者设置监听者 mSurfaceFlingerConsumer-&gt;setContentsChangedListener(this); mSurfaceFlingerConsumer-&gt;setName(mName);&#125; &#160; &#160; &#160; &#160;为了方便起见，我们将BufferQueueConsumer和SurfaceFlingerConsumer的类图都贴出来： &#160; &#160; &#160; &#160;为了通知消费消费GraphicBuffer，这里提供了另外一个类ConsumerListener，位于frameworks/native/include/gui/IConsumerListener.h：12345678910111213141516171819202122232425262728 ConsumerListener() &#123; &#125; virtual ~ConsumerListener() &#123; &#125; // onFrameAvailable is called from queueBuffer each time an additional // frame becomes available for consumption. This means that frames that // are queued while in asynchronous mode only trigger the callback if no // previous frames are pending. Frames queued while in synchronous mode // always trigger the callback. // // This is called without any lock held and can be called concurrently // by multiple threads. virtual void onFrameAvailable() = 0; /* Asynchronous */ // onBuffersReleased is called to notify the buffer consumer that the // BufferQueue has released its references to one or more GraphicBuffers // contained in its slots. The buffer consumer should then call // BufferQueue::getReleasedBuffers to retrieve the list of buffers // // This is called without any lock held and can be called concurrently // by multiple threads. virtual void onBuffersReleased() = 0; /* Asynchronous */ // onSidebandStreamChanged is called to notify the buffer consumer that the // BufferQueue's sideband buffer stream has changed. This is called when a // stream is first attached and when it is either detached or replaced by a // different stream. virtual void onSidebandStreamChanged() = 0; /* Asynchronous */&#125;; &#160; &#160; &#160; &#160;两个纯虚函数，当一块buffer可以被消费时，onFrameAvailable会被调用；当BufferQueue通知consumer它已经释放其mSlot中的一个或多个GraphicBuffer的引用时，会调用onBuffersReleased。 &#160; &#160; &#160; &#160;那么什么时候注册的这个监听呢？我们需要跟随一下SurfaceFlingerConsumer 的构造函数，位于frameworks/native/services/surfaceflinger/SurfaceFlingerConsumer.h中：123456SurfaceFlingerConsumer(const sp&lt;IGraphicBufferConsumer&gt;&amp; consumer, uint32_t tex) //调用了GLConsumer的构造函数 : GLConsumer(consumer, tex, GLConsumer::TEXTURE_EXTERNAL, false, false), mTransformToDisplayInverse(false)&#123;&#125; &#160; &#160; &#160; &#160;我们需要继续查看GLConsumer的构造函数，位于frameworks/native/libs/gui/GLConsumer.cpp：1234567891011121314151617181920212223242526GLConsumer::GLConsumer(const sp&lt;IGraphicBufferConsumer&gt;&amp; bq, uint32_t tex, uint32_t texTarget, bool useFenceSync, bool isControlledByApp) : ConsumerBase(bq, isControlledByApp), //有调用了ConsumerBase的构造函数，猫腻应该在这里 mCurrentTransform(0), mCurrentScalingMode(NATIVE_WINDOW_SCALING_MODE_FREEZE), mCurrentFence(Fence::NO_FENCE), mCurrentTimestamp(0), mCurrentFrameNumber(0), mDefaultWidth(1), mDefaultHeight(1), mFilteringEnabled(true), mTexName(tex), mUseFenceSync(useFenceSync), mTexTarget(texTarget), mEglDisplay(EGL_NO_DISPLAY), mEglContext(EGL_NO_CONTEXT), mCurrentTexture(BufferQueue::INVALID_BUFFER_SLOT), mAttached(true)&#123; ST_LOGV("GLConsumer"); memcpy(mCurrentTransformMatrix, mtxIdentity, sizeof(mCurrentTransformMatrix)); mConsumer-&gt;setConsumerUsageBits(DEFAULT_USAGE_FLAGS);&#125; &#160; &#160; &#160; &#160;又调用了ConsumerBase的构造函数，猫腻应该在这里，我们继续查看ConsumerBase的构造函数，位于frameworks/native/libs/gui/ConsumerBase.cpp中：1234567891011121314151617181920212223ConsumerBase::ConsumerBase(const sp&lt;IGraphicBufferConsumer&gt;&amp; bufferQueue, bool controlledByApp) : mAbandoned(false), mConsumer(bufferQueue) &#123; // Choose a name using the PID and a process-unique ID. mName = String8::format("unnamed-%d-%d", getpid(), createProcessUniqueId()); // Note that we can't create an sp&lt;...&gt;(this) in a ctor that will not keep a // reference once the ctor ends, as that would cause the refcount of 'this' // dropping to 0 at the end of the ctor. Since all we need is a wp&lt;...&gt; // that's what we create. //创建监听者对象 wp&lt;ConsumerListener&gt; listener = static_cast&lt;ConsumerListener*&gt;(this); //包装成一个代理 sp&lt;IConsumerListener&gt; proxy = new BufferQueue::ProxyConsumerListener(listener); //mConsumer调用consumerConnect函数，通过mConsumer，把consumerListener注册到服务者类bufferQueuecore中 status_t err = mConsumer-&gt;consumerConnect(proxy, controlledByApp); if (err != NO_ERROR) &#123; CB_LOGE("ConsumerBase: error connecting to BufferQueue: %s (%d)", strerror(-err), err); &#125; else &#123; mConsumer-&gt;setConsumerName(mName); &#125;&#125; &#160; &#160; &#160; &#160;上面代码主要工作有：&#160; &#160; &#160; &#160;1）创建监听者对象；&#160; &#160; &#160; &#160;2）包装成一个代理；&#160; &#160; &#160; &#160;3）mConsumer调用consumerConnect函数，通过mConsumer，把consumerListener注册到服务者类bufferQueuecore中。&#160; &#160; &#160; &#160;那么注册操作就在于第三部。这里BufferQueueConsumer调用consumerConnect函数，位于frameworks/native/libs/gui/BufferQueueConsumer.h中：1234virtual status_t consumerConnect(const sp&lt;IConsumerListener&gt;&amp; consumer, bool controlledByApp) &#123; return connect(consumer, controlledByApp);&#125; &#160; &#160; &#160; &#160;在头文件里定义了consumerConnect函数，调用connect函数，位于frameworks/native/libs/gui/BufferQueueConsumer.cpp中：123456789101112131415161718192021222324status_t BufferQueueConsumer::connect( const sp&lt;IConsumerListener&gt;&amp; consumerListener, bool controlledByApp) &#123; ATRACE_CALL(); if (consumerListener == NULL) &#123; BQ_LOGE("connect(C): consumerListener may not be NULL"); return BAD_VALUE; &#125; BQ_LOGV("connect(C): controlledByApp=%s", controlledByApp ? "true" : "false"); Mutex::Autolock lock(mCore-&gt;mMutex); if (mCore-&gt;mIsAbandoned) &#123; BQ_LOGE("connect(C): BufferQueue has been abandoned"); return NO_INIT; &#125; //这里把consumerListener注册到服务者类bufferQueuecore中 mCore-&gt;mConsumerListener = consumerListener; mCore-&gt;mConsumerControlledByApp = controlledByApp; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;connect连接，经过层层调用，把listener监听注册到bufferQueuecore中，这样bufferQueuecore就可以利用这个监听通知到具体的消费者。 &#160; &#160; &#160; &#160;接着就是设置监听者：mSurfaceFlingerConsumer-&gt;setContentsChangedListener(this);我们可以看看他的操作：1234567891011121314void SurfaceFlingerConsumer::setContentsChangedListener( const wp&lt;ContentsChangedListener&gt;&amp; listener) &#123; //调用了ConsumerBase的setFrameAvailableListener函数 setFrameAvailableListener(listener); Mutex::Autolock lock(mMutex); mContentsChangedListener = listener;&#125;/**ConsumerBase的setFrameAvailableListener函数**/void ConsumerBase::setFrameAvailableListener( const wp&lt;FrameAvailableListener&gt;&amp; listener) &#123; CB_LOGV("setFrameAvailableListener"); Mutex::Autolock lock(mMutex); mFrameAvailableListener = listener;&#125; &#160; &#160; &#160; &#160;这样就构成了完整的监听者模式了，有Observer，也有Observable。 &#160; &#160; &#160; &#160;这样就清楚了buffer的生产者，消费者，管理者之间的关系。 &#160; &#160; &#160; &#160;所以稍微总结一下BufferQueue机制，借助Chris Simmonds老爷子的话，就是如下内容： Mechanism for passing GraphicBuffers to SurfaceFlinger Contains an array of between 2 and 64 GraphicBuffers Uses interface IGraphicBufferAlloc to allocate buffers (see later) Provides two Binder interfaces： IGraphicBufferProducer for the client (Activity) IGraphicBufferConsumer for the consumer (SurfaceFlinger) Buffers cycle between producer and consumer 分配图形缓冲区&#160; &#160; &#160; &#160;首先buffer是共享缓冲区，故肯定会涉及到互斥锁，所以buffer的状态也有很多种，一般buffer大致会经过FREE-&gt;DEQUEUED-&gt;QUEUED-&gt;ACQUIRED-&gt;FREE这个过程。如果所示： &#160; &#160; &#160; &#160;或者这样表示这个过程： 申请图形缓冲区&#160; &#160; &#160; &#160;前面看到，bufferqueuecore中mSlots数组管理缓冲区，最大容量是64，这个mSlots一开始静态分配了64个bufferslot大小的空间，但是其中的数据缓冲区不是一次性分配的，不然就太浪费空间了，所以缓冲区的空间分配是动态的，具体就是producer在dequeuebuffer时，如果没有获取到可用的缓冲区，那就要重新分配空间了。 &#160; &#160; &#160; &#160;分配图形缓冲区，即GraphicBuffer，我们需要查看BufferQueueProducer的dequeuebuffer函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147status_t BufferQueueProducer::dequeueBuffer(int *outSlot, sp&lt;android::Fence&gt; *outFence, bool async, uint32_t width, uint32_t height, uint32_t format, uint32_t usage) &#123; ATRACE_CALL(); &#123; // Autolock scope Mutex::Autolock lock(mCore-&gt;mMutex); mConsumerName = mCore-&gt;mConsumerName; &#125; // Autolock scope BQ_LOGV("dequeueBuffer: async=%s w=%u h=%u format=%#x, usage=%#x", async ? "true" : "false", width, height, format, usage); //宽高不正常，则返回失败 if ((width &amp;&amp; !height) || (!width &amp;&amp; height)) &#123; BQ_LOGE("dequeueBuffer: invalid size: w=%u h=%u", width, height); return BAD_VALUE; &#125; //一些变量赋初值 status_t returnFlags = NO_ERROR; EGLDisplay eglDisplay = EGL_NO_DISPLAY; EGLSyncKHR eglFence = EGL_NO_SYNC_KHR; bool attachedByConsumer = false; &#123; // Autolock scope Mutex::Autolock lock(mCore-&gt;mMutex); //如果正在申请Grabuffer，则要wait。里面是一个mIsAllocating标志位判断，如果为true则要等待 mCore-&gt;waitWhileAllocatingLocked(); //如果format为0，则赋为初值PIXEL_FORMAT_RGBA_8888 if (format == 0) &#123; format = mCore-&gt;mDefaultBufferFormat; &#125; // Enable the usage bits the consumer requested usage |= mCore-&gt;mConsumerUsageBits; int found;//先在mSlot数组中查找FREE状态的slot，如果找到了就返回这个slot中的index。 status_t status = waitForFreeSlotThenRelock("dequeueBuffer", async, &amp;found, &amp;returnFlags); if (status != NO_ERROR) &#123; return status; &#125; // This should not happen //这种情况一般不会发生，因为如何found是INVALID_BUFFER_SLOT状态，上面的status就不会是NO_Error,就不会走到这里 if (found == BufferQueueCore::INVALID_BUFFER_SLOT) &#123; BQ_LOGE("dequeueBuffer: no available buffer slots"); return -EBUSY; &#125; //将找到的slot的index赋给外面传入的outSlot指向内容 *outSlot = found; ATRACE_BUFFER_INDEX(found); attachedByConsumer = mSlots[found].mAttachedByConsumer; const bool useDefaultSize = !width &amp;&amp; !height; if (useDefaultSize) &#123; width = mCore-&gt;mDefaultWidth; height = mCore-&gt;mDefaultHeight; &#125; //改变BufferState状态为DEQUEUED mSlots[found].mBufferState = BufferSlot::DEQUEUED; //然后将返回index的slot的Graphicbuffer赋给buffer变量 const sp&lt;GraphicBuffer&gt;&amp; buffer(mSlots[found].mGraphicBuffer); //判断这个GraphicBuffer是否需要重新分配空间，判断条件就是buffer为空，因为它初始值是null，第一次使用它需要分配空间， //如果不为null，但是buffer的width、height、format、usage属性跟要求的不一致，也要重新分配 if ((buffer == NULL) || (static_cast&lt;uint32_t&gt;(buffer-&gt;width) != width) || (static_cast&lt;uint32_t&gt;(buffer-&gt;height) != height) || (static_cast&lt;uint32_t&gt;(buffer-&gt;format) != format) || ((static_cast&lt;uint32_t&gt;(buffer-&gt;usage) &amp; usage) != usage)) &#123; //如果需要重新分配GraphicBuffer，先初始化这些变量 mSlots[found].mAcquireCalled = false; mSlots[found].mGraphicBuffer = NULL; mSlots[found].mRequestBufferCalled = false; mSlots[found].mEglDisplay = EGL_NO_DISPLAY; mSlots[found].mEglFence = EGL_NO_SYNC_KHR; mSlots[found].mFence = Fence::NO_FENCE; //增加BUFFER_NEEDS_REALLOCATION标志，已重新分配GraphicBuffer returnFlags |= BUFFER_NEEDS_REALLOCATION; &#125; if (CC_UNLIKELY(mSlots[found].mFence == NULL)) &#123; BQ_LOGE("dequeueBuffer: about to return a NULL fence - " "slot=%d w=%d h=%d format=%u", found, buffer-&gt;width, buffer-&gt;height, buffer-&gt;format); &#125; //给一些临时变量赋值 eglDisplay = mSlots[found].mEglDisplay; eglFence = mSlots[found].mEglFence; *outFence = mSlots[found].mFence; //给fence变量赋初值 mSlots[found].mEglFence = EGL_NO_SYNC_KHR; mSlots[found].mFence = Fence::NO_FENCE; &#125; // Autolock scope //如果需要重新分配GraphicBuffer if (returnFlags &amp; BUFFER_NEEDS_REALLOCATION) &#123; status_t error; BQ_LOGV("dequeueBuffer: allocating a new buffer for slot %d", *outSlot); //需要重新分配，调用BufferQueueCore中的GraphicBufferAlloc中函数createGraphicBuffer，生成一个newGraphicBuffer sp&lt;GraphicBuffer&gt; graphicBuffer(mCore-&gt;mAllocator-&gt;createGraphicBuffer( width, height, format, usage, &amp;error)); if (graphicBuffer == NULL) &#123; BQ_LOGE("dequeueBuffer: createGraphicBuffer failed"); return error; &#125; &#123; // Autolock scope Mutex::Autolock lock(mCore-&gt;mMutex); if (mCore-&gt;mIsAbandoned) &#123; BQ_LOGE("dequeueBuffer: BufferQueue has been abandoned"); return NO_INIT; &#125; ////新分配的buffer，将slot的mFrameNumber 置为最大。mFrameNumber 用于LRU排序 mSlots[*outSlot].mFrameNumber = UINT32_MAX; //将分配的buffer赋给slot中的mGraphicBuffer mSlots[*outSlot].mGraphicBuffer = graphicBuffer; &#125; // Autolock scope &#125; if (attachedByConsumer) &#123; returnFlags |= BUFFER_NEEDS_REALLOCATION; &#125; //如果eglFence不等于EGL_NO_SYNC_KHR。Fence我们后面会讲到，一种围栏机制 if (eglFence != EGL_NO_SYNC_KHR) &#123; //等待Buffer状态就绪，然后fence围栏放行 EGLint result = eglClientWaitSyncKHR(eglDisplay, eglFence, 0, 1000000000); // If something goes wrong, log the error, but return the buffer without // synchronizing access to it. It's too late at this point to abort the // dequeue operation. if (result == EGL_FALSE) &#123; BQ_LOGE("dequeueBuffer: error %#x waiting for fence", eglGetError()); &#125; else if (result == EGL_TIMEOUT_EXPIRED_KHR) &#123; BQ_LOGE("dequeueBuffer: timeout waiting for fence"); &#125; eglDestroySyncKHR(eglDisplay, eglFence); &#125; BQ_LOGV("dequeueBuffer: returning slot=%d/%" PRIu64 " buf=%p flags=%#x", *outSlot, mSlots[*outSlot].mFrameNumber, mSlots[*outSlot].mGraphicBuffer-&gt;handle, returnFlags); return returnFlags;&#125; &#160; &#160; &#160; &#160;代码虽然多，但是逻辑不复杂，我们分步查看： &#160; &#160; &#160; &#160;1）先在mSlot数组中查找FREE状态的slot，如果找到了就返回这个slot中的index。这个操作是调用了waitForFreeSlotThenRelock函数，我们看看这个函数的实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139status_t BufferQueueProducer::waitForFreeSlotThenRelock(const char* caller, bool async, int* found, status_t* returnFlags) const &#123; bool tryAgain = true;//默认是true while (tryAgain) &#123; if (mCore-&gt;mIsAbandoned) &#123;//如果没有初始化，BufferQueue被abandoned BQ_LOGE("%s: BufferQueue has been abandoned", caller); return NO_INIT; &#125; //getMaxBufferCountLocked returns the maximum number of buffers that can be allocated at once //注释还有一些，不贴了。就是获取一次申请的buffer的最大值。查看了代码，就是所有状态的buffer个数。 const int maxBufferCount = mCore-&gt;getMaxBufferCountLocked(async); //如果async为true，即异步操作，并且mOverrideMaxBufferCount不为0 //（一般mOverrideMaxBufferCount是在BufferQueueProducer调用setBufferCount函数改变， //这个函数会更新mSlot中所有Slot的状态为未分配的，并把ower还给BufferQueue） if (async &amp;&amp; mCore-&gt;mOverrideMaxBufferCount) &#123; // FIXME: Some drivers are manually setting the buffer count // (which they shouldn't), so we do this extra test here to // handle that case. This is TEMPORARY until we get this fixed. //因为是异步的关系，一些驱动会修改buffer count，但是返回不及时，所以这两个值可能不相等 if (mCore-&gt;mOverrideMaxBufferCount &lt; maxBufferCount) &#123; BQ_LOGE("%s: async mode is invalid with buffer count override", caller); return BAD_VALUE; &#125; &#125; // Free up any buffers that are in slots beyond the max buffer count //释放所有超出buffer数组最大数64的buffer for (int s = maxBufferCount; s &lt; BufferQueueDefs::NUM_BUFFER_SLOTS; ++s) &#123; assert(mSlots[s].mBufferState == BufferSlot::FREE); if (mSlots[s].mGraphicBuffer != NULL) &#123; mCore-&gt;freeBufferLocked(s); *returnFlags |= RELEASE_ALL_BUFFERS; &#125; &#125; // Look for a free buffer to give to the client //开始查找FREE状态的Buffer，并返回 *found = BufferQueueCore::INVALID_BUFFER_SLOT; int dequeuedCount = 0; int acquiredCount = 0; for (int s = 0; s &lt; maxBufferCount; ++s) &#123; switch (mSlots[s].mBufferState) &#123; case BufferSlot::DEQUEUED: ++dequeuedCount;//DEQUEUED + 1 break; case BufferSlot::ACQUIRED: ++acquiredCount;//ACQUIRED + 1 break; case BufferSlot::FREE: // We return the oldest of the free buffers to avoid // stalling the producer if possible, since the consumer // may still have pending reads of in-flight buffers //返回最老的FREE buffer，为了避免当consumer还在等待消费pending的正在起飞的buffer时候，producer突然GG了 if (*found == BufferQueueCore::INVALID_BUFFER_SLOT ||//第一次找到FREE buffer，会将found指针指向这个index //下次来的FREE buffer，就和上此找到的buffer对比slot的mFrameNumber ，找到mFrameNumber 最小的。 //这个就是根绝mFrameNumber 的大小进行LRU排序，最新的就是mFrameNumber 最大的，最老的就是mFrameNumber 最小的 mSlots[s].mFrameNumber &lt; mSlots[*found].mFrameNumber) &#123; *found = s; &#125; break; default: break; &#125; &#125; // Producers are not allowed to dequeue more than one buffer if they // did not set a buffer count //Producers 如果没有调用setBufferCount是不允许dequeue buffer的 if (!mCore-&gt;mOverrideMaxBufferCount &amp;&amp; dequeuedCount) &#123; BQ_LOGE("%s: can't dequeue multiple buffers without setting the " "buffer count", caller); return INVALID_OPERATION; &#125; // See whether a buffer has been queued since the last // setBufferCount so we know whether to perform the min undequeued // buffers check below //在queueBuffer调用后，mBufferHasBeenQueued会置为true //如果在上次setBufferCount调用后，当buffer被queue进入BufferQueue， //这时候需要检查undequeue的数量是否大于最小的临界值 if (mCore-&gt;mBufferHasBeenQueued) &#123; // Make sure the producer is not trying to dequeue more buffers // than allowed //总count - dequeue的count = undequeue的count const int newUndequeuedCount = maxBufferCount - (dequeuedCount + 1); const int minUndequeuedCount = mCore-&gt;getMinUndequeuedBufferCountLocked(async); if (newUndequeuedCount &lt; minUndequeuedCount) &#123; BQ_LOGE("%s: min undequeued buffer count (%d) exceeded " "(dequeued=%d undequeued=%d)", caller, minUndequeuedCount, dequeuedCount, newUndequeuedCount); return INVALID_OPERATION; &#125; &#125; // If we disconnect and reconnect quickly, we can be in a state where // our slots are empty but we have many buffers in the queue. This can // cause us to run out of memory if we outrun the consumer. Wait here if // it looks like we have too many buffers queued up. //如果我们调用disconnect后迅速的reconnect，会出现mSlot数组为空但是mQueue队列中的BufferItem不为空 //所以这样会造成内存耗尽的后果 //就是判断放入mQueue中BufferItem的个数是否大于mSlot中的个数 //mQueue是用来存放申请的缓冲区的，是一个Vector bool tooManyBuffers = mCore-&gt;mQueue.size() &gt; static_cast&lt;size_t&gt;(maxBufferCount); if (tooManyBuffers) &#123; BQ_LOGV("%s: queue size is %zu, waiting", caller, mCore-&gt;mQueue.size()); &#125; // If no buffer is found, or if the queue has too many buffers // outstanding, wait for a buffer to be acquired or released, or for the // max buffer count to change. //如果没有找到FREE的buffer，或者mQueue中有太多buffer占据内存未释放 //那么等待buffer被acquired or released； //或者等待最大buffer个数改变 tryAgain = (*found == BufferQueueCore::INVALID_BUFFER_SLOT) || tooManyBuffers; if (tryAgain) &#123; // Return an error if we're in non-blocking mode (producer and // consumer are controlled by the application). // However, the consumer is allowed to briefly acquire an extra // buffer (which could cause us to have to wait here), which is // okay, since it is only used to implement an atomic acquire + // release (e.g., in GLConsumer::updateTexImage()) if (mCore-&gt;mDequeueBufferCannotBlock &amp;&amp; (acquiredCount &lt;= mCore-&gt;mMaxAcquiredBufferCount)) &#123; return WOULD_BLOCK; &#125; //等待有buffer可以返回 mCore-&gt;mDequeueCondition.wait(mCore-&gt;mMutex); &#125; &#125; // while (tryAgain) return NO_ERROR;&#125; &#160; &#160; &#160; &#160;代码虽然多，但是大多数都是处理特殊情况的。核心就是从现存maxBufferCount找出FREE状态的slot，然后根据LRU匹配，取出最老的slot，然后返回index给found指针内容。&#160; &#160; &#160; &#160;这样取出来FREE的slot（如果能找到）。&#160; &#160; &#160; &#160;如果所有的buffer都不是free的，则等待。 &#160; &#160; &#160; &#160;2）将找到的buf所对应的状态修改为DEQUEUED； &#160; &#160; &#160; &#160;3）然后将上面查询mSlot中返回index的slot的Graphicbuffer赋给buffer变量； &#160; &#160; &#160; &#160;4）判断这个GraphicBuffer是否需要重新分配空间，判断条件就是buffer为空，因为它初始值是null，第一次使用它需要分配空间；如果不为null，但是buffer的width、height、format、usage属性跟要求的不一致，也要重新分配，并打上BUFFER_NEEDS_REALLOCATION标志位。 &#160; &#160; &#160; &#160;5）如果上面的buffer为null，则需要重新分配GraphicBuffer，调用BufferQueueCore中的GraphicBufferAlloc中函数createGraphicBuffer，生成一个new GraphicBuffer。 &#160; &#160; &#160; &#160;6）等待eglFence状态就绪。Fence机制我们以后会讲到。 创建图形缓冲区GraphicBuffer创建&#160; &#160; &#160; &#160;所以上述流程的核心是GraphicBuffer的分配，我们需要沿着上面继续分析。因此最后BufferQueueProducer中的dequeueBuffer函数中调用mCore-&gt;mAllocator的createGraphicBuffer函数就是调用了GraphicBufferAlloc的createGraphicBufferAlloc函数。mAllocator的类型为sp&lt; IGraphicBufferAlloc &gt;，要跨进程调用SurfaceFlinger的createGraphicBufferAlloc函数，上面讲过了。 &#160; &#160; &#160; &#160;上面刚刚说mAllocator的类型为sp&lt; IGraphicBufferAlloc &gt;，所以创建GraphicBuffer的函数createGraphicBuffer也需要Bp/Bn 的IPC跨进程调用过程。GraphicBuffer通过Binder传递对象是Parcel类型可序列化的，所以他要从模板类Flattenable派生。因为这个牵扯到内存缓冲区的fd传递到客户进程，所以我们放到后面讲。 &#160; &#160; &#160; &#160;所以我们先忽略IPC过程，直接查看生成结果，位于frameworks/native/libs/gui/GraphicBufferAlloc.cpp中：1234567891011121314151617sp&lt;GraphicBuffer&gt; GraphicBufferAlloc::createGraphicBuffer(uint32_t w, uint32_t h, PixelFormat format, uint32_t usage, status_t* error) &#123; //创建一个GraphicBuffer对象 sp&lt;GraphicBuffer&gt; graphicBuffer(new GraphicBuffer(w, h, format, usage)); status_t err = graphicBuffer-&gt;initCheck(); *error = err; if (err != 0 || graphicBuffer-&gt;handle == 0) &#123; if (err == NO_MEMORY) &#123; GraphicBuffer::dumpAllocationsToSystemLog(); &#125; ALOGE("GraphicBufferAlloc::createGraphicBuffer(w=%d, h=%d) " "failed (%s), handle=%p", w, h, strerror(-err), graphicBuffer-&gt;handle); return 0; &#125; return graphicBuffer;&#125; &#160; &#160; &#160; &#160;上面创建一个GraphicBuffer对象，我们看看GraphicBuffer的构造函数，位于frameworks/native/libs/ui/GraphicBuffer.cpp：1234567891011121314GraphicBuffer::GraphicBuffer(uint32_t w, uint32_t h, PixelFormat reqFormat, uint32_t reqUsage) : BASE(), mOwner(ownData), mBufferMapper(GraphicBufferMapper::get()), mInitCheck(NO_ERROR), mId(getUniqueId())&#123; width = height = stride = format = usage = 0; handle = NULL; //这里调用了initSize函数 mInitCheck = initSize(w, h, reqFormat, reqUsage);&#125; &#160; &#160; &#160; &#160;构造函数给一些变量赋了初值，并调用了initSize函数，我们看看这个函数：123456789101112131415status_t GraphicBuffer::initSize(uint32_t w, uint32_t h, PixelFormat format, uint32_t reqUsage)&#123; //获取一个GraphicBufferAllocator对象，它是个单例 GraphicBufferAllocator&amp; allocator = GraphicBufferAllocator::get(); //调用allocator 的alloc函数 status_t err = allocator.alloc(w, h, format, reqUsage, &amp;handle, &amp;stride); if (err == NO_ERROR) &#123; this-&gt;width = w; this-&gt;height = h; this-&gt;format = format; this-&gt;usage = reqUsage; &#125; return err;&#125; &#160; &#160; &#160; &#160;先创建一个GraphicBufferAllocator的单例对象，我们先看看它的构造函数，位于frameworks/native/libs/ui/GraphicBufferAllocator.cpp中：123456789101112131415alloc_device_t *mAllocDev;GraphicBufferAllocator::GraphicBufferAllocator() : mAllocDev(0)&#123; hw_module_t const* module; //加载gralloc设备 int err = hw_get_module(GRALLOC_HARDWARE_MODULE_ID, &amp;module); ALOGE_IF(err, "FATAL: can't find the %s module", GRALLOC_HARDWARE_MODULE_ID); if (err == 0) &#123; //打开gralloc设备 gralloc_open(module, &amp;mAllocDev); &#125;&#125; &#160; &#160; &#160; &#160;GraphicBufferAllocator构造函数里主要是加载了gralloc设备，然后打开它，因此mAllocDev指向了Gralloc模块。这一部分可以查看Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现，Gralloc模块的加载过程、Gralloc设备的打开过程。 &#160; &#160; &#160; &#160;加载完Gralloc模块，并打开之后，接着就是分配图形缓冲区了。我们可以继续往下看看是不是，查看GraphicBufferAllocator的alloc函数：1234567891011121314151617181920212223242526272829303132333435363738status_t GraphicBufferAllocator::alloc(uint32_t w, uint32_t h, PixelFormat format, int usage, buffer_handle_t* handle, int32_t* stride)&#123; ATRACE_CALL(); // make sure to not allocate a N x 0 or 0 x N buffer, since this is // allowed from an API stand-point allocate a 1x1 buffer instead. if (!w || !h) w = h = 1; // we have a h/w allocator and h/w buffer is requested status_t err; //这里内部是调用Gralloc模块中的函数gralloc_alloc来分配一块图形缓冲区 err = mAllocDev-&gt;alloc(mAllocDev, w, h, format, usage, handle, stride); ALOGW_IF(err, "alloc(%u, %u, %d, %08x, ...) failed %d (%s)", w, h, format, usage, err, strerror(-err)); if (err == NO_ERROR) &#123; Mutex::Autolock _l(sLock); KeyedVector&lt;buffer_handle_t, alloc_rec_t&gt;&amp; list(sAllocList); int bpp = bytesPerPixel(format); if (bpp &lt; 0) &#123; // probably a HAL custom format. in any case, we don't know // what its pixel size is. bpp = 0; &#125; alloc_rec_t rec; rec.w = w; rec.h = h; rec.s = *stride; rec.format = format; rec.usage = usage; rec.size = h * stride[0] * bpp; list.add(*handle, rec); &#125; return err;&#125; &#160; &#160; &#160; &#160;这里调用alloc分配了一块共享的内存缓冲区，就是我们的图形缓冲区。内部是调用Gralloc模块中的函数gralloc_alloc，可以产查看Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现，分配图形缓冲区。 &#160; &#160; &#160; &#160;这样我们GraphicBuffer的创建工作就完成了。 &#160; &#160; &#160; &#160;既然GraphicBuffer中的缓冲区是共享内存，我们知道使用共享内存需要传递共享内存的句柄fd。下面我们看看是如何传到客户进程的，填掉我们上面的坑。 内存缓冲区映射到应用进程&#160; &#160; &#160; &#160;GraphicBuffer类是从模板类Flattenable派生，这个派生类可以通过Parcel传递，通常派生类需要重载flatten和unflatten方法，用于对象的序列化和反序列化。 &#160; &#160; &#160; &#160;为了方便分析，我们可以看看GraphicBuffer的类图： &#160; &#160; &#160; &#160;上面说到mAllocator的类型为sp&lt; IGraphicBufferAlloc &gt;，它的创建GraphicBuffer的函数createGraphicBuffer也需要Bp/Bn 的IPC跨进程调用过程，序列化传递和反序列话取出这里用于IPC传递数据，我们可以在这里看看客户端和服务端的工作流程。&#160; &#160; &#160; &#160;我们先看看客户端Bp端，位于frameworks/native/libs/gui/IGraphicBufferAlloc.cpp中：12345678910111213141516171819202122232425262728293031323334class BpGraphicBufferAlloc : public BpInterface&lt;IGraphicBufferAlloc&gt;&#123;public: BpGraphicBufferAlloc(const sp&lt;IBinder&gt;&amp; impl) : BpInterface&lt;IGraphicBufferAlloc&gt;(impl) &#123; &#125; //客户端创建图形缓冲区函数 virtual sp&lt;GraphicBuffer&gt; createGraphicBuffer(uint32_t w, uint32_t h, PixelFormat format, uint32_t usage, status_t* error) &#123; Parcel data, reply; data.writeInterfaceToken(IGraphicBufferAlloc::getInterfaceDescriptor()); data.writeInt32(w); data.writeInt32(h); data.writeInt32(format); data.writeInt32(usage); //通过binder驱动，执行服务端对应函数。这里服务端是将创建的GraphicBuffer序列化后，塞给replay remote()-&gt;transact(CREATE_GRAPHIC_BUFFER, data, &amp;reply); sp&lt;GraphicBuffer&gt; graphicBuffer; status_t result = reply.readInt32(); if (result == NO_ERROR) &#123; //创建一个空壳子 graphicBuffer = new GraphicBuffer(); //然后客户端从服务端（SurfaceFlinger）返回的序列化后的replay， //反序列化出Graphicbuffer，放入空壳子中 result = reply.read(*graphicBuffer); // reply.readStrongBinder(); // here we don't even have to read the BufferReference from // the parcel, it'll die with the parcel. &#125; *error = result; return graphicBuffer; &#125;&#125;; &#160; &#160; &#160; &#160;客户端这里主要通过binder驱动，取出从服务端返回的序列化后的对象replay，然后自己造一个空壳子，然后读取replay信息，反序列化出GraphicBuffer。 &#160; &#160; &#160; &#160;那么服务端（SurfaceFlinger）创建GraphicBuffer并反序列化就在Bn端，我们看看Bn端实现，同样位于位于frameworks/native/libs/gui/IGraphicBufferAlloc.cpp中：123456789101112131415161718192021222324252627282930313233343536373839404142434445status_t BnGraphicBufferAlloc::onTransact( uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags)&#123; // codes that don't require permission check /* BufferReference just keeps a strong reference to a * GraphicBuffer until it is destroyed (that is, until * no local or remote process have a reference to it). */ class BufferReference : public BBinder &#123; sp&lt;GraphicBuffer&gt; buffer; public: BufferReference(const sp&lt;GraphicBuffer&gt;&amp; buffer) : buffer(buffer) &#123; &#125; &#125;; switch(code) &#123; case CREATE_GRAPHIC_BUFFER: &#123; CHECK_INTERFACE(IGraphicBufferAlloc, data, reply); uint32_t w = data.readInt32(); uint32_t h = data.readInt32(); PixelFormat format = data.readInt32(); uint32_t usage = data.readInt32(); status_t error; //调用GraphicBufferAlloc的createGraphicBuffer函数，创建一个GraphicBuffer实例 sp&lt;GraphicBuffer&gt; result = createGraphicBuffer(w, h, format, usage, &amp;error); reply-&gt;writeInt32(error); if (result != 0) &#123; ////GraphicBuffer进行序列化，传给client reply-&gt;write(*result); // We add a BufferReference to this parcel to make sure the // buffer stays alive until the GraphicBuffer object on // the other side has been created. // This is needed so that the buffer handle can be // registered before the buffer is destroyed on implementations // that do not use file-descriptors to track their buffers. reply-&gt;writeStrongBinder( new BufferReference(result) ); &#125; return NO_ERROR; &#125; break; default: return BBinder::onTransact(code, data, reply, flags); &#125;&#125; &#160; &#160; &#160; &#160;GraphicBufferAlloc的createGraphicBuffer函数我们上面分析过了。这样就将创建的GraphicBuffer实例序列化后，在通过binder驱动传给client。 &#160; &#160; &#160; &#160;我们知道，一个对象要在进程间传输必须继承于Flattenable类，并且实现flatten和unflatten方法，flatten方法用于序列化该对象，unflatten方法用于反序列化对象。 &#160; &#160; &#160; &#160;上面说到GraphicBuffer中的缓冲区是共享内存，使用共享内存需要传递共享内存的句柄fd，那么这个fd的传递就应该在GraphicBuffer的序列化/反序列化中，我们就来看看这个过程。 &#160; &#160; &#160; &#160;这里我先回顾一下Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现的内容： &#160; &#160; &#160; &#160;图形缓冲区可以从系统帧缓冲区分配也可以从内存中分配，分配一个图形缓冲区后还需要将该图形缓冲区映射到分配该buffer的进程地址空间来，在Android系统中，图形缓冲区的管理由SurfaceFlinger服务来负责。在系统帧缓冲区中分配的图形缓冲区是在SurfaceFlinger服务中使用，而在内存中分配的图形缓冲区既可以在SurfaceFlinger服务中使用，也可以在其它的应用程序中使用。当其它的应用程序需要使用图形缓冲区的时候，它们就会请求SurfaceFlinger服务为它们分配并将SurfaceFlinger服务返回来的图形缓冲区映射到应用程序进程地址空间。在从内存中分配buffer时，已经将分配的buffer映射到了SurfaceFlinger服务进程地址空间，如果该buffer是应用程序请求SurfaceFlinger服务为它们分配的，那么还需要将SurfaceFlinger服务返回来的图形缓冲区映射到应用程序进程地址空间。 &#160; &#160; &#160; &#160;GraphicBuffer类从模板类Flattenable派生，这个派生类可以通过Parcel传递，通常派生类需要重载flatten和unflatten方法，用于对象的序列化和反序列化。 &#160; &#160; &#160; &#160;1）将一个对象写入到Parcel中，需要使用flatten函数序列化该对象，我们先来看下flatten函数：12345678910111213141516171819202122232425262728293031323334353637status_t GraphicBuffer::flatten(void*&amp; buffer, size_t&amp; size, int*&amp; fds, size_t&amp; count) const &#123; size_t sizeNeeded = GraphicBuffer::getFlattenedSize(); if (size &lt; sizeNeeded) return NO_MEMORY; size_t fdCountNeeded = GraphicBuffer::getFdCount(); if (count &lt; fdCountNeeded) return NO_MEMORY; int32_t* buf = static_cast&lt;int32_t*&gt;(buffer); buf[0] = 'GBFR'; buf[1] = width; buf[2] = height; buf[3] = stride; buf[4] = format; buf[5] = usage; buf[6] = static_cast&lt;int32_t&gt;(mId &gt;&gt; 32); buf[7] = static_cast&lt;int32_t&gt;(mId &amp; 0xFFFFFFFFull); buf[8] = 0; buf[9] = 0; if (handle) &#123; buf[8] = handle-&gt;numFds; buf[9] = handle-&gt;numInts; native_handle_t const* const h = handle; //把handle中的data复制到fds中 memcpy(fds, h-&gt;data, h-&gt;numFds*sizeof(int)); memcpy(&amp;buf[10], h-&gt;data + h-&gt;numFds, h-&gt;numInts*sizeof(int)); &#125; buffer = reinterpret_cast&lt;void*&gt;(static_cast&lt;int*&gt;(buffer) + sizeNeeded); size -= sizeNeeded; if (handle) &#123; fds += handle-&gt;numFds; count -= handle-&gt;numFds; &#125; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;这个handle类型为native_handle_t ，且typedef成了buffer_handle_t，如果忘记了这一部分，可以查看Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现。我们贴一下它的定义：1234567typedef struct native_handle &#123; int version; //设置为结构体native_handle_t的大小，用来标识结构体native_handle_t的版本 int numFds; //表示结构体native_handle_t所包含的文件描述符的个数，这些文件描述符保存在成员变量data所指向的一块缓冲区中。 int numInts; //表示结构体native_handle_t所包含的整数值的个数，这些整数保存在成员变量data所指向的一块缓冲区中。 int data[0]; //指向的一块缓冲区中 &#125; native_handle_t; &#160; &#160; &#160; &#160;所以我们回到flatten函数中，fds参数用来传递文件句柄，函数把handle中的表示指向图形缓冲区文件描述符句柄复制到fds中，因此这些句柄就能通过binder传递到目标进程中去。 &#160; &#160; &#160; &#160;2）在应用程序读取来自服务进程的GraphicBuffer对象时，也就是result = reply.read(*p)，会调用GraphicBuffer类的unflatten函数进行反序列化过程：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162status_t GraphicBuffer::unflatten( void const*&amp; buffer, size_t&amp; size, int const*&amp; fds, size_t&amp; count) &#123; if (size &lt; 8*sizeof(int)) return NO_MEMORY; int const* buf = static_cast&lt;int const*&gt;(buffer); if (buf[0] != 'GBFR') return BAD_TYPE; const size_t numFds = buf[8]; const size_t numInts = buf[9]; const size_t sizeNeeded = (10 + numInts) * sizeof(int); if (size &lt; sizeNeeded) return NO_MEMORY; size_t fdCountNeeded = 0; if (count &lt; fdCountNeeded) return NO_MEMORY; if (handle) &#123; // free previous handle if any free_handle(); &#125; if (numFds || numInts) &#123; width = buf[1]; height = buf[2]; stride = buf[3]; format = buf[4]; usage = buf[5]; //创建一个native_handle对象 native_handle* h = native_handle_create(numFds, numInts); //将fds复制到native_handle对象的data中，和flatten操作相反 memcpy(h-&gt;data, fds, numFds*sizeof(int)); memcpy(h-&gt;data + numFds, &amp;buf[10], numInts*sizeof(int)); handle = h; &#125; else &#123; width = height = stride = format = usage = 0; handle = NULL; &#125; mId = static_cast&lt;uint64_t&gt;(buf[6]) &lt;&lt; 32; mId |= static_cast&lt;uint32_t&gt;(buf[7]); mOwner = ownHandle; if (handle != 0) &#123; //使用GraphicBufferMapper将服务端创建的图形缓冲区映射到当前进程地址空间 status_t err = mBufferMapper.registerBuffer(handle); if (err != NO_ERROR) &#123; width = height = stride = format = usage = 0; handle = NULL; ALOGE("unflatten: registerBuffer failed: %s (%d)", strerror(-err), err); return err; &#125; &#125; buffer = reinterpret_cast&lt;void const*&gt;(static_cast&lt;int const*&gt;(buffer) + sizeNeeded); size -= sizeNeeded; fds += numFds; count -= numFds; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;调用unflatten函数时，共享区的文件句柄已经准备好了，但是内存还没有进行映射，调用了mBufferMapper.registerBuffer函数来进行内存映射。&#160; &#160; &#160; &#160;GraphicBufferMapper是单例的模板类派生的，mBufferMapper对象在GraphicBuffer的构造函数中初始化赋值了，上面我们列出过了。我们看看GraphicBufferMapper的构造函数，位于frameworks/native/libs/ui/GraphicBufferMapper.cpp：123456789101112GraphicBufferMapper::GraphicBufferMapper() : mAllocMod(0)&#123; hw_module_t const* module; //加载gralloc模块 int err = hw_get_module(GRALLOC_HARDWARE_MODULE_ID, &amp;module); ALOGE_IF(err, "FATAL: can't find the %s module", GRALLOC_HARDWARE_MODULE_ID); if (err == 0) &#123; ////将hw_module_t的指针转换为gralloc_module_t类型指针 mAllocMod = (gralloc_module_t const *)module; &#125;&#125; &#160; &#160; &#160; &#160;这里根据模块ID加载Gralloc模块，并得到Gralloc模块的HMI符号首地址，并强制转换为gralloc_module_t类型指针。 &#160; &#160; &#160; &#160;我们接着看下GraphicBufferMapper::registerBuffer函数：1234567891011status_t GraphicBufferMapper::registerBuffer(buffer_handle_t handle)&#123; ATRACE_CALL(); status_t err; //将服务端创建的图形缓冲区映射到当前进程地址空间 err = mAllocMod-&gt;registerBuffer(mAllocMod, handle); ALOGW_IF(err, "registerBuffer(%p) failed %d (%s)", handle, err, strerror(-err)); return err;&#125; &#160; &#160; &#160; &#160;调用了mAllocMod的registerBuffer函数，mAllocMod同样是指向了Gralloc模块的指针，因此实际是调用了Gralloc模块的gralloc_register_buffer函数。这个函数就是调用了mmap来进行共享内存的映射。&#160; &#160; &#160; &#160;这一部分仍然可以查看Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现。我们仍然回顾一下以前的内容： &#160; &#160; &#160; &#160;registerBuffer和unregisterBuffer分别用来注册和注销一个指定的图形缓冲区，所谓注册图形缓冲区，实际上就是将一块图形缓冲区映射到一个进程的地址空间去，而注销图形缓冲区就是执行相反的操作。lock和unlock分别用来锁定和解锁一个指定的图形缓冲区，在访问一块图形缓冲区的时候，例如，向一块图形缓冲写入内容的时候，需要将该图形缓冲区锁定，用来避免访问冲突。在锁定一块图形缓冲区的时候，可以指定要锁定的图形绘冲区的位置以及大小，这是通过参数l、t、w和h来指定的，其中，参数l和t指定的是要访问的图形缓冲区的左上角位置，而参数w和h指定的是要访问的图形缓冲区的宽度和长度。锁定之后，就可以获得由参数参数l、t、w和h所圈定的一块缓冲区的起始地址，保存在输出参数vaddr中。另一方面，在访问完成一块图形缓冲区之后，需要解除这块图形缓冲区的锁定。 &#160; &#160; &#160; &#160;这就是内存缓冲区映射到应用进程的过程。因为需要映射共享内存，所以必须要序列化和反序列Pracel对象。如下图： &#160; &#160; &#160; &#160;这里我们创建GraphicBuffer流程就分析完了。 小结&#160; &#160; &#160; &#160;本节主要分析GraphicBuffer的创建过程，下一节我们分析对它的管理流程。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(六)----SurfaceFlinger创建Surface]]></title>
    <url>%2F2017%2F06%2F17%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E5%85%AD-SurfaceFlinger%E5%88%9B%E5%BB%BASurface%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;这次需要了解一下SurfaceFlinger创建Surface的流程，可能比较短，因为后面GraphicBuffer管理可能会比较多。另外，应用层创建Surface以后会继续学习，并且流畅会比较长，因为设计到了WMS等等。 图层分析&#160; &#160; &#160; &#160;在分析Android应用程序请求SurfaceFlinger创建Surface之前，我们首先了解一下Surface是由什么组成的。我们可以将Surface理解为一个绘图表面，Android应用程序负责往这个绘图表面上填内容，而SurfaceFlinger服务负责将这个绘图表面的内容取出来，并且渲染在显示屏上。 猜测&#160; &#160; &#160; &#160;每个应用程序对应着一个或者多个图形界面，而每一个界面我们称之为surface，或者说是window，如下图： &#160; &#160; &#160; &#160;上图我们可以看到三个surface，在这里我们需要弄明白的问题是： 每个surface在屏幕上有它的位置、大小，然后每个surface 里面还有要显示的内容，内容、大小、位置 这些元素，在我们改变应用程序的时候都可能会改变，改变时应该如何处理？ 然后就各个surface 之间可能有重叠，比如说在上面的简略图中，灰色覆盖了橙色，绿色覆盖了橙色 ，而且还具有一定透明度。这种层之间的关系应该如何描述？ &#160; &#160; &#160; &#160;对于第一个问题，我们需要一个结构来记录应用程序界面的位置，大小，以及一个buffer 来记录需要显示的内容，所以这就是我们surface 的概念，surface 实际我们可以把它理解成一个容器，这个容器记录着应用程序界面的控制信息，比如说大小啊，位置啊，而它还有buffer 来专门存储需要显示的内容。&#160; &#160; &#160; &#160;对于第二个问题，我们可以想象在屏幕平面的垂直方向还有一个Z 轴，所有的surface 根据在Z 轴上的坐标来确定前后，这样就可以描述各个surface 之间的上下覆盖关系了，而这个在Z 轴上的顺序，图形上有个专业术语叫Z-order 。 &#160; &#160; &#160; &#160;在这里还存在一个问题，那就是当存在图形重合的时候应该如何处理呢，而且可能有些surface 还带有透明信息，这里就是我们SurfaceFlinger 需要解决问题，它要把各个surface 组合(compose/merge) 成一个main Surface ，最后将Main Surface 的内容发送给FB，这样屏幕上就能看到我们想要的效果。 概述&#160; &#160; &#160; &#160;在Android中，Window与Surface一一对应。 如果说Window关心的是层次和布局，是从设计者角度定义的类，Surface则从实现角度出发，是工程师关系和考虑的类。Window的内容是变化 的，Surface需要有空间来记录每个时刻Window的内容。在Android的SurfaceFlinger实现里，通常一个Surface有两块 Buffer, 一块用于绘画，一块用于显示，两个Buffer按照固定的频率进行交换，从而实现Window的动态刷新。 &#160; &#160; &#160; &#160;但是，在SurfaceFlinger服务这一侧，绘图表面使用Layer类来描述。Layer是SurfaceFlinger 进行合成的基本操作单元。Layer在应用请求创建Surface的时候在SurfaceFlinger内部创建，因此一个Surface对应一个 Layer, 但注意，Surface不一定对应于Window，Android中有些Surface并不跟某个Window相关，而是有程序直接创建，比如说 StrictMode， 一块红色的背景，用于提示示Java代码中的一些异常, 还有SurfaceView, 用于显示有硬件输出的视频内容等。 &#160; &#160; &#160; &#160;Each Layer has： Z order Alpha value from 0 to 255 visibleRegion crop region transformation: rotate 0, 90, 180, 270: flip H, V: scale &#160; &#160; &#160; &#160;当多个Layer进行合成的时候，并不是整个Layer的空间都会被完全显示，根据这个Layer最终的显示效果，一个Layer可以被划分成很多的Region, Android SurfaceFlinger 定义了以下一些Region类型： TransparantRegion： 完全透明的区域，在它之下的区域将被显示出来。 OpaqueRegion: 完全不透明的区域，是否显示取决于它上面是否有遮挡或是否透明。 VisibleRegion: 可见区域，包括完全不透明无遮挡区域或半透明区域。 visibleRegion = Region - above OpaqueRegion. CoveredRegion: 被遮挡区域，在它之上，有不透明或半透明区域。 DirtyRegion: 可见部分改变区域，包括新的被遮挡区域，和新的露出区域。 &#160; &#160; &#160; &#160;Android 系统支持多种显示设备，比如说，输出到手机屏幕，或者通过WiFi 投射到电视屏幕。Android用DisplayDevice类来表示这样的设备。不是所有的Layer都会输出到所有的Display, 比如说，我们可以只将Video Layer投射到电视， 而非整个屏幕。LayerStack 就是为此设计，LayerStack 是一个Display 对象的一个数值， 而类Layer里成员State结构体也有成员变量mLayerStack， 只有两者的mLayerStack 值相同，Layer才会被输出到给该Display设备。所以LayerStack 决定了每个Display设备上可以显示的Layer数目。 &#160; &#160; &#160; &#160;SurfaceFlinger的工作内容，就是定期检查所有Layer的参数更新（LayerStack等），计算新的DirtyRegion，然后将结果推送给底层显示驱动进行显示。这里面有很多的细节，我们将在后续会专门研究。 &#160; &#160; &#160; &#160;上面描述的几个概念，均是针对于显示这个层面，更多是涉及到中下层模块，应用层并不参与也无需关心。对于应用而言，它关心的是如何将内容画出来。Canvas 是Java层定义的一个类，它对应与Surface上的某个区域并提供了很多的2D绘制函数（借助于底层的Skia或OpenGL)。应用只需通过 LockCanvas() 来获取一个Canvas对象，并调用它的绘画方法，然后 unLockCanvasAndPost（）来通知底层将更新内容进行显示。当然，并不是所有应用程序都需要直接操作Canva， 事实上只有少量应用需要直接操作Canvas, Android提供了很多封装好的控件 Widget，应用只需提供素材，如文字，图片，属性等等，这些控件会调用Canvas提供的接口帮用户完成绘制工作。 &#160; &#160; &#160; &#160;SurfaceFlinger 是一个独立的Service， 它接收所有Window的Surface作为输入，根据Z-Order， 透明度，大小，位置等参数，计算出每个Surface在最终合成图像中的位置，然后交由HWComposer或OpenGL生成最终的显示Buffer, 然后显示到特定的显示设备上。 Surface创建流程SurfaceFlinger服务侧Layer创建&#160; &#160; &#160; &#160;上面我们提，到在SurfaceFlinger服务这一侧，绘图表面使用Layer类来描述。 &#160; &#160; &#160; &#160;Layer类内部定义了两个结构体Geometry、State，位于frameworks/native/services/surfaceflinger/Layer.h中：12345678910111213141516171819202122232425262728struct Geometry &#123; uint32_t w; uint32_t h; Rect crop; inline bool operator ==(const Geometry&amp; rhs) const &#123; return (w == rhs.w &amp;&amp; h == rhs.h &amp;&amp; crop == rhs.crop); &#125; inline bool operator !=(const Geometry&amp; rhs) const &#123; return !operator ==(rhs); &#125;&#125;;struct State &#123; Geometry active; Geometry requested; uint32_t z; uint32_t layerStack; uint8_t alpha; uint8_t flags; uint8_t reserved[2]; int32_t sequence; // changes when visible regions can change Transform transform; // the transparentRegion hint is a bit special, it's latched only // when we receive a buffer -- this is because it's "content" // dependent. Region activeTransparentRegion; Region requestedTransparentRegion;&#125;; &#160; &#160; &#160; &#160;用变量mCurrentState和mDrawingState连个类型为State的成员变量保存当前和上一次的绘制状态，记录大小、可视区域、透明度、标志位、z-order等信息。 &#160; &#160; &#160; &#160;我们可以回顾一下以前 Android SurfaceFlinger 学习之路(三)—-Android开机动画流程简述 ，开机动画中创建surface流程位于BootAnimation.cpp的readyToRun函数，位于frameworks/base/cmds/bootanimation/BootAnimation.cpp中：12345678910111213141516171819202122status_t BootAnimation::readyToRun() &#123; ...... // create the native surface sp&lt;SurfaceControl&gt; control = session()-&gt;createSurface(String8("BootAnimation"), dinfo.w, dinfo.h, PIXEL_FORMAT_RGB_565); SurfaceComposerClient::openGlobalTransaction(); control-&gt;setLayer(0x40000000); SurfaceComposerClient::closeGlobalTransaction(); sp&lt;Surface&gt; s = control-&gt;getSurface(); ......&#125;sp&lt;SurfaceComposerClient&gt; BootAnimation::session() const &#123; return mSession;&#125;BootAnimation::BootAnimation() : Thread(false), mZip(NULL)&#123; mSession = new SurfaceComposerClient();&#125; &#160; &#160; &#160; &#160;这里session函数返回一个SurfaceComposerClient对象，在Android SurfaceFlinger 学习之路(四)—-SurfaceFlinger服务的启动与连接过程一文中，我们已经看到过SurfaceComposerClient类的作用了，Android应用程序主要就是通过它来和SurfaceFlinger服务建立连接的，连接的结果就是得到一个类型为Client的Binder代理对象，保存它的成员变量mClient中。 &#160; &#160; &#160; &#160;我们查看SurfaceComposerClient的createSurface函数，位于frameworks/native/libs/gui/SurfaceComposerClient.cpp中：123456789101112131415161718192021sp&lt;SurfaceControl&gt; SurfaceComposerClient::createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags)&#123; sp&lt;SurfaceControl&gt; sur; if (mStatus == NO_ERROR) &#123; sp&lt;IBinder&gt; handle; sp&lt;IGraphicBufferProducer&gt; gbp; //我们先分析这里，里面有创建Layer的部分 status_t err = mClient-&gt;createSurface(name, w, h, format, flags, &amp;handle, &amp;gbp); ALOGE_IF(err, "SurfaceComposerClient::createSurface error %s", strerror(-err)); if (err == NO_ERROR) &#123; sur = new SurfaceControl(this, handle, gbp); &#125; &#125; return sur;&#125; &#160; &#160; &#160; &#160;因此又要到Client的createSurface函数中，位于frameworks/native/services/surfaceflinger/Client.cpp中：123456789101112131415161718192021222324252627282930313233343536373839404142434445status_t Client::createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp)&#123; /* * createSurface must be called from the GL thread so that it can * have access to the GL context. */ class MessageCreateLayer : public MessageBase &#123; SurfaceFlinger* flinger; Client* client; sp&lt;IBinder&gt;* handle; sp&lt;IGraphicBufferProducer&gt;* gbp; status_t result; const String8&amp; name; uint32_t w, h; PixelFormat format; uint32_t flags; public: MessageCreateLayer(SurfaceFlinger* flinger, const String8&amp; name, Client* client, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp) : flinger(flinger), client(client), handle(handle), gbp(gbp), name(name), w(w), h(h), format(format), flags(flags) &#123; &#125; status_t getResult() const &#123; return result; &#125; virtual bool handler() &#123; //给SurfaceFlinger的MessageQueue发送同步消息，createLayer函数被调用 result = flinger-&gt;createLayer(name, client, w, h, format, flags, handle, gbp); return true; &#125; &#125;; sp&lt;MessageBase&gt; msg = new MessageCreateLayer(mFlinger.get(), name, this, w, h, format, flags, handle, gbp); mFlinger-&gt;postMessageSync(msg); return static_cast&lt;MessageCreateLayer*&gt;( msg.get() )-&gt;getResult();&#125; &#160; &#160; &#160; &#160;给SurfaceFlinger的MessageQueue发送同步消息，createLayer函数被调用，位于frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp中：12345678910111213141516171819202122232425262728293031323334353637383940414243status_t SurfaceFlinger::createLayer( const String8&amp; name, const sp&lt;Client&gt;&amp; client, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp)&#123; //ALOGD("createLayer for (%d x %d), name=%s", w, h, name.string()); if (int32_t(w|h) &lt; 0) &#123; ALOGE("createLayer() failed, w or h is negative (w=%d, h=%d)", int(w), int(h)); return BAD_VALUE; &#125; status_t result = NO_ERROR; sp&lt;Layer&gt; layer; switch (flags &amp; ISurfaceComposerClient::eFXSurfaceMask) &#123; case ISurfaceComposerClient::eFXSurfaceNormal: //创建普通的Layer result = createNormalLayer(client, name, w, h, flags, format, handle, gbp, &amp;layer); break; case ISurfaceComposerClient::eFXSurfaceDim: ////创建模糊的Layer result = createDimLayer(client, name, w, h, flags, handle, gbp, &amp;layer); break; default: result = BAD_VALUE; break; &#125; if (result == NO_ERROR) &#123; //将创建的Layer按顺序添加进list中，我们以后讲z-order会讲 addClientLayer(client, *handle, *gbp, layer); //修改标志位 setTransactionFlags(eTransactionNeeded); &#125; return result;&#125; &#160; &#160; &#160; &#160;这里会根据传进来的flag判断创建什么类型的Layer，一个是普通的，一个是模糊的Layer。这里我们只看看普通的Layer，createNormalLayer函数。123456789101112131415161718192021222324252627status_t SurfaceFlinger::createNormalLayer(const sp&lt;Client&gt;&amp; client, const String8&amp; name, uint32_t w, uint32_t h, uint32_t flags, PixelFormat&amp; format, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp, sp&lt;Layer&gt;* outLayer)&#123; // initialize the surfaces switch (format) &#123;//像素格式 case PIXEL_FORMAT_TRANSPARENT: case PIXEL_FORMAT_TRANSLUCENT: format = PIXEL_FORMAT_RGBA_8888; break; case PIXEL_FORMAT_OPAQUE: format = PIXEL_FORMAT_RGBX_8888; break; &#125; //创建一个Layer对象 *outLayer = new Layer(this, client, name, w, h, flags); //调用Layer的setBuffers函数，这只一些变量 status_t err = (*outLayer)-&gt;setBuffers(w, h, format, flags); if (err == NO_ERROR) &#123; //给成员变量handle和gbp赋值 *handle = (*outLayer)-&gt;getHandle(); *gbp = (*outLayer)-&gt;getProducer(); &#125; ALOGE_IF(err, "createNormalLayer() failed (%s)", strerror(-err)); return err;&#125; &#160; &#160; &#160; &#160;这里创建了一个Layer对象，我们可以看看Layer的构造函数，位于frameworks/native/services/surfaceflinger/Layer.cpp中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960Layer::Layer(SurfaceFlinger* flinger, const sp&lt;Client&gt;&amp; client, const String8&amp; name, uint32_t w, uint32_t h, uint32_t flags) : contentDirty(false), sequence(uint32_t(android_atomic_inc(&amp;sSequence))), mFlinger(flinger), mTextureName(-1U), mPremultipliedAlpha(true), mName("unnamed"), mDebug(false), mFormat(PIXEL_FORMAT_NONE), mTransactionFlags(0), mQueuedFrames(0), mSidebandStreamChanged(false), mCurrentTransform(0), mCurrentScalingMode(NATIVE_WINDOW_SCALING_MODE_FREEZE), mCurrentOpacity(true), mRefreshPending(false), mFrameLatencyNeeded(false), mFiltering(false), mNeedsFiltering(false), mMesh(Mesh::TRIANGLE_FAN, 4, 2, 2), mSecure(false), mProtectedByApp(false), mHasSurface(false), mClientRef(client), mPotentialCursor(false)&#123; mCurrentCrop.makeInvalid(); mFlinger-&gt;getRenderEngine().genTextures(1, &amp;mTextureName); mTexture.init(Texture::TEXTURE_EXTERNAL, mTextureName); uint32_t layerFlags = 0; if (flags &amp; ISurfaceComposerClient::eHidden) layerFlags |= layer_state_t::eLayerHidden; if (flags &amp; ISurfaceComposerClient::eOpaque) layerFlags |= layer_state_t::eLayerOpaque; if (flags &amp; ISurfaceComposerClient::eNonPremultiplied) mPremultipliedAlpha = false; mName = name; mCurrentState.active.w = w; mCurrentState.active.h = h; mCurrentState.active.crop.makeInvalid(); mCurrentState.z = 0; mCurrentState.alpha = 0xFF; mCurrentState.layerStack = 0; mCurrentState.flags = layerFlags; mCurrentState.sequence = 0; mCurrentState.transform.set(0, 0); mCurrentState.requested = mCurrentState.active; // drawing state &amp; current state are identical mDrawingState = mCurrentState; nsecs_t displayPeriod = flinger-&gt;getHwComposer().getRefreshPeriod(HWC_DISPLAY_PRIMARY); mFrameTracker.setDisplayRefreshPeriod(displayPeriod);&#125; &#160; &#160; &#160; &#160;Layer的构造函数中就是给一些变量赋了初值，事情不多。&#160; &#160; &#160; &#160;我们从上面的类图看到Layer间接继承于RefBase类，所以对象第一次被赋值给强指针会调用onFirstRef函数，我们看看它里面做了那些事情：123456789101112131415161718192021222324252627void Layer::onFirstRef() &#123; // Creates a custom BufferQueue for SurfaceFlingerConsumer to use sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferConsumer&gt; consumer; //BufferQueue创建图形缓冲区管理成员，我们以后分析图形缓冲区管理会讲到 BufferQueue::createBufferQueue(&amp;producer, &amp;consumer); //mProducer 不为空了，赋值 mProducer = new MonitoredProducer(producer, mFlinger); //mSurfaceFlingerConsumer 不为空了，赋值 mSurfaceFlingerConsumer = new SurfaceFlingerConsumer(consumer, mTextureName); //设置消费者相关设置 mSurfaceFlingerConsumer-&gt;setConsumerUsageBits(getEffectiveUsage(0)); mSurfaceFlingerConsumer-&gt;setContentsChangedListener(this); mSurfaceFlingerConsumer-&gt;setName(mName); // TARGET_DISABLE_TRIPLE_BUFFERING为false，所以使用了三缓冲，project butter计划嘛#ifdef TARGET_DISABLE_TRIPLE_BUFFERING#warning "disabling triple buffering" mSurfaceFlingerConsumer-&gt;setDefaultMaxBufferCount(2);#else mSurfaceFlingerConsumer-&gt;setDefaultMaxBufferCount(3);#endif //获取默认显示器 const sp&lt;const DisplayDevice&gt; hw(mFlinger-&gt;getDefaultDisplayDevice()); //更新显示图像方向 updateTransformHint(hw);&#125; &#160; &#160; &#160; &#160;onFirstRef函数中做的事情主要是创建图形缓冲区管理成员相关，这个我们后面会讲到。&#160; &#160; &#160; &#160;然后就是根据“黄油计划”定义了三缓冲，以及显示器方向的调整。 &#160; &#160; &#160; &#160;Layer对象创建完后，就调用Layer的setBuffers函数：1234567891011121314151617181920212223242526status_t Layer::setBuffers( uint32_t w, uint32_t h, PixelFormat format, uint32_t flags)&#123; uint32_t const maxSurfaceDims = min( mFlinger-&gt;getMaxTextureSize(), mFlinger-&gt;getMaxViewportDims()); // never allow a surface larger than what our underlying GL implementation // can handle. if ((uint32_t(w)&gt;maxSurfaceDims) || (uint32_t(h)&gt;maxSurfaceDims)) &#123; ALOGE("dimensions too large %u x %u", uint32_t(w), uint32_t(h)); return BAD_VALUE; &#125; mFormat = format; mPotentialCursor = (flags &amp; ISurfaceComposerClient::eCursorWindow) ? true : false; mSecure = (flags &amp; ISurfaceComposerClient::eSecure) ? true : false; mProtectedByApp = (flags &amp; ISurfaceComposerClient::eProtectedByApp) ? true : false; mCurrentOpacity = getOpacityForFormat(format); mSurfaceFlingerConsumer-&gt;setDefaultBufferSize(w, h); mSurfaceFlingerConsumer-&gt;setDefaultBufferFormat(format); mSurfaceFlingerConsumer-&gt;setConsumerUsageBits(getEffectiveUsage(0)); return NO_ERROR;&#125; &#160; &#160; &#160; &#160;这里也是一些变量的设置，东西不多。&#160; &#160; &#160; &#160;然后就是给gbp和handle赋值。gbp已经不为空了，我们在onFirstRef函数中对它付了值。那么我们看看给handle赋值，Layer的getHandle函数：123456789101112131415161718192021222324252627sp&lt;IBinder&gt; Layer::getHandle() &#123; Mutex::Autolock _l(mLock); LOG_ALWAYS_FATAL_IF(mHasSurface, "Layer::getHandle() has already been called"); mHasSurface = true; /* * The layer handle is just a BBinder object passed to the client * (remote process) -- we don't keep any reference on our side such that * the dtor is called when the remote side let go of its reference. * * LayerCleaner ensures that mFlinger-&gt;onLayerDestroyed() is called for * this layer when the handle is destroyed. */ class Handle : public BBinder, public LayerCleaner &#123; wp&lt;const Layer&gt; mOwner; public: Handle(const sp&lt;SurfaceFlinger&gt;&amp; flinger, const sp&lt;Layer&gt;&amp; layer) : LayerCleaner(flinger, layer), mOwner(layer) &#123; &#125; &#125;; return new Handle(mFlinger, this);&#125; &#160; &#160; &#160; &#160;只是新建一个Handle，而这个Handle只是一个Binder的实现，就是标识Surface的全局唯一性，没有什么实际的内容。 &#160; &#160; &#160; &#160;上述就是Layer的创建，是基于SurfaceFlinger服务端这一侧。 应用侧Surface创建&#160; &#160; &#160; &#160;我们继续回到上面的SurfaceComposerClient的createSurface函数，为了不往上翻我再贴一遍：12345678910111213141516171819202122sp&lt;SurfaceControl&gt; SurfaceComposerClient::createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags)&#123; sp&lt;SurfaceControl&gt; sur; if (mStatus == NO_ERROR) &#123; sp&lt;IBinder&gt; handle; sp&lt;IGraphicBufferProducer&gt; gbp; //上面分析完了SurfaceFlinger创建完了一个Layer status_t err = mClient-&gt;createSurface(name, w, h, format, flags, &amp;handle, &amp;gbp); ALOGE_IF(err, "SurfaceComposerClient::createSurface error %s", strerror(-err)); if (err == NO_ERROR) &#123; //根据上面创建Layer时创建的handle和gbp再创建一个SurfaceControl，并返回 sur = new SurfaceControl(this, handle, gbp); &#125; &#125; return sur;&#125; &#160; &#160; &#160; &#160;上面分析完了SurfaceFlinger创建完了一个Layer，接着根据上面创建Layer时创建的handle和gbp再创建一个SurfaceControl，并返回。 SurfaceControl创建&#160; &#160; &#160; &#160;在Android应用程序这一侧，每一个绘图表面都使用一个Surface对象来描述，每一个Surface对象都是由一个SurfaceControl对象来创建的。Surface类和SurfaceControl类的关系以及实现如图所示： &#160; &#160; &#160; &#160;SurfaceControl类的成员变量mClient是一个类型为SurfaceComposerClient对象，在Android SurfaceFlinger 学习之路(四)—-SurfaceFlinger服务的启动与连接过程一文中，我们已经看到过SurfaceComposerClient类的作用了，Android应用程序主要就是通过它来和SurfaceFlinger服务建立连接的，连接的结果就是得到一个类型为Client的Binder代理对象，保存它的成员变量mClient中。 &#160; &#160; &#160; &#160;SurfaceControl类的成员变量mHandle是指向的就是我们上面分析的创建Layer时，最后将Layer和SurfaceFlinger作为构造函数的参数创建一个Handle对象，这个Handle是一个Binder的实现，就是标识Surface的全局唯一性。当Android应用程序请求SurfaceFlinger服务创建一个绘图表面的时候，SurfaceFlinger服务就会在内部创建一个Layer对象，然后将Layer和SurfaceFlinger包装成一个Handle的一个Binder代理对象返回来给Android应用程序，然后Android应用程序再将这个Binder代理对象保存在一个SurfaceControl对象的成员变量mHandle中。 &#160; &#160; &#160; &#160;SurfaceControl类的成员变量mSurfaceData是一个类型为Surface的强指针，它指向了一个Surface对象。 &#160; &#160; &#160; &#160;SurfaceControl的构造函数也比较简单，位于frameworks/native/libs/gui/SurfaceControl.cpp中：1234567SurfaceControl::SurfaceControl( const sp&lt;SurfaceComposerClient&gt;&amp; client, const sp&lt;IBinder&gt;&amp; handle, const sp&lt;IGraphicBufferProducer&gt;&amp; gbp) : mClient(client), mHandle(handle), mGraphicBufferProducer(gbp)&#123;&#125; &#160; &#160; &#160; &#160;SurfaceControl类创建就到这里。 &#160; &#160; &#160; &#160;附：这是native层的SurfaceControl，对于java层，也有一个SurfaceControl，Android 4.3 里新引进的类。Google从之前的Surface类里拆出部分接口，变成SurfaceControl，为什么要这样？为了让结构更清晰，WindowManagerService 只能对Surface进行控制，但并不更新Surface里的内容，分拆之后，WindowManagerService 只能访问SurfaceControl，它主要控制Surface的创建，销毁，Z-order，透明度，显示或隐藏，等等。而真正的更新者，View会通过Canvas的接口将内容画到Surface上。那View怎么拿到WMService创建的Surface，答案是outSurface.copyFrom(surfaceControl)；，surfaceControl 被转换成一个Surface对象，然后传回给ViewRoot, 前面创建的空的Surface现在有了实质内容。Surface通过这种方式被创建出来，Surface对应的Buffer 也相应的在SurfaceFlinger内部通过HAL层模块（GRAlloc)分配并维护在SurfaceFlinger 内部，Canvas() 通过dequeueBuffer（）接口拿到Surface的一个Buffer，绘制完成后通过queueBuffer（）还给SurfaceFlinger进行绘制。（这一部分我们后期学习WMS时候再分析） Surface创建&#160; &#160; &#160; &#160;从上面的类图，可以看到Surface类的成员变量mGraphicBufferProducer指向一个sp&lt; IGraphicBufferProducer &gt; 类型的对象，在Layer的onFirstRef函数中，mProducer复制后，类型为MonitoredProducer，将它有传给了Surface中。我们注意MonitoredProducer构造函数的一个参数producer，事实上MonitoredProducer只是一个代理类，真正的实现在这个producer参数。它是在BufferQueue::createBufferQueue中创造的。这个我们下一篇会分析这个。 &#160; &#160; &#160; &#160;Surface类继承了ANativeObjectBase类，而ANativeObjectBase类又继承了ANativeWindow类。我们知道，Android系统是通过OpenGL库来绘制UI的。OpenGL库在绘制UI的时候，需要底层的系统提供一个本地窗口给它，以便它可以将UI绘制在这个本地窗口上。Android系统为OpenGL库定提供的本地窗口使用ANativeWindow类来描述，Surface类通过ANativeObjectBase类间接地继承了ANativeWindow类，因此，Surface类也是用来描述OpenGL绘图所需要的一个本地窗口的。从这个角度出发，我们可以将Surface类看作OpenGL库与Android的UI系统之间的一个桥梁。 &#160; &#160; &#160; &#160;创建Surface的函数也很简单，查看SurfaceControl的getSurface函数：12345678910sp&lt;Surface&gt; SurfaceControl::getSurface() const&#123; Mutex::Autolock _l(mLock); if (mSurfaceData == 0) &#123; // This surface is always consumed by SurfaceFlinger, so the // producerControlledByApp value doesn't matter; using false. mSurfaceData = new Surface(mGraphicBufferProducer, false); &#125; return mSurfaceData;&#125; &#160; &#160; &#160; &#160;以及看看Surface的构造函数，位于frameworks/native/libs/gui/Surface.cpp：12345678910111213141516171819202122232425262728293031323334353637383940Surface::Surface( const sp&lt;IGraphicBufferProducer&gt;&amp; bufferProducer, bool controlledByApp) : mGraphicBufferProducer(bufferProducer)&#123; // Initialize the ANativeWindow function pointers. ANativeWindow::setSwapInterval = hook_setSwapInterval; ANativeWindow::dequeueBuffer = hook_dequeueBuffer; ANativeWindow::cancelBuffer = hook_cancelBuffer; ANativeWindow::queueBuffer = hook_queueBuffer; ANativeWindow::query = hook_query; ANativeWindow::perform = hook_perform; ANativeWindow::dequeueBuffer_DEPRECATED = hook_dequeueBuffer_DEPRECATED; ANativeWindow::cancelBuffer_DEPRECATED = hook_cancelBuffer_DEPRECATED; ANativeWindow::lockBuffer_DEPRECATED = hook_lockBuffer_DEPRECATED; ANativeWindow::queueBuffer_DEPRECATED = hook_queueBuffer_DEPRECATED; const_cast&lt;int&amp;&gt;(ANativeWindow::minSwapInterval) = 0; const_cast&lt;int&amp;&gt;(ANativeWindow::maxSwapInterval) = 1; mReqWidth = 0; mReqHeight = 0; mReqFormat = 0; mReqUsage = 0; mTimestamp = NATIVE_WINDOW_TIMESTAMP_AUTO; mCrop.clear(); mScalingMode = NATIVE_WINDOW_SCALING_MODE_FREEZE; mTransform = 0; mStickyTransform = 0; mDefaultWidth = 0; mDefaultHeight = 0; mUserWidth = 0; mUserHeight = 0; mTransformHint = 0; mConsumerRunningBehind = false; mConnectedToCpu = false; mProducerControlledByApp = controlledByApp; mSwapIntervalZero = false;&#125; &#160; &#160; &#160; &#160;主要是设置了一些钩子方法，用于创建GraphicBuffer等等。还有一些变量的初始化。所以重点应该在这些钩子方法当中，我们下一节会分析。 小结&#160; &#160; &#160; &#160;本节我们主要讲了SurfaceFlinger创建Surface的过程，文章末尾我们我发现管理图形缓冲区的一个重要工具：BufferQueue。我们下一节会分析这个。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(五)----VSync 工作原理]]></title>
    <url>%2F2017%2F05%2F25%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E4%BA%94-VSync-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;VSync信号的科普我们上一篇已经介绍过了，这篇我们要分析在SurfaceFlinger中的作用。（愈发觉得做笔记对自己记忆模块巩固有很多帮助，整理文章不一定是用来给别人看的，但一定是为加强自己记忆的~） 流程基础&#160; &#160; &#160; &#160;从上一篇得知，Android 4.1一个很大的更新是Project Butter，黄油计划，为了解决用户交互体验差的问题(Jelly Bean is crazy fast)。Project Butter对Android Display系统进行了重构，引入了三个核心元素，即VSYNC、Triple Buffer和Choreographer。 硬件加载&#160; &#160; &#160; &#160;上上一节我们初略分析了SurfaceFlinger的启动流程，从SurfaceFlinger类的初始化流程得知，其init方法初始化了很多参数。我们分析Vsync流程需要这些信息，位于frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp中。 显示设备&#160; &#160; &#160; &#160;SurfaceFlinger中需要显示的图层（layer）将通过DisplayDevice对象传递到OpenGLES中进行合成，合成之后的图像再通过HWComposer对象传递到Framebuffer中显示。DisplayDevice对象中的成员变量mVisibleLayersSortedByZ保存了所有需要显示在本显示设备中显示的Layer对象，同时DisplayDevice对象也保存了和显示设备相关的显示方向、显示区域坐标等信息。 &#160; &#160; &#160; &#160;DisplayDevice是显示设备的抽象，定义了3中类型的显示设备。引用枚举类位于frameworks/native/services/surfaceflinger/DisplayDevice.h中，定义枚举位于hardware/libhardware/include/hardware/Hwcomposer_defs.h中：12345678910111213141516171819 //DisplayDevice.h enum DisplayType &#123; DISPLAY_ID_INVALID = -1, DISPLAY_PRIMARY = HWC_DISPLAY_PRIMARY, DISPLAY_EXTERNAL = HWC_DISPLAY_EXTERNAL, DISPLAY_VIRTUAL = HWC_DISPLAY_VIRTUAL, NUM_BUILTIN_DISPLAY_TYPES = HWC_NUM_PHYSICAL_DISPLAY_TYPES, &#125;; //Hwcomposer_defs.h /* Display types and associated mask bits. */ enum &#123; HWC_DISPLAY_PRIMARY = 0, HWC_DISPLAY_EXTERNAL = 1, // HDMI, DP, etc. HWC_DISPLAY_VIRTUAL = 2, HWC_NUM_PHYSICAL_DISPLAY_TYPES = 2, HWC_NUM_DISPLAY_TYPES = 3,&#125;; DISPLAY_PRIMARY：主显示设备，通常是LCD屏 DISPLAY_EXTERNAL：扩展显示设备。通过HDMI输出的显示信号 DISPLAY_VIRTUAL：虚拟显示设备，通过WIFI输出信号 &#160; &#160; &#160; &#160;这三种设备，第一种就是我们手机、电视的主显示屏，另外两种需要硬件扩展。 &#160; &#160; &#160; &#160;初始化显示设备模块位于SurfaceFlinger中的init函数：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758void SurfaceFlinger::init() &#123; ...... //初始化HWComposer硬件设备，我们下面会讲到 // actual hardware composer underneath. mHwc = new HWComposer(this, *static_cast&lt;HWComposer::EventHandler *&gt;(this)); //初始化物理显示设备 // initialize our non-virtual displays //物理设备类型总数为2 for (size_t i=0 ; i&lt;DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES ; i++) &#123; DisplayDevice::DisplayType type((DisplayDevice::DisplayType)i); // set-up the displays that are already connected //先调用了HWComposer的isConnected来检查显示设备是否已连接， //只有和显示设备连接的DisplayDevice对象才会被创建出来。 //即使没有任何物理显示设备被检测到， //SurfaceFlinger都需要一个DisplayDevice对象才能正常工作， //因此，DISPLAY_PRIMARY类型的DisplayDevice对象总是会被创建出来。 if (mHwc-&gt;isConnected(i) || type==DisplayDevice::DISPLAY_PRIMARY) &#123; // All non-virtual displays are currently considered secure. bool isSecure = true; ////给显示设备分配一个token,一般都是一个binder createBuiltinDisplayLocked(type); wp&lt;IBinder&gt; token = mBuiltinDisplays[i]; //然后会调用createBufferQueue函数创建一个producer和consumer sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferConsumer&gt; consumer; BufferQueue::createBufferQueue(&amp;producer, &amp;consumer, new GraphicBufferAlloc()); //然后又创建了一个FramebufferSurface对象，把consumer参数传入代表是一个消费者。 sp&lt;FramebufferSurface&gt; fbs = new FramebufferSurface(*mHwc, i, consumer); int32_t hwcId = allocateHwcDisplayId(type); //创建DisplayDevice对象，代表一类显示设备 sp&lt;DisplayDevice&gt; hw = new DisplayDevice(this, type, hwcId, mHwc-&gt;getFormat(hwcId), isSecure, token, fbs, producer, mRenderEngine-&gt;getEGLConfig()); //如果不是主屏幕 if (i &gt; DisplayDevice::DISPLAY_PRIMARY) &#123; // FIXME: currently we don't get blank/unblank requests // for displays other than the main display, so we always // assume a connected display is unblanked. ALOGD("marking display %zu as acquired/unblanked", i); //通常我们不会在次屏幕获得熄灭/点亮LCD的请求， //所以我们总是认为次显屏是点亮的 //设置PowerMode为HWC_POWER_MODE_NORMAL hw-&gt;setPowerMode(HWC_POWER_MODE_NORMAL); &#125; mDisplays.add(token, hw); &#125; &#125; ......&#125; &#160; &#160; &#160; &#160;上述过程就是初始化显示设备，我们分部查看。 &#160; &#160; &#160; &#160;1）检查设备连接：&#160; &#160; &#160; &#160;所有显示设备的输出都要通过HWComposer对象完成，因此上面这段代码先调用了HWComposer的isConnected来检查显示设备是否已连接，只有和显示设备连接的DisplayDevice对象才会被创建出来。即使没有任何物理显示设备被检测到，SurfaceFlinger都需要一个DisplayDevice对象才能正常工作，因此，DISPLAY_PRIMARY类型的DisplayDevice对象总是会被创建出来。 &#160; &#160; &#160; &#160;createBuiltinDisplayLocked函数就是为显示设备对象创建一个BBinder类型的Token：12345678910void SurfaceFlinger::createBuiltinDisplayLocked(DisplayDevice::DisplayType type) &#123; ALOGW_IF(mBuiltinDisplays[type], "Overwriting display token for display type %d", type); //一般这种token之类的东西，都是一个binder mBuiltinDisplays[type] = new BBinder(); DisplayDeviceState info(type); // All non-virtual displays are currently considered secure. info.isSecure = true; mCurrentState.displays.add(mBuiltinDisplays[type], info);&#125; &#160; &#160; &#160; &#160;一般这种token之类的东西，都是一个binder，就像我们创建window需要一个token，也是new一个binder然后在WindowManagerService里注册。 &#160; &#160; &#160; &#160;2）初始化GraphicBuffer相关内容：（这一部分我们后续分析GraphicBuffer管理相关会仔细分析它，这里只是粗略介绍）&#160; &#160; &#160; &#160;然后会调用createBufferQueue函数创建一个producer和consumer。然后又创建了一个FramebufferSurface对象。这里我们看到在新建FramebufferSurface对象时把consumer参数传入了代表是一个消费者。&#160; &#160; &#160; &#160;而在DisplayDevice的构造函数中（下面会讲到），会创建一个Surface对象传递给底层的OpenGL ES使用，而这个Surface是一个生产者。在OpenGl ES中合成好了图像之后会将图像数据写到Surface对象中，这将触发consumer对象的onFrameAvailable函数被调用。&#160; &#160; &#160; &#160;这就是Surface数据好了就通知消费者来拿数据做显示用，在onFrameAvailable函数汇总，通过nextBuffer获得图像数据，然后调用HWComposer对象mHwc的fbPost函数输出。 &#160; &#160; &#160; &#160;FramebufferSurface类位于frameworks/native/services/surfaceflinger/displayhardware/FramebufferSurface.cpp中，我们查看它的的onFrameAvailable函数：1234567891011121314151617// Overrides ConsumerBase::onFrameAvailable(), does not call base class impl.void FramebufferSurface::onFrameAvailable() &#123; sp&lt;GraphicBuffer&gt; buf; sp&lt;Fence&gt; acquireFence; //通过nextBuffer获得图像数据 status_t err = nextBuffer(buf, acquireFence); if (err != NO_ERROR) &#123; ALOGE("error latching nnext FramebufferSurface buffer: %s (%d)", strerror(-err), err); return; &#125; //调用HWComposer的fbPost函数输出图像，fbPost函数最后通过调用Gralloc模块的post函数来输出图像 err = mHwc.fbPost(mDisplayType, acquireFence, buf); if (err != NO_ERROR) &#123; ALOGE("error posting framebuffer: %d", err); &#125;&#125; &#160; &#160; &#160; &#160;这个流程我们下几节会仔细分析，这里我们粗略过个流程：&#160; &#160; &#160; &#160;通过nextBuffer获得图像数据；&#160; &#160; &#160; &#160;然后调用HWComposer对象mHwc的fbPost函数输出，fbPost函数最后通过调用Gralloc模块的post函数来输出图像。这一部分我们在Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现分析过了，可以查看这个链接。 &#160; &#160; &#160; &#160;3）创建DisplayDevice对象，用来描述显示设备：&#160; &#160; &#160; &#160;我们再来看看DisplayDevice的构造函数，位于frameworks/native/services/surfaceflinger/DisplayDevice.cpp中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293/* * Initialize the display to the specified values. * */DisplayDevice::DisplayDevice( const sp&lt;SurfaceFlinger&gt;&amp; flinger, DisplayType type, int32_t hwcId, int format, bool isSecure, const wp&lt;IBinder&gt;&amp; displayToken, const sp&lt;DisplaySurface&gt;&amp; displaySurface, const sp&lt;IGraphicBufferProducer&gt;&amp; producer, EGLConfig config) : lastCompositionHadVisibleLayers(false), mFlinger(flinger), mType(type), mHwcDisplayId(hwcId), mDisplayToken(displayToken), mDisplaySurface(displaySurface), mDisplay(EGL_NO_DISPLAY), mSurface(EGL_NO_SURFACE), mDisplayWidth(), mDisplayHeight(), mFormat(), mFlags(), mPageFlipCount(), mIsSecure(isSecure), mSecureLayerVisible(false), mLayerStack(NO_LAYER_STACK), mOrientation(), mPowerMode(HWC_POWER_MODE_OFF), mActiveConfig(0)&#123; //创建surface，传递给底层的OpenGL ES使用，而这个Surface是一个生产者 mNativeWindow = new Surface(producer, false); ANativeWindow* const window = mNativeWindow.get(); /* * Create our display's surface */ //在OpenGl ES中合成好了图像之后会将图像数据写到Surface对象中 EGLSurface surface; EGLint w, h; EGLDisplay display = eglGetDisplay(EGL_DEFAULT_DISPLAY); if (config == EGL_NO_CONFIG) &#123; config = RenderEngine::chooseEglConfig(display, format); &#125; surface = eglCreateWindowSurface(display, config, window, NULL); eglQuerySurface(display, surface, EGL_WIDTH, &amp;mDisplayWidth); eglQuerySurface(display, surface, EGL_HEIGHT, &amp;mDisplayHeight); // Make sure that composition can never be stalled by a virtual display // consumer that isn't processing buffers fast enough. We have to do this // in two places: // * Here, in case the display is composed entirely by HWC. // * In makeCurrent(), using eglSwapInterval. Some EGL drivers set the // window's swap interval in eglMakeCurrent, so they'll override the // interval we set here. ////虚拟设备不支持图像合成 if (mType &gt;= DisplayDevice::DISPLAY_VIRTUAL) window-&gt;setSwapInterval(window, 0); mConfig = config; mDisplay = display; mSurface = surface; mFormat = format; mPageFlipCount = 0; mViewport.makeInvalid(); mFrame.makeInvalid(); // virtual displays are always considered enabled //虚拟设备屏幕认为是不关闭的 mPowerMode = (mType &gt;= DisplayDevice::DISPLAY_VIRTUAL) ? HWC_POWER_MODE_NORMAL : HWC_POWER_MODE_OFF; // Name the display. The name will be replaced shortly if the display // was created with createDisplay(). //根据显示设备类型赋名称 switch (mType) &#123; case DISPLAY_PRIMARY: mDisplayName = "Built-in Screen"; break; case DISPLAY_EXTERNAL: mDisplayName = "HDMI Screen"; break; default: mDisplayName = "Virtual Screen"; // e.g. Overlay #n break; &#125; // initialize the display orientation transform. //调整显示设备视角的大小、位移、旋转等参数 setProjection(DisplayState::eOrientationDefault, mViewport, mFrame);&#125; &#160; &#160; &#160; &#160;DisplayDevice的构造函数初始化了一些参数，但是最重要的是：创建了一个Surface对象mNativeWindow，同时用它作为参数创建EGLSurface对象，这个EGLSurface对象是OpenGL ES中绘图需要的。 &#160; &#160; &#160; &#160;这样，在DisplayDevice中就建立了一个通向Framebuffer的通道，只要向DisplayDevice的mSurface写入数据。就会到消费者FrameBufferSurface的onFrameAvailable函数，然后到HWComposer在到Gralloc模块，最后输出到显示设备。 &#160; &#160; &#160; &#160;附（以后会讲到）：DisplayDevice有个函数swapBuffers。swapBuffers函数将内部缓冲区的图像数据刷新到显示设备的Framebuffer中，它通过调用eglSwapBuffers函数来完成缓冲区刷新工作：123456789101112131415161718192021222324252627282930void DisplayDevice::swapBuffers(HWComposer&amp; hwc) const &#123; // We need to call eglSwapBuffers() if: // (1) we don't have a hardware composer, or // (2) we did GLES composition this frame, and either // (a) we have framebuffer target support (not present on legacy // devices, where HWComposer::commit() handles things); or // (b) this is a virtual display if (hwc.initCheck() != NO_ERROR || (hwc.hasGlesComposition(mHwcDisplayId) &amp;&amp; (hwc.supportsFramebufferTarget() || mType &gt;= DISPLAY_VIRTUAL))) &#123; EGLBoolean success = eglSwapBuffers(mDisplay, mSurface); if (!success) &#123; EGLint error = eglGetError(); if (error == EGL_CONTEXT_LOST || mType == DisplayDevice::DISPLAY_PRIMARY) &#123; LOG_ALWAYS_FATAL("eglSwapBuffers(%p, %p) failed with 0x%08x", mDisplay, mSurface, error); &#125; else &#123; ALOGE("eglSwapBuffers(%p, %p) failed with 0x%08x", mDisplay, mSurface, error); &#125; &#125; &#125; status_t result = mDisplaySurface-&gt;advanceFrame(); if (result != NO_ERROR) &#123; ALOGE("[%s] failed pushing new frame to HWC: %d", mDisplayName.string(), result); &#125;&#125; &#160; &#160; &#160; &#160;但是注意调用swapBuffers输出图像是在显示设备不支持硬件composer的情况下。 HWComposer设备&#160; &#160; &#160; &#160;通过上一篇文章得知，Android通过VSync机制来提高显示效果，那么VSync是如何产生的？通常这个信号是由显示驱动产生，这样才能达到最佳效果。但是Android为了能运行在不支持VSync机制的设备上，也提供了软件模拟产生VSync信号的手段。&#160; &#160; &#160; &#160;在SurfaceFlinger中，是通过HWComposer类来表示硬件设备。我们继续产看init函数：12345678void SurfaceFlinger::init() &#123; ...... // Initialize the H/W composer object. There may or may not be an // actual hardware composer underneath. mHwc = new HWComposer(this, *static_cast&lt;HWComposer::EventHandler *&gt;(this)); ......&#125; &#160; &#160; &#160; &#160;我们继续查看HWComposer的构造函数，位于frameworks/native/services/surfaceflinger/displayhardware/HWComposer.cpp中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108// ---------------------------------------------------------------------------HWComposer::HWComposer( const sp&lt;SurfaceFlinger&gt;&amp; flinger, EventHandler&amp; handler) : mFlinger(flinger), mFbDev(0), mHwc(0), mNumDisplays(1), mCBContext(new cb_context),//这里直接new了一个设备上下文对象 mEventHandler(handler), mDebugForceFakeVSync(false)&#123; ...... bool needVSyncThread = true; // Note: some devices may insist that the FB HAL be opened before HWC. //装载FrameBuffer的硬件模块 int fberr = loadFbHalModule(); //装载HWComposer的硬件模块,这个函数中会将mHwc置为true loadHwcModule(); //如果有FB和HWC，并且也有pre-1.1 HWC，就要先关闭FB设备 //但这个是暂时的，直到HWC准备好 if (mFbDev &amp;&amp; mHwc &amp;&amp; hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &#123; // close FB HAL if we don't needed it. // FIXME: this is temporary until we're not forced to open FB HAL // before HWC. framebuffer_close(mFbDev); mFbDev = NULL; &#125; // If we have no HWC, or a pre-1.1 HWC, an FB dev is mandatory. //如果我们没有HWC硬件支持，或者之前的pre-1.1 HWC，如hwc 1.0,那么FrameBuffer就是必须的 if ((!mHwc || !hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &amp;&amp; !mFbDev) &#123; //如果没有framebuffer，就退出gg ALOGE("ERROR: failed to open framebuffer (%s), aborting", strerror(-fberr)); abort(); &#125; // these display IDs are always reserved for (size_t i=0 ; i&lt;NUM_BUILTIN_DISPLAYS ; i++) &#123; mAllocatedDisplayIDs.markBit(i); &#125; //硬件vsync信号 if (mHwc) &#123; ALOGI("Using %s version %u.%u", HWC_HARDWARE_COMPOSER, (hwcApiVersion(mHwc) &gt;&gt; 24) &amp; 0xff, (hwcApiVersion(mHwc) &gt;&gt; 16) &amp; 0xff); if (mHwc-&gt;registerProcs) &#123; //HWComposer设备上下文变量mCBContext赋值 mCBContext-&gt;hwc = this; //函数指针钩子函数hook_invalidate放入上下文 mCBContext-&gt;procs.invalidate = &amp;hook_invalidate; //vsync钩子函数放入上下文 mCBContext-&gt;procs.vsync = &amp;hook_vsync; if (hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) //hotplug狗子函数放入上下文 mCBContext-&gt;procs.hotplug = &amp;hook_hotplug; else mCBContext-&gt;procs.hotplug = NULL; memset(mCBContext-&gt;procs.zero, 0, sizeof(mCBContext-&gt;procs.zero)); //将钩子函数注册进硬件设备，硬件驱动回调这些钩子函数 mHwc-&gt;registerProcs(mHwc, &amp;mCBContext-&gt;procs); &#125; // don't need a vsync thread if we have a hardware composer //如果有硬件vsync信号， 则不需要软件vsync实现 needVSyncThread = false; ...... &#125; if (mFbDev) &#123;//如果mFbDev为不为null，情况为：没有hwc或者hwc是1.0老的版本 ALOG_ASSERT(!(mHwc &amp;&amp; hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)), "should only have fbdev if no hwc or hwc is 1.0"); DisplayData&amp; disp(mDisplayData[HWC_DISPLAY_PRIMARY]); disp.connected = true; disp.format = mFbDev-&gt;format; DisplayConfig config = DisplayConfig(); config.width = mFbDev-&gt;width; config.height = mFbDev-&gt;height; config.xdpi = mFbDev-&gt;xdpi; config.ydpi = mFbDev-&gt;ydpi; config.refresh = nsecs_t(1e9 / mFbDev-&gt;fps); disp.configs.push_back(config); disp.currentConfig = 0; &#125; else if (mHwc) &#123;//如果mFbDev为null，情况为：fb打开失败，或者暂时关闭(上面逻辑分析了)。打开了hwc // here we're guaranteed to have at least HWC 1.1 //这里我们至少有HWC 1.1，上面分析过了。NUM_BUILTIN_DISPLAYS 为2 for (size_t i =0 ; i&lt;NUM_BUILTIN_DISPLAYS ; i++) &#123; //获取显示设备相关属性 queryDisplayProperties(i); &#125; &#125; ...... //如果没有硬件vsync，则需要用使用软件vsync模拟 if (needVSyncThread) &#123; // we don't have VSYNC support, we need to fake it mVSyncThread = new VSyncThread(*this); &#125;&#125; &#160; &#160; &#160; &#160;构造函数主要做了以下几件事情： &#160; &#160; &#160; &#160;1）加载FrameBuffer硬件驱动：&#160; &#160; &#160; &#160;调用loadFbHalModule函数实现，我们继续查看：12345678910111213// Load and prepare the FB HAL, which uses the gralloc module. Sets mFbDev.int HWComposer::loadFbHalModule()&#123; hw_module_t const* module; //hal层封装的hal函数，参数GRALLOC_HARDWARE_MODULE_ID，加载gralloc模块 int err = hw_get_module(GRALLOC_HARDWARE_MODULE_ID, &amp;module); if (err != 0) &#123; ALOGE("%s module not found", GRALLOC_HARDWARE_MODULE_ID); return err; &#125; //打开fb设备 return framebuffer_open(module, &amp;mFbDev);&#125; &#160; &#160; &#160; &#160;hw_get_module函数是HAL层框架中封装的加载gralloc的函数，这个我们之前讲过，可以查看Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现中的Gralloc模块的加载过程部分。 &#160; &#160; &#160; &#160;framebuffer_open函数同上，是用来打开fb设备，同样查看fb设备的打开过程模块。 &#160; &#160; &#160; &#160;2）装载HWComposer的硬件模块：&#160; &#160; &#160; &#160;调用loadHwcModule函数来加载HWC模块，我们继续查看：123456789101112131415161718192021222324252627// Load and prepare the hardware composer module. Sets mHwc.void HWComposer::loadHwcModule()&#123; hw_module_t const* module; //同样是HAL层封装的函数，参数是HWC_HARDWARE_MODULE_ID，加载hwc模块 if (hw_get_module(HWC_HARDWARE_MODULE_ID, &amp;module) != 0) &#123; ALOGE("%s module not found", HWC_HARDWARE_MODULE_ID); return; &#125; //打开hwc设备 int err = hwc_open_1(module, &amp;mHwc); if (err) &#123; ALOGE("%s device failed to initialize (%s)", HWC_HARDWARE_COMPOSER, strerror(-err)); return; &#125; if (!hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_0) || hwcHeaderVersion(mHwc) &lt; MIN_HWC_HEADER_VERSION || hwcHeaderVersion(mHwc) &gt; HWC_HEADER_VERSION) &#123; ALOGE("%s device version %#x unsupported, will not be used", HWC_HARDWARE_COMPOSER, mHwc-&gt;common.version); hwc_close_1(mHwc); mHwc = NULL; return; &#125;&#125; &#160; &#160; &#160; &#160;同样是借助hw_get_module这个HAL函数，不过参数是HWC_HARDWARE_MODULE_ID，加载HWC模块；然后hwc_open_1函数打开hwc设备。 &#160; &#160; &#160; &#160;3）FB和HWC相关逻辑判断(1)：&#160; &#160; &#160; &#160;如果有FB和HWC，并且也有pre-1.1 HWC，就要先关闭FB设备。这个是暂时的，直到HWC准备好；&#160; &#160; &#160; &#160;如果我们没有HWC硬件支持，或者之前的pre-1.1 HWC,，如HWC 1.0，那么FrameBuffer就是必须的。如果脸FB都没有，那就GG。 &#160; &#160; &#160; &#160;4）硬件设备打开成功：&#160; &#160; &#160; &#160;如果硬件设备打开成功，则将钩子函数hook_invalidate、hook_vsync和hook_hotplug注册进硬件设备，作为回调函数。这三个都是硬件产生事件信号，通知上层SurfaceFlinger的回调函数，用于处理这个信号。 &#160; &#160; &#160; &#160;因为我们本节是Vsync信号相关，所以我们只看看hook_vsync钩子函数。这里指定了vsync的回调函数是hook_vsync，如果硬件中产生了VSync信号，将通过这个函数来通知上层，看看它的代码：123456void HWComposer::hook_vsync(const struct hwc_procs* procs, int disp, int64_t timestamp) &#123; cb_context* ctx = reinterpret_cast&lt;cb_context*&gt;( const_cast&lt;hwc_procs_t*&gt;(procs)); ctx-&gt;hwc-&gt;vsync(disp, timestamp);&#125; &#160; &#160; &#160; &#160;hook_vsync钩子函数会调用vsync函数，我们继续看：1234567891011121314151617181920212223242526void HWComposer::vsync(int disp, int64_t timestamp) &#123; if (uint32_t(disp) &lt; HWC_NUM_PHYSICAL_DISPLAY_TYPES) &#123; &#123; Mutex::Autolock _l(mLock); // There have been reports of HWCs that signal several vsync events // with the same timestamp when turning the display off and on. This // is a bug in the HWC implementation, but filter the extra events // out here so they don't cause havoc downstream. if (timestamp == mLastHwVSync[disp]) &#123; ALOGW("Ignoring duplicate VSYNC event from HWC (t=%" PRId64 ")", timestamp); return; &#125; mLastHwVSync[disp] = timestamp; &#125; char tag[16]; snprintf(tag, sizeof(tag), "HW_VSYNC_%1u", disp); ATRACE_INT(tag, ++mVSyncCounts[disp] &amp; 1); //这里调用EventHandler类型变量mEventHandler就是SurfaceFlinger， //所以调用了SurfaceFlinger的onVSyncReceived函数 mEventHandler.onVSyncReceived(disp, timestamp); &#125;&#125; &#160; &#160; &#160; &#160;mEventHandler对象类型为EventHandler，我们在SurfaceFlinger的init函数创建HWComposer类实例时候讲SurfaceFlinger强转为EventHandler作为构造函数的参数传入其中。再者SurfaceFlinger继承HWComposer::EventHandler，所以这里没毛病，老铁~所以最终会调用SurfaceFlinger的onVSyncReceived函数。这就是硬件vsync信号的产生，关于vsync信号流程工作我们下面马上会讲到。 &#160; &#160; &#160; &#160;5）FB和HWC相关逻辑判断(2)：&#160; &#160; &#160; &#160;如果mFbDev为不为null，情况为：没有hwc或者hwc是1.0老的版本。此时就要设置一些FB的属性了，从主屏幕获取；&#160; &#160; &#160; &#160;如果mFbDev为null，情况为：fb打开失败，或者暂时关闭(上面逻辑分析了)。仅仅打开了hwc，这里我们至少有HWC 1.1，上面分析过了。此时我们要获取显示设备相关属性，进入else if逻辑判断，里面的NUM_BUILTIN_DISPLAYS 为2，则只有主屏幕和扩展屏幕需要查询显示设备属性，调用queryDisplayProperties函数，我们可以产看这个函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172status_t HWComposer::queryDisplayProperties(int disp) &#123; LOG_ALWAYS_FATAL_IF(!mHwc || !hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)); // use zero as default value for unspecified attributes int32_t values[NUM_DISPLAY_ATTRIBUTES - 1]; memset(values, 0, sizeof(values)); const size_t MAX_NUM_CONFIGS = 128; uint32_t configs[MAX_NUM_CONFIGS] = &#123;0&#125;; size_t numConfigs = MAX_NUM_CONFIGS; //hwc设备获取显屏配置信息，都是HAL层封装的函数，并保存在这些指针指向内容中 status_t err = mHwc-&gt;getDisplayConfigs(mHwc, disp, configs, &amp;numConfigs); //如果一个不可插入的显屏没有连接，则将connected赋值为false if (err != NO_ERROR) &#123; // this can happen if an unpluggable display is not connected mDisplayData[disp].connected = false; return err; &#125; mDisplayData[disp].currentConfig = 0; //获取显示相关属性 for (size_t c = 0; c &lt; numConfigs; ++c) &#123; err = mHwc-&gt;getDisplayAttributes(mHwc, disp, configs[c], DISPLAY_ATTRIBUTES, values); if (err != NO_ERROR) &#123; // we can't get this display's info. turn it off. mDisplayData[disp].connected = false; return err; &#125; //获取成功后，创建一个config对象，用于保存这些属性 DisplayConfig config = DisplayConfig(); for (size_t i = 0; i &lt; NUM_DISPLAY_ATTRIBUTES - 1; i++) &#123; switch (DISPLAY_ATTRIBUTES[i]) &#123; case HWC_DISPLAY_VSYNC_PERIOD://屏幕刷新率 config.refresh = nsecs_t(values[i]); break; case HWC_DISPLAY_WIDTH://屏幕宽 config.width = values[i]; break; case HWC_DISPLAY_HEIGHT://屏幕高 config.height = values[i]; break; case HWC_DISPLAY_DPI_X://DPI像素X config.xdpi = values[i] / 1000.0f; break; case HWC_DISPLAY_DPI_Y://DPI像素Y config.ydpi = values[i] / 1000.0f; break; default: ALOG_ASSERT(false, "unknown display attribute[%zu] %#x", i, DISPLAY_ATTRIBUTES[i]); break; &#125; &#125; //如果没有获取到DPI则给个默认的，跟进去是213或者320 if (config.xdpi == 0.0f || config.ydpi == 0.0f) &#123; float dpi = getDefaultDensity(config.width, config.height); config.xdpi = dpi; config.ydpi = dpi; &#125; //将每个设备的显示信息存起来 mDisplayData[disp].configs.push_back(config); &#125; // FIXME: what should we set the format to? //像素编码都为ARGB_8888 mDisplayData[disp].format = HAL_PIXEL_FORMAT_RGBA_8888; //这里讲connected复制为true mDisplayData[disp].connected = true; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;queryDisplayProperties查询每个显示设备的属性，主要有以下几步： hwc设备获取显屏配置信息，都是HAL层封装的函数，并保存在这些指针指向内容中; 获取显示先关属性，并保存起来。 &#160; &#160; &#160; &#160;这一部分注释写的比较清晰，可以查看注释。 &#160; &#160; &#160; &#160;6）软件VSync信号产生：&#160; &#160; &#160; &#160;如果硬件设备打开失败，则需要软件vsync信号，因此needVSyncThread为true，会进入最后一个if条件判断逻辑，则会创建一个VSyncThread类实例，用来模拟软件vsync信号。 &#160; &#160; &#160; &#160;所以我们看看VSyncThread这个类，它集成与Thread，又是一个线程，定义位于HWComposer.h中：123456789101112131415// this class is only used to fake the VSync event on systems that don't// have it.class VSyncThread : public Thread &#123; HWComposer&amp; mHwc; mutable Mutex mLock; Condition mCondition; bool mEnabled; mutable nsecs_t mNextFakeVSync; nsecs_t mRefreshPeriod; virtual void onFirstRef(); virtual bool threadLoop();public: VSyncThread(HWComposer&amp; hwc); void setEnabled(bool enabled);&#125;; &#160; &#160; &#160; &#160;构造函数位于HWComposer.cpp中：123456HWComposer::VSyncThread::VSyncThread(HWComposer&amp; hwc) : mHwc(hwc), mEnabled(false),//false mNextFakeVSync(0),//0 mRefreshPeriod(hwc.getRefreshPeriod(HWC_DISPLAY_PRIMARY))//获取主屏幕刷新率&#123;&#125; &#160; &#160; &#160; &#160;构造函数中初始化了mEnable变量为false，mNextFakeVSync为0。还有获取了主屏幕的刷新率，并保存在mRefreshPeriod。屏幕刷新率我们在上一篇VSync信号已经科普过了。&#160; &#160; &#160; &#160;我们看看屏幕刷新率的获取，hwc.getRefreshPeriod(HWC_DISPLAY_PRIMARY)函数：1234nsecs_t HWComposer::getRefreshPeriod(int disp) const &#123; size_t currentConfig = mDisplayData[disp].currentConfig; return mDisplayData[disp].configs[currentConfig].refresh;&#125; &#160; &#160; &#160; &#160;这里获取的屏幕刷新率refresh就是我们之前queryDisplayProperties函数里获取的。其实queryDisplayProperties不止在HWComposer构造函数里调用，还会在钩子函数hook_hotplug里调用，我们查看hotplug函数：123456789void HWComposer::hotplug(int disp, int connected) &#123; if (disp == HWC_DISPLAY_PRIMARY || disp &gt;= VIRTUAL_DISPLAY_ID_BASE) &#123; ALOGE("hotplug event received for invalid display: disp=%d connected=%d", disp, connected); return; &#125; queryDisplayProperties(disp); mEventHandler.onHotplugReceived(disp, bool(connected));&#125; &#160; &#160; &#160; &#160;如果是主屏幕或者虚拟设备则不会查询设备属性。&#160; &#160; &#160; &#160;我猜测是插入扩展屏幕后，会重新再查询一下屏幕信息，从硬件发出的信号。 &#160; &#160; &#160; &#160;好了我们继续回到上面，因为VSyncThread对象付给了一个强指针，所以会调用onFirstRef函数：123void HWComposer::VSyncThread::onFirstRef() &#123; run("VSyncThread", PRIORITY_URGENT_DISPLAY + PRIORITY_MORE_FAVORABLE);&#125; &#160; &#160; &#160; &#160;这样这个软件vsync产生线程就运行起来了，因此我们需要查看threadLoop函数：1234567891011121314151617181920212223242526272829303132333435363738bool HWComposer::VSyncThread::threadLoop() &#123; &#123; // scope for lock Mutex::Autolock _l(mLock); while (!mEnabled) &#123;//这个是false，在构造函数里赋值 //因此会阻塞在这里 mCondition.wait(mLock); &#125; &#125; const nsecs_t period = mRefreshPeriod;//屏幕刷新率 const nsecs_t now = systemTime(CLOCK_MONOTONIC);//now nsecs_t next_vsync = mNextFakeVSync;//下一次vsync时间，初始值为0 nsecs_t sleep = next_vsync - now;//距离下次vsync信号中间的睡眠时间 if (sleep &lt; 0) &#123;//如果sleep小于0，比如第一次进来mNextFakeVSync为0；或者错过n次vsync信号 // we missed, find where the next vsync should be //如果错过了，则重新计算下一次vsync的偏移时间 sleep = (period - ((now - next_vsync) % period)); next_vsync = now + sleep; &#125; //偏移时间加上刷新率周期时间 mNextFakeVSync = next_vsync + period; struct timespec spec; spec.tv_sec = next_vsync / 1000000000; spec.tv_nsec = next_vsync % 1000000000; int err; do &#123; //然后睡眠等待下次vsync信号的时间 err = clock_nanosleep(CLOCK_MONOTONIC, TIMER_ABSTIME, &amp;spec, NULL); &#125; while (err&lt;0 &amp;&amp; errno == EINTR); if (err == 0) &#123;//睡眠ok后，则去驱动SurfaceFlinger函数触发 mHwc.mEventHandler.onVSyncReceived(0, next_vsync); &#125; return true;&#125; &#160; &#160; &#160; &#160;上面的代码，我们需要注意两个地方： mEnabled默认为false，mCondition在这里阻塞，直到有人调用了signal()，同时mEnabled为true； 如果阻塞解开，则会定期休眠，然后去驱动SF，这就相当于产生了持续的Vsync信号。 &#160; &#160; &#160; &#160;这个函数会间隔模拟产生VSync的信号的原理是在固定时间发送消息给HWCompoer的消息对象mEventHandler，这个其实就到SurfaceFlinger的onVSyncReceived函数了。用软件模拟VSync信号在系统比较忙的时候可能会丢失一些信号，所以才有中间的sleep小于0的情况。 &#160; &#160; &#160; &#160;到目前为止，HWC中新建了一个线程VSyncThread，阻塞中，下面我们看下打开Vsync开关的闸刀是如何建立的，以及何处去开闸刀。 VSync 信号闸刀控制&#160; &#160; &#160; &#160;上面我分析了Vsync软件和硬件信号的产生过程，但是HWC中新建的线程VSyncThread是处于阻塞当中，因此我们需要一个闸刀去控制这个开关。 Vsync信号开关&#160; &#160; &#160; &#160;这个闸刀还是在SurfaceFlinger的init函数中建立的，我们查看这个位置：1234567void SurfaceFlinger::init() &#123; ...... mEventControlThread = new EventControlThread(this); mEventControlThread-&gt;run("EventControl", PRIORITY_URGENT_DISPLAY); ......&#125; &#160; &#160; &#160; &#160;这是一个Thread线程类，因此都会实现threadLoop函数。不过我们先看看它的构造函数：1234EventControlThread::EventControlThread(const sp&lt;SurfaceFlinger&gt;&amp; flinger): mFlinger(flinger), mVsyncEnabled(false) &#123;&#125; &#160; &#160; &#160; &#160;比较简单，将SurfaceFlinger对象传了进来，并且将mVsyncEnabled标志初始为false。&#160; &#160; &#160; &#160;然后我们看看他的threadLoop线程函数：1234567891011121314151617181920212223242526bool EventControlThread::threadLoop() &#123; Mutex::Autolock lock(mMutex); bool vsyncEnabled = mVsyncEnabled;//初始为false //先调用一遍SurfaceFlinger的eventControl函数 mFlinger-&gt;eventControl(HWC_DISPLAY_PRIMARY, SurfaceFlinger::EVENT_VSYNC, mVsyncEnabled); //这里是个死循环 while (true) &#123; status_t err = mCond.wait(mMutex);//会阻塞在这里 if (err != NO_ERROR) &#123; ALOGE("error waiting for new events: %s (%d)", strerror(-err), err); return false; &#125; //vsyncEnabled 开始和mVsyncEnabled都为false，如果有其他地方改变了mVsyncEnabled, //会去调用SF的eventControl函数 if (vsyncEnabled != mVsyncEnabled) &#123; mFlinger-&gt;eventControl(HWC_DISPLAY_PRIMARY, SurfaceFlinger::EVENT_VSYNC, mVsyncEnabled); vsyncEnabled = mVsyncEnabled; &#125; &#125; return false;&#125; &#160; &#160; &#160; &#160;threadLoop函数主要逻辑分两步：&#160; &#160; &#160; &#160;1.进来时候在mCond处阻塞，可以看到外面是个死循环，所以如果有其他地方对这个mCond调用了signal，执行完下面的代码又会阻塞 ；&#160; &#160; &#160; &#160;2.解除阻塞后，如果vsyncEnabled != mVsyncEnabled，也就是开关状态不同，由开到关和由关到开，都回去调用mFlinger-&gt;eventControl，调用完成后把这次的开关状态保存，vsyncEnabled = mVsyncEnabled;，与下次做比较。&#160; &#160; &#160; &#160;从上面我们能明显认识到EventControlThread线程，其实就起到了个闸刀的作用，等待着别人去开、关。 &#160; &#160; &#160; &#160;所以这个开关操作就在SurfaceFlinger的eventControl函数，我们看看它怎么实现：1234void SurfaceFlinger::eventControl(int disp, int event, int enabled) &#123; ATRACE_CALL(); getHwComposer().eventControl(disp, event, enabled);&#125; &#160; &#160; &#160; &#160;它又会调用HWComposer的eventControl函数，我们继续查看：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647void HWComposer::eventControl(int disp, int event, int enabled) &#123; //如果是没有申请注册的设备，GG if (uint32_t(disp)&gt;31 || !mAllocatedDisplayIDs.hasBit(disp)) &#123; ALOGD("eventControl ignoring event %d on unallocated disp %d (en=%d)", event, disp, enabled); return; &#125; //不是EVENT_VSYNC事件，GG if (event != EVENT_VSYNC) &#123; ALOGW("eventControl got unexpected event %d (disp=%d en=%d)", event, disp, enabled); return; &#125; status_t err = NO_ERROR; //如果是硬件Vsync闸刀，并且不是调试软件vsync if (mHwc &amp;&amp; !mDebugForceFakeVSync) &#123; // NOTE: we use our own internal lock here because we have to call // into the HWC with the lock held, and we want to make sure // that even if HWC blocks (which it shouldn't), it won't // affect other threads. Mutex::Autolock _l(mEventControlLock); const int32_t eventBit = 1UL &lt;&lt; event; const int32_t newValue = enabled ? eventBit : 0; const int32_t oldValue = mDisplayData[disp].events &amp; eventBit; if (newValue != oldValue) &#123; ATRACE_CALL(); //硬件vsync闸刀控制开关 err = mHwc-&gt;eventControl(mHwc, disp, event, enabled); if (!err) &#123; int32_t&amp; events(mDisplayData[disp].events); events = (events &amp; ~eventBit) | newValue; char tag[16]; snprintf(tag, sizeof(tag), "HW_VSYNC_ON_%1u", disp); ATRACE_INT(tag, enabled); &#125; &#125; // error here should not happen -- not sure what we should // do if it does. ALOGE_IF(err, "eventControl(%d, %d) failed %s", event, enabled, strerror(-err)); &#125; //如果没有硬件Vsync支持，则调用软件vsync信号闸刀 if (err == NO_ERROR &amp;&amp; mVSyncThread != NULL) &#123; mVSyncThread-&gt;setEnabled(enabled); &#125;&#125; &#160; &#160; &#160; &#160;这里可以看到还是分软件闸刀和硬件闸刀： 硬件闸刀控制就到了HAL层函数了，调用了mHwc-&gt;eventControl(mHwc, disp, event, enabled)这个逻辑； 如果没有硬件Vsync支持，则会调用软件闸刀控制手段，VsyncThread的setEnable函数。 &#160; &#160; &#160; &#160;硬件就不用了看，反正也看不到代码。。。我们看看软件Vsync闸刀控制，即VsyncThread的setEnable函数：1234567void HWComposer::VSyncThread::setEnabled(bool enabled) &#123; Mutex::Autolock _l(mLock); if (mEnabled != enabled) &#123; mEnabled = enabled; mCondition.signal(); &#125;&#125; &#160; &#160; &#160; &#160;将软件模拟线程VSyncThread的mCondition释放，这时候VSyncThread就会定期产生“信号”，去驱动SF了。 EventControlThread开关&#160; &#160; &#160; &#160;但是我们上边漏了东西，就是EventControlThread本身的闸刀，它自己还在mCond.wait(mMutex)这里阻塞呢。那么EventControlThread这个闸刀到底在哪儿打开呢？ &#160; &#160; &#160; &#160;所以我们还得的查看SurfaceFlinger的init函数，再这里它有一处逻辑：12345678void SurfaceFlinger::init() &#123;...... // set initial conditions (e.g. unblank default device) initializeDisplays();......&#125; &#160; &#160; &#160; &#160;我们查看initializeDisplays函数：12345678910111213void SurfaceFlinger::initializeDisplays() &#123; class MessageScreenInitialized : public MessageBase &#123; SurfaceFlinger* flinger; public: MessageScreenInitialized(SurfaceFlinger* flinger) : flinger(flinger) &#123; &#125; virtual bool handler() &#123; flinger-&gt;onInitializeDisplays(); return true; &#125; &#125;; sp&lt;MessageBase&gt; msg = new MessageScreenInitialized(this); postMessageAsync(msg); // we may be called from main thread, use async message&#125; &#160; &#160; &#160; &#160;这里放了一个消息给SF的MessageQueue，然后消息回调onInitializeDisplays函数，我们继续查看：12345678910111213141516171819202122232425void SurfaceFlinger::onInitializeDisplays() &#123; // reset screen orientation and use primary layer stack //重置屏幕方向，并使用主屏幕图层 Vector&lt;ComposerState&gt; state; Vector&lt;DisplayState&gt; displays; DisplayState d; d.what = DisplayState::eDisplayProjectionChanged | DisplayState::eLayerStackChanged; d.token = mBuiltinDisplays[DisplayDevice::DISPLAY_PRIMARY]; d.layerStack = 0; d.orientation = DisplayState::eOrientationDefault; d.frame.makeInvalid(); d.viewport.makeInvalid(); d.width = 0; d.height = 0; displays.add(d); //重设屏幕Transaction状态 setTransactionState(state, displays, 0); //我们要找到闸刀开关在这个里面 setPowerModeInternal(getDisplayDevice(d.token), HWC_POWER_MODE_NORMAL); //主显示器刷新率 const nsecs_t period = getHwComposer().getRefreshPeriod(HWC_DISPLAY_PRIMARY); mAnimFrameTracker.setDisplayRefreshPeriod(period);&#125; &#160; &#160; &#160; &#160;其它都不重要，重要在setPowerModeInternal函数，我们继续查看：123456789101112131415161718192021222324252627282930313233343536373839404142434445void SurfaceFlinger::setPowerModeInternal(const sp&lt;DisplayDevice&gt;&amp; hw, int mode) &#123;//HWC_POWER_MODE_NORMAL ALOGD("Set power mode=%d, type=%d flinger=%p", mode, hw-&gt;getDisplayType(), this); int32_t type = hw-&gt;getDisplayType();//DISPLAY_PRIMARY int currentMode = hw-&gt;getPowerMode();//HWC_POWER_MODE_OFF if (mode == currentMode) &#123;//skip ALOGD("Screen type=%d is already mode=%d", hw-&gt;getDisplayType(), mode); return; &#125; hw-&gt;setPowerMode(mode); if (type &gt;= DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES) &#123; ALOGW("Trying to set power mode for virtual display"); return; &#125; if (currentMode == HWC_POWER_MODE_OFF) &#123;//会走到这里 getHwComposer().setPowerMode(type, mode); if (type == DisplayDevice::DISPLAY_PRIMARY) &#123; // FIXME: eventthread only knows about the main display right now //EventThread是后面讲到VSync工作流程的部分，后面讲，这里影响不大 mEventThread-&gt;onScreenAcquired(); //这里才是重点，是打开闸刀的地方 resyncToHardwareVsync(true); &#125; mVisibleRegionsDirty = true; repaintEverything(); &#125; else if (mode == HWC_POWER_MODE_OFF) &#123;//不会到这里 if (type == DisplayDevice::DISPLAY_PRIMARY) &#123; disableHardwareVsync(true); // also cancels any in-progress resync // FIXME: eventthread only knows about the main display right now mEventThread-&gt;onScreenReleased(); &#125; getHwComposer().setPowerMode(type, mode); mVisibleRegionsDirty = true; // from this point on, SF will stop drawing on this display &#125; else &#123;//也不会到这儿 getHwComposer().setPowerMode(type, mode); &#125;&#125; &#160; &#160; &#160; &#160;上面逻辑也不难，只要我们记住以前赋值的变量时多少就行了。&#160; &#160; &#160; &#160;我们获取主显屏的信息，因为是灭的HWC_POWER_MODE_OFF模式，所以进入这个if判断里面，然后调用resyncToHardwareVsync函数。我们继续查看它：123456789101112131415161718192021222324void SurfaceFlinger::resyncToHardwareVsync(bool makeAvailable) &#123;//true Mutex::Autolock _l(mHWVsyncLock); //true if (makeAvailable) &#123; mHWVsyncAvailable = true; &#125; else if (!mHWVsyncAvailable) &#123; ALOGE("resyncToHardwareVsync called when HW vsync unavailable"); return; &#125; //主屏幕硬件刷新率 const nsecs_t period = getHwComposer().getRefreshPeriod(HWC_DISPLAY_PRIMARY); //设置硬件刷新率 mPrimaryDispSync.reset(); mPrimaryDispSync.setPeriod(period); if (!mPrimaryHWVsyncEnabled) &#123;//false,进入 mPrimaryDispSync.beginResync(); //eventControl(HWC_DISPLAY_PRIMARY, SurfaceFlinger::EVENT_VSYNC, true); //打开闸刀 mEventControlThread-&gt;setVsyncEnabled(true); mPrimaryHWVsyncEnabled = true; &#125;&#125; &#160; &#160; &#160; &#160;先将mHWVsyncAvailable 标志位置为true，然后设置硬件刷新周期，最后打开mEventControlThread闸刀，我们可以查看它的setVsyncEnabled函数：12345void EventControlThread::setVsyncEnabled(bool enabled) &#123; Mutex::Autolock lock(mMutex); mVsyncEnabled = enabled; mCond.signal();&#125; &#160; &#160; &#160; &#160;可以看到，在这里释放了前面的mCond，让EventControlThread去调用mFlinger-&gt;eventControl()函数，从而去HWC中打开Vsync开关。 VSync信号构成&#160; &#160; &#160; &#160;本来我们应该继续顺着代码去跟踪vsync信号的分发过程，但是在这之前我们有必要了解一下vsync信号的构成，以便我们后续分析没有盲点。 &#160; &#160; &#160; &#160;我们知道Android是用Vsync来驱动系统的画面更新包括APPview draw ,surfaceflinger 画面的合成,display把surfaceflinger合成的画面呈现在LCD上。要分析Vsync信号的组成，可以了解一下systrace工具。 Systrace简介&#160; &#160; &#160; &#160;Systrace是Android4.1中新增的性能数据采样和分析工具。它可帮助开发者收集Android关键子系统（如surfaceflinger、WindowManagerService等Framework部分关键模块、服务，View系统等）的运行信息，从而帮助开发者更直观的分析系统瓶颈，改进性能。 &#160; &#160; &#160; &#160;systrace的使用可以查看官网，或者问娘问哥。&#160; &#160; &#160; &#160;官网使用教程如下：&#160; &#160; &#160; &#160;https://developer.android.com/studio/profile/systrace-walkthru.html &#160; &#160; &#160; &#160;命令行使用方式：&#160; &#160; &#160; &#160;https://developer.android.com/studio/profile/systrace-commandline.html &#160; &#160; &#160; &#160;生成html分析：&#160; &#160; &#160; &#160;https://developer.android.com/studio/profile/systrace.html &#160; &#160; &#160; &#160;分析systrace生成文件，要在eng版本rom，其他的要么无法生成，要么没有权限。 &#160; &#160; &#160; &#160;下面我抄一个例子分析。 VSync的构成&#160; &#160; &#160; &#160;就挑一个相册这个应用，是系统自带的system app，包名是com.android.gallery3d。 &#160; &#160; &#160; &#160;在systrace中,我们经常可以看到如上图的信息。 红色框1是与Vsync相关event信息.这些Vsync event构成了android系统画面更新基础； 红色框2和红色框3是Vsync-app的信息.我们可以看到红色框2比红色框3稀疏.我们将会在本文说明其原因； 红色框4和红色框5是Vsync-sf的信息.我们可以看到红色框4比红色框5稀疏.我们将会在本文说明其原因。 &#160; &#160; &#160; &#160;从上图可以看出来，VSync信号由HW_VSYNC_ON_0,HW_VSYNC_0, Vsync-app和Vsync-sf四部分构成： HW_VSYNC_ON_0：代表PrimaryDisplay的VSync被enable或disable。0这个数字代表的是display的编号, 0是PrimaryDisplay,如果是Externalmonitor,就会是HW_VSYNC_ON_1。当SF要求HWComposer将Display的VSync打开或关掉时,这个event就会记录下来； HW_VSYNC_0：代表PrimaryDisplay的VSync发生时间点, 0同样代表display编号。其用来调节Vsync-app和Vsync-sfevent的输出； Vsync-app：App,SystemUI和systemserver 等viewdraw的触发器； Vsync-sf：Surfaceflinger合成画面的触发器。 &#160; &#160; &#160; &#160;通常为了避免Tearing的现象，画面更新(Flip)的动作通常会在VSync开始的时候才做，因为在VSync开始到它结束前，Display不会把framebuffer资料显示在display上，所以在这段时间做Flip可以避免使用者同时看到前后两个部份画面的现象。目前user看到画面呈现的过程是这样的，app更新它的画面后，它需要透过BufferQueue通知SF，SF再将更新过的app画面与其它的App或SystemUI组合后，再显示在User面前。在这个过程里,有3个component牵涉进来，分别是App、SF、与Display。以目前Android的设计，这三个Component都是在VSync发生的时候才开始做事。我们将它们想成一个有3个stage的pipeline，这个pipeline的clock就LCD的TE信号(60HZ)也即HW_VSYNC_0。 &#160; &#160; &#160; &#160;我们来看看android draw的pipeline，如下： T = 0时, App正在画N, SF与Display都没内容可用 T = 1时, App正在画N+1, SF组合N, Display没Buffer可显示 T = 2时, App正在画N+2, SF组合N+1, Display显示N T = 3时, App正在画N, SF组合N+2, Display显示N+1 …… &#160; &#160; &#160; &#160;如果按照这个步骤，当user改变一个画面时，要等到2个VSync后，画面才会显示在user面前，latency大概是33ms (2个frame的时间)。但是对大部份的操作来讲，可能app加SF画的时间一个frame(16.6ms)就可以完成。因此，Android就从HW_VSYNC_0中产生出两个VSync信号，VSYNC-app是给App用的，VSYNC-sf是给SF用的， Display则是使用HW_VSYNC_0。VSYNC-app与VSYNC-sf可以分别给定一个phase，简单的说： &#160; &#160; &#160; &#160;VSYNC-app = HW_VSYNC_0 + phase_app&#160; &#160; &#160; &#160;VSYNC-sf =HW_VSYNC_0 + phase_sf &#160; &#160; &#160; &#160;从而使App draw和surfaceflinger的合成，错开时间运行。这样就有可能整个系统draw的pipeline更加有效率，从而提高用户体验。 &#160; &#160; &#160; &#160;也就是说，如果phase_app与phase_sf设定的好的话，可能大部份user使用的状况， App+SF可以在一个frame里完成，然后在下一个HW_VSYNC_0来的时候,显示在display上。 &#160; &#160; &#160; &#160;理论上透过VSYNC-sf与VSYNC-app的确是可以有机会在一个frame里把App+SF做完，但是实际上不是一定可以在一个frame里完成。因为有可能因为CPU调度，GPUperformance不够，以致App或SF没办法及时做完。但是即便如此，把app，surfaceflinger和displayVsync分开也比用一个Vsync来trigger appdraw，surfaceflinger合成，display在LCD上draw的性能要好很多(FPS更高)。因为如果放在同一时间，CPU调度压力更大，可能会卡顿、延迟。 &#160; &#160; &#160; &#160;那么是否我们收到LCD的Vsync event就会触发Vsync-app和Vsync-SF呢？如果是这样我们就不会看到本文开头的Vsync-app和Vsync-SF的节奏不一致的情况(红色框2比红色框3稀疏)。事实上，在大多数情况下，APP的画面并不需要一直更新。比如我们看一篇文章，大部份时间，画面是维持一样的，如果我们在每个LCD的VSYNC来的时候都触发SF或APP，就会浪费时间和Power。可是在画面持续更新的情况下，我们又需要触发SF和App的Vsync event。例如玩game或播影片时。就是说我们需要根据是否有内容的更新来选择Vsync event出现的机制。&#160; &#160; &#160; &#160;Vsync event 的触发需要按需出现。所以在Android里，HW_VSYNC_0，Vsync-sf和Vsync-app都会按需出现。HW_VSYNC_0是根据是否需要调节sf和app的Vsync event而出现，而SF和App则会call requestNextVsync()来告诉系统我要在下一个VSYNC需要被trigger。也是虽然系统每秒有60个HW VSYNC，但不代表APP和SF在每个VSYNC都要更新画面。因此，在Android里，是根据SoftwareVSYNC(Vsync-sf，Vsync-app)来更新画面。Software VSYNC是根据HWVSYNC过去发生的时间，推测未来会发生的时间。因此，当APP或SF利用requestNextVsync时，Software VSYNC才会触发VSYNC-sf或VSYNC-app。 &#160; &#160; &#160; &#160;下面我们就继续跟着代码，分析VSync信号工作流程。 VSync工作流程整体概述&#160; &#160; &#160; &#160;我们知道Vsync是android display系统的重要基石,其驱动android display系统不断的更新App侧的绘画,并把相关内容及时的更新到LCD上.其包含的主要代码如下:&#160; &#160; &#160; &#160;frameworks\native\services\surfaceflinger\DispSync.cpp&#160; &#160; &#160; &#160;frameworks\native\services\surfaceflinger\SurfaceFlinger.cpp&#160; &#160; &#160; &#160;frameworks\native\services\surfaceflinger\SurfaceFlinger.cpp$DispSyncSource&#160; &#160; &#160; &#160;frameworks\native\services\surfaceflinger\EventThread.cpp&#160; &#160; &#160; &#160;frameworks\native\services\surfaceflinger\MessageQueue.cpp&#160; &#160; &#160; &#160;frameworks\native\libs\gui\BitTube.cpp&#160; &#160; &#160; &#160;frameworks\native\libs\gui\BufferQueueProducer.cpp DispSync.cpp：这个class包含了DispSyncThread，是SW-SYNC的心脏，所有的SW-SYNCevent均由其产生。在Android系统中,只有一个DispSync； SurfaceFlinger.cpp：这个类主要处理layer的合成，它合成好相关的layer后发送command给HW display进行进行显示； DispSyncSource：Vsync source 在Android系统中有两个instance，一是Vsync-app，另一个Vsync-sf。当SW-SYNC发生时，Vsyncsource会callback到其相应的EventThread，并且会在Systrace上显示出Vsync-sf和Vsync-app的跳变； EventThread.cpp：Vsync event处理线程，在系统中有两个EventThread，一个用于Vsync-app，另一个用于Vsync-sf。其记录App和SurfaceFlinger的requestnextVsync()请求，当SW-VSYNCevent触发时，EventThread线程通过BitTube通知App或SurfaceFlinger，App开始drawview，SurfaceFlinger 开始合成 dirty layer； MessageQueue.cpp：MessageQueue 主要处理surfaceflinger的Vsync请求和发生Vsync事件给surfaceFlinger； BitTube.cpp：Vsync 事件的传输通道。App或Surfaceflinger首先与EventThread建立DisplayEventConnection(EventThread::Connection::Connection,Connection 是BnDisplayEventConnection子类，BitTube是其dataChannel)。App或surfaceFlinger通过call DisplayEventConnection::requestNextVsync()(binder 通信)向EventThread请求Vsync event，当EventThread收到SW-VSYNCevent时，其通过BitTube把Vsync evnet发送给App或SufaceFlinger； BufferQueueProducer.cpp：在Vsync架构中，其主要作用是向EventThread请求Vsync-sfevent。当App画完一个frame后，其会把画完的buffer放到bufferqueue中通过call BufferQueueProducer::queueBuffer()，进而surfaceflinger进程会通过call DisplayEventConnection:: requestNextVsync()向EventThread请求Vsync event。 &#160; &#160; &#160; &#160;Vsyncevent产生的示意图如下： &#160; &#160; &#160; &#160;App需要draw一个frame时，其会向EventThread(appEventThread)请求Vysncevent，当EventThread收到Vsync event时，EventThread通过BitTueb把Vsync event通知到App，同时跳变systrace中的Vsync-app。App收到Vsync-app，开始draw frame。当App画完一个frame后，把画好的内容放到bufferqueue中，就会要求一个Vsync-sf event，以便surfaceflinger在收到Vsync-sf event时合成相关的dirty的内容并通知DisplayHW。Display HW 会在下一个HWvsync时间点，把相关的内容更新到LCD上完成内容的更新。 &#160; &#160; &#160; &#160;下面是其大概的流程图： (SF-1)APP 画完一个frame以后,就会把其绘画的buffer放到buffer queue中.从生产者和消费者的关系来看,App是生产者,surfaceflinger进程是消费者. (SF-2)在SurfaceFlinger 进程内,当收到有更新的可用的frame时(需要render到LCD ),就会向EventThread请求Vsync event(requestNextVsync()).EventThread会记录下此请求,当下次Vsync event到来时,便会triggerVsync-sf event. DispSync 不断产生SW-VSYNCEVENT.当SW-VSYNCEVENT产生时,会检查是否有EventListener关心该event(SF和APP请求Vsync 时会在DispSync中创建EventListener并把其相应的DispSyncSource作为EventListener的callback),如有则调用EventListenercallback(DispSyncSource)的onDispSyncEvent.(callbacks[i].mCallback-&gt;onDispSyncEvent(callbacks[i].mEventTime)). DispSyncSource会引起Vsync-sf或Vsync-app跳变,当onDispSyncEvent()被调用时,并且会直接调用EventThread的onVSyncEvent(). EventThread会把Vsync-sf或Vsync-app event通知到App或surfaceFlinger当Vsync-sfevent产生时(callonDispSyncEvent),surfaceflinger进程合成dirty layer的内容(SF-5)并通知Display HW把相关的更新到LCD上.App则在Vsync-ap时开始drawview. Display HW 便会在HW 的vsync 到来时,更新LCD 的内容(SF-6). 如果有Layer 的内容需要更新,surfaceflinger 便会把相关内容合成在一起,并且通知DisplayHW ,有相关的更新内容. 程序实现&#160; &#160; &#160; &#160;我们来看看其代码的实现，继续回到SurfaceFlinger.cpp中的init函数：1234567891011121314151617181920212223void SurfaceFlinger::init() &#123;...... // start the EventThread sp&lt;VSyncSource&gt; vsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, vsyncPhaseOffsetNs, true, "app"); mEventThread = new EventThread(vsyncSrc); sp&lt;VSyncSource&gt; sfVsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, sfVsyncPhaseOffsetNs, true, "sf"); mSFEventThread = new EventThread(sfVsyncSrc); mEventQueue.setEventThread(mSFEventThread); mEventControlThread = new EventControlThread(this); mEventControlThread-&gt;run("EventControl", PRIORITY_URGENT_DISPLAY); // set a fake vsync period if there is no HWComposer if (mHwc-&gt;initCheck() != NO_ERROR) &#123; mPrimaryDispSync.setPeriod(16666667); &#125;......&#125; &#160; &#160; &#160; &#160;当surfaceFlinger初始化时，其创建了两个DispSyncSource和两个EventThread，一个用于APP，而另一个用于SurfaceFlinger本身。我们知道DispSyncSource和DispSync协同工作，共同完成Vsyncevent的fire。而EventThread会trigger App draw frame和surfaceFlinger合成”dirty” layer在SW-VSYNCevent产生时。 &#160; &#160; &#160; &#160;上面图形是Vsync信号从产生到驱动SF去工作的一个过程，其中绿色部分是HWC中的Vsync信号软件模拟进程，其他都是SF中的：&#160; &#160; &#160; &#160;其中包含4个线程，EventControlThread，就像是Vsync信号产生的闸刀，当然闸刀肯定需要人去打开和关闭，这个人就是SF；&#160; &#160; &#160; &#160;VsyncThread，下面的代码只介绍软件模拟的Vsync信号，这个线程主要工作就是循环定期产生信号，然后调用SF中的函数，这就相当于触发了；&#160; &#160; &#160; &#160;DispSyncThread，是Vsync信号的模型，VsyncThread首先触发DispSyncThread，然后DispSyncThread再去驱动其他事件，它就是Vsync在SF中的代表；&#160; &#160; &#160; &#160;EventThread，具体的事件线程，由DispSyncThread去驱动。 &#160; &#160; &#160; &#160;我们接下来根据Vsync-sf先关分析一下流程。 DispSync和DispSyncThread&#160; &#160; &#160; &#160;从上面SurfaceFlinger的init函数中，在构造DispSyncSource会传入一个mPrimaryDispSync的引用，它是SF类中有一个field为DispSync mPrimaryDispSync，因此在SF类创建时会在栈上生成这个对象。我们首先看看这个类的构造函数：1234567891011DispSync::DispSync() : mRefreshSkipCount(0), mThread(new DispSyncThread()) &#123; //创建DispSyncThread线程，并运行 mThread-&gt;run("DispSync", PRIORITY_URGENT_DISPLAY + PRIORITY_MORE_FAVORABLE); //清除一些变量为0 reset(); beginResync(); ......&#125; &#160; &#160; &#160; &#160;在DispSync 构造函数中，创建了DispSyncThread线程，并运行。DispSyncThread实现也位于DispSync.cpp中：123456DispSyncThread(): mStop(false), mPeriod(0), mPhase(0), mWakeupLatency(0) &#123;&#125; &#160; &#160; &#160; &#160;这里面将mStop设置为false，mPeriod设置为0，还有其他的也置为0。因为DispSyncThread是Thread派生的，所以我们需要看看它的线程函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172virtual bool threadLoop() &#123; status_t err; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); nsecs_t nextEventTime = 0; while (true) &#123; Vector&lt;CallbackInvocation&gt; callbackInvocations; nsecs_t targetTime = 0; &#123; // Scope for lock Mutex::Autolock lock(mMutex); if (mStop) &#123;//false return false; &#125; if (mPeriod == 0) &#123;//0 //mCond阻塞，当signal，同时mPeriod 不为0时，继续往下执行， err = mCond.wait(mMutex); if (err != NO_ERROR) &#123; ALOGE("error waiting for new events: %s (%d)", strerror(-err), err); return false; &#125; continue; &#125; //计算下一次vsync信号时间时间 nextEventTime = computeNextEventTimeLocked(now); targetTime = nextEventTime; bool isWakeup = false; //如果还没到下一次vsync时间，则等待中间间隔时间 if (now &lt; targetTime) &#123; err = mCond.waitRelative(mMutex, targetTime - now); //等待超时，则唤醒 if (err == TIMED_OUT) &#123; isWakeup = true; &#125; else if (err != NO_ERROR) &#123; ALOGE("error waiting for next event: %s (%d)", strerror(-err), err); return false; &#125; &#125; now = systemTime(SYSTEM_TIME_MONOTONIC); //唤醒触发 if (isWakeup) &#123; //如果唤醒了，则计算Latency，如果Latency大于500000则强制Latency为500000 mWakeupLatency = ((mWakeupLatency * 63) + (now - targetTime)) / 64; if (mWakeupLatency &gt; 500000) &#123; // Don't correct by more than 500 us mWakeupLatency = 500000; &#125; if (kTraceDetailedInfo) &#123; ATRACE_INT64("DispSync:WakeupLat", now - nextEventTime); ATRACE_INT64("DispSync:AvgWakeupLat", mWakeupLatency); &#125; &#125; //gatherCallbackInvocationsLocked函数获取本次VSync信号的回调函数列表 //些回调函数是通过DispSync类的addEventListener函数加入的 callbackInvocations = gatherCallbackInvocationsLocked(now); &#125; //接着就调用fireCallbackInvocations来依次调用列表中所有对象的onDispSyncEvent函数 if (callbackInvocations.size() &gt; 0) &#123; fireCallbackInvocations(callbackInvocations); &#125; &#125; return false;&#125; &#160; &#160; &#160; &#160;DispSyncThread的threadLoop函数，主要这个函数比较计算时间来决定是否要发送信号。主要工作如下：&#160; &#160; &#160; &#160;1. 模型线程DispSyncThread阻塞在mCond，等待别人给mPeriod 赋值和signal；&#160; &#160; &#160; &#160;2. gatherCallbackInvocationsLocked函数获取本次VSync信号的回调函数列表，这些回调函数是通过DispSync类的addEventListener函数加入的；&#160; &#160; &#160; &#160;3. 接着就调用fireCallbackInvocations来依次调用列表中所有对象的onDispSyncEvent函数。 &#160; &#160; &#160; &#160;新建的DispSyncThread线程，目前被阻塞，先不管，我们先看模型DispSync和要驱动的事件(DispSyncSource，EventThread等)是如何联系起来的。 DispSyncSource和EventThread&#160; &#160; &#160; &#160;在SF的init函数中，有如下代码，涉及到了DispSync，DispSyncSource，EventThread和mEventQueue的纠缠关系。我们可以看到在驱动事件DispSyncSource的构造中，我们输入了模型DispSync，这样就为回调创造了机会，下面看具体如何实现的。12345678910111213void SurfaceFlinger::init() &#123; ...... // start the EventThread //把模型mPrimaryDispSync(DispSync)保存在DispSyncSource中 sp&lt;VSyncSource&gt; vsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, vsyncPhaseOffsetNs, true, "app"); mEventThread = new EventThread(vsyncSrc); sp&lt;VSyncSource&gt; sfVsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, sfVsyncPhaseOffsetNs, true, "sf"); mSFEventThread = new EventThread(sfVsyncSrc); mEventQueue.setEventThread(mSFEventThread); ......&#125; &#160; &#160; &#160; &#160;这个我们分布分析：&#160; &#160; &#160; &#160;1）先看下DispSyncSource对象的建立，其实这个对象从名字上看就是模型所驱动的事件。我们看看它的构造函数，也位于SurfaceFlinger.cpp中：12345678DispSyncSource(DispSync* dispSync, nsecs_t phaseOffset, bool traceVsync, const char* label) : mValue(0), mPhaseOffset(phaseOffset), mTraceVsync(traceVsync), mVsyncOnLabel(String8::format("VsyncOn-%s", label)), mVsyncEventLabel(String8::format("VSYNC-%s", label)), mDispSync(dispSync) &#123;&#125; &#160; &#160; &#160; &#160;构造函数中主要设置了模型mDispSync(dispSync)，以及触发时间的偏移量mPhaseOffset(phaseOffset)。这个实参vsyncPhaseOffsetNs和sfVsyncPhaseOffsetNs定义如下，也位于SurfaceFlinger.cpp中：123456789101112131415161718192021222324// This is the phase offset in nanoseconds of the software vsync event// relative to the vsync event reported by HWComposer. The software vsync// event is when SurfaceFlinger and Choreographer-based applications run each// frame.//// This phase offset allows adjustment of the minimum latency from application// wake-up (by Choregographer) time to the time at which the resulting window// image is displayed. This value may be either positive (after the HW vsync)// or negative (before the HW vsync). Setting it to 0 will result in a// minimum latency of two vsync periods because the app and SurfaceFlinger// will run just after the HW vsync. Setting it to a positive number will// result in the minimum latency being://// (2 * VSYNC_PERIOD - (vsyncPhaseOffsetNs % VSYNC_PERIOD))//// Note that reducing this latency makes it more likely for the applications// to not have their window content image ready in time. When this happens// the latency will end up being an additional vsync period, and animations// will hiccup. Therefore, this latency should be tuned somewhat// conservatively (or at least with awareness of the trade-off being made).static const int64_t vsyncPhaseOffsetNs = VSYNC_EVENT_PHASE_OFFSET_NS;// This is the phase offset at which SurfaceFlinger's composition runs.static const int64_t sfVsyncPhaseOffsetNs = SF_VSYNC_EVENT_PHASE_OFFSET_NS; &#160; &#160; &#160; &#160;这个phase就是用来调节vsync信号latency 时间，其定义我也是搜了整个工程才找到如下定义： &#160; &#160; &#160; &#160;generic版本中phase-app为0，phase-sf为5000000。（不知道对不对=。=） &#160; &#160; &#160; &#160;2）我们继续往下看，mSFEventThread = new EventThread(sfVsyncSrc);先看看EventThread的构造函数：1234567891011121314151617181920EventThread::EventThread(const sp&lt;VSyncSource&gt;&amp; src) : mVSyncSource(src), mUseSoftwareVSync(false),//软vsync初始为false mVsyncEnabled(false),//vsync使能初始为false mDebugVsyncEnabled(false), mVsyncHintSent(false) &#123; //i&lt;2,只遍历主屏幕和扩展屏幕 for (int32_t i=0 ; i&lt;DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES ; i++) &#123; mVSyncEvent[i].header.type = DisplayEventReceiver::DISPLAY_EVENT_VSYNC; mVSyncEvent[i].header.id = 0; mVSyncEvent[i].header.timestamp = 0; mVSyncEvent[i].vsync.count = 0; &#125; struct sigevent se; se.sigev_notify = SIGEV_THREAD; se.sigev_value.sival_ptr = this; se.sigev_notify_function = vsyncOffCallback; se.sigev_notify_attributes = NULL; timer_create(CLOCK_MONOTONIC, &amp;se, &amp;mTimerId);&#125; &#160; &#160; &#160; &#160;EventThread是Thread的派生类，还是RefBase的间接派生类，所以我们需要产看它的onFirstRef函数：123void EventThread::onFirstRef() &#123; run("EventThread", PRIORITY_URGENT_DISPLAY + PRIORITY_MORE_FAVORABLE);&#125; &#160; &#160; &#160; &#160;将这个线程启动了起来，那么我们需要看下线程循环函数：12345678910111213141516171819202122232425262728293031bool EventThread::threadLoop() &#123; DisplayEventReceiver::Event event; Vector&lt; sp&lt;EventThread::Connection&gt; &gt; signalConnections; //等待事件到来 signalConnections = waitForEvent(&amp;event); // dispatch events to listeners... //把事件分发给listener const size_t count = signalConnections.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Connection&gt;&amp; conn(signalConnections[i]); // now see if we still need to report this event //然后调用每个连接的postEvent来发送Event status_t err = conn-&gt;postEvent(event); if (err == -EAGAIN || err == -EWOULDBLOCK) &#123; // The destination doesn't accept events anymore, it's probably // full. For now, we just drop the events on the floor. // FIXME: Note that some events cannot be dropped and would have // to be re-sent later. // Right-now we don't have the ability to do this. ALOGW("EventThread: dropping event (%08x) for connection %p", event.header.type, conn.get()); &#125; else if (err &lt; 0) &#123; // handle any other error on the pipe as fatal. the only // reasonable thing to do is to clean-up this connection. // The most common error we'll get here is -EPIPE. removeDisplayEventConnection(signalConnections[i]); &#125; &#125; return true;&#125; &#160; &#160; &#160; &#160;这个线程函数里面主要做了三件事：&#160; &#160; &#160; &#160;1. 调用waitForEvent等待事件到来；&#160; &#160; &#160; &#160;2. 把事件分发给listener；&#160; &#160; &#160; &#160;3. 然后调用每个连接的postEvent来发送Event。 &#160; &#160; &#160; &#160;我们分部查看，先看看waitForEvent实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140 // This will return when (1) a vsync event has been received, and (2) there was// at least one connection interested in receiving it when we started waiting.Vector&lt; sp&lt;EventThread::Connection&gt; &gt; EventThread::waitForEvent( DisplayEventReceiver::Event* event)&#123; Mutex::Autolock _l(mLock); Vector&lt; sp&lt;EventThread::Connection&gt; &gt; signalConnections; do &#123; bool eventPending = false; bool waitForVSync = false; size_t vsyncCount = 0; nsecs_t timestamp = 0; //上面初始化EventThread时候，都是0 for (int32_t i=0 ; i&lt;DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES ; i++) &#123; timestamp = mVSyncEvent[i].header.timestamp; if (timestamp) &#123; // we have a vsync event to dispatch *event = mVSyncEvent[i]; mVSyncEvent[i].header.timestamp = 0; vsyncCount = mVSyncEvent[i].vsync.count; break; &#125; &#125; if (!timestamp) &#123;//0 // no vsync event, see if there are some other event eventPending = !mPendingEvents.isEmpty(); if (eventPending) &#123;//初始时候为false // we have some other event to dispatch *event = mPendingEvents[0]; mPendingEvents.removeAt(0); &#125; &#125; // find out connections waiting for events //// 初始也是空的 SortedVector&lt; wp&lt;Connection&gt; &gt; mDisplayEventConnections; size_t count = mDisplayEventConnections.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; sp&lt;Connection&gt; connection(mDisplayEventConnections[i].promote()); if (connection != NULL) &#123; bool added = false; if (connection-&gt;count &gt;= 0) &#123; // we need vsync events because at least // one connection is waiting for it waitForVSync = true; if (timestamp) &#123; // we consume the event only if it's time // (ie: we received a vsync event) if (connection-&gt;count == 0) &#123; // fired this time around connection-&gt;count = -1; signalConnections.add(connection); added = true; &#125; else if (connection-&gt;count == 1 || (vsyncCount % connection-&gt;count) == 0) &#123; // continuous event, and time to report it signalConnections.add(connection); added = true; &#125; &#125; &#125; if (eventPending &amp;&amp; !timestamp &amp;&amp; !added) &#123; // we don't have a vsync event to process // (timestamp==0), but we have some pending // messages. signalConnections.add(connection); &#125; &#125; else &#123; // we couldn't promote this reference, the connection has // died, so clean-up! mDisplayEventConnections.removeAt(i); --i; --count; &#125; &#125; // Here we figure out if we need to enable or disable vsyncs if (timestamp &amp;&amp; !waitForVSync) &#123; // we received a VSYNC but we have no clients // don't report it, and disable VSYNC events disableVSyncLocked(); &#125; else if (!timestamp &amp;&amp; waitForVSync) &#123; // we have at least one client, so we want vsync enabled // (TODO: this function is called right after we finish // notifying clients of a vsync, so this call will be made // at the vsync rate, e.g. 60fps. If we can accurately // track the current state we could avoid making this call // so often.) enableVSyncLocked(); &#125; // note: !timestamp implies signalConnections.isEmpty(), because we // don't populate signalConnections if there's no vsync pending if (!timestamp &amp;&amp; !eventPending) &#123; // wait for something to happen if (waitForVSync) &#123; // This is where we spend most of our time, waiting // for vsync events and new client registrations. // // If the screen is off, we can't use h/w vsync, so we // use a 16ms timeout instead. It doesn't need to be // precise, we just need to keep feeding our clients. // // We don't want to stall if there's a driver bug, so we // use a (long) timeout when waiting for h/w vsync, and // generate fake events when necessary. bool softwareSync = mUseSoftwareVSync; nsecs_t timeout = softwareSync ? ms2ns(16) : ms2ns(1000); if (mCondition.waitRelative(mLock, timeout) == TIMED_OUT) &#123; if (!softwareSync) &#123; ALOGW("Timed out waiting for hw vsync; faking it"); &#125; // FIXME: how do we decide which display id the fake // vsync came from ? mVSyncEvent[0].header.type = DisplayEventReceiver::DISPLAY_EVENT_VSYNC; mVSyncEvent[0].header.id = DisplayDevice::DISPLAY_PRIMARY; mVSyncEvent[0].header.timestamp = systemTime(SYSTEM_TIME_MONOTONIC); mVSyncEvent[0].vsync.count++; &#125; &#125; else &#123; // Nobody is interested in vsync, so we just want to sleep. // h/w vsync should be disabled, so this will wait until we // get a new connection, or an existing connection becomes // interested in receiving vsync again. //所以EventThread初始的时候会在这阻塞 mCondition.wait(mLock); &#125; &#125; &#125; while (signalConnections.isEmpty()); //因为mCondition有可能异常返回，所以要看下这个while，就知道何时mCondition被正常的signal //如果signalConnections不为空了，这时候就会从while中退出来 //也就是上面的mDisplayEventConnections有东西了 // here we're guaranteed to have a timestamp and some connections to signal // (The connections might have dropped out of mDisplayEventConnections // while we were asleep, but we'll still have strong references to them.) return signalConnections;&#125; &#160; &#160; &#160; &#160;waitForEvent函数中下面代码段，timestamp为0表示没有时间，waitForSync为true表示至少有一个客户和EventThread建立了连接。这段代码一旦有客户连接，就调用enableVSyncLocked接收DispSyncSource的VSync信号。如果在接受信号中，所有客户都断开了连接，则调用disableVSyncLocked函数停止接受DispSyncSource对象的信号。 &#160; &#160; &#160; &#160;虽然代码量有些多，但是我们是初始化时候，所以省去了很多判断逻辑。因此主要工作就不多了，主要如下：&#160; &#160; &#160; &#160;①初始的时候，mCondition会阻塞；&#160; &#160; &#160; &#160;② 因为mCondition有可能异常返回，所以要看下外围的while循环，就知道何时mCondition会被signal。即使mCondition异常返回，也会再去判断signalConnections是否为空。空的话继续阻塞，如果signalConnections不为空了，这时候就会从while中退出来，也就是上面的mDisplayEventConnections有东西了。所以mDisplayEventConnections需要留意何时赋值啦。 &#160; &#160; &#160; &#160;至此，创建了一个mSFEventThread = new EventThread(sfVsyncSrc);，也阻塞着了。。 &#160; &#160; &#160; &#160;至于下面的 2. 把事件分发给listener；3. 然后调用每个连接的postEvent来发送Event。这两个步骤我们下面会讲到，这里先不看它。 &#160; &#160; &#160; &#160;3）从上面得知在DsipSync里面创建的DispSyncThread线程阻塞等待着；还有上面的EventThread线程，也阻塞等待了。&#160; &#160; &#160; &#160;下面我们继续看mEventQueue.setEventThread(mSFEventThread);会不会解除阻塞。这里EventThread和SF主线程的MessageQueue又纠缠到了一起：1234567891011void MessageQueue::setEventThread(const sp&lt;EventThread&gt;&amp; eventThread)&#123; mEventThread = eventThread; //SF与EventThread建立连接 mEvents = eventThread-&gt;createEventConnection(); //创建通信通道 mEventTube = mEvents-&gt;getDataChannel(); //Looper连接通道，进行消息循环 mLooper-&gt;addFd(mEventTube-&gt;getFd(), 0, Looper::EVENT_INPUT, MessageQueue::cb_eventReceiver, this);&#125; &#160; &#160; &#160; &#160;我们依然分部查看：&#160; &#160; &#160; &#160;1. 首先， eventThread-&gt;createEventConnection()，新建了一个Connection对象：123sp&lt;EventThread::Connection&gt; EventThread::createEventConnection() const &#123; return new Connection(const_cast&lt;EventThread*&gt;(this));&#125; &#160; &#160; &#160; &#160;Connection对象如下图所示: &#160; &#160; &#160; &#160;Connection中保存了了一个EventThread对象，和一个生成的BitTube对象mChannel，下面看下Connection的构造函数：12345EventThread::Connection::Connection( const sp&lt;EventThread&gt;&amp; eventThread) : count(-1), mEventThread(eventThread), mChannel(new BitTube())&#123;&#125; &#160; &#160; &#160; &#160;调用了BitTube的无参构造函数：12345BitTube::BitTube() : mSendFd(-1), mReceiveFd(-1)&#123; init(DEFAULT_SOCKET_BUFFER_SIZE, DEFAULT_SOCKET_BUFFER_SIZE);&#125; &#160; &#160; &#160; &#160;构造函数里调用了init函数：123456789101112131415161718void BitTube::init(size_t rcvbuf, size_t sndbuf) &#123; int sockets[2]; if (socketpair(AF_UNIX, SOCK_SEQPACKET, 0, sockets) == 0) &#123; size_t size = DEFAULT_SOCKET_BUFFER_SIZE; setsockopt(sockets[0], SOL_SOCKET, SO_RCVBUF, &amp;rcvbuf, sizeof(rcvbuf)); setsockopt(sockets[1], SOL_SOCKET, SO_SNDBUF, &amp;sndbuf, sizeof(sndbuf)); // sine we don't use the "return channel", we keep it small... setsockopt(sockets[0], SOL_SOCKET, SO_SNDBUF, &amp;size, sizeof(size)); setsockopt(sockets[1], SOL_SOCKET, SO_RCVBUF, &amp;size, sizeof(size)); fcntl(sockets[0], F_SETFL, O_NONBLOCK); fcntl(sockets[1], F_SETFL, O_NONBLOCK); mReceiveFd = sockets[0]; mSendFd = sockets[1]; &#125; else &#123; mReceiveFd = -errno; ALOGE("BitTube: pipe creation failed (%s)", strerror(-mReceiveFd)); &#125;&#125; &#160; &#160; &#160; &#160;建立一个域套接字，用fcntl设置成非阻塞模式，然后两个fd，一个读一个写，mSendFd和mReceiveFd。 &#160; &#160; &#160; &#160;我们上面创建一个Connection，但是又是一个sp指针，所以就得看看它的onFirstRef函数：1234void EventThread::Connection::onFirstRef() &#123; // NOTE: mEventThread doesn't hold a strong reference on us mEventThread-&gt;registerDisplayEventConnection(this);&#125; &#160; &#160; &#160; &#160;这里又调用EventThread的registerDisplayEventConnection函数，我们继续查看：123456789status_t EventThread::registerDisplayEventConnection( const sp&lt;EventThread::Connection&gt;&amp; connection) &#123; Mutex::Autolock _l(mLock); //把connection添加到mDisplayEventConnections mDisplayEventConnections.add(connection); //mCondition解除 mCondition.broadcast(); return NO_ERROR;&#125; &#160; &#160; &#160; &#160;前面提到，EventThread一直阻塞在waitForEvent中，正是这个mCondition，这里也对mDisplayEventConnections添加了东西，不为空了。&#160; &#160; &#160; &#160;为了方便分析，我们再把前面的waitForEvent函数列出来，这次就是另一套逻辑了：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136// This will return when (1) a vsync event has been received, and (2) there was// at least one connection interested in receiving it when we started waiting.Vector&lt; sp&lt;EventThread::Connection&gt; &gt; EventThread::waitForEvent( DisplayEventReceiver::Event* event)&#123; Mutex::Autolock _l(mLock); Vector&lt; sp&lt;EventThread::Connection&gt; &gt; signalConnections; do &#123; bool eventPending = false; bool waitForVSync = false; size_t vsyncCount = 0; nsecs_t timestamp = 0; for (int32_t i=0 ; i&lt;DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES ; i++) &#123; timestamp = mVSyncEvent[i].header.timestamp; if (timestamp) &#123; // we have a vsync event to dispatch *event = mVSyncEvent[i]; mVSyncEvent[i].header.timestamp = 0; vsyncCount = mVSyncEvent[i].vsync.count; break; &#125; &#125; if (!timestamp) &#123;//这次这里还是0 // no vsync event, see if there are some other event eventPending = !mPendingEvents.isEmpty(); if (eventPending) &#123; // we have some other event to dispatch *event = mPendingEvents[0]; mPendingEvents.removeAt(0); &#125; &#125; // find out connections waiting for events //有东西了，就是保存的Connection size_t count = mDisplayEventConnections.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; sp&lt;Connection&gt; connection(mDisplayEventConnections[i].promote()); if (connection != NULL) &#123; bool added = false; if (connection-&gt;count &gt;= 0) &#123; // we need vsync events because at least // one connection is waiting for it waitForVSync = true; if (timestamp) &#123; // we consume the event only if it's time // (ie: we received a vsync event) if (connection-&gt;count == 0) &#123; // fired this time around connection-&gt;count = -1; signalConnections.add(connection); added = true; &#125; else if (connection-&gt;count == 1 || (vsyncCount % connection-&gt;count) == 0) &#123; // continuous event, and time to report it signalConnections.add(connection); added = true; &#125; &#125; &#125; if (eventPending &amp;&amp; !timestamp &amp;&amp; !added) &#123; // we don't have a vsync event to process // (timestamp==0), but we have some pending // messages. signalConnections.add(connection); &#125; &#125; else &#123; // we couldn't promote this reference, the connection has // died, so clean-up! mDisplayEventConnections.removeAt(i); --i; --count; &#125; &#125; // Here we figure out if we need to enable or disable vsyncs if (timestamp &amp;&amp; !waitForVSync) &#123;//0 true // we received a VSYNC but we have no clients // don't report it, and disable VSYNC events disableVSyncLocked(); &#125; else if (!timestamp &amp;&amp; waitForVSync) &#123;//所以走到了这里 // we have at least one client, so we want vsync enabled // (TODO: this function is called right after we finish // notifying clients of a vsync, so this call will be made // at the vsync rate, e.g. 60fps. If we can accurately // track the current state we could avoid making this call // so often.) //这次走到了这里 enableVSyncLocked(); &#125; // note: !timestamp implies signalConnections.isEmpty(), because we // don't populate signalConnections if there's no vsync pending if (!timestamp &amp;&amp; !eventPending) &#123;//0 false // wait for something to happen if (waitForVSync) &#123;//true // This is where we spend most of our time, waiting // for vsync events and new client registrations. // // If the screen is off, we can't use h/w vsync, so we // use a 16ms timeout instead. It doesn't need to be // precise, we just need to keep feeding our clients. // // We don't want to stall if there's a driver bug, so we // use a (long) timeout when waiting for h/w vsync, and // generate fake events when necessary. bool softwareSync = mUseSoftwareVSync; nsecs_t timeout = softwareSync ? ms2ns(16) : ms2ns(1000); if (mCondition.waitRelative(mLock, timeout) == TIMED_OUT) &#123; if (!softwareSync) &#123; ALOGW("Timed out waiting for hw vsync; faking it"); &#125; // FIXME: how do we decide which display id the fake // vsync came from ? mVSyncEvent[0].header.type = DisplayEventReceiver::DISPLAY_EVENT_VSYNC; mVSyncEvent[0].header.id = DisplayDevice::DISPLAY_PRIMARY; mVSyncEvent[0].header.timestamp = systemTime(SYSTEM_TIME_MONOTONIC); mVSyncEvent[0].vsync.count++; &#125; &#125; else &#123; // Nobody is interested in vsync, so we just want to sleep. // h/w vsync should be disabled, so this will wait until we // get a new connection, or an existing connection becomes // interested in receiving vsync again. mCondition.wait(mLock);//这个之前的wait已经解开了 &#125; &#125; &#125; while (signalConnections.isEmpty()); // here we're guaranteed to have a timestamp and some connections to signal // (The connections might have dropped out of mDisplayEventConnections // while we were asleep, but we'll still have strong references to them.) return signalConnections;&#125; &#160; &#160; &#160; &#160;所以在waitEvent里，这次的逻辑和上次不同了，主要是一下几点：&#160; &#160; &#160; &#160;①建立Connection后，mCondition返回了；&#160; &#160; &#160; &#160;②这里timestamp 还是0 ;&#160; &#160; &#160; &#160;③mDisplayEventConnections非空了，将waitForVSync = true;&#160; &#160; &#160; &#160;④所以会执行到enableVSyncLocked();。 &#160; &#160; &#160; &#160;那么我们看看enableVSyncLocked函数：1234567891011121314void EventThread::enableVSyncLocked() &#123; if (!mUseSoftwareVSync) &#123;//false // never enable h/w VSYNC when screen is off if (!mVsyncEnabled) &#123;//false mVsyncEnabled = true;//将mVsyncEnabled 置为true //DispSyncSource的setCallback，把mCallback设置为EventThread mVSyncSource-&gt;setCallback(static_cast&lt;VSyncSource::Callback*&gt;(this)); //调用DispSyncSource的setVSyncEnabled mVSyncSource-&gt;setVSyncEnabled(true); &#125; &#125; mDebugVsyncEnabled = true; sendVsyncHintOnLocked();&#125; &#160; &#160; &#160; &#160;这个也分两步：&#160; &#160; &#160; &#160;①将mVsyncEnabled 设为true，调用mVSyncSource-&gt;setCallback(static_cast(this));，this为EventThread，既是调用DispSyncSource的setCallback，把mCallback设置为EventThread：1234virtual void setCallback(const sp&lt;VSyncSource::Callback&gt;&amp; callback) &#123; Mutex::Autolock lock(mMutex); mCallback = callback;&#125; &#160; &#160; &#160; &#160;②调用mVSyncSource-&gt;setVSyncEnabled(true);，既是调用DispSyncSource的setVSyncEnabled：12345678910111213141516171819202122virtual void setVSyncEnabled(bool enable) &#123;//true // Do NOT lock the mutex here so as to avoid any mutex ordering issues // with locking it in the onDispSyncEvent callback. if (enable) &#123; ////在硬件模型mDispSync中添加addEventListener，一个参数为偏移量，一个为DispSyncSource status_t err = mDispSync-&gt;addEventListener(mPhaseOffset, static_cast&lt;DispSync::Callback*&gt;(this)); if (err != NO_ERROR) &#123; ALOGE("error registering vsync callback: %s (%d)", strerror(-err), err); &#125; //ATRACE_INT(mVsyncOnLabel.string(), 1); &#125; else &#123; status_t err = mDispSync-&gt;removeEventListener( static_cast&lt;DispSync::Callback*&gt;(this)); if (err != NO_ERROR) &#123; ALOGE("error unregistering vsync callback: %s (%d)", strerror(-err), err); &#125; //ATRACE_INT(mVsyncOnLabel.string(), 0); &#125;&#125; &#160; &#160; &#160; &#160;从上面的代码可以看出，这里是将驱动事件DispSyncSource和硬件模型mDispSync建立起关系。既然调用了DispSync的addEventListener，那么我们就继续查看：12345678910111213141516171819202122232425262728293031323334//DispSync的addEventListenerstatus_t DispSync::addEventListener(nsecs_t phase, const sp&lt;Callback&gt;&amp; callback) &#123; Mutex::Autolock lock(mMutex); return mThread-&gt;addEventListener(phase, callback);&#125; //DispSyncThread的addEventListener status_t addEventListener(nsecs_t phase, const sp&lt;DispSync::Callback&gt;&amp; callback) &#123; Mutex::Autolock lock(mMutex); for (size_t i = 0; i &lt; mEventListeners.size(); i++) &#123; if (mEventListeners[i].mCallback == callback) &#123; return BAD_VALUE; &#125; &#125; EventListener listener; listener.mPhase = phase; listener.mCallback = callback; // We want to allow the firstmost future event to fire without // allowing any past events to fire. Because // computeListenerNextEventTimeLocked filters out events within a half // a period of the last event time, we need to initialize the last // event time to a half a period in the past. listener.mLastEventTime = systemTime(SYSTEM_TIME_MONOTONIC) - mPeriod / 2; ////把listener放到mEventListeners中 mEventListeners.push(listener); ////释放mCond mCond.signal(); return NO_ERROR; &#125; &#160; &#160; &#160; &#160;这里可以看出，驱动事件DispSyncSource是硬件模型DispSync的“listener”，监听者，把两者联系了起来。并把DispSyncThread线程中的阻塞mCond解除，但是，前面我们分析过，还要mPeriod 非0。我们可以回顾一下上面的代码片段，DispSyncThread的threadLoop函数：1234567891011121314151617virtual bool threadLoop() &#123; ...... while(true)&#123; ...... if (mPeriod == 0) &#123; err = mCond.wait(mMutex); if (err != NO_ERROR) &#123; ALOGE("error waiting for new events: %s (%d)", strerror(-err), err); return false; &#125; continue; &#125; ...... &#125; ......&#125; &#160; &#160; &#160; &#160;那么哪里给mPeriod 赋值呢？我们上面讲EventControlThread闸刀控制线程的时候，initializeDisplays()函数会被调用，最终会调用resyncToHardwareVsync函数，这个里面会获取mPeriod屏幕刷新率，然后给mPeriod 赋值：123456789void SurfaceFlinger::resyncToHardwareVsync(bool makeAvailable) &#123;...... const nsecs_t period = getHwComposer().getRefreshPeriod(HWC_DISPLAY_PRIMARY); mPrimaryDispSync.reset(); mPrimaryDispSync.setPeriod(period);......&#125; &#160; &#160; &#160; &#160;然后调用DispSync的setPeriod函数：1234567void DispSync::setPeriod(nsecs_t period) &#123; Mutex::Autolock lock(mMutex); mPeriod = period;//mPeriod 赋值后，已经不为0 mPhase = 0; //调用线程的更新模型函数 mThread-&gt;updateModel(mPeriod, mPhase);&#125; &#160; &#160; &#160; &#160;mPeriod 赋值后，已经不为0；然后调用线程的更新模型函数updateModel：1234567void updateModel(nsecs_t period, nsecs_t phase) &#123; Mutex::Autolock lock(mMutex); mPeriod = period; mPhase = phase; //mCond阻塞解除 mCond.signal();&#125; &#160; &#160; &#160; &#160;至此，DispSync中设置了监听者DispSyncSource，mPeriod 也不为0，硬件模型线程不再阻塞，不阻塞就会去处理vsync事件，这个我们后面会讲。 &#160; &#160; &#160; &#160;经过调用mEvents = eventThread-&gt;createEventConnection();完成了一下几个功能：&#160; &#160; &#160; &#160;①MessageQueue中保存了一个Connection，mEvents ；&#160; &#160; &#160; &#160;②EventThread中保存了这个Connection，mDisplayEventConnections.add(connection);&#160; &#160; &#160; &#160;③mVSyncSource-&gt;setCallback 把mCallback = callback 设置为EventThread；&#160; &#160; &#160; &#160;④在mDispSync中注册listener， 放到DispSyncthread的mEventListeners中，这个listener的callback就是mVSyncSource。 &#160; &#160; &#160; &#160;2. 然后就是创建通信通道。接着继续MessageQueue::setEventThread()函数，调用mEventTube = mEvents-&gt;getDataChannel()。&#160; &#160; &#160; &#160;mEvents类型为sp&lt; IDisplayEventConnection &gt; ;在上一步mEvents = eventThread-&gt;createEventConnection();返回的直接是Connection对象。Connection继承于BnDisplayEventConnection，BnDisplayEventConnection继承于BnInterface&lt; IDisplayEventConnection &gt;。因此我们找到bp端的getDataChannel函数，位于BpDisplayEventConnection类当中，继承于BpInterface&lt; IDisplayEventConnection &gt;位于frameworks/native/libs/gui/IDisplayEventConnection.cpp：1234567virtual sp&lt;BitTube&gt; getDataChannel() const&#123; Parcel data, reply; data.writeInterfaceToken(IDisplayEventConnection::getInterfaceDescriptor()); remote()-&gt;transact(GET_DATA_CHANNEL, data, &amp;reply); return new BitTube(reply);&#125; &#160; &#160; &#160; &#160;相应的bn端位onTransact方法于也位于IDisplayEventConnection.cpp：1234567891011121314status_t BnDisplayEventConnection::onTransact( uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags)&#123; switch(code) &#123; case GET_DATA_CHANNEL: &#123; CHECK_INTERFACE(IDisplayEventConnection, data, reply); sp&lt;BitTube&gt; channel(getDataChannel());//这个getDataChannel是Connection类的函数 channel-&gt;writeToParcel(reply); return NO_ERROR; &#125; break; ...... &#125; return BBinder::onTransact(code, data, reply, flags);&#125; &#160; &#160; &#160; &#160;上面那个getDataChannel是Connection类的函数，我们看看Connection的getDataChannel函数：123sp&lt;BitTube&gt; EventThread::Connection::getDataChannel() const &#123; return mChannel;&#125; &#160; &#160; &#160; &#160;mChannel就是我们在构造函数里面的new BitTube()。&#160; &#160; &#160; &#160;这样mEventTube 中只包含了读fd，而mEvents这个connection中的mChannel只剩下写fd，两个依然是一对读写，，但是分开了，如下图所示： &#160; &#160; &#160; &#160;3. 继续调用，这里就是把mEventTube这个读tube注册到SF主线程的Looper中去，回调函数为MessageQueue::cb_eventReceiver：12mLooper-&gt;addFd(mEventTube-&gt;getFd(), 0, ALOOPER_EVENT_INPUT, MessageQueue::cb_eventReceiver, this); &#160; &#160; &#160; &#160;这里用到了Android消息处理零散分析的内容，如果忘记了，可以翻一翻。&#160; &#160; &#160; &#160;因为注册了回调事件，因此我们需要查看MessageQueue::cb_eventReceiver函数：1234int MessageQueue::cb_eventReceiver(int fd, int events, void* data) &#123; MessageQueue* queue = reinterpret_cast&lt;MessageQueue *&gt;(data); return queue-&gt;eventReceiver(fd, events);&#125; &#160; &#160; &#160; &#160;然后是eventReceiver函数：1234567891011121314151617int MessageQueue::eventReceiver(int /*fd*/, int /*events*/) &#123; ssize_t n; DisplayEventReceiver::Event buffer[8]; while ((n = DisplayEventReceiver::getEvents(mEventTube, buffer, 8)) &gt; 0) &#123; for (int i=0 ; i&lt;n ; i++) &#123; if (buffer[i].header.type == DisplayEventReceiver::DISPLAY_EVENT_VSYNC) &#123;#if INVALIDATE_ON_VSYNC //1 mHandler-&gt;dispatchInvalidate();#else mHandler-&gt;dispatchRefresh();#endif break; &#125; &#125; &#125; return 1;&#125; &#160; &#160; &#160; &#160;这里会从MessageQueue的读BitTube中读出event，然后调用mHandler-&gt;dispatchInvalidate():12345void MessageQueue::Handler::dispatchInvalidate() &#123; if ((android_atomic_or(eventMaskInvalidate, &amp;mEventMask) &amp; eventMaskInvalidate) == 0) &#123; mQueue.mLooper-&gt;sendMessage(this, Message(MessageQueue::INVALIDATE)); &#125;&#125; &#160; &#160; &#160; &#160;接收消息就是handleMessage函数：12345678910111213141516void MessageQueue::Handler::handleMessage(const Message&amp; message) &#123; switch (message.what) &#123; case INVALIDATE: android_atomic_and(~eventMaskInvalidate, &amp;mEventMask); mQueue.mFlinger-&gt;onMessageReceived(message.what); break; case REFRESH: android_atomic_and(~eventMaskRefresh, &amp;mEventMask); mQueue.mFlinger-&gt;onMessageReceived(message.what); break; case TRANSACTION: android_atomic_and(~eventMaskTransaction, &amp;mEventMask); mQueue.mFlinger-&gt;onMessageReceived(message.what); break; &#125;&#125; &#160; &#160; &#160; &#160;进而去调用SF的onMessageReceived函数，最终每次Vsync信号来了，SF都会去执行handleMessageTransaction()等函数。12345678910111213141516void SurfaceFlinger::onMessageReceived(int32_t what) &#123; ATRACE_CALL(); switch (what) &#123; case MessageQueue::TRANSACTION: handleMessageTransaction(); break; case MessageQueue::INVALIDATE: handleMessageTransaction(); handleMessageInvalidate(); signalRefresh(); break; case MessageQueue::REFRESH: handleMessageRefresh(); break; &#125;&#125; &#160; &#160; &#160; &#160;至此我们对于vsync消息的准备逻辑工作都做完了。但是细心的读者应该发现了，我们还没有按正常流程处理一遍vsync信号，就是当vsync产生后，SurfaceFlinger的回调函数onVSyncReceived处理过程。接下来我们就分析一下这个过程。 VSync信号处理矫正VSync时间&#160; &#160; &#160; &#160;我们再次借助上面那幅Vsync event产生的示意图： &#160; &#160; &#160; &#160;这里HW_VSYNC就是HW_VSYNC_0。 &#160; &#160; &#160; &#160;当SF从HWComposer收到VSYNC(HW_VSYNC_0)时，会调用onVSyncReceived函数处理：12345678910111213141516void SurfaceFlinger::onVSyncReceived(int type, nsecs_t timestamp) &#123; bool needsHwVsync = false; &#123; // Scope for the lock Mutex::Autolock _l(mHWVsyncLock); if (type == 0 &amp;&amp; mPrimaryHWVsyncEnabled) &#123;//将新的VSYNC时间交给DispSync needsHwVsync = mPrimaryDispSync.addResyncSample(timestamp); &#125; &#125; if (needsHwVsync) &#123;//是否还需要vsync enableHardwareVsync(); &#125; else &#123; disableHardwareVsync(false); &#125;&#125; &#160; &#160; &#160; &#160;它会利用DispSync::addResyncSample将新的VSYNC时间交给DispSync。addResyncSample决定是否还需要HW_VSYNC的输入,如果不需要,就会将HW_VSYNC关掉。 &#160; &#160; &#160; &#160;然后我们继续查看DispSync::addResyncSample：1234567891011121314151617181920212223242526272829bool DispSync::addResyncSample(nsecs_t timestamp) &#123; Mutex::Autolock lock(mMutex); //MAX_RESYNC_SAMPLES = 32 size_t idx = (mFirstResyncSample + mNumResyncSamples) % MAX_RESYNC_SAMPLES; mResyncSamples[idx] = timestamp; if (mNumResyncSamples &lt; MAX_RESYNC_SAMPLES) &#123; mNumResyncSamples++; &#125; else &#123; mFirstResyncSample = (mFirstResyncSample + 1) % MAX_RESYNC_SAMPLES; &#125; //这个函数很重要，用来计算SW_VSYNC的值，直到误差小于threshold updateModelLocked(); //MAX_RESYNC_SAMPLES_WITHOUT_PRESENT = 12 if (mNumResyncSamplesSincePresent++ &gt; MAX_RESYNC_SAMPLES_WITHOUT_PRESENT) &#123; resetErrorLocked(); &#125; if (kIgnorePresentFences) &#123; // If we don't have the sync framework we will never have // addPresentFence called. This means we have no way to know whether // or not we're synchronized with the HW vsyncs, so we just request // that the HW vsync events be turned on whenever we need to generate // SW vsync events. return mThread-&gt;hasAnyEventListeners(); &#125; return mPeriod == 0 || mError &gt; kErrorThreshold;&#125; &#160; &#160; &#160; &#160;DispSync是利用HW_VSYNC和PresentFence来判断是否需要开启HW_VSYNC。HW_VSYNC最少要3个，最多是32个，实际上要用几个则不一定，DispSync拿到3个HW_VSYNC后就会计算出SW_VSYNC，只要收到的PresentFence没有超过误差，则HW_VSYNC就会关掉，以便节省功耗.不然会继续开启HW_VSYNC计算SW_VSYNC的值，直到误差小于threshold。其计算的方法是DispSync::updateModelLocked()：123456789101112131415161718192021222324252627282930313233343536373839404142void DispSync::updateModelLocked() &#123; if (mNumResyncSamples &gt;= MIN_RESYNC_SAMPLES_FOR_UPDATE) &#123; nsecs_t durationSum = 0; for (size_t i = 1; i &lt; mNumResyncSamples; i++) &#123; size_t idx = (mFirstResyncSample + i) % MAX_RESYNC_SAMPLES; size_t prev = (idx + MAX_RESYNC_SAMPLES - 1) % MAX_RESYNC_SAMPLES; durationSum += mResyncSamples[idx] - mResyncSamples[prev]; &#125; mPeriod = durationSum / (mNumResyncSamples - 1); double sampleAvgX = 0; double sampleAvgY = 0; double scale = 2.0 * M_PI / double(mPeriod); for (size_t i = 0; i &lt; mNumResyncSamples; i++) &#123; size_t idx = (mFirstResyncSample + i) % MAX_RESYNC_SAMPLES; nsecs_t sample = mResyncSamples[idx]; double samplePhase = double(sample % mPeriod) * scale; sampleAvgX += cos(samplePhase); sampleAvgY += sin(samplePhase); &#125; sampleAvgX /= double(mNumResyncSamples); sampleAvgY /= double(mNumResyncSamples); mPhase = nsecs_t(atan2(sampleAvgY, sampleAvgX) / scale); if (mPhase &lt; 0) &#123; mPhase += mPeriod; &#125; if (kTraceDetailedInfo) &#123; ATRACE_INT64("DispSync:Period", mPeriod); ATRACE_INT64("DispSync:Phase", mPhase); &#125; // Artificially inflate the period if requested. mPeriod += mPeriod * mRefreshSkipCount; mThread-&gt;updateModel(mPeriod, mPhase); &#125;&#125; &#160; &#160; &#160; &#160;基本思想如下: 计算目前收到HW_VSYNC间隔,取平均值(AvgPeriod) HW_VSYNC; 将每个收到的VSYNC时间与AvgPeriod算出误差. (Delta = Time %AvgPeriod); 将Delta转换成角度(DeltaPhase),如果AvgPeriod是360度,DeltaPhase = 2PIDelta/AvgPeriod; 从DeltaPhase可以得到DeltaX与DeltaY (DeltaX =cos(DeltaPhase), DeltaY = sin(DeltaPhase)); 将每个收到的VSYNC的DeltaX与DeltaY取平均,可以得到AvgX与AvgY; 利用atan与AvgX, AvgY可以得到平圴的phase (AvgPhase); AvgPeriod + AvgPhase就是SW_VSYNC。 &#160; &#160; &#160; &#160;当DispSync收到addPresentFence时(最多记录8个sample)，每一个fence的时间算出(Time% AvgPeriod)的平方当作误差，将所有的Fence误差加总起来如果大于某个Threshold，就表示需要校正(DispSync::updateErrorLocked)。校正的方法是呼叫DispSync::beginResync()将所有的HW_VSYNC清掉,开启HW_VSYNC。等至少3个HW_VSYNC再重新计算。 处理VSync消息&#160; &#160; &#160; &#160;消息处理流程依然需要分步骤：&#160; &#160; &#160; &#160;1）我们继续回到DispSync的threadLoop函数中，回顾上面的内容，因为在这里我们需要搜集vsync触发事件：1234567891011121314virtual bool threadLoop() &#123;...... while (true) &#123; Vector&lt;CallbackInvocation&gt; callbackInvocations; ...... callbackInvocations = gatherCallbackInvocationsLocked(now); &#125; if (callbackInvocations.size() &gt; 0) &#123; fireCallbackInvocations(callbackInvocations); &#125; &#125; ......&#125; &#160; &#160; &#160; &#160;mPeriod 不为0，也有signal，DispSyncThread线程不阻塞了，执行gatherCallbackInvocationsLocked(now)和fireCallbackInvocations(callbackInvocations)。我们先查看gatherCallbackInvocationsLocked函数：12345678910111213141516171819Vector&lt;CallbackInvocation&gt; gatherCallbackInvocationsLocked(nsecs_t now) &#123; Vector&lt;CallbackInvocation&gt; callbackInvocations; nsecs_t ref = now - mPeriod; //这里的mEventListeners[i].mCallback都是驱动的事件DispSyncSource for (size_t i = 0; i &lt; mEventListeners.size(); i++) &#123; nsecs_t t = computeListenerNextEventTimeLocked(mEventListeners[i], ref); if (t &lt; now) &#123; CallbackInvocation ci; ci.mCallback = mEventListeners[i].mCallback; ci.mEventTime = t; callbackInvocations.push(ci); mEventListeners.editItemAt(i).mLastEventTime = t; &#125; &#125; return callbackInvocations;&#125; &#160; &#160; &#160; &#160;然后发送事件，fireCallbackInvocations函数：123456void fireCallbackInvocations(const Vector&lt;CallbackInvocation&gt;&amp; callbacks) &#123; for (size_t i = 0; i &lt; callbacks.size(); i++) &#123; //这里的callbacks[i].mCallback，就是驱动的事件DispSyncSource callbacks[i].mCallback-&gt;onDispSyncEvent(callbacks[i].mEventTime); &#125;&#125; &#160; &#160; &#160; &#160;这里会回调DispSyncSource的onDispSyncEvent函数：12345678910111213141516virtual void onDispSyncEvent(nsecs_t when) &#123; sp&lt;VSyncSource::Callback&gt; callback; &#123; Mutex::Autolock lock(mMutex); callback = mCallback; if (mTraceVsync) &#123; mValue = (mValue + 1) % 2; ATRACE_INT(mVsyncEventLabel.string(), mValue); &#125; &#125; ////这里的callback为EventThread if (callback != NULL) &#123; callback-&gt;onVSyncEvent(when); &#125;&#125; &#160; &#160; &#160; &#160;继续调用，这里已经从驱动事件，转化到驱动事件的线程EventThread中，填充EventThread的mVSyncEvent，因此我们查看onVSyncEvent函数：12345678void EventThread::onVSyncEvent(nsecs_t timestamp) &#123; Mutex::Autolock _l(mLock); mVSyncEvent[0].header.type = DisplayEventReceiver::DISPLAY_EVENT_VSYNC; mVSyncEvent[0].header.id = 0; mVSyncEvent[0].header.timestamp = timestamp; mVSyncEvent[0].vsync.count++; mCondition.broadcast();&#125; &#160; &#160; &#160; &#160;这里将mVSyncEvent事件相关属性复制，且threadLoop等待阻塞唤醒，以便往下继续执行。 &#160; &#160; &#160; &#160;2）EventThread的waitForEvent()，返回signalConnections，就是开始建立的Connection，这个Connection里面有个BitTube的写fd，另外的读fd在MessageQueue中。 &#160; &#160; &#160; &#160;此时我们回到上面没有讲的，当waitForEvent返回后，2. 把事件分发给listener；3. 然后调用每个连接的postEvent来发送Event。这两个步骤。&#160; &#160; &#160; &#160;waitForEvent返回，调用conn-&gt;postEvent(event)，我们查看Connection的postEvent函数：1234567891011status_t EventThread::Connection::postEvent( const DisplayEventReceiver::Event&amp; event) &#123; ssize_t size = DisplayEventReceiver::sendEvents(mChannel, &amp;event, 1); return size &lt; 0 ? status_t(size) : status_t(NO_ERROR);&#125;//位于frameworks/native/libs/gui/DisplayEventReceiver.cpp中ssize_t DisplayEventReceiver::sendEvents(const sp&lt;BitTube&gt;&amp; dataChannel, Event const* events, size_t count)&#123; return BitTube::sendObjects(dataChannel, events, count);&#125; &#160; &#160; &#160; &#160;也就是通过Connection的写fd将event发送给MessageQueue。&#160; &#160; &#160; &#160;这时候MessageQueue的looper epoll返回，最终会去调用response.request.callback-&gt;handleEvent，最终调用的是mCallback(fd, events, data);而这个mCallback，既是MessageQueue::cb_eventReceiver。这一步可以查看Android消息处理零散分析的内容，如果忘记了，可以翻一翻。&#160; &#160; &#160; &#160;所以最终还是回到了MessageQueue::cb_eventReceiver：1234int MessageQueue::cb_eventReceiver(int fd, int events, void* data) &#123; MessageQueue* queue = reinterpret_cast&lt;MessageQueue *&gt;(data); return queue-&gt;eventReceiver(fd, events);&#125; &#160; &#160; &#160; &#160;这个步骤上面分析过了。最终去调用SF的onMessageReceived函数，最终每次Vsync信号来了，SF都会去执行handleMessageTransaction()等函数。12345678910111213141516void SurfaceFlinger::onMessageReceived(int32_t what) &#123; ATRACE_CALL(); switch (what) &#123; case MessageQueue::TRANSACTION: handleMessageTransaction(); break; case MessageQueue::INVALIDATE: handleMessageTransaction(); handleMessageInvalidate(); signalRefresh(); break; case MessageQueue::REFRESH: handleMessageRefresh(); break; &#125;&#125; &#160; &#160; &#160; &#160;这就是vsync信号的处理过程，和上面章节的结合在了一起。 VSync实例重要点小结&#160; &#160; &#160; &#160;我们列出了EventThread的重要function.下面一一说明其功能： EventThread::Connection::Connection()：Connection的构造函数.用于进程间的通信by BitTube..在此处主要是搭建一个通路(BitTube)来完成client(App或SurfaceFlinger)对Vsync event事件的请求(通过requestNextVsync())和EventThread把SW-Vsync event callback到其感兴趣的client。需要注意的是App是通过SurfaceFlinger::createDisplayEventConnection()创建此连接的，而sufaceflinge是在其初始化时call EventQueue.setEventThread(mSFEventThread)创建的。所以对App 的EventThread 来说可能有多个connection ,也有可能没有，而对sufaceflinger目前来说有且只有一个； spEventThread::createEventConnection()：创建 Connection连接； status_tEventThread::registerDisplayEventConnection()：如其名所描述.其功能是把创建的Connection注册到一个容器中。当SW-VSYNCevent发生时，EventThread会从Connection注册的容器中，找到那些对SW-VSYNC event感兴趣的connection并把vsyncevent通过BitTube传到client； void EventThread::requestNextVsync()：Clinet 端通过Connection call 这函数通知EventThread，其对SW-SYNCevent的请求； voidEventThread::onVSyncEvent(nsecs_t timestamp)：SW-VSYNCEVENT 发生时，DispSyncSource 会call此函数，告知EventThread，Vsync event已经发生，如果此时有connect对Vsync感兴趣，EventThread便会通过connect-&gt;postEvent(event)把Vsync事件发送到client端(App或surfaceflinger)； bool EventThread::threadLoop()：线程的主体函数.其完成两件事，一是把对SW-VSYNC event有请求并且还没有处理的connect找出来，二是把Vsyncevent通过connect通知到client； Vector&lt; sp&gt; EventThread::waitForEvent()：EventThread 的主要功能都在此函数里，此函数由threadLoop()调用。EventThread在大部分时间里是sleep的，如果系统的性能比较好，那么其sleep的节奏是和SW-VSYNC event的节奏一致，即16.6mssleep一次。然而由于其App或surfaceflinger没有Vsync的请求，其sleep的时间为更长。此函数的名为waitForEvent，其到底在等什么event？原来此函数在等待的event就是Dispsync产生的SW-SYNC event，其功能check所有的connect是否有Vsync事件请求根据不同的情况做如下处理。 所有的connect都没有Vsync请求，则其通过disableVSyncLocked()，disableVsync event，那么此EventThread将不会收到SW-SYNCevent，一直sleep直到有connect有Vsync请求为止。 在所有的connect中，有SW-SYNC event请求，但是当其请求SW-SYNCevent时，SW-SYNCevent还没有fire，则其通过enableVSyncLocked() enable Vsync并进入sleep。当下一个SW-SYNCevent来到时，便把所有有SW-SYNCevent请求的connection返回给threadLoop。 &#160; &#160; &#160; &#160;下图是整个流程的大图： &#160; &#160; &#160; &#160;我们接下来依然以gallery3d（相册）为例子，分析Vsync-app和Vsync-sf的工作流程。 Vsync-app实例 App 通过Connection向EventThread请求SW-VSYNCevent(最终并且通过callEventThread::requestNextVsync()).一般来说App是通过Choreographer.doScheduleVsync()来请求Vsyncevent. DispSync 产生SW-VSYNC event并call DispSyncSource的onDispSyncEvent(). DispSyncSource产生VSYNC-app信号跳变. 同时DispSyncSource使EventThread(app)的waitForEvent()结束sleep,并把VSYNC-app通知到App进程(gallery3d). Gallery3d 进程收到VSYNC-app信号,在其main线程(UI Thread)中开始draw一个frame(发送draw command给GL thread). GL Thread 开始draw view. 当GL thread 线程Draw 完一frame后,把draw buffer放到buffer queue中. SufaceView中画好的buffer 数据增加为1.这个buffer 将被surfaceflinger 在适当的时候合成. Vsync-sf实例 App draw 完一个frame (SurfaceView) 并且把此frame通过调用BufferQueueProducer::queueBuffer()放到 bufferqueue中,并且通过call EventThread::requestNextVsync(),请求SW-VSYNCevent.此时由于DispSync没有fireSW-Sync event所以EventThread在EventThread::waitForEvent()中sleep. DispSync触发SW-SYNC event信号并且call DispSyncSource::onDispSyncEvent(). 在DispSyncSource::onDispSyncEvent()中首先引起Vsync-sf跳变,然后在call EventThread::onVSyncEvent(nsecs_ttimestamp). 在EventThread::onVSyncEvent(nsecs_ttimestamp)中,会唤醒EventThread::waitForEvent()中的sleep,从而发送消息到Surfaceflinger. SurfaceFlinger收到EventThread发过来的Vsync-sf跳变信息,开始合成dirty layer, 本例是SurfaceView.合成完以后发送更新的消息到DisplayHW. Display HW 收到更新的消息时,HW vsync event还没有来,因此DisplayHW sleep了一段事件.当HWvsync event到来时,DisplayHW 便更新其LCD的内容.把最新的内容显示在LCD上. &#160; &#160; &#160; &#160;自此我们完成了Vsync子系统的分析。 性能影响&#160; &#160; &#160; &#160;回到系统性能上,我们可以看出系统的性能(FPS)可能于如下因素有关： DispSync的调度. DispSync是Vsync系统的心脏,如果DispSync来不及调度,则有可能由于去SW-SYNC event的产生不及时而影响系统的性能. View Draw 花费了过多的时间也会引性能问题, 如果其不能在一个SW-SYNC时间内完成, 那么此应用就会有一个Janks.由于现在的Androd系统都采用了HWUI,viewdraw往往是GPU直接draw,所以很多时候是GPUperformance问题. HW display 的性能问题.LCD 上所绘画的内容, 最终需要HW display在HW vsync触发是把其内容显示在LCD上. Binder 的调度问题, 我们可以看到无论是requestNextVsync () 还是queueBuffer()都是App通过binder与surfaceflinger进程通信的.而这些binder在大多数时间是sleep的.如果binder由于CPU调度而错过了一个Vsync跳变点,那么就有一个frame发生Janks. 结语&#160; &#160; &#160; &#160;对于VSync信号的学习就到这里，我们可以看到还是十分复杂的。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VSync信号]]></title>
    <url>%2F2017%2F05%2F21%2FVSync%E4%BF%A1%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;在我们详细分析SurfaceFlinger之前要了解一下VSync信号，为下一节分析Vsync工作原理打下基础。 VSync信号相关&#160; &#160; &#160; &#160;首先我们要了解以下几个概念： 屏幕刷新率&#160; &#160; &#160; &#160;即 Refresh Rate 或 Scanning Frequency，单位赫兹/Hz，是指设备刷新屏幕的频率，该值对于特定的设备来说是个常量，如 60hz。 &#160; &#160; &#160; &#160;每一台CRT显示器都有自己的刷新率，其单位是Hz，其数值是显示器每秒钟更新画面的次数。不同的显示器支持再不同分辨率下的不同刷新率。它的范围可以从低到60高到100。注意它不是你游戏中所提到的那个FPS。如果你设置了一个特定的刷新率，显示器将一直按照这个速率刷新画面。甚至画面没有任何的改变。&#160; &#160; &#160; &#160;液晶显示器就不同了。LCD的每个像素在被告知改变的时候将一直是亮着的。他们不需要刷新。但是因为VGA(或是DVI)的工作原理，LCD不得不从显示卡那里按一定的速率得到新的新画面。这就是虽然LCD不必要更新，但是他还是有自己的刷新率。 &#160; &#160; &#160; &#160;如下图，屏幕的刷新过程是每一行从左到右（行刷新，水平刷新，Horizontal Scanning），从上到下（屏幕刷新，垂直刷新，Vertical Scanning）。当整个屏幕刷新完毕，即一个垂直刷新周期完成，会有短暂的空白期，此时发出 VSync 信号。所以，VSync 中的 V 指的是垂直刷新中的垂直/Vertical。&#160; &#160; &#160; &#160;(显示器的扫描方式分为“逐行扫描”和“隔行扫描”两种。逐行扫描比隔行扫描拥有列稳定显示效果。早期的显示器因为成本所限，使用逐行扫描方式的产品要比隔行扫描的贵许多，但随着技术进步，隔行扫描显示器现在已经被淘汰。我们可以翻一翻《电视原理》) &#160; &#160; &#160; &#160;对于一个特定的设备，帧率和刷新频率没有必然的大小关系。 帧率&#160; &#160; &#160; &#160;即 Frame Rate，单位 fps，是指 gpu 生成帧的速率，如 33 fps，60fps，越高越好。 &#160; &#160; &#160; &#160;我想这里的每一人都明白FPS。它显示显示卡在每秒钟可以描画多少画面。这显然是越高越好。但是对于快速变化的游戏而言，你的FPS很难一直保持同样的数值，他会随着你所看到的显示卡所要描画的画面的复杂程度而变化。 VSync&#160; &#160; &#160; &#160;安卓系统中有 2 种 VSync 信号：屏幕产生的硬件 VSync 和由 SurfaceFlinger 将其转成的软件 Vsync 信号。后者经由 Binder 传递给 Choreographer。&#160; &#160; &#160; &#160;硬件 VSync 是一个脉冲信号，起到开关或触发某种操作的作用。 显示器参数&#160; &#160; &#160; &#160;TFT LCD 有 vsync,hsync,hspw,hbpd,hfpd, vspw,vbpd, vfpd 等参数，这些参数都是由以前的 CRT(阴极射线显像管)带过来的, 而 TFT 液晶跟 CRT 显示方法根本不同, 至于为什么这些参数也会引入到 TFT 中,大概是因为VGA(或是DVI)工作原理，上文提到过。&#160; &#160; &#160; &#160;这网站有更权威的描述 http://www.linux-fbdev.org/HOWTO/2.html ，然后是英文。。。 &#160; &#160; &#160; &#160;CRT 侧面看是个漏斗状的真空的东东, 根部就是电子枪, 打出的电子撞击前面的玻璃面上的荧光物质, 荧光物发光. 控制电子枪按规律射出电子, 逐行的打到荧光物质上, 打完一行(也即扫描完一行), 就回头扫描下一行….. 扫描完一个显示屏所有的行后, 就是一幅完整的画面了, 称为一帧(frame), 扫描过程如果非常快, 人眼看到是一幅完整画面, 但实际是一个个点发不同光组成的. 扫描得慢时, 就会觉得闪烁了(以前听老师讲课, 说在他们的年代, 能明显看到一行一行刷过的壮观场景)。&#160; &#160; &#160; &#160;一帧扫描完, 再回头从第一行开始继续扫描, 重复过程. 看到的就是持续在显示的画面了(实际上, 电子的运动轨迹是由磁场控制的, 电子枪没有机械的运动, 为好理解, 就当是电子枪做了机械运动吧)。 &#160; &#160; &#160; &#160;描述方式多数显示器选择从左上角开始, 从左至右, 到了右边界, 再偏转到左边界的下一行, 这是所谓的”Z”型扫描。类似地扫描完最后一帧时, 要偏转回左上角起始处, 准备扫描下一帧。这个上面那一幅图描述过了。 &#160; &#160; &#160; &#160;(科普一下，ignore it。start------) HSYNC 信号用于告诉电子枪该扫描下一行了, 即要转到下一行起始处了； VSYNC 信号告诉电子枪该显示下一帧了, 即该转回左上角起始处了； &#160; &#160; &#160; &#160;H for Horizontal, V for vertical。在这里 Hsync, Vsync 两者各表示一种信号, 分别由 HSPW 及 VSPW 两个参数确定信号持续时间, 也就是脉冲的宽度. &#160; &#160; &#160; &#160;在扫描一行中, 首先 HSYNC 脉冲信号为高电平, 一发出此信号, 电子枪迅速移回左边界, 期间电子枪不发射电子. 在 HSYNC 信号持续时间上的某点, 电子枪开始再次向右扫描了, 等 HSYNC 信号结束, 表示该开始显示下一行的数据了, 电子枪又要开始发射电子. 在HSYNC 信号结束与开始显示数据之间, 可以插入一段延时(由 HBPD 参数控制)让显示屏仍然不显示有效数据(效果就是黑色), 是为显示屏左边框. 到 HFPD 结束, 电子枪才可以发射电子, 显示该行的有效数据. 显示完毕. 又该开始发出 HSYNC 信号了. 在一行中有效数据扫描完毕与 HSYNC 信号发出之间也插入一段延时 HFPD, 是为显示屏右边框,之后, 就是重复过程了. &#160; &#160; &#160; &#160;因此, 显示一行时序为： HSPW -&gt; HBPD -&gt; 扫描数据 -&gt; HFPD . &#160; &#160; &#160; &#160;类似地, 垂直扫描一帧的时序：VSPW -&gt; VBPD -&gt; 扫描有效行 -&gt; VFPD . &#160; &#160; &#160; &#160;为什么要有边框(vbpd, vfpd, hbpd, hfpd)? 按上边贴出网页的说法:&#160; &#160; &#160; &#160;Usually, one doesn’t use that feature nowadays, as we have tunable monitors that allow stretching the mode to the physical limits of the monitor.&#160; &#160; &#160; &#160;我的理解是:确实是以前的显示器调整显示屏边界用的,而新的显示有调整能力了, 所以不再需要刻意关注了,但术语被保留了下来. 举个栗子&#160; &#160; &#160; &#160;对于 TFT LCD, 但这些参数作用是同样的. 但如何确定 ? TFT 的 LCD 的 datasheet 中一定得标有。&#160; &#160; &#160; &#160;如型号 WXCAT35-TG3 3.5 寸的液晶中有表如下: &#160; &#160; &#160; &#160;对照下边的时序图(注: 时序图的 Vsync, Hsync 信号(红框圈出的)跟上边讲的有点出入, 信号都是低电平, 而非高电平, 因此编程时要设置信号反相, 如s3c244a 的 LCDCON5的INVLINE 及 INVFRAME 即是干这活的): tvp 即 VSPW tvb 即 VBPD tvf 即 VFPD thp 即 HSPW thb 即 HBPD thf 即 HFPD &#160; &#160; &#160; &#160;参考的时钟就是 CLK, 一个 CLK 时钟, 完成一个像素点的显示。 &#160; &#160; &#160; &#160;计算帧频率(刷新频率)的方法就是所有的像素点跟边沿(边框,hbpd 之类),同步脉冲的时间相加, 结果就是显示完整一帧所需时间, 其倒数即是帧频率。 Linux 的 LCD 驱动&#160; &#160; &#160; &#160;LCD 驱动主要得完成两部分, 一是跟 framebuffer 注册驱动； 二是设置 LCD 控制器的寄存器, 以适配 LCD。&#160; &#160; &#160; &#160;struct fb_info 是关键, 有显示驱动的所有信息, 要拿此结构跟 framebuffer 注册, 代表着本驱动, 该结构定义在 ,在驱动程序中, 主体部分就是实现 struct fb_info .&#160; &#160; &#160; &#160;在Android SurfaceFlinger 学习之路(一)—-Android图形显示之HAL层Gralloc模块实现中提到过这一部分。&#160; &#160; &#160; &#160;在该结构中, 以下定义的三个字段也是比较重要的, 要填一些数据入去:123struct fb_var_screeninfo var; /* Current var */struct fb_fix_screeninfo fix; /* Current fix */struct fb_ops *fbops; &#160; &#160; &#160; &#160;struct fb_fix_screeninfo 里定义的 pixclock 是像素点的周期, 单位是皮秒, 数值等于像素点显示频率的倒数. 如上图贴出的表格中, Dclk 那行中, 6.4 Mhz 就是频率, 频率倒数即为周期, 换算出来为 156250 ps, 约为表中给出的 156 ns. &#160; &#160; &#160; &#160;驱动具体的实现, 要看开发板对应的驱动源码文件, 一般位于 kernel_src/drivers/video/ 下,我们以前下的Android源码，找到对应设备分支下的，应为位于kernel/{branch}/drivers/video/吧。 简单描述&#160; &#160; &#160; &#160;在手机平台，LCD,Camera，TV的接线上，都会用到PCLK，VSYNC和HSYNC这三个信号。可见这三个信号和显示关系非常大。首先我们先看这三个信号的作用： PCLK：有些方案给他起名字叫：DotCLK。是像素点同步时钟信号。也就是每个PCLK对应一个像素点。 VSYNC：是场同步信号。以高电平有效为例，VSYNC置高直到被拉低，这个区段所输出的所有影像数据组成一个frame。 HSYNC：是行同步信号。就是在告诉接收端：“HSYNC”有效时段内接收端接收到的所有的信号输出属同一行。 &#160; &#160; &#160; &#160;若要显示一个640x480的画面，显示不正确的时候，若量PCLK，VSYNC和HSYNC这三个信号，就可以知道这三个信号配置是否有问题，一般来讲，这种情况是有公式的： VSync = HSYNC x 320； Hsync = PCLK x 640； &#160; &#160; &#160; &#160;sensor的同步信号可以简单的理解为sensor向其信号接收端所发送的宣告信号。比如HSYNC，就是sensor这告诉接收端：“HSYNC”有效时段内sensor所有的信号输出属同一行。VSYNC同理，以高电平有效为例，VSYNC置高直到被拉低，这个区段sensor所输出的所有影像数据组成一个frame。同步信号的频率决定于pixel clock，比如一行有640个pixel，那么HSYNC的频率为：PCLK/(640+dummy);Vsync同理。 &#160; &#160; &#160; &#160;(科普结束，ignore it，too。end------) VSync 信号的作用tearing 画面撕裂&#160; &#160; &#160; &#160;首先，我们来看下，没有引入 VSync 时，屏幕显示图像的工作流程。 &#160; &#160; &#160; &#160;如上图，CPU/GPU 向 Buffer 中生成图像，屏幕从 Buffer 中取图像、刷新后显示。这是一个典型的生产者——消费者模型。理想的情况是帧率和刷新频率相等，每绘制一帧，屏幕显示一帧。而实际情况是，二者之间没有必然的大小关系，如果没有锁来控制同步，很容易出现问题。 &#160; &#160; &#160; &#160;所谓”撕裂”就是一种画面分离的现象，就象你照一张照片，在旋转哪怕一度再照一张照片，然后把两张照片的从中间裁开，用一张照片的上半部与另一张的下半部对接起来。这样得到的画像虽然相似但是上半部和下半部确实明显的不同。这就被称之为视觉现实上的撕裂。它不会一直从中间分开，它可能靠近上面也可能下面，分离点可能在屏幕上下移动，也可能在两点间前后移动。（译者：原文的作者实在是啰嗦，其实就是画面移动较快的时候，画面看上去是两截。这种现象恐怕打游戏的都看到过，最好玩过PS2游戏的，用模拟器，在比较差的显卡和CPU上面，撕裂现象更为明显）。 &#160; &#160; &#160; &#160;为什么会发生这种现象呢？让我们举一个特定的例子。让我们假定你的显示器的刷新率是75Hz, 你真在玩你最喜欢的游戏，而且你现在有100的FPS.这就意味着你的显示器每秒更新75次画面，而你的显示卡每秒更新100次，比你的显示器快33%。这就意味着在你的显示器更新画面的时间里，显示卡描画了1+1/3的画面。这样在画面显示的时候，那个1/3的画面就会覆盖那个完整画面上部的1/3。在下次的图像刷新的时候，显示卡会描画剩下来得2/3和新的2/3的画面。这样，因为屏幕的更新只能跟上画面更新的2/3，这样图像的上部的1/3或是下部的1/3就会和剩下的画面合不上。如果画面的变化不大可能不太会注意到这一点，但是如果你快速的环顾四周那就会非常的明显。 Double Buffer 双缓冲&#160; &#160; &#160; &#160;现在，一个很普遍的误解就产生了。一些人认为解决这个问题的方法就是简单设置一个FPS的限制让FPS不超过显示器的刷新率，这样显示卡就不会超过75FPS,这样就可以了。真的吗？错！ &#160; &#160; &#160; &#160;在我解释为什么之前，让我来讲一下双倍缓冲。双倍缓冲一种用来减轻撕裂问题，虽然不是很完全。基本上来说你有一个显示缓冲和一个后备缓冲。当显示器要显示画面的时候，就会从显示缓冲里“推出”显示画面。显示卡则在后备缓冲里描画另外一个新画面，当描画完成后则将新画面考入显示缓冲里。但是这个过程需要时间，如果显示器的刷新在拷贝过程中进行的话，显示器上显示的仍然是个”撕裂”的画面。 &#160; &#160; &#160; &#160;VSync 通过建立一个不让在显示器刷新前将后备缓冲中的画面拷贝到显示缓冲中的规定来解决这个问题。如果FPS高于刷新率的话，没有问题。后备缓冲的更新完成后，系统处于等待状态。当显示器刷新后，后备缓存考入显示缓存，显示卡则可以在后备缓存里描画新的画面，这样就很有效的将你的FPS限制在显示器的刷新率的范围内。 &#160; &#160; &#160; &#160;为了解决单缓存的“tearing”问题，双缓存和 VSync 应运而生。双重缓存模型如下图： &#160; &#160; &#160; &#160;两个缓存区分别为 Back Buffer 和 Frame Buffer。GPU 向 Back Buffer 中写数据，屏幕从 Frame Buffer 中读数据。VSync 信号负责调度从 Back Buffer 到 Frame Buffer 的复制操作，可认为该复制操作在瞬间完成。其实，该复制操作是等价后的效果，实际上双缓冲的实现方式是交换 Back Buffer 和 Frame Buffer 的名字，更具体的说是交换内存地址（有没有联想到那道经典的笔试题目：“有两个整型数，如何用最优的方法交换二者的值？”），通过二进制运算“异或”即可完成，所以可认为是瞬间完成。(《数字电路技术》当中有一章节应该讲过运算器还是控制器来着，可以设计一个异或电路。忘了，回去翻一翻~) &#160; &#160; &#160; &#160;附加小福利：&#160; &#160; &#160; &#160;假设两个数x和y，则有：方法1，算术运算（加减）：x=x+y; //x暂存两数之和y=x-y; //y为两数之和减去y，即原来的xx=x-y; //x为两数之和减去现在的y（原来的x），变成原来的y方法2，逻辑运算（异或）：x^=y; //x先存x和y两者的信息y^=x; //保持x不变，利用x异或反转y的原始值使其等于x的原始值x^=y; //保持y不变，利用x异或反转y的原始值使其等于y的原始值 &#160; &#160; &#160; &#160;双缓冲的模型下，工作流程这样的：&#160; &#160; &#160; &#160;在某个时间点，一个屏幕刷新周期完成，进入短暂的刷新空白期。此时，VSync 信号产生，先完成复制操作，然后通知 CPU/GPU 绘制下一帧图像。复制操作完成后屏幕开始下一个刷新周期，即将刚复制到 Frame Buffer 的数据显示到屏幕上。 &#160; &#160; &#160; &#160;在这种模型下，只有当 VSync 信号产生时，CPU/GPU 才会开始绘制。这样，当帧率大于刷新频率时，帧率就会被迫跟刷新频率保持同步，从而避免“tearing”现象。 Jank 掉帧&#160; &#160; &#160; &#160;注意，当 VSync 信号发出时，如果 GPU/CPU 正在生产帧数据，此时不会发生复制操作。屏幕进入下一个刷新周期时，从 Frame Buffer 中取出的是“老”数据，而非正在产生的帧数据，即两个刷新周期显示的是同一帧数据。这是我们称发生了“掉帧”（Dropped Frame，Skipped Frame，Jank）现象。 &#160; &#160; &#160; &#160;让我们来看一个另外一个不同的例子。让我们假定你已经玩到了你最喜欢的游戏的最后一关，这个游戏有很好的图像.你显示器的刷新率还是在75。但是你的FPS现在只有50了，比刷新率要低33%.这就意味着每次显示器刷新图像，你的显示卡只能画出下一桢画面的2/3。让我们看看它是如何工作的。 显示器刚刚更新，第一桢的画面已经拷贝到显示缓冲，第二桢的画面的2/3被写入后备缓冲。 这时显示器重新刷新，它会第一次从显示缓冲里提取第一桢的画面。然后显示卡开始完成的第二桢剩下的部分。但是它必须等待，应为再下一次刷新之前它是不会上传的。 显示器再次刷新，显示器不得不第二次从显示缓冲里提取第一桢的画面，然后第二桢的画面被写入显示缓冲。显示卡在后备缓冲中写入第三桢的2/3。 等到显示器刷新，第一次从显示缓冲里提取第二桢的画面，显示卡开始完成的第三桢剩下的部分。然后又是第二次从显示缓冲里提取第二桢的画面，然后第三桢的画面被写入显示缓冲。 &#160; &#160; &#160; &#160;如此类推。这样4次显示器刷新，我们只能的到2桢的画面。如果刷新率是75的话，我们只能得到35的FPS.很明显这个数值要低于显示卡可以带到的50FPS.这主要就是应为显示卡不得不在描画后备缓冲上浪费时间。而在此过程中，后备缓冲上的画面是不能被拷贝到显示缓冲。理论上讲，双缓冲的VSync,FPS将是一组不连续的整数，其等于刷新率/n,n是正整数。也就是说，如果你的刷新率是60hz,你能得到的FPS只能是 60，30，20，15，12，10 等等。你可以注意到60到30是一个相当大的差距。只要的显示卡的FPS在60到30之间，你说得到的真实FPS都将只能等于30！ &#160; &#160; &#160; &#160;如下图，A、B 和 C 都是 Buffer。蓝色代表 CPU 生成 Display List，绿色代表 GPU 执行 Display List 中的命令从而生成帧，黄色代表生成帧完成。 &#160; &#160; &#160; &#160;现在，你明白为什么有人不喜欢它了。让我们回到一开始的那个例子。你在玩你最喜欢的游戏，刷新率是75HZ,100FPS。你打开VSync.游戏就被限制在75FPS,没有问题，没有撕裂图像，看起来不错。你到了一个图像特别复杂的地方，在不用VSync的时候，你的FPS下降到了60左右。但是你打开了VSync，你的FPS实际就只有37.5。这样你的游戏突然从75FPS变成了37.5FPS,不管37.5仍然很流畅但是你一定会注意到刷新率突然减少了一半。当让如果以下变到25FPS的话，实际的现实率可能就只有17.5。本来还可以玩的游戏，就变成了幻灯片。这就是大家不喜欢它的原因。 &#160; &#160; &#160; &#160;如果你的游戏的FPS可以一直稳定的大于显示器的刷新率，VSync是个不错的东西。但是如果FPS忽大忽小。VSync就是让人烦的东西。如果你的游戏FPS一直都小于刷新率的话，实际的FPS要远远小于显示卡可以显示的FPS.看上去就象是VSync降低了你的FPS,但是从技术角度讲，不是应为图像太复杂，而是因为VSync就是这样工作的。 Triple Buffer 三缓冲&#160; &#160; &#160; &#160;双重缓存的缺陷在于：当 CPU/GPU 绘制一帧的时间超过 16 ms 时，会产生 Jank。更要命的是，产生 Jank 的那一帧的显示期间，GPU/CPU 都是在闲置的。 &#160; &#160; &#160; &#160;也不是说所有的希望都没有了。现在的triple-buffering技术可以用来解决这个问题。让我们再来看刷新率75。FPS50的例子。 第一桢在显示缓冲，第二桢的2/3在后备缓冲。 显示器刷新第一桢第一次被显示，在后备缓冲里描画第二桢的剩下的1/3，在第二后备缓冲里描画第三桢的1/3(因为我们有三级缓冲了)。 显示器再次刷新第一桢第二次被显示，第二桢放入在显示缓冲，第三桢的的1/3放入后备缓冲，第二后备缓冲里描画第三桢剩下的2/3。 接下来显示器再次刷新的时候，第二桢被显示，第三桢就可以放入显示缓冲，这样我们就可以在3次刷新中得到2桢的画面。 &#160; &#160; &#160; &#160;也就是刷新率的2/3,也就是50FPS.triple-buffering理论上讲可以避免缓冲写入是带来的延迟现象，这样就不会浪费时间。但是triple-buffering并不是适用于所有的游戏。实际上它并不是普及(这个文章可能写的太早，现在triple-buffering已经很普及了)，而且它也会影响显示卡的性能，应为它需要更多的显示内存，需要更多时间在内存之间降数据拷贝来拷贝去。但是triple-buffering确实是一个很好的方法，既可以消除撕裂画面又可以不像普通VSync一样影响你的FPS. &#160; &#160; &#160; &#160;如果有第三个 Buffer 能让 CPU/GPU 在这个时候继续工作，那就完全可以避免第二个 Jank 的发生了！ &#160; &#160; &#160; &#160;于是就有了三缓存： &#160; &#160; &#160; &#160;工作原理同双缓冲类似，只是多了一个 Back Buffer。&#160; &#160; &#160; &#160;需要注意的是，第三个缓存并不是总是存在的，只要当需要的时候才会创建。之所以这样，是因为三缓存会显著增加用户输入到显示的延迟时间。如上图，帧 C 是在第 2 个刷新周期产生的，但却是在第 4 个周期显示的。最坏的情况下，你会同时遇到输入延迟和卡顿现象。 三缓冲局限&#160; &#160; &#160; &#160;我希望这篇文章是有用的，可以帮出你理解VSync的工作原理。（特别是不再犹豫是否打开VSync）总之，如果没有triple-buffering的情况下，如何权衡Vsync的FPS限制和消除撕裂画面带来的视觉感受，那将完全取决于你个人的喜好。&#160; &#160; &#160; &#160;译者按：如果这篇文章的机理是正确的。triple-buffering也不是万能的，实际上就是把减少1/2变成了减少1/3而已，如果是FPS恰好卡到了一定的数值的时候没有问题，一旦没有，那就绝对要损失FPS.所以对于那种FPS刚刚超过24的游戏，不管有没有triple-buffering，都应该关. &#160; &#160; &#160; &#160;所以一定会有人问：液晶显示器的刷新频率为何不能调高？&#160; &#160; &#160; &#160;这里有一个误解。&#160; &#160; &#160; &#160;原来我们使用的是CRT技术的显示器,为保证长时间地注视屏幕而眼睛不疲劳,我们一般都会把刷新率调到75H甚至是85Hz,这是由于CRT技术的特性决定的,刷新率越高也就意味着图像越清楚、越稳定。但是对于LCD来说,由于液晶板本身并不发光,只是液晶分子控制光线的偏转或通过,发光的是背光源,即荧光灯管,在使用的时候即使把刷新率调到60Hz你也不会感到屏幕在闪烁,“刷新率”对LCD来说已经没有多大意义了,所以在使用液晶显示器的时候,我们是不必过于苛求刷新率的高低的。 参考Getting To Know Android 4.1, Part 3: Project Butter - How It Works And What It Addedhttp://www.androidpolice.com/2012/07/12/getting-to-know-android-4-1-part-3-project-butter-how-it-works-and-what-it-added/ 总结&#160; &#160; &#160; &#160;本篇科普一下VSync，下一节开始分析它在SurfaceFlinger中的工作流程。]]></content>
      <categories>
        <category>科普分享</category>
      </categories>
      <tags>
        <tag>VSync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(四)----SurfaceFlinger服务的启动与连接过程]]></title>
    <url>%2F2017%2F05%2F13%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E5%9B%9B-SurfaceFlinger%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%90%AF%E5%8A%A8%E4%B8%8E%E8%BF%9E%E6%8E%A5%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;上一篇我们分析了Android的开机动画启动流程，这一篇我们基于上一篇的基础，分析一下SurfaceFlinger的启动，还有连接它的过程。 SurfaceFlinger的启动启动概述&#160; &#160; &#160; &#160;SurfaceFlinger服务是一个独立进程，并且负责统一管理设备的帧缓冲区。通过上一篇开机动画流程分析，我们可以在init.rc中找到SurfaceFlinger服务配置的地方，位于system/core/rootdir/Init.rc中：12345service surfaceflinger /system/bin/surfaceflinger class core user system group graphics drmrpc onrestart restart zygote &#160; &#160; &#160; &#160;在硬件设备/system/bin/下，可以找到SurfaceFlinger的应用程序。我们查看源码frameworks/native/services/surfaceflinger/Android.mk文件：123456789101112131415161718192021222324################################################################ build surfaceflinger's executableinclude $(CLEAR_VARS)LOCAL_CFLAGS:= -DLOG_TAG=\"SurfaceFlinger\"# SurfaceFlinger启动文件LOCAL_SRC_FILES:= \ main_surfaceflinger.cpp LOCAL_SHARED_LIBRARIES := \ libsurfaceflinger \ libcutils \ liblog \ libbinder \ libutils # SurfaceFlinger是个动态库LOCAL_MODULE:= surfaceflingerifdef TARGET_32_BIT_SURFACEFLINGERLOCAL_32_BIT_ONLY := trueendifinclude $(BUILD_EXECUTABLE) &#160; &#160; &#160; &#160;从Makefile文件可以看出，相关依赖和主文件会被编译成libsurfaceflinger.so，然后SurfaceFlinger是对库的一个“封装调用”，里面有个main_surfaceflinger.cpp，我们可以沿着它的main函数往下分析。 &#160; &#160; &#160; &#160;启动流程大概如下图，我才刚刚学了时序图，不知道画的对不对。。。。 启动过程&#160; &#160; &#160; &#160;SurfaceFlinger的main函数在framework/native/services/surfaceflinger/main_surfaceflinger.cpp中：1234567891011121314151617181920212223242526272829303132333435int main(int, char**) &#123; // When SF is launched in its own process, limit the number of // binder threads to 4. //在该进程设置了binder线程池最大数为4 ProcessState::self()-&gt;setThreadPoolMaxThreadCount(4); // start the thread pool //这里其实只是将当前线程加入到这个Binder线程池中去 sp&lt;ProcessState&gt; ps(ProcessState::self()); ps-&gt;startThreadPool(); // instantiate surfaceflinger //创建一个SurfaceFlinger强引用对象 sp&lt;SurfaceFlinger&gt; flinger = new SurfaceFlinger();#if defined(HAVE_PTHREADS) setpriority(PRIO_PROCESS, 0, PRIORITY_URGENT_DISPLAY);#endif set_sched_policy(0, SP_FOREGROUND); // initialize before clients can connect //调用SurfaceFlinger的init函数 flinger-&gt;init(); // publish surface flinger //把SurfaceFlinger服务注册到ServiceManager中 sp&lt;IServiceManager&gt; sm(defaultServiceManager()); sm-&gt;addService(String16(SurfaceFlinger::getServiceName()), flinger, false); // run in this thread //运行这个UI渲染流程 flinger-&gt;run(); return 0;&#125; &#160; &#160; &#160; &#160;main函数包含以下几件事情： 调用当前进程中的ProcessState单例的成员函数startThreadPool来启动一个Binder线程池，将线程池最大数量设为4，并且调用当前线程中的IPCThreadState单例来将当前线程加入到前面所启动的Binder线程池中去； 创建一个SurfaceFlinger的对象，并赋给他的强引用指针； 执行SurfaceFlinger的init函数； 将SurfaceFlinger服务注册到ServiceManager当中； 运行SurfaceFlinger的UI渲染流程。 &#160; &#160; &#160; &#160;比较重要的步骤就是创建SurfaceFlinger对象，执行init函数，和运行UI渲染流程。我们逐个分析。 创建SurfaceFlinger对象&#160; &#160; &#160; &#160;new一个SurfaceFlinger对象，并赋给强引用指针。我们先看看它的构造函数，位于frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748SurfaceFlinger::SurfaceFlinger() : BnSurfaceComposer(), mTransactionFlags(0), mTransactionPending(false), mAnimTransactionPending(false), mLayersRemoved(false), mRepaintEverything(0), mRenderEngine(NULL), mBootTime(systemTime()), mVisibleRegionsDirty(false), mHwWorkListDirty(false), mAnimCompositionPending(false), mDebugRegion(0), mDebugDDMS(0), mDebugDisableHWC(0), mDebugDisableTransformHint(0), mDebugInSwapBuffers(0), mLastSwapBufferTime(0), mDebugInTransaction(0), mLastTransactionTime(0), mBootFinished(false), mPrimaryHWVsyncEnabled(false),//主显屏硬件VSync信号关闭 mHWVsyncAvailable(false), mDaltonize(false), mHasColorMatrix(false)&#123; ALOGI("SurfaceFlinger is starting"); //一些调试变量，忽略一下内容 // debugging stuff... char value[PROPERTY_VALUE_MAX]; property_get("ro.bq.gpu_to_cpu_unsupported", value, "0"); mGpuToCpuSupported = !atoi(value); property_get("debug.sf.showupdates", value, "0"); mDebugRegion = atoi(value); property_get("debug.sf.ddms", value, "0"); mDebugDDMS = atoi(value); if (mDebugDDMS) &#123; if (!startDdmConnection()) &#123; // start failed, and DDMS debugging not enabled mDebugDDMS = 0; &#125; &#125; ALOGI_IF(mDebugRegion, "showupdates enabled"); ALOGI_IF(mDebugDDMS, "DDMS debugging enabled");&#125; &#160; &#160; &#160; &#160;构造函数中主要初始化一系列变量，没什么重要信息。SurfaceFlinger类继承了BnSurfaceComposer类，而后者是一个实现了ISurfaceComposer接口的Binder本地对象类。 &#160; &#160; &#160; &#160;由于Service Manager的Binder代理对象的成员函数addService的第二个参数是一个类型为IBinder的强指针引用。当一个对象第一次被一个强指针引用时，那么这个对象的成员函数onFirstRef就会被调用。因此，接下来前面所创建的SurfaceFlinger实例的成员函数onFirstRef就会被调用，以便可以继续执行初始化操作。我们继续查看：12345678// mEventQueue在SurfaceFlinger.h中定义// these are thread safemutable MessageQueue mEventQueue;void SurfaceFlinger::onFirstRef()&#123; mEventQueue.init(this);&#125; &#160; &#160; &#160; &#160;MessageQueue 类在frameworks/native/services/surfaceflinger/MessageQueue.h中定义，实现位于frameworks/native/services/surfaceflinger/MessageQueue.cpp中，init函数如下：123456void MessageQueue::init(const sp&lt;SurfaceFlinger&gt;&amp; flinger)&#123; mFlinger = flinger; mLooper = new Looper(true); mHandler = new Handler(*this);&#125; &#160; &#160; &#160; &#160;调用MessageQueue的init，在MessageQueue中建了一个Looper和Handler，注意不是Java中的，native实现的。到后面就可以看到SF的核心就是接收消息，处理消息。对于消息处理，可以参考之前的一片文章Android消息处理零散分析。 调用init函数&#160; &#160; &#160; &#160;回到SurfaceFlinger.cpp中，继续分析init函数：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798void SurfaceFlinger::init() &#123; ALOGI( "SurfaceFlinger's main thread ready to run. " "Initializing graphics H/W..."); status_t err; Mutex::Autolock _l(mStateLock); //初始化OpenGL 图形库相关配置 // initialize EGL for the default display 将EGL初始化成默认的显示，默认是主屏幕，编号为0 mEGLDisplay = eglGetDisplay(EGL_DEFAULT_DISPLAY); eglInitialize(mEGLDisplay, NULL, NULL); //创建显示设备的抽象代表，负责和显示设备打交道 // Initialize the H/W composer object. There may or may not be an // actual hardware composer underneath. mHwc = new HWComposer(this, *static_cast&lt;HWComposer::EventHandler *&gt;(this)); // get a RenderEngine for the given display / config (can't fail) mRenderEngine = RenderEngine::create(mEGLDisplay, mHwc-&gt;getVisualID()); // retrieve the EGL context that was selected/created mEGLContext = mRenderEngine-&gt;getEGLContext(); LOG_ALWAYS_FATAL_IF(mEGLContext == EGL_NO_CONTEXT, "couldn't create EGLContext"); // initialize our non-virtual displays //创建显示设备对象 for (size_t i=0 ; i&lt;DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES ; i++) &#123; DisplayDevice::DisplayType type((DisplayDevice::DisplayType)i); // set-up the displays that are already connected if (mHwc-&gt;isConnected(i) || type==DisplayDevice::DISPLAY_PRIMARY) &#123; // All non-virtual displays are currently considered secure. bool isSecure = true; createBuiltinDisplayLocked(type); wp&lt;IBinder&gt; token = mBuiltinDisplays[i]; sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferConsumer&gt; consumer; BufferQueue::createBufferQueue(&amp;producer, &amp;consumer, new GraphicBufferAlloc()); sp&lt;FramebufferSurface&gt; fbs = new FramebufferSurface(*mHwc, i, consumer); int32_t hwcId = allocateHwcDisplayId(type); sp&lt;DisplayDevice&gt; hw = new DisplayDevice(this, type, hwcId, mHwc-&gt;getFormat(hwcId), isSecure, token, fbs, producer, mRenderEngine-&gt;getEGLConfig()); if (i &gt; DisplayDevice::DISPLAY_PRIMARY) &#123; // FIXME: currently we don't get blank/unblank requests // for displays other than the main display, so we always // assume a connected display is unblanked. ALOGD("marking display %zu as acquired/unblanked", i); hw-&gt;setPowerMode(HWC_POWER_MODE_NORMAL); &#125; mDisplays.add(token, hw); &#125; &#125; // make the GLContext current so that we can create textures when creating Layers // (which may happens before we render something) getDefaultDisplayDevice()-&gt;makeCurrent(mEGLDisplay, mEGLContext); // start the EventThread //启动EventThread。监听和处理SurfaceFlinger中的事件 //app的VSync信号 sp&lt;VSyncSource&gt; vsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, vsyncPhaseOffsetNs, true, "app"); mEventThread = new EventThread(vsyncSrc); //SF的VSync信号 sp&lt;VSyncSource&gt; sfVsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, sfVsyncPhaseOffsetNs, true, "sf"); mSFEventThread = new EventThread(sfVsyncSrc); //SF的VSync信号控制逻辑也要放入mEventQueue消息队列 mEventQueue.setEventThread(mSFEventThread); //VSync信号闸刀控制线程 mEventControlThread = new EventControlThread(this); mEventControlThread-&gt;run("EventControl", PRIORITY_URGENT_DISPLAY); // set a fake vsync period if there is no HWComposer //如果硬件设备检测有问题，或者没有硬件设备驱动提供Vsync信号，则设置软件VSync信号 if (mHwc-&gt;initCheck() != NO_ERROR) &#123; mPrimaryDispSync.setPeriod(16666667); &#125; // initialize our drawing state mDrawingState = mCurrentState; // set initial conditions (e.g. unblank default device) //初始化显示设备，调用initializeDisplays完成 initializeDisplays(); // start boot animation //启动开机动画，调用了startBootAnim函数，只是设置了两个属性，其中一个ctl.start是启动了bootanim进程 startBootAnim();&#125; &#160; &#160; &#160; &#160;init函数主要做了以下事情： 初始化OpenGL ES图形库； 创建显示设备的抽象代表，负责和显示设备打交道； 创建显示设备对象； 启动EventThread。监听和处理SurfaceFlinger中的事件； 设置软件VSync信号周期； 初始化显示设备，调用initializeDisplays完成； 启动开机动画，调用了startBootAnim函数，只是设置了两个属性，其中一个ctl.start是启动了bootanim进程。 &#160; &#160; &#160; &#160;其中初始化OpenGL ES可以参考Android SurfaceFlinger 学习之路(二)—-SurfaceFlinger概述，后期如果有机会再次会细讲。创建显示设备我们后面分析管理图形缓冲区时候会细讲。还有VSync信号，后面会单独开章节分析。这里我们大概过一个流程。 执行run函数&#160; &#160; &#160; &#160;然后继续往下看，执行SurfaceFlinger的run函数：123456789void SurfaceFlinger::run() &#123; do &#123; waitForEvent(); &#125; while (true);&#125;void SurfaceFlinger::waitForEvent() &#123; mEventQueue.waitMessage();&#125; &#160; &#160; &#160; &#160;run函数非常简单，但却是SF的核心，是个while循环，循环处理消息等。&#160; &#160; &#160; &#160;然后又调用了EventQueue的waitMessage方法，记住这里是在主线程中循环调用的。12345678910111213141516171819202122void MessageQueue::waitMessage() &#123; do &#123; //flushCommands主要是清理工作的 IPCThreadState::self()-&gt;flushCommands(); //pollOnce是消息机制，主要调用了epoll_wait函数，会阻塞，阻塞完了会分发消息队列中的消息 int32_t ret = mLooper-&gt;pollOnce(-1); switch (ret) &#123; case Looper::POLL_WAKE: case Looper::POLL_CALLBACK: continue; case Looper::POLL_ERROR: ALOGE("Looper::POLL_ERROR"); case Looper::POLL_TIMEOUT: // timeout (should not happen) continue; default: // should not happen ALOGE("Looper::pollOnce() returned unknown status %d", ret); continue; &#125; &#125; while (true);&#125; &#160; &#160; &#160; &#160;我们来看下waitMessage方法，flushCommands主要是清理工作的，和Binder驱动的交互关了。而pollOnce是消息机制，主要调用了epoll_wait函数，会阻塞，阻塞完了会分发消息队列中的消息。这里的消息只有自己在Handler中发的消息，还有在setEventThread中自己添加的fd。 消息处理给SurfaceFlinger发送消息&#160; &#160; &#160; &#160;我们以SurfaceFlinger与客户端通信创建Surface为例，看看如何给SurfaceFlinger发送消息。这个我们下面会讲到，这里先举个栗子。 &#160; &#160; &#160; &#160;从上一篇开机动画的简述流程可以得知，BootAnimation的readyToRun函数中有一句：123// create the native surface//调用SurfaceComposerClient对象mSession的成员函数createSurface可以获得一个SurfaceControl对象controlsp&lt;SurfaceControl&gt; control = session()-&gt;createSurface(String8("BootAnimation"), &#160; &#160; &#160; &#160;BootAnimation类的成员函数session用来返回BootAnimation类的成员变量mSession所描述的一个SurfaceComposerClient对象。通过调用SurfaceComposerClient对象mSession的成员函数createSurface可以获得一个SurfaceControl对象control。&#160; &#160; &#160; &#160;SurfaceComposerClient类的成员函数createSurface首先调用内部的Binder代理对象mClient(frameworks/native/services/surfaceflinger/Client.cpp)来请求SurfaceFlinger返回一个类型为(class Handle : public BBinder, public LayerCleaner)Binder代理对象（封装了SurfaceFlinger的sp指针和Layer对象）handle，和一个IGraphicBufferProducer的sp指针（封装了SurfaceFlinger的sp指针）gbp，接着再使用这两个对象来创建一个SurfaceControl对象。创建出来的SurfaceControl对象的成员变量handle就指向了从SurfaceFlinger返回来的类型为Handle 的Binder代理对象。有了这个Binder代理对象之后，SurfaceControl对象就可以和SurfaceFlinger服务通信了。 &#160; &#160; &#160; &#160;我们关注的就是Client的createSurface函数，frameworks/native/services/surfaceflinger/Client.cpp：12345678910111213141516171819202122232425262728293031323334353637383940414243444546status_t Client::createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp)&#123; /* * createSurface must be called from the GL thread so that it can * have access to the GL context. */ class MessageCreateLayer : public MessageBase &#123; SurfaceFlinger* flinger; Client* client; sp&lt;IBinder&gt;* handle; sp&lt;IGraphicBufferProducer&gt;* gbp; status_t result; const String8&amp; name; uint32_t w, h; PixelFormat format; uint32_t flags; public: MessageCreateLayer(SurfaceFlinger* flinger, const String8&amp; name, Client* client, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp) : flinger(flinger), client(client), handle(handle), gbp(gbp), name(name), w(w), h(h), format(format), flags(flags) &#123; &#125; status_t getResult() const &#123; return result; &#125; //handler是执行消息动作的地方 virtual bool handler() &#123; result = flinger-&gt;createLayer(name, client, w, h, format, flags, handle, gbp); return true; &#125; &#125;; //首先封装消息 sp&lt;MessageBase&gt; msg = new MessageCreateLayer(mFlinger.get(), name, this, w, h, format, flags, handle, gbp); //调用SF的postMessageSync发送同步消息 mFlinger-&gt;postMessageSync(msg); return static_cast&lt;MessageCreateLayer*&gt;( msg.get() )-&gt;getResult();&#125; &#160; &#160; &#160; &#160;类MessageBase就是封装了类似于一个Handler，里面有个Barrier，我们能够猜到，这个Barrier 肯定是用来进行同步发送消息的，利用Barrier 去等待”wait”。位于framework/native/services/surfaceflinger/MessageQueue.h中:12345678910111213141516171819class MessageBase : public MessageHandler&#123;public: MessageBase(); // return true if message has a handler virtual bool handler() = 0; // waits for the handler to be processed void wait() const &#123; barrier.wait(); &#125;protected: virtual ~MessageBase();private: virtual void handleMessage(const Message&amp; message); mutable Barrier barrier;&#125;; &#160; &#160; &#160; &#160;MessageBase的handleMessage函数，可以看到MessageBase的handler()函数是真正消息处理的地方，执行完成后，调用barrier.open();，打开barrier，这样调用barrier.wait()的地方就能退出了。实现位于framework/native/services/surfaceflinger/MessageQueue.cpp中：1234void MessageBase::handleMessage(const Message&amp;) &#123; this-&gt;handler(); barrier.open();&#125;; &#160; &#160; &#160; &#160;接着分析mFlinger-&gt;postMessageSync(msg);，这是给SF发同步消息的入口，当然也可以发异步消息，实现是类似的:12345678910status_t SurfaceFlinger::postMessageSync(const sp&lt;MessageBase&gt;&amp; msg, nsecs_t reltime, uint32_t /* flags */) &#123; //向mEventQueue，即MessageQueue中发送消息 status_t res = mEventQueue.postMessage(msg, reltime); //这里等着，同步就在同步函数中等着 if (res == NO_ERROR) &#123; msg-&gt;wait(); &#125; return res;&#125; &#160; &#160; &#160; &#160;可以看到在同步发送消息中，barrier在postMessageSync函数中一直等着呢（wait），等待SF调用handleMessage()函数去将barrier这个栅栏打开(open)。 SurfaceFlinger处理消息&#160; &#160; &#160; &#160;从上面waitMessage得知，消息处理都位于里面无限循环处的的int32_t ret = mLooper-&gt;pollOnce(-1);我们追寻到Looper中的pollOnce函数，位于system/core/libutils/Looper.cpp中：123456789int Looper::pollOnce(int timeoutMillis, int* outFd, int* outEvents, void** outData) &#123; int result = 0; for (;;) &#123; ...... result = pollInner(timeoutMillis); &#125;&#125; &#160; &#160; &#160; &#160;函数中，而pollOnce又会调用pollInner：12345678910111213141516171819202122232425262728293031323334353637int Looper::pollInner(int timeoutMillis) &#123; // Invoke pending message callbacks. mNextMessageUptime = LLONG_MAX; while (mMessageEnvelopes.size() != 0) &#123; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); const MessageEnvelope&amp; messageEnvelope = mMessageEnvelopes.itemAt(0); if (messageEnvelope.uptime &lt;= now) &#123; // Remove the envelope from the list. // We keep a strong reference to the handler until the call to handleMessage // finishes. Then we drop it so that the handler can be deleted *before* // we reacquire our lock. &#123; // obtain handler sp&lt;MessageHandler&gt; handler = messageEnvelope.handler; Message message = messageEnvelope.message; //把头删除啊 mMessageEnvelopes.removeAt(0); mSendingMessage = true; mLock.unlock();#if DEBUG_POLL_AND_WAKE || DEBUG_CALLBACKS ALOGD("%p ~ pollOnce - sending message: handler=%p, what=%d", this, handler.get(), message.what);#endif //处理消息啊 handler-&gt;handleMessage(message); &#125; // release handler mLock.lock(); mSendingMessage = false; result = ALOOPER_POLL_CALLBACK; &#125; else &#123; // The last message left at the head of the queue determines the next wakeup time. mNextMessageUptime = messageEnvelope.uptime; break; &#125; &#125;&#125; &#160; &#160; &#160; &#160;从上面代码可以看到，发给SF的消息被封装在MessageEnvelope结构中，SF一直在mMessageEnvelopes队列中从头部取出消息，然后执行，即handler-&gt;handleMessage(message)，这个即是我们上面提到的framework/native/services/surfaceflinger/MessageQueue.cpp中：12345void MessageBase::handleMessage(const Message&amp;) &#123; this-&gt;handler(); //打开栅栏 barrier.open();&#125;; &#160; &#160; &#160; &#160;调用handleMessage执行handler()，所以SurfaceFlinger创建Surface的核心代码就是SurfaceFlinger的createLayer函数，回到刚才的createSurface函数中的片段：12result = flinger-&gt;createLayer(name, client, w, h, format, flags, handle, gbp); &#160; &#160; &#160; &#160;执行完成后，打开barrier。 &#160; &#160; &#160; &#160;这个只是个简单例子，我们后续会详细讲解它的消息处理流程。 SurfaceFlinger服务连接过程概述&#160; &#160; &#160; &#160;第一篇文章描述Android应用程序和SurfaceFlinger服务的关系时提到，每一个有UI的Android应用程序都需要与SurfaceFlinger服务建立一个连接，以便可以通过这个连接来请求SurfaceFlinger服务为它创建和渲染Surface。我们将以Android系统的开机动画应用程序为例，详细描述Android应用程序是如何与SurfaceFlinger服务建立连接的。 &#160; &#160; &#160; &#160;从从上一篇文章可以知道，Android系统的开机动画是主要一个BootAnimation对象来实现，这个BootAnimation对象在构造的时候，会在内部创建一个SurfaceComposerClient对象来负责创建一个到SurfaceFlinger服务的连接。 &#160; &#160; &#160; &#160;BootAnimation类的构造函数实现在文件frameworks/base/cmds/bootanimation/BootAnimation.cpp中：1234BootAnimation::BootAnimation() : Thread(false), mZip(NULL)&#123; mSession = new SurfaceComposerClient();&#125; &#160; &#160; &#160; &#160;mSession是BootAnimation类的成员变量，它是一个类型为SurfaceComposerClient的强指针，即sp &lt; SurfaceComposerClient &gt; 。在SurfaceComposerClient类内部，有一个类型为sp&lt; ISurfaceComposerClient &gt;的成员变量mClient，如下图： &#160; &#160; &#160; &#160;SurfaceComposerClient类的成员变量mClient指向的实际上是一个类型为BpSurfaceComposerClient的Binder代理对象，而这个类型为BpSurfaceComposerClient的Binder代理对象引用的是一个类型为Client的Binder本地对象，位于frameworks/native/services/surfaceflinger/Client.cpp。类型为Client的Binder本地对象是由SurfaceFlinger服务来负责创建的，并且运行在SurfaceFlinger服务中，用来代表使用SurfaceFlinger服务的一个客户端，即一个与UI相关的Android应用程序。 &#160; &#160; &#160; &#160;由于Client类和BpSurfaceComposerClient类分别是一个Binder本地对象类和一个Binder代理对象类，它们都是根据Android系统在应用程序框架层提供的Binder进程间通信库来实现的。类图如下： &#160; &#160; &#160; &#160;Client类的实现结构图 &#160; &#160; &#160; &#160;BpSurfaceComposerClient类的实现结构图 &#160; &#160; &#160; &#160;Client类和BpSurfaceComposerClient类均实现了类型为ISurfaceComposerClient的Binder接口。ISurfaceComposerClient接口有个createSurface接口，它们定义在文件frameworks/base/include/surfaceflinger/ISurfaceComposerClient.h中，如下所示：123456789101112class ISurfaceComposerClient : public IInterface&#123;/* * Requires ACCESS_SURFACE_FLINGER permission */ virtual status_t createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp) = 0;&#125; &#160; &#160; &#160; &#160;在接下来的文章中，我们再详细分析ISurfaceComposerClient接口的成员函数createSurface的实现。 理解了SurfaceComposerClient、Client以及BpSurfaceComposerClient这三个类的关系之后，接下来我们就可以分析Android系统的开机动画应用程序bootanimation是如何与SurfaceFlinger服务建立连接的。 &#160; &#160; &#160; &#160;SurfaceComposerClient类继承了RefBase类，因此，当BootAnimation类在构造函数创建了一个SurfaceComposerClient对象，并且将这个对象赋值给类型为sp&lt; SurfaceComposerClient &gt;的智能指针mSession时，就会导致SurfaceComposerClient类的成员函数onFirstRef被调用，而SurfaceComposerClient类的成员函数onFirstRef在调用的过程中，就会在应用程序bootanimation与SurfaceFlinger服务建立一个连接。这个流程如下： &#160; &#160; &#160; &#160;接下来，我们就详细分析每一个步骤。 获取SurfaceFlinger服务代理接口&#160; &#160; &#160; &#160;进入frameworks/native/libs/gui/SurfaceComposerClient.cpp中，找到onFristRef函数：12345678910void SurfaceComposerClient::onFirstRef() &#123; sp&lt;ISurfaceComposer&gt; sm(ComposerService::getComposerService()); if (sm != 0) &#123; sp&lt;ISurfaceComposerClient&gt; conn = sm-&gt;createConnection(); if (conn != 0) &#123; mClient = conn; mStatus = NO_ERROR; &#125; &#125;&#125; &#160; &#160; &#160; &#160;SurfaceComposerClient类的成员函数getComposerService用来获得SurfaceFlinger服务的一个代理接口，它的实现同样位于frameworks/native/libs/gui/SurfaceComposerClient.cpp中：12345678910/*static*/ sp&lt;ISurfaceComposer&gt; ComposerService::getComposerService() &#123; ComposerService&amp; instance = ComposerService::getInstance(); Mutex::Autolock _l(instance.mLock); if (instance.mComposerService == NULL) &#123; ComposerService::getInstance().connectLocked(); assert(instance.mComposerService != NULL); ALOGD("ComposerService reconnected"); &#125; return instance.mComposerService;&#125; &#160; &#160; &#160; &#160;ComposerService类是单例模式，当我们第一次调用它的静态函数getInstance的时候，它就会在构造函数中获得SurfaceFlinger服务的一个代理接口，并且保存在它的成员变量mComposerService中，如下所示：12345678910111213141516171819202122232425262728ComposerService::ComposerService(): Singleton&lt;ComposerService&gt;() &#123; Mutex::Autolock _l(mLock); connectLocked();&#125;void ComposerService::connectLocked() &#123; const String16 name("SurfaceFlinger"); while (getService(name, &amp;mComposerService) != NO_ERROR) &#123; usleep(250000); &#125; assert(mComposerService != NULL); // Create the death listener. class DeathObserver : public IBinder::DeathRecipient &#123; ComposerService&amp; mComposerService; virtual void binderDied(const wp&lt;IBinder&gt;&amp; who) &#123; ALOGW("ComposerService remote (surfaceflinger) died [%p]", who.unsafe_get()); mComposerService.composerServiceDied(); &#125; public: DeathObserver(ComposerService&amp; mgr) : mComposerService(mgr) &#123; &#125; &#125;; mDeathObserver = new DeathObserver(*const_cast&lt;ComposerService*&gt;(this)); mComposerService-&gt;asBinder()-&gt;linkToDeath(mDeathObserver);&#125; &#160; &#160; &#160; &#160;在ComposerService类的构造函数中，会获得SurfaceFlinger服务的代理接口。 连接SurfaceFlinger服务&#160; &#160; &#160; &#160;回到SurfaceComposerClient类的成员函数onFirstRef中，由于SurfaceFlinger服务实现了ISurfaceComposer接口，因此，我们可以将前面获得的SurfaceFlinger服务的代理接口赋值给一个类型为ISurfaceComposer的强指针sm，并且调用它的成员函数createConnection来请求SurfaceFlinger服务创建一个连接，即创建一个类型为Client的Binder对象，并且将这个Binder对象的一个代理接口conn返回来。SurfaceComposerClient类获得了SurfaceFlinger服务返回来的Client代理接口conn之后，就将它保存自己的成员变量mClient中，这样开机动画应用程序bootanimation后续就可以通过它来请求SurfaceFlinger创建和渲染Surface了。 &#160; &#160; &#160; &#160;接下来，我们就继续分析SurfaceFlinger服务的成员函数createConnection的实现，以便可以了解它是如何为Android应用程序创建一个连接的。 &#160; &#160; &#160; &#160;进入SurfaceFlinger.cpp中查看createConnection函数：12345678910sp&lt;ISurfaceComposerClient&gt; SurfaceFlinger::createConnection()&#123; sp&lt;ISurfaceComposerClient&gt; bclient; sp&lt;Client&gt; client(new Client(this)); status_t err = client-&gt;initCheck(); if (err == NO_ERROR) &#123; bclient = client; &#125; return bclient;&#125; &#160; &#160; &#160; &#160;它的实现很简单，只是创建了一个类型为Client的Binder对象client，并且获得它的一个ISurfaceComposerClient接口，最后将这个ISurfaceComposerClient接口，即一个Client代理对象，返回给开机动画应用程序bootanimation。 &#160; &#160; &#160; &#160;接下来，我们再继续分析Client对象的创建过程,，即Client类的构造函数的实现：1234Client::Client(const sp&lt;SurfaceFlinger&gt;&amp; flinger) : mFlinger(flinger)&#123;&#125; &#160; &#160; &#160; &#160;这个很简单，就是保存了SurfaceFlinger的强引用对象。&#160; &#160; &#160; &#160;回到SurfaceFlinger类的成员函数createConnection中，它将一个指向了一个Client对象的ISurfaceComposerClient接口返回到开机动画应用程序bootanimation之后，开机动画应用程序bootanimation就可以将它封装成一个类型为BpSurfaceComposerClient的Binder代理对象。 &#160; &#160; &#160; &#160;类型为BpSurfaceComposerClient的Binder代理对象的封装过程实现在SurfaceFlinger服务的Binder代理对象类BpSurfaceComposer的成员函数createConnection中，位于frameworks/native/libs/gui/ISurfaceCompose.cpp中：123456789101112131415class BpSurfaceComposer : public BpInterface&lt;ISurfaceComposer&gt;&#123;public: ...... virtual sp&lt;ISurfaceComposerClient&gt; createConnection() &#123; uint32_t n; Parcel data, reply; data.writeInterfaceToken(ISurfaceComposer::getInterfaceDescriptor()); remote()-&gt;transact(BnSurfaceComposer::CREATE_CONNECTION, data, &amp;reply); return interface_cast&lt;ISurfaceComposerClient&gt;(reply.readStrongBinder()); &#125;......&#125; &#160; &#160; &#160; &#160;interface_cast是一个模板函数，它定义在framework/native/include/binder/IInterface.h文件中：12345template&lt;typename INTERFACE&gt;inline sp&lt;INTERFACE&gt; interface_cast(const sp&lt;IBinder&gt;&amp; obj)&#123; return INTERFACE::asInterface(obj);&#125; &#160; &#160; &#160; &#160;从这里就可以看出，当模板参数为ISurfaceComposerClient的时候，模板函数interface_cast实际就是通过调用ISurfaceComposerClient类的静态成员函数asInterface来将参数obj所描述的一个Binder代理对象，即一个BpBinder对象，封装成一个BpSurfaceComposerClient对象。 &#160; &#160; &#160; &#160;ISurfaceComposerClient类的静态成员函数asInterface是由frameworks/native/libs/gui/ISurfaceComposerClient.cpp文件中的IMPLEMENT_META_INTERFACE宏来定义的，如下所示：1IMPLEMENT_META_INTERFACE(SurfaceComposerClient, "android.ui.ISurfaceComposerClient"); &#160; &#160; &#160; &#160;IMPLEMENT_META_INTERFACE宏展开后，得到ISurfaceComposerClient类的静态成员函数asInterface的实现如下所示：12345678910111213android::sp&lt;ISurfaceComposerClient&gt; ISurfaceComposerClient::asInterface(const android::sp&lt;android::IBinder&gt;&amp; obj) &#123; android::sp&lt;ISurfaceComposerClient&gt; intr; if (obj != NULL) &#123; intr = static_cast&lt;ISurfaceComposerClient*&gt;( obj-&gt;queryLocalInterface(ISurfaceComposerClient::descriptor).get()); if (intr == NULL) &#123; intr = new BpSurfaceComposerClient(obj); &#125; ｝ return intr; &#125; &#160; &#160; &#160; &#160;参数obj是从BpSurfaceComposer类的成员函数createConnection传进来的，它指向的实际上是一个BpBinder对象。当我们调用一个BpBinder对象的成员函数queryLocalInterface时，获得的是一个NULL指针，因此，ISurfaceComposerClient类的静态成员函数asInterface最后就会将参数obj所指向的一个BpBinder对象封装成一个BpSurfaceComposerClient对象，并且返回给调用者。 &#160; &#160; &#160; &#160;至此，开机动画应用程序bootanimation就通过SurfaceComposerClient类来与SurfaceFlinger服务建立一个连接了。 小结&#160; &#160; &#160; &#160;本篇主要学习了SurfaceFlinger的启动和连接过程，应该算不太难的部分。下几节我们将逐步分析消息处理、创建surface、管理GraphicBuffer、VSync信号、Fence机制等等内容。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(三)----Android开机动画流程简述]]></title>
    <url>%2F2017%2F05%2F02%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E4%B8%89-Android%E5%BC%80%E6%9C%BA%E5%8A%A8%E7%94%BB%E6%B5%81%E7%A8%8B%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;在详细分析SurfaceFlinger模块之前要先看看Android的开机动画，因为这个BootAnimation是一个C++应用程序，需要使用SurfaceFlinger服务来创建和渲染自己的Surface，并且不涉及与用户进行交互，所以能以最简洁的方式体现Android与SurfaceFlinger服务的关系。 开机动画的启动&#160; &#160; &#160; &#160;第一个开机画面是在内核启动的过程中出现的，它是一个静态的画面。第二个开机画面是在init进程启动的过程中出现的，它也是一个静态的画面。第三个开机画面BootAnimation是在系统服务启动的过程中出现的，它是一个动态的画面。无论是哪一个画面，它们都是在一个称为帧缓冲区（frame buffer，简称fb）的硬件设备上进行渲染的。接下来，我们就分别分析第三个画面是如何在fb上显示的。 bootanimation的启动过程&#160; &#160; &#160; &#160;第三个开机动画使用应用程序BootAnimation来负责显示的，它在启动脚本init.rc中被配置成了一个服务，位于system/core/rootdir/Init.rc：12345678......service bootanim /system/bin/bootanimation class core user graphics //用户 group graphics audio //用户组 disabled //init进程启动时，不会自动启动bootanimation oneshot...... &#160; &#160; &#160; &#160;应用程序bootanimation的用户和用户组名称分别被设置为graphics。注意， 用来启动应用程序bootanimation的服务是disable的，即init进程在启动的时候，不会主动将应用程序bootanimation启动起来。&#160; &#160; &#160; &#160;当SurfaceFlinger服务启动的时候，它会通过修改系统属性ctl.start的值来通知init进程启动应用程序bootanimation，以便可以显示第三个开机画面。 &#160; &#160; &#160; &#160;在早期的Android版本中，SurfaceFlinger服务是由SystemServer启动的。但在Android5.0中，该服务是init进程启动过程中就启动了。在init.rc中能看到对该服务的描述：1234567......service surfaceflinger /system/bin/surfaceflinger class core user system group graphics drmrpc onrestart restart zygote...... &#160; &#160; &#160; &#160;SurfaceFlinger服务的源码路径位于framework/native/services/surfaceflinger/下面，服务的入口在下面的main_surfaceflinger.cpp中，我们查看它的main函数：1234567891011121314151617181920212223242526272829int main(int, char**) &#123; // When SF is launched in its own process, limit the number of // binder threads to 4. ProcessState::self()-&gt;setThreadPoolMaxThreadCount(4); // start the thread pool sp&lt;ProcessState&gt; ps(ProcessState::self()); ps-&gt;startThreadPool(); // instantiate surfaceflinger sp&lt;SurfaceFlinger&gt; flinger = new SurfaceFlinger();#if defined(HAVE_PTHREADS) setpriority(PRIO_PROCESS, 0, PRIORITY_URGENT_DISPLAY);#endif set_sched_policy(0, SP_FOREGROUND); // initialize before clients can connect flinger-&gt;init(); //我们主要看这里，其他忽略 // publish surface flinger sp&lt;IServiceManager&gt; sm(defaultServiceManager()); sm-&gt;addService(String16(SurfaceFlinger::getServiceName()), flinger, false); // run in this thread flinger-&gt;run(); return 0;&#125; &#160; &#160; &#160; &#160;主要工作就是新建一个SurfaceFlinger对象，然后调用其中的init函数，最后调用run函数。我们的重点在init方法，位于framework/native/services/surfaceflinger/SurfaceFlinger.cpp中：12345void SurfaceFlinger::init() &#123;...省略大量代码... // start boot animation startBootAnim();&#125; &#160; &#160; &#160; &#160;最后一行调用了startBootAnim函数，我们继续往下看：12345void SurfaceFlinger::startBootAnim() &#123; // start boot animation property_set("service.bootanim.exit", "0"); property_set("ctl.start", "bootanim");&#125; &#160; &#160; &#160; &#160;这里讲系统属性”service.bootanim.exit”设置为”0”，并将”ctl.start”设置为”bootanim”。重点是第二个。 &#160; &#160; &#160; &#160;当系统属性发生改变时，init进程就会接收到一个系统属性变化通知，这个通知最终是由在init进程中的函数handle_property_set_fd来处理的。我们可以查看init进程的入口main函数，位于system/core/init/Init.c中：1234567891011121314151617181920212223242526int main(int argc, char **argv)&#123;...省略大量代码... for(;;) &#123; int nr, i, timeout = -1; ...省略一些代码... //poll函数用来轮询事件 nr = poll(ufds, fd_count, timeout); if (nr &lt;= 0) continue; for (i = 0; i &lt; fd_count; i++) &#123; if (ufds[i].revents &amp; POLLIN) &#123; //我们关注这里，当系统属性值被修改时，得到该事件，会执行handle_property_set_fd函数 if (ufds[i].fd == get_property_set_fd()) handle_property_set_fd(); else if (ufds[i].fd == get_keychord_fd()) handle_keychord(); else if (ufds[i].fd == get_signal_fd()) handle_signal(); &#125; &#125; &#125; return 0;&#125; &#160; &#160; &#160; &#160;可以看到，init进程会使用poll机制来轮询事件，其中一个事件是系统属性值被修改。得到该事件后，会执行handle_property_set_fd()，位于system/core/init/Property_service.c中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105void handle_property_set_fd()&#123; prop_msg msg; //init进程是通过一个socket来接收系统属性变化事件的 int s; int r; int res; struct ucred cr; struct sockaddr_un addr; socklen_t addr_size = sizeof(addr); socklen_t cr_size = sizeof(cr); char * source_ctx = NULL; struct pollfd ufds[1]; const int timeout_ms = 2 * 1000; /* Default 2 sec timeout for caller to send property. */ int nr; //接收TCP连接 服务端阻塞？等待客户端连接 if ((s = accept(property_set_fd, (struct sockaddr *) &amp;addr, &amp;addr_size)) &lt; 0) &#123; return; &#125; /* Check socket options here */ //取出socket的可选内容，可能包括客户端进程的权限等属性 if (getsockopt(s, SOL_SOCKET, SO_PEERCRED, &amp;cr, &amp;cr_size) &lt; 0) &#123; close(s); ERROR("Unable to receive socket options\n"); return; &#125; ufds[0].fd = s; ufds[0].events = POLLIN; ufds[0].revents = 0; //轮询客户端事件 nr = TEMP_FAILURE_RETRY(poll(ufds, 1, timeout_ms)); //轮询事件超时 if (nr == 0) &#123; ERROR("sys_prop: timeout waiting for uid=%d to send property message.\n", cr.uid); close(s); return; //等待错误 &#125; else if (nr &lt; 0) &#123; ERROR("sys_prop: error waiting for uid=%d to send property message. err=%d %s\n", cr.uid, errno, strerror(errno)); close(s); return; &#125; //接收socket的主体数据 r = TEMP_FAILURE_RETRY(recv(s, &amp;msg, sizeof(msg), MSG_DONTWAIT)); //接收事件不匹配，return if(r != sizeof(prop_msg)) &#123; ERROR("sys_prop: mis-match msg size received: %d expected: %zu errno: %d\n", r, sizeof(prop_msg), errno); close(s); return; &#125; //开始条件判断接收事件 switch(msg.cmd) &#123; case PROP_MSG_SETPROP: //如果是属性发生变化 msg.name[PROP_NAME_MAX-1] = 0; msg.value[PROP_VALUE_MAX-1] = 0; //检查属性名，不能有特殊字符，或者两个点..这样的名字 if (!is_legal_property_name(msg.name, strlen(msg.name))) &#123; ERROR("sys_prop: illegal property name. Got: \"%s\"\n", msg.name); close(s); return; &#125; getpeercon(s, &amp;source_ctx);/* 如果是ctl开头的消息，则认为是控制消息，控制消息用来执行一些命令，例如用adb shell登录后，输入setprop ctl.start bootanim就可以查看开机动画了;输入setprop service.bootanim.exit 1就可以退出开机动画了 */ if(memcmp(msg.name,"ctl.",4) == 0) &#123; // Keep the old close-socket-early behavior when handling // ctl.* properties. close(s); //改变系统属性是需要权限,所以需要减产是否有权限 if (check_control_mac_perms(msg.value, source_ctx)) &#123; //通过了权限检查之后，另外一个函数handle_control_message就会被调用，以便可以执行一个名称为“bootanim”的命令 handle_control_message((char*) msg.name + 4, (char*) msg.value); &#125; else &#123; ERROR("sys_prop: Unable to %s service ctl [%s] uid:%d gid:%d pid:%d\n", msg.name + 4, msg.value, cr.uid, cr.gid, cr.pid); &#125; &#125; else &#123; /* 不是ctl开头的属性，则首先检查其权限。例如，设置net.开头的属性需要AID_SYSTEM权限，log.开头的属性需要AID_SHELL属性等。 */ if (check_perms(msg.name, source_ctx)) &#123; // 最后通过property_set函数设置客户端需要设置的属性 property_set((char*) msg.name, (char*) msg.value); &#125; else &#123; ERROR("sys_prop: permission denied uid:%d name:%s\n", cr.uid, msg.name); &#125; // Note: bionic's property client code assumes that the // property server will not close the socket until *AFTER* // the property is written to memory. close(s); &#125; freecon(source_ctx); break; default: close(s); break; &#125;&#125; &#160; &#160; &#160; &#160;init进程是通过一个socket来接收系统属性变化事件的。每一个系统属性变化事件的内容都是通过一个prop_msg对象来描述的。在prop_msg对象对，成员变量name用来描述发生变化的系统属性的名称，而成员变量value用来描述发生变化的系统属性的值。系统属性分为两种类型，一种是普通类型的系统属性，另一种是控制类型的系统属性（属性名称以“ctl.”开头）。控制类型的系统属性在发生变化时，会触发init进程执行一个命令，而普通类型的系统属性就不具有这个特性。注意，改变系统属性是需要权限，因此，函数handle_property_set_fd在处理一个系统属性变化事件之前，首先会检查修改系统属性的进程是否具有相应的权限，这是通过调用函数check_control_perms或者check_perms来实现的。 &#160; &#160; &#160; &#160;从前面的调用过程可以知道，当前发生变化的系统属性的名称为“ctl.start”，它的值被设置为“bootanim”。由于这是一个控制类型的系统属性，因此，在通过了权限检查之后，另外一个函数handle_control_message就会被调用，以便可以执行一个名称为“bootanim”的命令。 &#160; &#160; &#160; &#160;函数handle_control_message实现在system/core/init/Init.c中，如下所示：123456789101112void handle_control_message(const char *msg, const char *arg)&#123; if (!strcmp(msg,"start")) &#123;//所以进入了这里 msg_start(arg);//arg是bootanimation &#125; else if (!strcmp(msg,"stop")) &#123; msg_stop(arg); &#125; else if (!strcmp(msg,"restart")) &#123; msg_restart(arg); &#125; else &#123; ERROR("unknown control msg '%s'\n", msg); &#125;&#125; &#160; &#160; &#160; &#160;控制类型的系统属性的名称是以”ctl.”开头，并且是以“start”或者“stop”结尾的，其中，“start”表示要启动某一个服务，而“stop”表示要停止某一个服务，它们是分别通过函数msg_start和msg_stop来实现的。由于当前发生变化的系统属性是以“start”来结尾的，因此，接下来就会调用函数msg_start来启动一个名称为“bootanim”的服务。 &#160; &#160; &#160; &#160;函数msg_start实现在文件system/core/init/Init.c中，如下所示：12345678910111213141516171819202122232425262728static void msg_start(const char *name)//bootanim&#123; struct service *svc = NULL; char *tmp = NULL; char *args = NULL; if (!strchr(name, ':'))//into //查找init.rc中配置的service列表 svc = service_find_by_name(name); else &#123; tmp = strdup(name); if (tmp) &#123; args = strchr(tmp, ':'); *args = '\0'; args++; svc = service_find_by_name(tmp); &#125; &#125; if (svc) &#123;//如果在列表中找到了bootanim，则启动它 service_start(svc, args); &#125; else &#123; ERROR("no such service '%s'\n", name); &#125; if (tmp) free(tmp);&#125; &#160; &#160; &#160; &#160;该函数首先调用service_find_by_name()，从service_list中查询要启动的服务是否有存在，若存在，返回服务的相关信息。因为init.rc中有bootanimation的定义，因此在init进程执行parse_config()时，会将该服务添加到service_list中，所以bootanimation应用是存在的。然后，如果找到了该服务，就调用service_start启动服务。 &#160; &#160; &#160; &#160;到此，bootanimation应用就启动了。 开机动画显示&#160; &#160; &#160; &#160;从前面的内容可以知道，名称等于“bootanim”的服务所对应的应用程序为/system/bin/bootanimation，这个应用程序实现在frameworks/base/cmds/bootanimation目录中，其中，应用程序入口函数main是实现在frameworks/base/cmds/bootanimation/Bootanimation_main.cpp中的，如下所示：12345678910111213141516171819202122232425int main(int argc, char** argv)&#123;#if defined(HAVE_PTHREADS) setpriority(PRIO_PROCESS, 0, ANDROID_PRIORITY_DISPLAY);#endif char value[PROPERTY_VALUE_MAX]; property_get("debug.sf.nobootanimation", value, "0"); int noBootAnimation = atoi(value); ALOGI_IF(noBootAnimation, "boot animation disabled"); if (!noBootAnimation) &#123;//检查系统属性“debug.sf.nobootnimaition”的值是否不等于0 //启动一个Binder线程池 sp&lt;ProcessState&gt; proc(ProcessState::self()); ProcessState::self()-&gt;startThreadPool(); // create the boot animation object //创建一个BootAnimation对象 sp&lt;BootAnimation&gt; boot = new BootAnimation(); IPCThreadState::self()-&gt;joinThreadPool(); &#125; return 0;&#125; &#160; &#160; &#160; &#160;这个函数首先检查系统属性“debug.sf.nobootnimaition”的值是否不等于0。如果不等于的话，那么接下来就会启动一个Binder线程池，并且创建一个BootAnimation对象。这个BootAnimation对象就是用来显示第三个开机画面的。由于BootAnimation对象在显示第三个开机画面的过程中，需要与SurfaceFlinger服务通信，因此，应用程序bootanimation就需要启动一个Binder线程池。 &#160; &#160; &#160; &#160;接着我们看看BootAnimation类的声明，位于frameworks/base/cmds/bootanimation/BootAnimation.h中：12345678910111213141516171819class BootAnimation : public Thread, public IBinder::DeathRecipient &#123; public: BootAnimation(); virtual ~BootAnimation(); ....... private: virtual bool threadLoop(); virtual status_t readyToRun(); virtual void onFirstRef(); virtual void binderDied(const wp&lt;IBinder&gt;&amp; who); status_t initTexture(Texture* texture, AssetManager&amp; asset, const char* name); status_t initTexture(const Animation::Frame&amp; frame); bool android(); bool readFile(const char* name, String8&amp; outString); bool movie(); ...... &#125;; &#160; &#160; &#160; &#160;BootAnimation类继承了Thread类和IBinder::DeathRecipient类，其中几个重要的函数说明如下： onFirstRef() —– 属于其父类RefBase，该函数在强引用sp新增引用计数時调用，就是当有sp包装的类初始化的时候调用； binderDied() —– 当对象死掉或者其他情况导致该Binder结束时，就会回调binderDied()方法; readyToRun() —– Thread执行前的初始化工作； threadLoop() —– 每个线程类都要实现的，在这里定义thread的执行内容。这个函数如果返回true，且没有requestExist()没有被调用，则该函数会再次执行；如果返回false，则threadloop中的内容仅仅执行一次，线程就会退出。&#160; &#160; &#160; &#160;其他主要函数的说明如下： android() —– 显示系统默认的开机画面； movie() —– 显示用户自定义的开机动画。 &#160; &#160; &#160; &#160;BootAnimation类成员函数的实现位于frameworks/base/cmds/bootanimation/BootAnimation.cpp。 &#160; &#160; &#160; &#160;BootAnimation类间接地继承了RefBase类，并且重写了RefBase类的成员函数onFirstRef，因此，当一个BootAnimation对象第一次被智能指针引用的时，这个BootAnimation对象的成员函数onFirstRef就会被调用：1234567void BootAnimation::onFirstRef() &#123; status_t err = mSession-&gt;linkToComposerDeath(this); ALOGE_IF(err, "linkToComposerDeath failed (%s) ", strerror(-err)); if (err == NO_ERROR) &#123; run("BootAnimation", PRIORITY_DISPLAY); &#125;&#125; &#160; &#160; &#160; &#160;mSession是BootAnimation类的一个成员变量，它的类型为SurfaceComposerClient，是用来和SurfaceFlinger执行Binder进程间通信的，它是在BootAnimation类的构造函数中创建的，如下所示：1234BootAnimation::BootAnimation() : Thread(false), mZip(NULL)&#123; mSession = new SurfaceComposerClient();&#125; &#160; &#160; &#160; &#160;SurfaceComposerClient类内部有一个实现了ISurfaceComposerClient接口的Binder代理对象mClient，这个Binder代理对象引用了SurfaceFlinger服务，SurfaceComposerClient类就是通过它来和SurfaceFlinger服务通信的。这个我们后面章节会详细分析。 &#160; &#160; &#160; &#160;回到BootAnimation类的成员函数onFirstRef中，由于BootAnimation类引用了SurfaceFlinger服务，因此，当SurfaceFlinger服务意外死亡时，BootAnimation类就需要得到通知，这是通过调用成员变量mSession的成员函数linkToComposerDeath来注册SurfaceFlinger服务的死亡接收通知来实现的。 &#160; &#160; &#160; &#160;BootAnimation类继承了Thread类，因此，当BootAnimation类的成员函数onFirstRef调用了父类Thread的成员函数run之后，系统就会创建一个线程，这个线程在第一次运行之前，会调用BootAnimation类的成员函数readyToRun来执行一些初始化工作，后面再调用BootAnimation类的成员函数htreadLoop来显示第三个开机画面。 &#160; &#160; &#160; &#160;BootAnimation类的成员函数readyToRun的实现如下所示：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879status_t BootAnimation::readyToRun() &#123; mAssets.addDefaultAssets(); //检查显示屏信息是否正确 sp&lt;IBinder&gt; dtoken(SurfaceComposerClient::getBuiltInDisplay( ISurfaceComposer::eDisplayIdMain)); DisplayInfo dinfo; status_t status = SurfaceComposerClient::getDisplayInfo(dtoken, &amp;dinfo); if (status) return -1; // create the native surface //调用SurfaceComposerClient对象mSession的成员函数createSurface可以获得一个SurfaceControl对象control sp&lt;SurfaceControl&gt; control = session()-&gt;createSurface(String8("BootAnimation"), dinfo.w, dinfo.h, PIXEL_FORMAT_RGB_565); SurfaceComposerClient::openGlobalTransaction(); control-&gt;setLayer(0x40000000); SurfaceComposerClient::closeGlobalTransaction(); //调用SurfaceControl对象control的成员函数getSurface会返回一个Surface对象s sp&lt;Surface&gt; s = control-&gt;getSurface(); // initialize opengl and egl //初始化OPENEGL和EGL const EGLint attribs[] = &#123; EGL_RED_SIZE, 8, EGL_GREEN_SIZE, 8, EGL_BLUE_SIZE, 8, EGL_DEPTH_SIZE, 0, EGL_NONE &#125;; EGLint w, h, dummy; EGLint numConfigs; EGLConfig config; EGLSurface surface; EGLContext context; EGLDisplay display = eglGetDisplay(EGL_DEFAULT_DISPLAY); eglInitialize(display, 0, 0); eglChooseConfig(display, attribs, &amp;config, 1, &amp;numConfigs); surface = eglCreateWindowSurface(display, config, s.get(), NULL); context = eglCreateContext(display, config, NULL, NULL); eglQuerySurface(display, surface, EGL_WIDTH, &amp;w); eglQuerySurface(display, surface, EGL_HEIGHT, &amp;h); if (eglMakeCurrent(display, surface, surface, context) == EGL_FALSE) return NO_INIT; mDisplay = display; mContext = context; mSurface = surface; mWidth = w; mHeight = h; mFlingerSurfaceControl = control; mFlingerSurface = s; // If the device has encryption turned on or is in process // of being encrypted we show the encrypted boot animation. //如果设备加密功能开启了，就要显示加密的开机动画 char decrypt[PROPERTY_VALUE_MAX]; property_get("vold.decrypt", decrypt, ""); bool encryptedAnimation = atoi(decrypt) != 0 || !strcmp("trigger_restart_min_framework", decrypt); //选取动画文件 ZipFileRO* zipFile = NULL; if ((encryptedAnimation &amp;&amp; (access(SYSTEM_ENCRYPTED_BOOTANIMATION_FILE, R_OK) == 0) &amp;&amp; ((zipFile = ZipFileRO::open(SYSTEM_ENCRYPTED_BOOTANIMATION_FILE)) != NULL)) || ((access(OEM_BOOTANIMATION_FILE, R_OK) == 0) &amp;&amp; ((zipFile = ZipFileRO::open(OEM_BOOTANIMATION_FILE)) != NULL)) || ((access(SYSTEM_BOOTANIMATION_FILE, R_OK) == 0) &amp;&amp; ((zipFile = ZipFileRO::open(SYSTEM_BOOTANIMATION_FILE)) != NULL))) &#123; mZip = zipFile; &#125; return NO_ERROR;&#125; &#160; &#160; &#160; &#160;readyToRun主要做了如下工作： &#160; &#160; &#160; &#160;1）BootAnimation类的成员函数session用来返回BootAnimation类的成员变量mSession所描述的一个SurfaceComposerClient对象。通过调用SurfaceComposerClient对象mSession的成员函数createSurface可以获得一个SurfaceControl对象control。 &#160; &#160; &#160; &#160;SurfaceComposerClient类的成员函数createSurface首先调用内部的Binder代理对象mClient(frameworks/native/services/surfaceflinger/Client.cpp)来请求SurfaceFlinger返回一个类型为(class Handle : public BBinder, public LayerCleaner)Binder代理对象（封装了SurfaceFlinger的sp指针和Layer对象）handle，和一个IGraphicBufferProducer的sp指针（封装了SurfaceFlinger的sp指针）gbp，接着再使用这两个对象来创建一个SurfaceControl对象。创建出来的SurfaceControl对象的成员变量handle就指向了从SurfaceFlinger返回来的类型为Handle 的Binder代理对象。有了这个Binder代理对象之后，SurfaceControl对象就可以和SurfaceFlinger服务通信了。(下一章节会分析) &#160; &#160; &#160; &#160;调用SurfaceControl对象control的成员函数getSurface会返回一个Surface对象s。这个Surface对象s内部也有一个类型为IGraphicBufferProducer的sp指针mGraphicBufferProducer，这个sp指针与前面所创建的SurfaceControl对象control的内部的sp指针（封装了SurfaceFlinger的sp指针）gbp引用的是同一个对象。这样，Surface对象s也可以通过其内部的sp指针mGraphicBufferProducer来和SurfaceFlinger服务通信。（下一章节会分析） &#160; &#160; &#160; &#160;Surface类继承了ANativeWindow类。ANativeWindow类是连接OpenGL和Android窗口系统的桥梁，即OpenGL需要通过ANativeWindow类来间接地操作Android窗口系统。这种桥梁关系是通过EGL库来建立的，所有以egl为前缀的函数名均为EGL库提供的接口。 &#160; &#160; &#160; &#160;2）为了能够在OpenGL和Android窗口系统之间的建立一个桥梁，我们需要一个EGLDisplay对象display，一个EGLConfig对象config，一个EGLSurface对象surface，以及一个EGLContext对象context，其中，EGLDisplay对象display用来描述一个EGL显示屏，EGLConfig对象config用来描述一个EGL帧缓冲区配置参数，EGLSurface对象surface用来描述一个EGL绘图表面，EGLContext对象context用来描述一个EGL绘图上下文（状态），它们是分别通过调用egl库函数eglGetDisplay、eglChooseConfig、eglCreateWindowSurface和eglCreateContext来获得的。注意，EGLConfig对象config、EGLSurface对象surface和EGLContext对象context都是用来描述EGLDisplay对象display的。有了这些对象之后，就可以调用函数eglMakeCurrent来设置当前EGL库所使用的绘图表面以及绘图上下文。 &#160; &#160; &#160; &#160;还有另外一个地方需要注意的是，每一个EGLSurface对象surface有一个关联的ANativeWindow对象。这个ANativeWindow对象是通过函数eglCreateWindowSurface的第三个参数来指定的。在我们这个场景中，这个ANativeWindow对象正好对应于前面所创建的 Surface对象s。每当OpenGL需要绘图的时候，它就会找到前面所设置的绘图表面，即EGLSurface对象surface。有了EGLSurface对象surface之后，就可以找到与它关联的ANativeWindow对象，即Surface对象s。有了Surface对象s之后，就可以通过其内部的sp指针mGraphicBufferProducer来请求SurfaceFlinger服务返回帧缓冲区硬件设备的一个图形访问接口。这样，OpenGL最终就可以将要绘制的图形渲染到帧缓冲区硬件设备中去，即显示在实际屏幕上。屏幕的大小，即宽度和高度，可以通过函数eglQuerySurface来获得。 &#160; &#160; &#160; &#160;3）动画文件的读取是按顺序进行的，如果读取成功，则不再读取后续的文件，如果失败，则读取下一个文件。顺序如下： 如果设备的加密功能已经开启，或者设备正在进行加密，则读取加密开机动画文件，路径为： 1#define SYSTEM_ENCRYPTED_BOOTANIMATION_FILE "/system/media/bootanimation-encrypted.zip" OEM厂商指定的开机动画，路径为： 1#define OEM_BOOTANIMATION_FILE "/oem/media/bootanimation.zip" 系统开机动画，路径为： 1#define SYSTEM_BOOTANIMATION_FILE "/system/media/bootanimation.zip" &#160; &#160; &#160; &#160;这一步执行完成之后，用来显示第三个开机画面的线程的初始化工作就执行完成了，接下来，就会执行这个线程的主体函数，即BootAnimation类的成员函数threadLoop。&#160; &#160; &#160; &#160;BootAnimation类的成员函数threadLoop的实现如下所示：1234567891011121314151617181920bool BootAnimation::threadLoop()&#123; bool r; // We have no bootanimation file, so we use the stock android logo // animation. if (mZip == NULL) &#123; r = android(); &#125; else &#123; r = movie(); &#125; eglMakeCurrent(mDisplay, EGL_NO_SURFACE, EGL_NO_SURFACE, EGL_NO_CONTEXT); eglDestroyContext(mDisplay, mContext); eglDestroySurface(mDisplay, mSurface); mFlingerSurface.clear(); mFlingerSurfaceControl.clear(); eglTerminate(mDisplay); IPCThreadState::self()-&gt;stopProcess(); return r;&#125; &#160; &#160; &#160; &#160;这个函数流程比较简单，首先判断自定义的开机动画文件mZip是否存在，如果存在就调用movie()完成自定义开机画面的显示；如果不存在，调用android()完成系统默认开机画面的显示。然后进行开机动画显示后的销毁、释放工作，主要就是readyToRun中初始化的一些EGL对象。最后终止线程，并return。注意，movie()和android()的返回值都是false，因此线程结束也会返回false。threadLoop()函数如果返回值为false，则该函数中的内容只会执行一次；如果返回true，则会不停的执行。这里返回false，因此只会执行一次。 &#160; &#160; &#160; &#160;接下来，我们就分别分析BootAnimation类的成员函数android和movie的实现。 系统默认开机动画android()&#160; &#160; &#160; &#160;BootAnimation类的成员函数android的实现如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071bool BootAnimation::android()&#123; //读取开机动画默认图片，根据图片创建两个纹理对象 //"android"字样图片 initTexture(&amp;mAndroid[0], mAssets, "images/android-logo-mask.png"); //闪光图片 initTexture(&amp;mAndroid[1], mAssets, "images/android-logo-shine.png"); // clear screen //清理屏幕 glShadeModel(GL_FLAT); glDisable(GL_DITHER); glDisable(GL_SCISSOR_TEST); glClearColor(0,0,0,1); glClear(GL_COLOR_BUFFER_BIT); eglSwapBuffers(mDisplay, mSurface); glEnable(GL_TEXTURE_2D); glTexEnvx(GL_TEXTURE_ENV, GL_TEXTURE_ENV_MODE, GL_REPLACE); //图片在屏幕中现实位置 const GLint xc = (mWidth - mAndroid[0].w) / 2; const GLint yc = (mHeight - mAndroid[0].h) / 2; const Rect updateRect(xc, yc, xc + mAndroid[0].w, yc + mAndroid[0].h); glScissor(updateRect.left, mHeight - updateRect.bottom, updateRect.width(), updateRect.height()); // Blend state glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); glTexEnvx(GL_TEXTURE_ENV, GL_TEXTURE_ENV_MODE, GL_REPLACE); const nsecs_t startTime = systemTime(); do &#123; //计算每次偏移时间，然后计算出偏移位置 nsecs_t now = systemTime(); double time = now - startTime; float t = 4.0f * float(time / us2ns(16667)) / mAndroid[1].w; GLint offset = (1 - (t - floorf(t))) * mAndroid[1].w; GLint x = xc - offset; glDisable(GL_SCISSOR_TEST); glClear(GL_COLOR_BUFFER_BIT); glEnable(GL_SCISSOR_TEST); glDisable(GL_BLEND); //绘制闪光图片，这个会根据上面计算的位置来显示 glBindTexture(GL_TEXTURE_2D, mAndroid[1].name); glDrawTexiOES(x, yc, 0, mAndroid[1].w, mAndroid[1].h); glDrawTexiOES(x + mAndroid[1].w, yc, 0, mAndroid[1].w, mAndroid[1].h); glEnable(GL_BLEND); //绘制Android字样的图片，这个不动 glBindTexture(GL_TEXTURE_2D, mAndroid[0].name); glDrawTexiOES(xc, yc, 0, mAndroid[0].w, mAndroid[0].h); //交换缓冲区，以显示到屏幕 EGLBoolean res = eglSwapBuffers(mDisplay, mSurface); if (res == EGL_FALSE) break; // 12fps: don't animate too fast to preserve CPU const nsecs_t sleepTime = 83333 - ns2us(systemTime() - now); if (sleepTime &gt; 0) usleep(sleepTime); //最后执行checkExit函数，判断是否退出了 checkExit(); &#125; while (!exitPending()); glDeleteTextures(1, &amp;mAndroid[0].name); glDeleteTextures(1, &amp;mAndroid[1].name); return false;&#125; &#160; &#160; &#160; &#160;一共有以下几个步骤：&#160; &#160; &#160; &#160;1） Android系统默认的开机动画是由两张图片android-logo-mask.png和android-logo-shine.png中。这两张图片保存在frameworks/base/core/res/assets/images目录中，它们最终会被编译在framework-res模块（frameworks/base/core/res）中，即编译在framework-res.apk文件中。编译在framework-res模块中的资源文件可以通过AssetManager类来访问。 &#160; &#160; &#160; &#160;BootAnimation类的成员函数android首先调用另外一个成员函数initTexture来将根据图片android-logo-mask.png和android-logo-shine.png的内容来分别创建两个纹理对象，这两个纹理对象就分别保存在BootAnimation类的成员变量mAndroid所描述的一个数组中。通过混合渲染这两个纹理对象，我们就可以得到一个开机动画，这是通过中间的while循环语句来实现的。 &#160; &#160; &#160; &#160;2）图片android-logo-mask.png用作动画前景，它是一个镂空的“ANDROID”图像。图片android-logo-shine.png用作动画背景，它的中间包含有一个高亮的呈45度角的条纹。在每一次循环中，图片android-logo-shine.png被划分成左右两部分内容来显示。左右两个部分的图像宽度随着时间的推移而此消彼长，这样就可以使得图片android-logo-shine.png中间高亮的条纹好像在移动一样。另一方面，在每一次循环中，图片android-logo-mask.png都作为一个整体来渲染，而且它的位置是恒定不变的。由于它是一个镂空的“ANDROID”图像，因此，我们就可以通过它的镂空来看到它背后的图片android-logo-shine.png的条纹一闪一闪地划过。这个while循环语句会一直被执行，直到应用程序/system/bin/bootanimation被结束为止。 &#160; &#160; &#160; &#160;3）在循环语句最后会执行checkExit()函数：123456789101112void BootAnimation::checkExit() &#123; // Allow surface flinger to gracefully request shutdown char value[PROPERTY_VALUE_MAX]; property_get(EXIT_PROP_NAME, value, "0"); int exitnow = atoi(value); if (exitnow) &#123; requestExit(); if (mAudioPlayer != NULL) &#123; mAudioPlayer-&gt;requestExit(); &#125; &#125;&#125; &#160; &#160; &#160; &#160;首先调用property_get获取属性EXIT_PROP_NAME的值:1#define EXIT_PROP_NAME "service.bootanim.exit" &#160; &#160; &#160; &#160;然后判断该值，如果为1，则调用requestExit（）要求退出当前线程，该函数是异步的。位于system/core/libutils/Thread.cpp中：123456void Thread::requestExit()&#123; Mutex::Autolock _l(mLock); //这里将mExitPending 赋值为true mExitPending = true;&#125; &#160; &#160; &#160; &#160;回到android()代码：1&#125; while (!exitPending()); &#160; &#160; &#160; &#160;调用exitPending()，改函数判断requestExit（）是否被调用过，如果调用过则返回true，否则为false。依然位于system/core/libutils/Thread.cpp中：123456bool Thread::exitPending() const&#123; Mutex::Autolock _l(mLock); //上面我们在requestExit赋过值了 return mExitPending;&#125; &#160; &#160; &#160; &#160;这样，当属性“service.bootanim.exit”值被设为”1”时，android()就会调用requestExit()，exitPending()返回值为true。于是do…while()循环就会退出，开机动画绘制就会结束。&#160; &#160; &#160; &#160;至于什么时候是哪个服务将属性“service.bootanim.exit”的值设置为1的，我们后面讲开机动画的停止的时候会提到。 自定义开机动画movie()&#160; &#160; &#160; &#160;BootAnimation类的成员函数movie的实现比较长，我们分段来阅读：&#160; &#160; &#160; &#160;Part.1：12345678910111213141516171819202122bool BootAnimation::movie()&#123;String8 desString; //读取desc.txt文件内容 if (!readFile("desc.txt", desString)) &#123; return false; &#125; char const* s = desString.string(); // Create and initialize an AudioPlayer if we have an audio_conf.txt file //如果存在audio_conf.txt文件，则会创建一个AudioPlayer，并根据读取的字符串初始化 //ignore it String8 audioConf; if (readFile("audio_conf.txt", audioConf)) &#123; mAudioPlayer = new AudioPlayer; if (!mAudioPlayer-&gt;init(audioConf.string())) &#123; ALOGE("mAudioPlayer.init failed"); mAudioPlayer = NULL; &#125; &#125; ......&#125; &#160; &#160; &#160; &#160;从前面BootAnimation类的成员函数readyToRun的实现可以知道，如果目标设备上存在压缩文件/system/media/bootanimation.zip(另外两种我们忽略)，那么BootAnimation类的成员变量mZip就会指向它，这段代码作用是读取开机动画文件mZip中的描述文件“desc.txt”。每个动画文件压缩包中必须要包含一个desc.txt，该文件用来描述开机动画如何显示。下面以一个示例来分析一下该文件：12345480 640 20 p 1 0 folder1 p 2 20 folder2 c 0 0 folder3 c 1 0 folder4 &#160; &#160; &#160; &#160;Part.2：&#160; &#160; &#160; &#160;第1行用来描述开机动画在屏幕显示的大小及速度。具体为：开机动画的宽度为480个像素，高度为640个像素，显示频率为每秒20帧，即每帧显示1/20秒。 &#160; &#160; &#160; &#160;下面的每一行代表一个片段，显示的时候会按照顺序从上到下依次显示。第1个字符为片段类型，有’c’和’p’两种，两者的区别后面会结合代码说明。 &#160; &#160; &#160; &#160;第2个数字为该片段重复显示的次数，如果为‘0’，表示会无限重复显示；第3个数字为两次显示之间的间隔，单位为第一行中定义的每帧显示的时间；第4个字符串为该片段所在的文件夹，一个片段可以由多个png图片组成，都存放在folder文件夹中。 &#160; &#160; &#160; &#160;“p 1 0 folder1”代表该片段显示1次，与下一个片段间隔0s，该片段的显示图片路径为bootanimation.zip/folder1。&#160; &#160; &#160; &#160;“p 2 20 folder2”代表该片段显示2次，且两次之间显示的间隔为20(1/20)=1s，与下一个片段间隔20(1/20)=1s，该片段的显示图片路径为bootanimation.zip/folder2。&#160; &#160; &#160; &#160;“c 0 0 folder3”代表该片段无限循环显示，且两次显示的间隔为0s，与下一个片段间隔0s，该片段的显示图路径为bootanimation.zip/folder3。&#160; &#160; &#160; &#160;“c 1 10 folder4”代表该片段显示1次，显示后暂停10*(1/20)=0.5s，该片段的显示图路径为bootanimation.zip/folder4。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546bool BootAnimation::movie()&#123; ...Part.1... Animation animation; // Parse the description file for (;;) &#123; const char* endl = strstr(s, "\n");//检测首次出现换行符的地址 if (!endl) break; //每次读取一行 String8 line(s, endl - s); const char* l = line.string(); int fps, width, height, count, pause; char path[ANIM_ENTRY_NAME_MAX]; char color[7] = "000000"; // default to black if unspecified char pathType; if (sscanf(l, "%d %d %d", &amp;width, &amp;height, &amp;fps) == 3) &#123;//每行如果有三个字符串，则依次是宽、高、帧率 // ALOGD("&gt; w=%d, h=%d, fps=%d", width, height, fps); animation.width = width; animation.height = height; animation.fps = fps; &#125; //如果是大于等于四个字符串，则依次是显示类型、显示次数、与下一次间隔时间、显示颜色 else if (sscanf(l, " %c %d %d %s #%6s", &amp;pathType, &amp;count, &amp;pause, path, color) &gt;= 4) &#123; // ALOGD("&gt; type=%c, count=%d, pause=%d, path=%s, color=%s", pathType, count, pause, path, color); Animation::Part part; part.playUntilComplete = pathType == 'c'; part.count = count; part.pause = pause; part.path = path; part.audioFile = NULL; if (!parseColor(color, part.backgroundColor)) &#123; ALOGE("&gt; invalid color '#%s'", color); part.backgroundColor[0] = 0.0f; part.backgroundColor[1] = 0.0f; part.backgroundColor[2] = 0.0f; &#125; animation.parts.add(part); &#125; s = ++endl; &#125; ......&#125; &#160; &#160; &#160; &#160;上面的for循环语句分析完成desc.txt文件的内容后，就得到了开机动画的显示大小、速度以及片断信息。这些信息都保存在Animation对象animation中，其中，每一个动画片断都使用一个Animation::Part对象来描述，并且保存在Animation对象animation的成员变量parts所描述的一个片断列表中。 &#160; &#160; &#160; &#160;Part.3：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657bool BootAnimation::movie()&#123; ...Part.1... ...Part.2... // read all the data structures //开始读取zip文件 const size_t pcount = animation.parts.size(); void *cookie = NULL; if (!mZip-&gt;startIteration(&amp;cookie)) &#123; return false; &#125; ZipEntryRO entry; char name[ANIM_ENTRY_NAME_MAX]; //开始循环遍历每一个文件 while ((entry = mZip-&gt;nextEntry(cookie)) != NULL) &#123; const int foundEntryName = mZip-&gt;getEntryFileName(entry, name, ANIM_ENTRY_NAME_MAX); if (foundEntryName &gt; ANIM_ENTRY_NAME_MAX || foundEntryName == -1) &#123; ALOGE("Error fetching entry file name"); continue; &#125; const String8 entryName(name); const String8 path(entryName.getPathDir()); const String8 leaf(entryName.getPathLeaf()); if (leaf.size() &gt; 0) &#123; for (size_t j=0 ; j&lt;pcount ; j++) &#123; if (path == animation.parts[j].path) &#123; int method; // supports only stored png files if (mZip-&gt;getEntryInfo(entry, &amp;method, NULL, NULL, NULL, NULL, NULL)) &#123; if (method == ZipFileRO::kCompressStored) &#123; FileMap* map = mZip-&gt;createEntryFileMap(entry); if (map) &#123; Animation::Part&amp; part(animation.parts.editItemAt(j)); if (leaf == "audio.wav") &#123; // a part may have at most one audio file part.audioFile = map; &#125; else &#123; Animation::Frame frame; frame.name = leaf; frame.map = map; part.frames.add(frame); &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; mZip-&gt;endIteration(cookie); ......&#125; &#160; &#160; &#160; &#160;接下来，BootAnimation类的成员函数movie再断续将每一个片断所对应的png图片读取出来。每一个png图片都表示一个动画帧，使用一个Animation::Frame对象来描述，并且保存在对应的Animation::Part对象的成员变量frames所描述的一个帧列表中。 &#160; &#160; &#160; &#160;Part.4：12345678910111213141516171819202122232425262728293031323334bool BootAnimation::movie()&#123; ...Part.1... ...Part.2... ...Part.3... // clear screen glShadeModel(GL_FLAT); glDisable(GL_DITHER); glDisable(GL_SCISSOR_TEST); glDisable(GL_BLEND); glClearColor(0,0,0,1); glClear(GL_COLOR_BUFFER_BIT); eglSwapBuffers(mDisplay, mSurface); glBindTexture(GL_TEXTURE_2D, 0); glEnable(GL_TEXTURE_2D); glTexEnvx(GL_TEXTURE_ENV, GL_TEXTURE_ENV_MODE, GL_REPLACE); glTexParameterx(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); glTexParameterx(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT); glTexParameterx(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameterx(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); const int xc = (mWidth - animation.width) / 2; const int yc = ((mHeight - animation.height) / 2); nsecs_t lastFrame = systemTime(); nsecs_t frameDuration = s2ns(1) / animation.fps; Region clearReg(Rect(mWidth, mHeight)); clearReg.subtractSelf(Rect(xc, yc, xc+animation.width, yc+animation.height)); ......&#125; &#160; &#160; &#160; &#160;前面的一系列gl函数首先用来清理屏幕，接下来的一系列gl函数用来设置OpenGL的纹理显示方式。&#160; &#160; &#160; &#160;变量xc和yc的值用来描述开机动画的显示位置，即需要在屏幕中间显示开机动画，另外一个变量frameDuration的值用来描述每一帧的显示时间，它是以纳秒为单位的。&#160; &#160; &#160; &#160;Region对象clearReg用来描述屏幕中除了开机动画之外的其它区域，它是用整个屏幕区域减去开机动画所点据的区域来得到的。 &#160; &#160; &#160; &#160;Part.5： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105bool BootAnimation::movie()&#123; ...Part.1... ...Part.2... ...Part.3... ...Part.4... //第一层for循环用来显示每一个动画片断 for (size_t i=0 ; i&lt;pcount ; i++) &#123; const Animation::Part&amp; part(animation.parts[i]); const size_t fcount = part.frames.size(); glBindTexture(GL_TEXTURE_2D, 0); //第二层的for循环用来循环显示每一个动画片断 for (int r=0 ; !part.count || r&lt;part.count ; r++) &#123; // Exit any non playuntil complete parts immediately if(exitPending() &amp;&amp; !part.playUntilComplete) break; // only play audio file the first time we animate the part if (r == 0 &amp;&amp; mAudioPlayer != NULL &amp;&amp; part.audioFile) &#123; mAudioPlayer-&gt;playFile(part.audioFile); &#125; glClearColor( part.backgroundColor[0], part.backgroundColor[1], part.backgroundColor[2], 1.0f); //第三层的for循环用来显示每一个动画片断所对应的png图片 //可以看到，如果exitPending()返回值为true且part.playUntilComplete=false，则会break。 //即：当SurfaceFlinger服务要求bootanimation停止显示动画时，以‘p’标识的片段会停止， //而以'c'标识的片段会继续显示。这就是两者之间的主要区别。 for (size_t j=0 ; j&lt;fcount &amp;&amp; (!exitPending() || part.playUntilComplete) ; j++) &#123; const Animation::Frame&amp; frame(part.frames[j]); nsecs_t lastFrame = systemTime(); if (r &gt; 0) &#123; glBindTexture(GL_TEXTURE_2D, frame.tid); &#125; else &#123;//如果一个动画片断的循环显示次数不等于1，那么就说明这个动画片断中的png图片需要重复地显示在屏幕中 //第一次显示一个png图片的时候，会调用函数glGenTextures来为这个png图片创建一个纹理对象, //并且将这个纹理对象的名称保存在对应的Animation::Frame对象的成员变量tid中 if (part.count != 1) &#123; glGenTextures(1, &amp;frame.tid); glBindTexture(GL_TEXTURE_2D, frame.tid); glTexParameterx(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameterx(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); &#125; initTexture(frame); &#125; // 如果Region对象clearReg所包含的区域不为空,首先要将它所包含的区域裁剪掉，避免开机动画可以显示在指定的位置以及大小中 if (!clearReg.isEmpty()) &#123; Region::const_iterator head(clearReg.begin()); Region::const_iterator tail(clearReg.end()); glEnable(GL_SCISSOR_TEST); while (head != tail) &#123; const Rect&amp; r(*head++); glScissor(r.left, mHeight - r.bottom, r.width(), r.height()); glClear(GL_COLOR_BUFFER_BIT); &#125; glDisable(GL_SCISSOR_TEST); &#125; glDrawTexiOES(xc, yc, 0, animation.width, animation.height); eglSwapBuffers(mDisplay, mSurface); nsecs_t now = systemTime(); nsecs_t delay = frameDuration - (now - lastFrame); //ALOGD("%lld, %lld", ns2ms(now - lastFrame), ns2ms(delay)); lastFrame = now; //调用函数usleep函数来让线程睡眠一下，以保证每一个png图片，即每一帧动画都按照预先指定好的速度来显示 if (delay &gt; 0) &#123; struct timespec spec; spec.tv_sec = (now + delay) / 1000000000; spec.tv_nsec = (now + delay) % 1000000000; int err; do &#123; err = clock_nanosleep(CLOCK_MONOTONIC, TIMER_ABSTIME, &amp;spec, NULL); &#125; while (err&lt;0 &amp;&amp; errno == EINTR); &#125; checkExit(); &#125; //每当循环显示完成一个片断时，需要调用usleep函数来使得线程睡眠part.pause * ns2us(frameDuration)毫秒，以便可以按照预先设定的节奏来显示开机动画。 usleep(part.pause * ns2us(frameDuration)); // For infinite parts, we've now played them at least once, so perhaps exit //如果无限循环，则检查是否有退出消息来了，上面android()函数分析过了 if(exitPending() &amp;&amp; !part.count) break; &#125; // free the textures for this part //最后一个if语句判断一个动画片断是否是循环显示的，即循环次数不等于1。 //如果是的话，那么就说明前面为它所对应的每一个png图片都创建过一个纹理对象。 //现在既然这个片断的显示过程已经结束了，因此，就需要释放前面为它所创建的纹理对象。 if (part.count != 1) &#123; for (size_t j=0 ; j&lt;fcount ; j++) &#123; const Animation::Frame&amp; frame(part.frames[j]); glDeleteTextures(1, &amp;frame.tid); &#125; &#125; &#125; return false;&#125; &#160; &#160; &#160; &#160;1）第一层for循环用来显示每一个动画片断，第二层的for循环用来循环显示每一个动画片断，第三层的for循环用来显示每一个动画片断所对应的png图片。这些png图片以纹理的方式来显示在屏幕中。 &#160; &#160; &#160; &#160;2）注意，如果一个动画片断的循环显示次数不等于1，那么就说明这个动画片断中的png图片需要重复地显示在屏幕中。由于每一个png图片都需要转换为一个纹理对象之后才能显示在屏幕中，因此，为了避免重复地为同一个png图片创建纹理对象，第三层的for循环在第一次显示一个png图片的时候，会调用函数glGenTextures来为这个png图片创建一个纹理对象，并且将这个纹理对象的名称保存在对应的Animation::Frame对象的成员变量tid中，这样，下次再显示相同的图片时，就可以使用前面已经创建好了的纹理对象，即调用函数glBindTexture来指定当前要操作的纹理对象。 &#160; &#160; &#160; &#160;3）如果Region对象clearReg所包含的区域不为空，那么在调用函数glDrawTexiOES和eglSwapBuffers来显示每一个png图片之前，首先要将它所包含的区域裁剪掉，避免开机动画可以显示在指定的位置以及大小中。 &#160; &#160; &#160; &#160;4） 每当显示完成一个png图片之后，都要将变量frameDuration的值从纳秒转换为毫秒。如果转换后的值大小于，那么就需要调用函数usleep函数来让线程睡眠一下，以保证每一个png图片，即每一帧动画都按照预先指定好的速度来显示。注意，函数usleep指定的睡眠时间只能精确到毫秒，因此，如果预先指定的帧显示时间小于1毫秒，那么BootAnimation类的成员函数movie是无法精确地控制地每一帧的显示时间的。 &#160; &#160; &#160; &#160;5）还有另外一个地方需要注意的是，每当循环显示完成一个片断时，需要调用usleep函数来使得线程睡眠part.pause * ns2us(frameDuration)毫秒，以便可以按照预先设定的节奏来显示开机动画。 &#160; &#160; &#160; &#160;6）最后一个if语句判断一个动画片断是否是循环显示的，即循环次数不等于1。如果是的话，那么就说明前面为它所对应的每一个png图片都创建过一个纹理对象。现在既然这个片断的显示过程已经结束了，因此，就需要释放前面为它所创建的纹理对象。 附注：&#160; &#160; &#160; &#160;可以看到，如果exitPending()返回值为true且part.playUntilComplete=false，则会break。即：当SurfaceFlinger服务要求bootanimation停止显示动画时，以‘p’标识的片段会停止，而以’c’标识的片段会继续显示。这就是两者之间的主要区别。 我猜想”c”标识的意思是continue，即：即使SurfaceFlinger要求bootanimation停止动画，bootanimation也不会立刻停止动画，它会等c标识片段都显示完毕后，再停止。 &#160; &#160; &#160; &#160;至此，第三个开机画面的显示过程就分析完成了。 开机动画的停止&#160; &#160; &#160; &#160;接下来，我们再继续分析第三个开机画面是如何停止显示的。 &#160; &#160; &#160; &#160;当System进程将系统中的关键服务启动起来之后，就会将Launcher启动起来。Android应用程序的启动过程实际上就是它的根Activity组件的启动过程。对于应用程序Launcher来说，它的根Activity组件即为Launcher组件。 &#160; &#160; &#160; &#160;一个Activity组件在启动起来之后，就会被记录起来，等到它所运行在的主线程空闲的时候，这个主线程就会向ActivityManagerService发送一个Activity组件空闲的通知。由于应用程序Launcher是系统中第一个被启动的应用程序，即它的根Activity组件是系统中第一个被启动的Activity组件，因此，当ActivityManagerService接收到它的空闲通知的时候，就可以知道系统是刚刚启动起来的。在这种情况下，ActivityManagerService就会停止显示开机动画，以便可以在屏幕中显示应用程序Lancher的界面。 &#160; &#160; &#160; &#160;如果一个线程想要在空闲的时候处理一些事务，那么就必须要向这个线程的消息队列注册一个空闲消息处理器。自定义的空闲消息处理器灯必须要从MessageQueue.IdleHandler类继承下来，并且重写成员函数queueIdle。当一个线程空闲的时候，即消息队列中没有新的消息需要处理的时候，那些注册了的空闲消息处理器的成员函数queueIdle就会被调用。 &#160; &#160; &#160; &#160;应用程序的主线程是通过ActivityThread类来描述的，它实现在文件frameworks/base/core/Java/android/app/ActivityThread.java中。每当有一个新的Activity组件启动起来的时候，ActivityThread类都会向它所描述的应用程序主线程的消息队列注册一个类型为Idler的空闲消息处理器。这样一个应用程序的主线程就可以在空闲的时候，向ActivityManagerService发送一个Activity组件空闲通知，相当于是通知ActivityManagerService，一个新的Activity组件已经准备就绪了。 &#160; &#160; &#160; &#160;Idler类定义在frameworks/base/core/java/android/app/ActivityThread.java中， 它的成员函数queueIdle的实现如下所示：12345678910111213141516171819202122232425262728private class Idler implements MessageQueue.IdleHandler &#123; @Override public final boolean queueIdle() &#123; ActivityClientRecord a = mNewActivities; ...... if (a != null) &#123; mNewActivities = null; IActivityManager am = ActivityManagerNative.getDefault(); ActivityClientRecord prev; do &#123; ..... if (a.activity != null &amp;&amp; !a.activity.mFinished) &#123; try &#123; am.activityIdle(a.token, a.createdConfig, stopProfiling); a.createdConfig = null; &#125; catch (RemoteException ex) &#123; // Ignore &#125; &#125; prev = a; a = a.nextIdle; prev.nextIdle = null; &#125; while (a != null); &#125; ...... return false; &#125;&#125; &#160; &#160; &#160; &#160;ActivityThread类有一个类型为ActivityClientRecord的成员变量mNewActivities，用来描述所有在当前应用程序主线程中新启动起来的Activity组件。这些新启动起来的Activity组件通过ActivityClientRecord类的成员变量nextIdle连接在一起。一旦当前应用程序主线程向ActivityManagerService发送了这些新启动的Activity组件的空闲通知之后，这些新启动起来的Activity组件就不会再被保存在ActivityThread类的成员变量mNewActivities中了，即每一个新启动的Activity组件只有一次机会向ActivityManagerService发送一个空闲通知。 &#160; &#160; &#160; &#160;向ActivityManagerService发送一个Activity组件空闲通知是通过调用ActivityManagerService代理对象的成员函数activityIdle来实现的，而ActivityManagerService代理对象可以通过调用ActivityManagerNative类的静态成员函数getDefault来获得。 &#160; &#160; &#160; &#160;ActivityManagerService代理对象的类型为ActivityManagerProxy，它的成员函数activityIdle实现在文件frameworks/base/core/java/android/app/ActivityManagerNative.java中，如下所示：123456789101112131415161718192021222324class ActivityManagerProxy implements IActivityManager &#123; ...... public void activityIdle(IBinder token, Configuration config) throws RemoteException &#123; Parcel data = Parcel.obtain(); Parcel reply = Parcel.obtain(); data.writeInterfaceToken(IActivityManager.descriptor); data.writeStrongBinder(token); if (config != null) &#123; data.writeInt(1); config.writeToParcel(data, 0); &#125; else &#123; data.writeInt(0); &#125; mRemote.transact(ACTIVITY_IDLE_TRANSACTION, data, reply, IBinder.FLAG_ONEWAY); reply.readException(); data.recycle(); reply.recycle(); &#125; ...... &#125; &#160; &#160; &#160; &#160;ActivityManagerProxy类的成员函数activityIdle实际上是向ActivityManagerService发送一个类型为ACTIVITY_IDLE_TRANSACTION的Binder进程间通信请求，其中，参数token用来描述与这个进程间通信请求所关联的一个Activity组件，在我们这个场景中，这个Activity组件即为应用程序Launcher的根Activity组件Launcher。 &#160; &#160; &#160; &#160;类型为ACTIVITY_IDLE_TRANSACTION的Binder进程间通信请求是由ActivityManagerService类的成员函数activityIdle来处理的，如下所示：1234567891011121314151617181920212223242526272829public final class ActivityManagerService extends ActivityManagerNative implements Watchdog.Monitor, BatteryStatsImpl.BatteryCallback &#123; ...... @Override public final void activityIdle(IBinder token, Configuration config, boolean stopProfiling) &#123; final long origId = Binder.clearCallingIdentity(); synchronized (this) &#123; ActivityStack stack = ActivityRecord.getStackLocked(token); if (stack != null) &#123; ActivityRecord r = mStackSupervisor.activityIdleInternalLocked(token, false, config); if (stopProfiling) &#123; if ((mProfileProc == r.app) &amp;&amp; (mProfileFd != null)) &#123; try &#123; mProfileFd.close(); &#125; catch (IOException e) &#123; &#125; clearProfilerLocked(); &#125; &#125; &#125; &#125; Binder.restoreCallingIdentity(origId); &#125; ...... &#125; &#160; &#160; &#160; &#160;ActivityManagerService有一个类型为ActivityStackSupervisor的成员变量mStackSupervisor，Run all ActivityStacks through this，运行所有的ActivityStacks 通过这个，它的成员函数activityIdleInternalLocked如下：123456789101112131415161718192021222324252627282930// Checked.final ActivityRecord activityIdleInternalLocked(final IBinder token, boolean fromTimeout, Configuration config) &#123; ...... boolean booting = false; boolean enableScreen = false; boolean activityRemoved = false; ActivityRecord r = ActivityRecord.forToken(token); if (r != null) &#123; ...... if (isFrontStack(r.task.stack) || fromTimeout) &#123; booting = mService.mBooting; mService.mBooting = false; if (!mService.mBooted) &#123; mService.mBooted = true; enableScreen = true; &#125; &#125; &#125; ...... if (booting || enableScreen) &#123; mService.postFinishBooting(booting, enableScreen); &#125; ...... return r;&#125; &#160; &#160; &#160; &#160;所以进入if判断中，将booting置为true，mService.mBooted和enableScreen也为true，进入下面的if判断，ActivityManagerService调用postFinishBooting方法完成系统启动。我们进入往下看：123456789101112131415161718192021222324void postFinishBooting(boolean finishBooting, boolean enableScreen) &#123; mHandler.sendMessage(mHandler.obtainMessage(FINISH_BOOTING_MSG, finishBooting? 1 : 0, enableScreen ? 1 : 0));&#125;final MainHandler mHandler;final class MainHandler extends Handler &#123; @Override public void handleMessage(Message msg) &#123; ...... case FINISH_BOOTING_MSG: &#123; if (msg.arg1 != 0) &#123; finishBooting(); &#125; if (msg.arg2 != 0) &#123; enableScreenAfterBoot(); &#125; break; &#125; ...... &#125;&#125; &#160; &#160; &#160; &#160;这里最后调用enableScreenAfterBoot方法，以便可以将屏幕让出来显示应用程序Launcher的界面：123456789void enableScreenAfterBoot() &#123; EventLog.writeEvent(EventLogTags.BOOT_PROGRESS_ENABLE_SCREEN, SystemClock.uptimeMillis()); mWindowManager.enableScreenAfterBoot(); synchronized (this) &#123; updateEventDispatchingLocked(); &#125;&#125; &#160; &#160; &#160; &#160;ActivityManagerService类的成员变量mWindowManager指向了系统中的Window管理服务WindowManagerService，ActivityManagerService服务通过调用它的成员函数enableScreenAfterBoot来停止显示开机动画。 &#160; &#160; &#160; &#160;WindowManagerService类的成员函数enableScreenAfterBoot的实现如下所示：123456789101112131415161718192021222324public void enableScreenAfterBoot() &#123; synchronized(mWindowMap) &#123; if (DEBUG_BOOT) &#123; RuntimeException here = new RuntimeException("here"); here.fillInStackTrace(); Slog.i(TAG, "enableScreenAfterBoot: mDisplayEnabled=" + mDisplayEnabled + " mForceDisplayEnabled=" + mForceDisplayEnabled + " mShowingBootMessages=" + mShowingBootMessages + " mSystemBooted=" + mSystemBooted, here); &#125; if (mSystemBooted) &#123; return; &#125; mSystemBooted = true; hideBootMessagesLocked(); // If the screen still doesn't come up after 30 seconds, give // up and turn it on. mH.sendEmptyMessageDelayed(H.BOOT_TIMEOUT, 30*1000); &#125; mPolicy.systemBooted(); performEnableScreen();&#125; &#160; &#160; &#160; &#160;WindowManagerService类的成员变量mSystemBooted用来记录系统是否已经启动完成的。如果已经启动完成的话，那么这个成员变量的值就会等于true，这时候WindowManagerService类的成员函数enableScreenAfterBoot什么也不做就返回了，否则的话，WindowManagerService类的成员函数enableScreenAfterBoot首先将这个成员变量的值设置为true，接着再调用另外一个成员函数performEnableScreen来执行停止显示开机动画的操作。&#160; &#160; &#160; &#160;WindowManagerService类的成员函数performEnableScreen的实现如下所示：1234567891011121314151617181920212223242526272829303132333435public void performEnableScreen() &#123; synchronized(mWindowMap) &#123; ...... if (mDisplayEnabled) &#123; return; &#125; if (!mSystemBooted &amp;&amp; !mShowingBootMessages) &#123; return; &#125; // Don't enable the screen until all existing windows have been drawn. if (!mForceDisplayEnabled &amp;&amp; checkWaitingForWindowsLocked()) &#123; return; &#125; if (!mBootAnimationStopped) &#123; // Do this one time. try &#123; IBinder surfaceFlinger = ServiceManager.getService("SurfaceFlinger"); if (surfaceFlinger != null) &#123; //Slog.i(TAG, "******* TELLING SURFACE FLINGER WE ARE BOOTED!"); Parcel data = Parcel.obtain(); data.writeInterfaceToken("android.ui.ISurfaceComposer"); surfaceFlinger.transact(IBinder.FIRST_CALL_TRANSACTION, // BOOT_FINISHED data, null, 0); data.recycle(); &#125; &#125; catch (RemoteException ex) &#123; Slog.e(TAG, "Boot completed: SurfaceFlinger is dead!"); &#125; mBootAnimationStopped = true; &#125; ......&#125; &#160; &#160; &#160; &#160;WindowManagerService类的另外一个成员变量mDisplayEnabled用来描述WindowManagerService是否已经初始化过系统的屏幕了，只有当它的值等于false，并且系统已经完成启动，即WindowManagerService类的成员变量mSystemBooted等于true的情况下，WindowManagerService类的成员函数performEnableScreen才通知SurfaceFlinger服务停止显示开机动画。 &#160; &#160; &#160; &#160;注意，WindowManagerService类的成员函数performEnableScreen是通过一个类型为IBinder.FIRST_CALL_TRANSACTION的进程间通信请求来通知SurfaceFlinger服务停止显示开机动画的。 &#160; &#160; &#160; &#160;在SurfaceFlinger服务，类型为IBinder.FIRST_CALL_TRANSACTION的进程间通信请求被定义为停止显示开机动画的请求，位于frameworks/native/include/gui/ISurfaceComposer.h中：123456789101112131415class BnSurfaceComposer : public BnInterface&lt;ISurfaceComposer&gt; &#123; public: enum &#123; // Note: BOOT_FINISHED must remain this value, it is called from // Java by ActivityManagerService. BOOT_FINISHED = IBinder::FIRST_CALL_TRANSACTION, ...... &#125;; virtual status_t onTransact( uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags = 0); &#125;; &#160; &#160; &#160; &#160;BnSurfaceComposer类它是SurfaceFlinger服务所要继承的Binder本地对象类，其中。当SurfaceFlinger服务接收到类型为IBinder::FIRST_CALL_TRANSACTION，即类型为BOOT_FINISHED的进程间通信请求时，它就会将该请求交给它的成员函数bootFinished来处理。 &#160; &#160; &#160; &#160;SurfaceFlinger服务的成员函数bootFinished实现在文件frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp中，如下所示：12345678910111213141516171819void SurfaceFlinger::bootFinished()&#123; const nsecs_t now = systemTime(); const nsecs_t duration = now - mBootTime; ALOGI("Boot is finished (%ld ms)", long(ns2ms(duration)) ); mBootFinished = true; // wait patiently for the window manager death const String16 name("window"); sp&lt;IBinder&gt; window(defaultServiceManager()-&gt;getService(name)); if (window != 0) &#123; window-&gt;linkToDeath(static_cast&lt;IBinder::DeathRecipient*&gt;(this)); &#125; // stop boot animation // formerly we would just kill the process, but we now ask it to exit so it // can choose where to stop the animation. property_set("service.bootanim.exit", "1");&#125; &#160; &#160; &#160; &#160;可以看到，该函数将属性“service.bootanim.exit”设置为”1”。在第2节分析android()代码的时候，我们讲到：当属性“service.bootanim.exit”值被设为”1”时，android()就会退出，开机动画显示自然也就结束了。由于android()退出且返回值为false，BootAnimation::threadLoop()线程也就结束了。再回到BootAnimation.cpp的main()函数中，threadLoop()线程结束，main函数也就结束，至此，bootanimaiton进程就自行结束，开机动画的显示完成了。 &#160; &#160; &#160; &#160;至此，Android系统的三个开机画面的显示过程就分析完成了。 小结&#160; &#160; &#160; &#160;本文的目的并不是单纯为了介绍Android系统的开机画面，而是希望能够以Android系统的开机画面来作为切入点来分析SurfaceFlinger。后续文章我们会详细分析SurfaceFlinger的每个模块和功能。没图了，等过一阵子再去拍一波~]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(二)----SurfaceFlinger概述]]></title>
    <url>%2F2017%2F04%2F27%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E4%BA%8C-SurfaceFlinger%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;因为还有很多烦人的事情需要处理，所以暂时没有离职，也挺忙的。最近一段时间稍微轻松些，所以更新一下这个进度。 前言&#160; &#160; &#160; &#160;Android的Graphic系统是设计的很精妙，但是也很复杂。SurfaceFlinger也是Graphic系统中十分重要的组成部分，要从正面分析不是一件容易的事情。所以我们先从侧面窥视它的重要模块，积累起足够的模块基础，再统一分析，就会一目了然。 GUI框架图形显示过程&#160; &#160; &#160; &#160;一般应用开发都要将UI数据使用Activity这个载体去展示，典型的Activity显示流程为： startActivity启动Activity； 为Activity创建一个window(PhoneWindow)，并在WindowManagerService中注册这个window； 切换到前台显示时，WindowManagerService会要求SurfaceFlinger为这个window创建一个surface用来绘图。SurfaceFlinger创建一个”layer”（surface）。（以想象一下C/S架构，SF对应Server，对应Layer；App对应Client，对应Surface）,这个layer的核心即是一个BufferQueue，这时候app就可以在这个layer上render了； 将所有的layer进行合成，显示到屏幕上。 &#160; &#160; &#160; &#160;一般app而言，在任何屏幕上起码有三个layer：屏幕顶端的status bar，屏幕下面的navigation bar，还有就是app的UI部分。一些特殊情况下，app的layer可能多余或者少于3个，例如对全屏显示的app就没有status bar，而对launcher，还有个为了wallpaper显示的layer。status bar和navigation bar是由系统进行去render，因为不是普通app的组成部分嘛。而app的UI部分对应的layer当然是自己去render，所以就有了第4条中的所有layer进行“合成”。 GUI框架 &#160; &#160; &#160; &#160;SurfaceFlinger：每当用户程序刷新UI的时候，会中介BufferQueue申请一个buffer（dequeueBuffer），然后把UI的信息填入，丢给SurfaceFlinger，SurfaceFlinger通过计算多重计算合成visibleRegion之后，丢给openGL层处理，处理之后送到显示器display上显示。 根据整个Android系统的GUI设计理念，我们不难猜想到至少需要两种本地窗口： 面向管理者(SurfaceFlinger)：既然SurfaceFlinger扮演了系统中所有UI界面的管理者，那么它无可厚非地需要直接或间接地持有“本地窗口”，这个窗口就是FramebufferNativeWindow 面向应用程序：这类窗口是Surface（这里和以前版本出入比较大，之前的版本本地窗口是SurfaceTextureClient） &#160; &#160; &#160; &#160;第二种窗口是能直接显示在终端屏幕上的——它使用了帧缓冲区，而第一种Window实际上是从内存缓冲区分配的空间。当系统中存在多个应用程序时，这能保证它们都可以获得一个“本地窗口”，并且这些窗口最终也能显示到屏幕上——SurfaceFlinger会收集所有程序的显示需求，对它们做统一的图像混合操作。 Render过程&#160; &#160; &#160; &#160;android提供了两种方式：Canvas ，OpenGL ES。这两种方式在我认为都应该算是标准，提供了一套固定的API，实现和平台无关。Java中类似于给你提供了一个interface接口，这个接口你可以用任何方式去实现，只要满足功能要求即可。不同的人可以有不同的实现方式，对10个数进行排序，A可以用冒泡法，B可以用选择法，最终结果只要保证把这10个数排好序了，唯一差别就是实现难度和效率问题。 Canvas&#160; &#160; &#160; &#160;android提供了Canvas 2D API用来进行普通图形的绘制的，类似TextView这种应该都是用Canvas API来完成的。而Canvas这个”标准”的具体实现是由/external/skia库来完成的，真正干活的是skia。上层Canvas调用的API到下层其实封装了skia的实现。 OpenGL ES&#160; &#160; &#160; &#160;OpenGL ES相关的API是为了3D图形的绘制而准备的。&#160; &#160; &#160; &#160;android上有个EGL库，EGL和OpenGL ES是什么关系？代码在frameworks\native\opengl\libs\，我们查看frameworks\native\opengl\libs\Android.mk中：123456789......LOCAL_SHARED_LIBRARIES += libcutils libutils liblog libGLES_trace#编译的库文件为/system/lib/libEGL.soLOCAL_MODULE:= libEGLLOCAL_LDFLAGS += -Wl,--exclude-libs=ALLLOCAL_SHARED_LIBRARIES += libdl# we need to access the private Bionic header &lt;bionic_tls.h&gt;LOCAL_C_INCLUDES += bionic/libc/private...... &#160; &#160; &#160; &#160;SurfaceFlinger这个非常重要的系统服务依赖EGL库，包含了EGL头文件，使用了EGL中的函数，查看frameworks/native/services/surfaceflinger/Android.mk：12345678910111213141516171819......LOCAL_SHARED_LIBRARIES := \ libcutils \ liblog \ libdl \ libhardware \ libutils \ libEGL \ libGLESv1_CM \ libGLESv2 \ libbinder \ libui \ libgui \ libpowermanagerLOCAL_MODULE:= libsurfaceflingerinclude $(BUILD_SHARED_LIBRARY)...... &#160; &#160; &#160; &#160;在SurfaceFlinger::init()中，调用了egl开头的函数，位于frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp中：12345678910111213void SurfaceFlinger::init() &#123; ALOGI( "SurfaceFlinger's main thread ready to run. " "Initializing graphics H/W..."); status_t err; Mutex::Autolock _l(mStateLock); // initialize EGL for the default display //#define EGL_DEFAULT_DISPLAY ((EGLNativeDisplayType)0) mEGLDisplay = eglGetDisplay(EGL_DEFAULT_DISPLAY); eglInitialize(mEGLDisplay, NULL, NULL); ......&#125; &#160; &#160; &#160; &#160;在eglGetDisplay()中调用了egl_init_drivers()，位于frameworks/native/opengl/libs/egl/eglApi.cpp：12345678910111213141516EGLDisplay eglGetDisplay(EGLNativeDisplayType display)&#123; clearError(); uintptr_t index = reinterpret_cast&lt;uintptr_t&gt;(display); if (index &gt;= NUM_DISPLAYS) &#123; return setError(EGL_BAD_PARAMETER, EGL_NO_DISPLAY); &#125; //egl_init_drivers函数载入OpenGL ES相关库 if (egl_init_drivers() == EGL_FALSE) &#123; return setError(EGL_BAD_PARAMETER, EGL_NO_DISPLAY); &#125; EGLDisplay dpy = egl_display_t::getFromNativeDisplay(display); return dpy;&#125; &#160; &#160; &#160; &#160;关于egl_init_drivers()函数，载入OpenGL ES相关库，限于本篇只是简单介绍，所以不详细分析。相关文件如下： frameworks/native/opengl/libs/egl/Egl.cpp frameworks/native/opengl/libs/egl/Loader.cpp &#160; &#160; &#160; &#160;代码也不多，主要做的事情： &#160; &#160; &#160; &#160;1. 首先在frameworks\native\opengl\libs\EGL\egl_entries.in中保存了EGL的相关API，函数都是以egl开头；在frameworks\native\opengl\libs\entries.in中保存了OpenGL ES相关API，函数都是以gl开头。函数egl_init_drivers()需要做的就是找到EGL和OpenGL ES本地的实现，然后对这些函数进行赋值。可以查看frameworks/native/opengl/libs/egl/Egl.cpp：123456789char const * const gl_names[] = &#123; #include "../entries.in" NULL&#125;;char const * const egl_names[] = &#123; #include "egl_entries.in" NULL&#125;; &#160; &#160; &#160; &#160;2. 在/vendor/lib/egl或/system/lib/egl下(不是/system/lib/下)，寻找libGLES.so，如果未找到，则寻找libGLES_*.so，上面两种库只要找到一个，则打开库，这个库里包含了EGL和OpenGL ES具体实现的库，然后“取出”库中具体的函数实现进行赋值。&#160; &#160; &#160; &#160;但是较新版本的android已经不使用软件实现，即libagl，所以即使找到libGLES_android.so也不会去用。位于frameworks/native/opengl/libs/egl/Loader.cpp中load_driver函数实现：12345678910void *Loader::load_driver(const char* kind, egl_connection_t* cnx, uint32_t mask)&#123; ...... if (!strcmp(e-&gt;d_name, "libGLES_android.so")) &#123; // always skip the software renderer continue; &#125; ...... &#125; &#160; &#160; &#160; &#160;如果libGLES.so和libGLES_.so都未找到，需要分别去加载/vendor/lib/egl或/system/lib/egl下libEGL.so, libGLESv1CM.so, libGLESv2.so这三个库或者libEGL.so, libGLESv1CM.so, libGLESv2_.so，然后分别对EGL、OpenGL ES V1、OpenGL ES V2的具体实现去赋值(那意思是libGLES_*.so中其实囊括了三个库所有的具体实现)。至此，就将OpenGL ES“标准”和“实现”挂钩了。 &#160; &#160; &#160; &#160;所以android中OpenGL ES的实现方式有2种： 一种是软件实现，用cpu去绘图，这就是所谓的agl(libGLES_android.so)，代码路径在frameworks/native/opengl/libagl，即the software OpenGL ES library； 另一种是硬件厂商根据自己GPU提供的实现，一般都不开放源代码，就是上面介绍的需要去/vendor/lib/egl或/system/lib/egl找的几个库，但是只要把API 函数赋值上正确的实现函数即可。 &#160; &#160; &#160; &#160;此外由于OpenGL ES的实现是系统无关的，所以EGL库的另一个作用就是将OpenGL ES和本地窗口系统结合起来，举个例子好理解，如果你要画个纹理多边形，调用OpenGL ES接口，如果要把图形render到屏幕，需要调用EGL接口。例如在使用OpenGL ES前首先需要调用EGL的相关函数去搭建好OpenGL ES的本地环境等，EGL是android使用OpenGL ES API绘图的助手！ hardware-accelerated Canvas&#160; &#160; &#160; &#160;从android 4.0开始，支持了硬件加速的Canvas，应该就是修改了Canvas的具体实现，不用skia了，而改调用EGL和OpenGL ES API了。 android的封装&#160; &#160; &#160; &#160;那么我们平时写app时为何不用调用上面的Canvas和OpenGL ES，也能出现漂亮的UI呢？因为我们使用的都是android上层封装好的类，例如TextView就是用了Canvas，而GLSurfaceView就是使用了OpenGL ES，android已经帮我们做了大部分的工作。当然完全可以不用调用上层的Java类，而用c++/c去直接调用Canvas和OpenGL ES。 如何去合成首先介绍个概念，hardware overlay，来自维基百科。 From Wikipedia, the free encyclopediaIn computing, hardware overlay, a type of video overlay, provides a method of rendering an image to a display screen with a dedicated memory buffer inside computer video hardware. The technique aims to improve the display of a fast-moving video image — such as a computer game, a DVD, or the signal from a TV card. Most video cards manufactured since about 1998 and most media players support hardware overlay.[1] The overlay is a dedicated buffer into which one app can render (typically video), without incurring the significant performance cost of checking for clipping and overlapping rendering by other apps. The framebuffer has hardware support for importing and rendering the buffer contents without going through the GPU.[citation needed] &#160; &#160; &#160; &#160;hardware overlay 是提供一种机制，直接render到display screen的硬件内存中，提高显示效率吧。&#160; &#160; &#160; &#160;而android对layer的合成主要包括2部分：在GPU中合成和在display的硬件中进行buffer的合成。&#160; &#160; &#160; &#160;在GPU中进行合成，既是利用OpenGL ES进行合成，需要注意，画图的时候我们用了OpenGL ES，这里合成时也用了，功能真是强大，开始一直奇怪为何SurfaceFlinger也要去调用EGL的函数，原来是需要用OpenGL ES去合成layer；在display的硬件中进行合成，也就是hardware overlay机制。 Hardware Composer&#160; &#160; &#160; &#160;那么android是如何使用这两种合成机制的呢？这里就是Hardware Composer的功劳。处理流程为： 1.SurfaceFlinger给HWC提供layer list，询问如何处理这些layer； 2.HWC将每个layer标记为overlay或者GLES composition，然后回馈给SurfaceFlinger； 3.SurfaceFlinger需要去处理那些GLES的合成，而不用去管overlay的合成，最后将overlay的layer和GLES合成后的buffer发送给HWC处理。 &#160; &#160; &#160; &#160;借用google一张图说明，可以将上面讲的很多概念展现，很清晰。地址位于 https://source.android.com/devices/graphics/&#160; &#160; &#160; &#160;在我认为使用overlay后，可以将SurfaceFlinger的工作减轻，即少一些GLES的合成，HWC承担了部分OpenGL ES 和GPU的工作， 从而减少了功耗。 注：如果屏幕上的画面基本不变化，这时候用GLES 合成的效率要高于overlay(overlay主要是为了render快速变化的图形等)；android 4.4往上支持4个oveylay，如果要合成超过4个layer，系统就会对剩余的使用GLES合成，所以app的layer个数对手机的功耗影响挺大。 SurfaceFlinger和BufferQueue生产模型&#160; &#160; &#160; &#160;一个UI完全显示到diplay的过程，SurfaceFlinger扮演着重要的角色但是它的职责是“Flinger”，即把系统中所有应用程序的最终的“绘图结果”进行“混合”，然后统一显示到物理屏幕上，而其他方面比如各个程序的绘画过程，就由其他东西来担任了。这个光荣的任务自然而然地落在了BufferQueue的肩膀上，它是每个应用程序“一对一”的辅导老师，指导着UI程序的“画板申请”、“作画流程”等一系列细节。下面的图描述了这三者的关系： &#160; &#160; &#160; &#160;虽说是三者的关系，但是他们所属的层却只有两个，app属于Java层，BufferQueue/SurfaceFlinger属于native层。也就是说BufferQueue也是隶属SurfaceFlinger，所有工作围绕SurfaceFlinger展开。&#160; &#160; &#160; &#160;这里IGraphicBufferProducer就是app和BufferQueue重要桥梁，GraphicBufferProducer承担着单个应用进程中的UI显示需求，与BufferQueue打交道的就是它。它的工作流程如下： &#160; &#160; &#160; &#160;BpGraphicBufferProducer是GraphicBufferProducer在客户端这边的代理对象，负责和SF交互，GraphicBufferProducer通过gbp（IGraphicBufferProducer类对象）向BufferQueue获取buffer，然后进行填充UI信息，当填充完毕会通知SF，SF知道后就对该Buffer进行下一步操作。典型的生产-消费者模式。 工作流程&#160; &#160; &#160; &#160;接下来具体说明客户端（producer）和服务端SurfaceFlinger（consumer）工作的模式：&#160; &#160; &#160; &#160;首先这里的buffer是共享缓冲区，故肯定会涉及到互斥锁，所以buffer的状态也会有多种，一般的buffer大致会经过FREE-&gt;DEQUEUED-&gt;QUEUED-&gt;ACQUIRED-&gt;FREE这个流程，如下图： BufferQueue：可以认为BufferQueue是一个服务中心，其它两个owner必须要通过它来管理buffer。比如说当producer想要获取一个buffer时，它不能越过BufferQueue直接与consumer进行联系，反之亦然。 Producer：生产者就是“填充”buffer空间的人，通常情况下当然就是应用程序。因为应用程序不断地刷新UI，从而将产生的显示数据源源不断地写到buffer中。当Producer需要使用一块buffer时，它首先会向中介BufferQueue发起dequeue申请，然后才能对指定的缓冲区进行操作。这种情况下buffer就属于producer一个人的了，它可以对buffer进行任何必要的操作，而其它owner此刻绝不能擅自插手。当生产者认为一块buffer已经写入完成后，它进一步调用BufferQueue的queue。从字面上看这个函数是“入列”的意思，形象地表达了buffer此时的操作——把buffer归还到BufferQueue的队列中。一旦queue成功后，owner也就随之改变为BufferQueue了。 Consumer：消费者是与生产者相对应的，它的操作同样受到BufferQueue的管控。当一块buffer已经就绪后，Consumer就可以开始工作了。这里需要特别留意的是，从各个对象所扮演的角色来看，BufferQueue是中介机构，属于服务提供方;Producer属于buffer内容的产出方，它对缓冲区的操作是一个“主动”的过程;反之，Consumer对buffer的处理则是“被动”的、“等待式”的——它必须要等到一块buffer填充完成后才能做工作。在这样的模型下，我们怎么保证Consumer可以及时的处理buffer呢？换句话说，当一块buffer数据ready后，应该怎么告知Consumer来操作呢？仔细观察的话，可以看到BufferQueue里还同时提供了一个特别的类，名称为ProxyConsumerListener，其中的函数接口包括： 123456789101112131415classProxyConsumerListener : public BnConsumerListener &#123; public: //省略构造函数 virtual void onFrameAvailable();/*当一块buffer可以被消费时，这个函数会被调用，特别注意此时没有共享锁的保护*/ virtual voidonBuffersReleased();/*BufferQueue通知consumer它已经释放其slot中的一个或多个 GraphicBuffer引用*/ private: wp&lt;ConsumerListener&gt;mConsumerListener; &#125; 这样子就很清楚了，当有一帧数据准备就绪后，BufferQueue就会调用onFrameAvailable()来通知Consumer进行消费。 消费模型&#160; &#160; &#160; &#160;BufferQueue和SurfaceFlinger之间的通信模式如下： &#160; &#160; &#160; &#160;也是有一对BpGraphicBufferConsumer/BnGraphicBufferConsumer支持他们之间的信息传输。 结语&#160; &#160; &#160; &#160;本篇只是简单科普一下SurfaceFlinger的轮廓，我们从下一篇开始详细分析每个流程。&#160; &#160; &#160; &#160;突然觉得上一篇开头太中二了=。= 唉，事情太多了，有点烦，感觉没了以前那份心若冰清的沉稳和奋不顾身的勇气了，唉(第二次唉)，慢慢来，需要时间去消化。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android性能优化之内存优化实战]]></title>
    <url>%2F2017%2F04%2F07%2FAndroid%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E4%B9%8B%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;事情依旧很多，生活的、工作的，成长总是需要代价的。一直憧憬童年和大学，那是两种异曲同工的生活态度。假装自己是个孩子，一直不想长大，被自己小小的愿望襁褓着，不去理会外面世界的沧海桑田。白驹过隙，岁月在脸上留下了痕迹，不再隐瞒早已不年轻的事实。伴随着时光点滴流逝，心也在渐渐老去。也许成长就是一种淡然吧，从锋芒毕露到心如止水，从棱角分明到不露声色。少了昔日偶尔破釜沉舟的勇气，更多的是经常瞻前顾后的思虑。时光荏苒，没有留下刻骨铭心的记忆，更多的是平淡、安静。我渐渐喜欢这种感觉，岁月静好，没有跌宕起伏的生活剧情，没有扣人心弦的情景环节，一切安好，便是生活最美好的恩赐。&#160; &#160; &#160; &#160;记忆渐渐模糊，没有留下印记。没有选择性忘记，但却真实在一点一点消逝。那些旧照片也没能留下，早已不见了踪影，那些故人，都逐渐失去了联系，那些雨天泥泞的小路，还有路边在儿时植树节时新栽的小树，也都变得那么陌生。只有那些花儿，却开的愈加鲜艳茂密。就像一颗顽强的心，一颗不服输的心，在抗拒随波逐流的人生。&#160; &#160; &#160; &#160;多年之前，我们都还是个孩子。经过岁月的洗礼，也许会变得更加沉稳，也会变得更加冷静，亦或者是……冷漠。回忆往昔，这一路走来，还是渐渐变成了孩时最讨厌的自己，一个冷漠的自己，一个绝情的自己。安静也许是一个借口，一种掩饰，透过镜子，终于看清了真实的自己，那个自私的自己。有人说，社会会逼着一个人成长，是啊，虽然不愿承认，但是至少褪去了青年时怕事嫌烦表选出来的虚伪，获得更多的是学会了处事不惊的态度，和勇于承担责任的勇气。如果选择对生活妥协，是不会答应的，即使没了追梦赤子心，至少还有追逐本心的动力。不忘初心，坚持自己的初衷。青春即使如同《老男孩》的歌词唱的那样：”青春如同奔流的江河,一去不回来不及道别,只剩下麻木的我,没有了当年的热血……”即便如此，依然要做不一样的我，一个不退缩、不妥协的我，一个默默前行的我，一个坚持本心的我————“我就是我，是颜色不一样的烟火，天空海阔，要做最坚强的泡沫，我喜欢我，让蔷薇开出一种结果，孤独的沙漠里一样盛放的赤裸裸” &#160; &#160; &#160; &#160;这次带来一篇性能优化的文章，内容来自APP架构师,我稍微修改了一些部分，加了一些内容。 Memory Leak&#160; &#160; &#160; &#160;内存泄漏：对于Java来说，就是new出来的Object 放在Heap上无法被GC回收（内存中存在无法被回收的对象）；内存泄漏发生时的主要表现为内存抖动，可用内存慢慢变少。 Memory Monitor&#160; &#160; &#160; &#160;AndroidStudio自带的Memory Monitor可以方便的观察堆内存的分配情况，并且可以粗略的观察有没有Memory Leak。 &#160; &#160; &#160; &#160;频繁的内存抖动，可能存在内存泄漏 A：initiate GC 手动触发GC操作； B：Dump Java Heap 获取当前的堆栈信息，生成一个.hprof文件，AndroidStudip会自动使用HeapViewer打开；一般用于操作之后检测内存泄漏的情况； C：Start Allocation Tracking 内存分配追踪工具，用于追踪一段时间的内存分配使用情况，能够知道执行一些列操作后，有哪些对象被分配空间。一般用于追踪某项操作之后的内存分配，调整相关的方法调用来优化app性能与内存使用； D：剩余可用内存； E：已经使用的内存。 &#160; &#160; &#160; &#160;点击Memory Monitor的Dump Java Heap，会生成一个.hprof文件，位于工程名/captures/文件夹下。AndroidStudio会自动使用HeapViewer打开。 &#160; &#160; &#160; &#160;左面板说明： Total Count 该类的实例个数 Heap Count 选定的Heap中实例的个数 Sizeof 每个实例占用的内存大小 Shallow Size 所有该类的实例占用的内存大小 Retained Size 该类的所有实例可支配的内存大小 &#160; &#160; &#160; &#160;右面板说明： Instance 该类的所有实例对象（左侧Total Count为15，此处就有15个对象） Depth 深度, GC Root点到该实例的最短链路数 Dominating Size 该实例可支配的内存大小 &#160; &#160; &#160; &#160;此处可以看出MainActivity存在了15个示例对象，怀疑此处有问题。 &#160; &#160; &#160; &#160;附：Android Device Monitor抓取：（可忽略，直接跳过） &#160; &#160; &#160; &#160;1. 首先打开AS中的Android Device Monitor工具 ，Tools—&gt;Android—&gt;Android Device Monitor,打开如下： &#160; &#160; &#160; &#160;2. 先选中你要检测的应用的包名，然后点击下图画圈的地方，会在程序包名后标记一个图标 &#160; &#160; &#160; &#160;3. 接下来要做的就是操作我们的app 。 之后点击下图的图标 就可导出hprof文件进行分析了 MAT&#160; &#160; &#160; &#160;上述只是可以粗略的看出是不是有问题，而要知道问题出在哪里就需要借助MAT了。 &#160; &#160; &#160; &#160;打开MAT工具 ，如果没有 可以在下面网址下载 ：&#160; &#160; &#160; &#160;MAT工具下载地址 &#160; &#160; &#160; &#160;打开我们先前导出的hprof文件 ，不出意外会报下面的错误 : &#160; &#160; &#160; &#160;这是因为MAT是用来分析Java程序的hprof文件的 与Android导出的hprof有一定的格式区别，因此我们需要把导出的hprof文件转换一下，sdk中提供给我们转换的工具 hprof-conv.exe 在下图的位置 &#160; &#160; &#160; &#160;我们看到adb.exe和hprof-conv.exe在同一个目录，这就比较幸运了，因为我们一般都会把adb路径加入环境变量，所以这个hprof-conv也会位于环境变量当中，因此我们不用cd进入这个目录去敲命令了，直接随便哪个控制台路径都可以。比如我们利用AS自带的Terminal。 &#160; &#160; &#160; &#160;将生成的.hprof文件进行转换 格式转换命令：hprof-conv 原文件路径 转换后文件路径 &#160; &#160; &#160; &#160;然后用MAT打开转换后的.hprof文件 &#160; &#160; &#160; &#160;注意下面的Actions： Histogram可以列出内存中每个对象的名字、数量以及大小。 Dominator Tree会将所有内存中的对象按大小进行排序，并且我们可以分析对象之间的引用结构。 &#160; &#160; &#160; &#160;一般使用最多的也是这两个功能。 &#160; &#160; &#160; &#160;Retained Heap表示这个对象以及它所持有的其它引用（包括直接和间接）所占的总内存 使用Histogram： 点击Histogram并在顶部的Regex中输入MainActivity会进行正则匹配，会将包含“MainActivity”的所有对象全部列出了出来，其中第一行就是MainActivity的实例。 对着想查看的对象点击右键 -&gt; List objects -&gt; with incoming references 查看具体MainActivity实例。 对想要查看的对象实例点击右键-&gt; Path To Gc Roots -&gt; exclude weak reference（排除掉软引用）。 &#160; &#160; &#160; &#160;注意：&#160; &#160; &#160; &#160;this$0前面的图标的左下角有个圆圈，这代表这个引用可以被Gc Roots引用到，由于MainActivity$LeakClass能被GC Roots访问到导致其不能被回收，从而它所持有的其它引用也无法被回收了，包括MainActivity，也包括MainActivity中所包含的其它资源。 &#160; &#160; &#160; &#160;此时我们就找到了内存泄漏的原因。 使用Dominator Tree &#160; &#160; &#160; &#160;使用上面Histogram的操作方式也可以找到泄漏的具体原因，此处不再累述。&#160; &#160; &#160; &#160;注意：每个对象前的图标的圆圈，并不代表一定是导致内存泄漏的原因，有些对象就是需要在内存中存活的，需要区别对待。 LeakCanary&#160; &#160; &#160; &#160;LeakCanary是square出品的一个检测内存泄漏的库，集成到App之后便无需关心，在发生内存泄漏之后会Toast、通知栏弹出等方式提示，可以指出泄漏的引用路径，而且可以抓取当前的堆栈信息供详细分析。 Out Of MemoryAndroid OOM&#160; &#160; &#160; &#160;Android系统的每个进程都有一个最大内存限制，如果申请的内存资源超过这个限制，系统就会抛出OOM错误。 Android 2.x系统，当dalvik allocated + external allocated + 新分配的大小 &gt;= dalvik heap 最大值时候就会发生OOM。其中bitmap是放于external中 。 Android 4.x系统，废除了external的计数器，类似bitmap的分配改到dalvik的java heap中申请，只要allocated + 新分配的内存 &gt;= dalvik heap 最大值的时候就会发生OOM（art运行环境的统计规则还是和dalvik保持一致）&#160; &#160; &#160; &#160;内存溢出是程序运行到某一阶段的最终结果，直接原因是剩余的内存不能满足内存的申请，但是再分析间接原因内存为什么没有了： 内存泄漏的存在可能导致可用内存越来越少； 内存申请的峰值超过了系统时间点剩余的内存；(例如：某手机单个进程可用最大内存为192M，目前分配内存80M，此时申请5M内存，但是当前时间点整个系统可用内存只有3M，此时没有超出单个进程可用最大内存，但是OOM也会发生) Avoid Android OOM&#160; &#160; &#160; &#160;除了避免内存泄漏之外，根据《Manage Your App’s Memory》，我们可以对内存的状态进行监听，在Activity中覆写此方法，根据不同的case进行不同的处理：1234@Override public void onTrimMemory(int level) &#123; super.onTrimMemory(level); &#125; TRIM_MEMORY_RUNNING_MODERATE：你的应用正在运行并且不会被列为可杀死的。但是设备此时正运行于低内存状态下，系统开始触发杀死LRU Cache中的Process的机制。TRIM_MEMORY_RUNNING_LOW：你的应用正在运行且没有被列为可杀死的。但是设备正运行于更低内存的状态下，你应该释放不用的资源用来提升系统性能。TRIM_MEMORY_RUNNING_CRITICAL：你的应用仍在运行，但是系统已经把LRU Cache中的大多数进程都已经杀死，因此你应该立即释放所有非必须的资源。如果系统不能回收到足够的RAM数量，系统将会清除所有的LRU缓存中的进程，并且开始杀死那些之前被认为不应该杀死的进程，例如那个包含了一个运行态Service的进程。当应用进程退到后台正在被Cached的时候，可能会接收到从onTrimMemory()中返回的下面的值之一：TRIM_MEMORY_BACKGROUND： 系统正运行于低内存状态并且你的进程正处于LRU缓存名单中最不容易杀掉的位置。尽管你的应用进程并不是处于被杀掉的高危险状态，系统可能已经开始杀掉LRU缓存中的其他进程了。你应该释放那些容易恢复的资源，以便于你的进程可以保留下来，这样当用户回退到你的应用的时候才能够迅速恢复。TRIM_MEMORY_MODERATE：系统正运行于低内存状态并且你的进程已经已经接近LRU名单的中部位置。如果系统开始变得更加内存紧张，你的进程是有可能被杀死的。TRIM_MEMORY_COMPLETE：系统正运行于低内存的状态并且你的进程正处于LRU名单中最容易被杀掉的位置。你应该释放任何不影响你的应用恢复状态的资源。 Memory Churn&#160; &#160; &#160; &#160;Memory Churn内存抖动：大量的对象被创建又在短时间内马上被释放。&#160; &#160; &#160; &#160;瞬间产生大量的对象会严重占用Young Generation的内存区域，当达到阀值，剩余空间不够的时候，也会触发GC。系统花费在GC上的时间越多，进行界面绘制或流音频处理的时间就越短。即使每次分配的对象占用了很少的内存，但是他们叠加在一起会增加Heap的压力，从而触发更多其他类型的GC。这个操作有可能会影响到帧率，并使得用户感知到性能问题。 &#160; &#160; &#160; &#160;常见的可能引发内存抖动的情形： 循环中创建临时对象； onDraw中创建Paint或Bitmap对象等； &#160; &#160; &#160; &#160;例如之前使用过的有些下拉刷新控件的实现方式，在onDraw中创建Bitmap等多个临时大对象会导致内存抖动。 Bitmap&#160; &#160; &#160; &#160;Bitmap的处理也是Android中的一个难点，当然使用第三方框架的话就屏蔽掉了这个难点。 Bitmap的内存模型； Bitmap的加载、压缩、缓存等策略； 版本的兼容等； &#160; &#160; &#160; &#160;关于Bitmap之后会写专门的一篇文章来介绍，此处可以参考《Handling Bitmaps》。也可以查看我之前的一片文章：浅析Bitmap占据内存大小。 Program Advice节制地使用Service&#160; &#160; &#160; &#160;内存管理最大的错误之一就是让Service一直运行。在后台使用service时，除非它需要被触发并执行一个任务，否则其他时候Service都应该是停止状态。另外需要注意Service工作完毕之后需要被停止，以免造成内存泄漏。 &#160; &#160; &#160; &#160;系统会倾向于保留有Service所在的进程，这使得进程的运行代价很高，因为系统没有办法把Service所占用的RAM空间腾出来让给其他组件，另外Service还不能被Paged out。这减少了系统能够存放到LRU缓存当中的进程数量，它会影响应用之间的切换效率，甚至会导致系统内存使用不稳定，从而无法继续保持住所有目前正在运行的service。 &#160; &#160; &#160; &#160;建议使用JobScheduler，而尽量避免使用持久性的Service。还有建议使用IntentService，它会在处理完交代给它的任务之后尽快结束自己。 使用优化过的集合&#160; &#160; &#160; &#160;Android API当中提供了一些优化过后的数据集合工具类，如SparseArray，SparseBooleanArray，以及LongSparseArray等，使用这些API可以让我们的程序更加高效。传统Java API中提供的HashMap工具类会相对比较低效，因为它需要为每一个键值对都提供一个对象入口，而SparseArray就避免掉了基本数据类型转换成对象数据类型的时间。 谨慎对待面向抽象&#160; &#160; &#160; &#160;开发者经常把抽象作为好的编程实践，因为抽象能够提升代码的灵活性与可维护性。然而，抽象会导致一个显著的开销：面向抽象需要额外的代码（不会被执行到），同样会被咨映射到内存中，耗费了更多的时间以及内存空间。因此如果面向抽象对你的代码没有显著的收益，那你应该避免使用。&#160; &#160; &#160; &#160;例如：使用枚举通常会比使用静态常量要消耗两倍以上的内存，在Android开发当中我们应当尽可能地不使用枚举。 使用nano protobufs序列化数据&#160; &#160; &#160; &#160;Protocol buffers是Google为序列化数据设计的一种语言无关、平台无关、具有良好扩展性的数据描述语言，与XML类似，但是更加轻量、快速、简单。如果使用protobufs来实现数据的序列化及反序列化，建议在客户端使用nano protobufs，因为通常的protobufs会生成冗余代码，会导致可用内存减少，Apk体积变大，运行速度减慢。 避免内存抖动&#160; &#160; &#160; &#160;垃圾回收通常不会影响应用的表现，但是短时间内多次的垃圾回收会消耗掉界面绘制的时间。系统花费在GC上的时间越多，进行界面绘制或流音频处理的时间就越短。通常内存抖动会导致多次的GC，实践中内存抖动代表了一段时间内分配了临时对象。&#160; &#160; &#160; &#160;例如：在For循环中分配了多个临时对象，或在onDraw()方法中创建了Paint、Bitmap对象，应用产生了大量的对象；这会很快耗尽young generation的可用内存，导致GC发生。&#160; &#160; &#160; &#160;使用Analyze your RAM usage中的工具找出代码里内存抖动的地方。考虑把操作移出内部循环，或者将其移动到基于工厂的分配结构中。 移除消耗内存的库、缩减Apk的大小&#160; &#160; &#160; &#160;查看Apk的大小，包括三方库和内嵌的资源，这些都会影响应用消耗的内存。通过减少冗余、非必须或大的组件、库、图片、资源、动画等，都可以改善应用的内存消耗。 使用Dagger 2进行依赖注入&#160; &#160; &#160; &#160;如果您打算在应用程序中使用依赖注入框架，请考虑使用Dagger 2。 Dagger不使用反射来扫描应用程序的代码。 Dagger的编译时注解技术实现意味着它不需要不必要的运行时成本。而使用反射的其它依赖注入框架通常通过扫描代码来初始化过程。 此过程可能需要显着更多的CPU周期和RAM，并可能导致应用程序启动时明显的卡顿。&#160; &#160; &#160; &#160;备注：之前的文档是不建议使用依赖注入框架，因为实现原理是使用反射，而进化为编译时注解之后，就不再有反射带来的影响了。 谨慎使用第三方库&#160; &#160; &#160; &#160;很多开源的library代码都不是为移动端而编写的，如果运用在移动设备上，并不一定适合。即使是针对Android而设计的library，也需要特别谨慎，特别是在你不知道引入的library具体做了什么事情的时候。例如，其中一个library使用的是nano protobufs, 而另外一个使用的是micro protobufs。这样一来，在你的应用里面就有2种protobuf的实现方式。这样类似的冲突还可能发生在输出日志，加载图片，缓存等等模块里面。另外不要为了1个或者2个功能而导入整个library，如果没有一个合适的库与你的需求相吻合，你应该考虑自己去实现，而不是导入一个大而全的解决方案。 Other谨慎使用LargeHeap属性 &#160; &#160; &#160; &#160;可以通过在manifest的application标签下添加largeHeap=true的属性来为应用声明一个更大的heap空间(可以通过getLargeMemoryClass()来获取到这个更大的heap size阈值)。然而，声明得到更大Heap阈值的本意是为了一小部分会消耗大量RAM的应用(例如一个大图片的编辑应用)。不要轻易的因为你需要使用更多的内存而去请求一个大的Heap Size。只有当你清楚的知道哪里会使用大量的内存并且知道为什么这些内存必须被保留时才去使用large heap，使用额外的内存空间会影响系统整体的用户体验，并且会使得每次gc的运行时间更长。在任务切换时，系统的性能会大打折扣。另外, large heap并不一定能够获取到更大的heap。在某些有严格限制的机器上，large heap的大小和通常的heap size是一样的。 谨慎使用多进程 &#160; &#160; &#160; &#160;多进程确实是一种可以帮助我们节省和管理内存的高级技巧。如果你要使用它的话一定要谨慎使用，因为绝大多数的应用程序都不应该在多个进程当中运行的，一旦使用不当，它甚至会增加额外的内存而不是帮我们节省内存；同时需要知晓多进程带来的缺点。这个技巧比较适用于那些需要在后台去完成一项独立的任务，和前台的功能是可以完全区分开的场景。 &#160; &#160; &#160; &#160;这里举一个比较适合去使用多进程技巧的场景，比如说我们正在做一个音乐播放器软件，其中播放音乐的功能应该是一个独立的功能，它不需要和UI方面有任何关系，即使软件已经关闭了也应该可以正常播放音乐。如果此时我们只使用一个进程，那么即使用户关闭了软件，已经完全由Service来控制音乐播放了，系统仍然会将许多UI方面的内存进行保留。在这种场景下就非常适合使用两个进程，一个用于UI展示，另一个则用于在后台持续地播放音乐。 实现方式可能存在的问题：例如启动页闪屏图，show完毕之后应该释放掉Bitmap。&#160; &#160; &#160; &#160;一些实现方式看起来没有问题实现了功能但是实际上可能对内存造成了影响。我在使用Heap Viewer查看Bitmap对象时发现了一张只需下载不应该被加载的图。 &#160; &#160; &#160; &#160;使用HeapViewer可直接查看Bitmap &#160; &#160; &#160; &#160;内存中出现的不应该被加载的图 &#160; &#160; &#160; &#160;通过查阅代码，发现问题出在：此处下载图片作为另一个模块的使用图，但是下载的方法竟然是使用图片加载器加载出来Bitmap然后再保存到本地；而且保存之后也没有将Bitmap对象释放掉。&#160; &#160; &#160; &#160;与之类似的还有：首页闪屏图展示之后，Bitmap对象应该及时释放掉。 使用try catch进行捕获&#160; &#160; &#160; &#160;对高风险OOM代码块如展示高清大图等进行try catch，在catch块加载非高清的图片并做相应内存回收的处理。注意OOM是OutOfMemoryError，不能使用Exception进行捕获。 Summary&#160; &#160; &#160; &#160;内存优化的套路：&#160; &#160; &#160; &#160;1. 解决所有的内存泄漏 集成LeakCanary，可以方便的定位出90%的内存泄漏问题； 通过反复进出可疑界面，观察内存增减的情况，Dump Java Heap获取当前堆栈信息使用MAT进行分析。 内存泄漏的常见情形可参照《Android 内存泄漏分析心得》 &#160; &#160; &#160; &#160;2. 避免内存抖动 避免在循环中创建临时对象； 避免在onDraw中创建Paint、Bitmap对象等。 &#160; &#160; &#160; &#160;3. Bitmap的使用 使用三方库加载图片一般不会出内存问题，但是需要注意图片使用完毕的释放，而不是被动等待释放。 &#160; &#160; &#160; &#160;4. 使用优化过的数据结构 &#160; &#160; &#160; &#160;5. 使用onTrimMemory根据不同的内存状态做相应处理 &#160; &#160; &#160; &#160;6. Library的使用 去掉无用的Library，对生成的Apk进行反编译查看使用到的Library，避免出现无用的Lib仍然被打进Apk； 避免引入巨大的Library； 使用Proguard进行混淆、压缩。 &#160; &#160; &#160; &#160;参考： Android性能优化典范 Manage Your App’s Memory 小结&#160; &#160; &#160; &#160;最近还是很忙，主要是生活上有多多事情需要处理，所以没空闲时间更新博客了，唉。。。。。。本篇没有妹子，见谅~~]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android SurfaceFlinger 学习之路(一)----Android图形显示之HAL层Gralloc模块实现]]></title>
    <url>%2F2017%2F03%2F12%2FAndroid-SurfaceFlinger-%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF-%E4%B8%80-Android%E5%9B%BE%E5%BD%A2%E6%98%BE%E7%A4%BA%E4%B9%8BHAL%E5%B1%82Gralloc%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;好久没更新了，主要是因为年前和年后太忙了，尤其是年后这一段时间，忙得把身体都忙坏了。。。。。。终于在我GG之前，提交了离职，总算赢得了一些空闲时间，赶紧养养身体。。。现在处于即将待业状态，正好有时间研究一下年前就想了解的surfaceflinger模块。。。不过最近我也看了下，surfaceflinger内容太多太复杂了，也许这个系列我坚持不了多久，不能像上一个MediaPlayer系列那样。。。。。。同时，因为目前我处于即将待业状况，所以求好心HR或者大牛带我入新坑，感激不尽(~ToT~) 下载内核代码(准备工作)&#160; &#160; &#160; &#160;因为接下来分析需要用到部分kernel代码，所有我们需要先把kernel代码下载下来。 更换hosts文件&#160; &#160; &#160; &#160;因为天朝GFW的原因，所以不能直接访问google放置android source code的网站 https://source.android.com/ 。这个上面有很详细的教程，教大家下载、编译源码，和其他变动相关的知识。所以我们需要Fan Qiang。 &#160; &#160; &#160; &#160;如果买了VPN的土豪；找到免费vpn（GreenVPN、FreeVPN、hideVPN、rssVPN等等，好多都失效了）；还有可以去亚马逊申请一个免费服务器（一年），可以搭建自己的FQ服务器（这个需要绑定一张VISA信用卡，银联的不行），这里 有个教程，我上传到了自己博客服务器，大家可以下载下来；或者有稳定且流量大的FQ软件（比如赛风、自由门、蓝灯、畅游无限浏览器、影梭等）；或者有靠谱的hosts文件的，可以跳过这一步。 &#160; &#160; &#160; &#160;因为要拉kernel代码，文件比较大，所以我推荐更换hosts文件。这里推荐老D hosts，地址是 https://laod.cn/hosts/2017-google-hosts.html 。都是傻瓜式教程，应该都会。 下载kernel代码&#160; &#160; &#160; &#160;更换完hosts文件后，进入android源码官网，然后找到里面的Explore the Source标题下面描述里面的AOSP repository，点击进入源码仓库 https://android.googlesource.com/ 。&#160; &#160; &#160; &#160;1) 因为我们要下kernel源码，所以选择kenrnel/common,下载这一个通用代码；&#160; &#160; &#160; &#160;2) 用git下载代码：git clone https://android.googlesource.com/kernel/common ；&#160; &#160; &#160; &#160;3) 切换分支，我选了一个release版本：android-4.4-n-release，所以要git checkout -b android-4.4-n-release remotes/origin/android-4.4-n-release，然后pull一下，common文件夹下就有代码了。&#160; &#160; &#160; &#160;4) 最后将下载好的代码导入source insight，这一步就不用细说了。 初识Gralloc模块Gralloc模块简介&#160; &#160; &#160; &#160;Android设备的显示屏被抽象为一个帧缓冲区，而Android系统中的SurfaceFlinger服务就是通过向这个帧缓冲区写入内容来绘制应用程序的用户界面的。Android系统在硬件抽象层中提供了一个Gralloc模块，封装了对帧缓冲区的所有访问操作。&#160; &#160; &#160; &#160;Linux内核在启动的过程中会创建一个类别和名称分别为“graphics”和“fb0”的设备，用来描述系统中的第一个帧缓冲区，即第一个显示屏，其中，数字0表示从设备号。注意，系统中至少要存在一个显示屏，因此，名称为“fb0”的设备是肯定会存在的，否则的话，就是出错了。&#160; &#160; &#160; &#160;init进程在启动的过程中，会启动另外一个进程ueventd来管理系统的设备文件。当ueventd进程启动起来之后，会通过netlink接口来Linux内核通信，以便可以获得内核中的硬件设备变化通知。而当ueventd进程发现内核中创建了一个类型和名称分别为“graphics”和“fb0”的设备的时候，就会这个设备创建一个/dev/graphics/fb0设备文件。这样，用户空间的应用程序就可以通过设备文件/dev/graphics/fb0来访问内核中的帧缓冲区，即在设备的显示屏中绘制指定的画面。注意，用户空间的应用程序一般是通过内存映射的方式来访问设备文件/dev/graphics/fb0的。&#160; &#160; &#160; &#160;Android系统在硬件抽象层中提供了一个Gralloc模块，封装了对帧缓冲区的所有访问操作。用户空间的应用程序在使用帧缓冲区之间，首先要加载Gralloc模块，并且获得一个gralloc设备和一个fb设备。有了gralloc设备之后，用户空间中的应用程序就可以申请分配一块图形缓冲区，并且将这块图形缓冲区映射到应用程序的地址空间来，以便可以向里面写入要绘制的画面的内容。最后，用户空间中的应用程序就通过fb设备来将已经准备好了的图形缓冲区渲染到帧缓冲区中去，即将图形缓冲区的内容绘制到显示屏中去。相应地，当用户空间中的应用程序不再需要使用一块图形缓冲区的时候，就可以通过gralloc设备来释放它，并且将它从地址空间中解除映射。 代码结构&#160; &#160; &#160; &#160;Gralloc模块实现源码位于：hardware/libhardware/modules/gralloc ： &#160; &#160; &#160; &#160;|——– Android.mk&#160; &#160; &#160; &#160;|——– framebuffer.cpp&#160; &#160; &#160; &#160;|——– gralloc.cpp&#160; &#160; &#160; &#160;|——– gralloc_priv.h&#160; &#160; &#160; &#160;|——– gr.h&#160; &#160; &#160; &#160;|——– mapper.cpp 功能分析&#160; &#160; &#160; &#160;接下来，我们就按照上述使用情景来分析Gralloc模块的实现。 Gralloc模块的加载过程&#160; &#160; &#160; &#160;每一个HAL模块都有一个ID值，以这些ID值为参数来调用硬件抽象层提供的函数hw_get_module就可以将指定的模块加载到内存来，并且获得一个hw_module_t接口来打开相应的设备。&#160; &#160; &#160; &#160;Gralloc模块的ID值定义在hardware/libhardware/include/hardware/gralloc.h文件中，如下所示：1234/** * The id of this module */#define GRALLOC_HARDWARE_MODULE_ID "gralloc" &#160; &#160; &#160; &#160;函数hw_get_module实现在hardware/libhardware/hardware.c文件中，如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081int hw_get_module(const char *id, const struct hw_module_t **module)&#123; return hw_get_module_by_class(id, NULL, module);//调用hw_get_module_by_class函数&#125;//hw_get_module_by_class函数实现int hw_get_module_by_class(const char *class_id, const char *inst, const struct hw_module_t **module)&#123; int i; char prop[PATH_MAX]; char path[PATH_MAX]; char name[PATH_MAX]; //PATH_MAX为256 char prop_name[PATH_MAX]; if (inst)//NULL snprintf(name, PATH_MAX, "%s.%s", class_id, inst); else strlcpy(name, class_id, PATH_MAX);//字符串拷贝 /* * Here we rely on the fact that calling dlopen multiple times on * the same .so will simply increment a refcount (and not load * a new copy of the library). * We also assume that dlopen() is thread-safe. */ //我们依赖的事实是，调用dlopen方法多次打开相同的so库会简单的增加引用计数(而不会重复加载多次库的拷贝) //我们认为dlopen方法是线程安全的 /* First try a property specific to the class and possibly instance */ //先查找确定系统属性的值是否为空 snprintf(prop_name, sizeof(prop_name), "ro.hardware.%s", name); if (property_get(prop_name, prop, NULL) &gt; 0) &#123; //如果不为空，则找到属性对应的so库是否存在 if (hw_module_exists(path, sizeof(path), name, prop) == 0) &#123; goto found; &#125; &#125; /* Loop through the configuration variants looking for a module */ //如果上面没找到指定的so库，则循环遍历指定数组里的属性，然后找到第一个存在的so并跳出 for (i=0 ; i&lt;HAL_VARIANT_KEYS_COUNT; i++) &#123; if (property_get(variant_keys[i], prop, NULL) == 0) &#123; continue; &#125; if (hw_module_exists(path, sizeof(path), name, prop) == 0) &#123; goto found; &#125; &#125; /* Nothing found, try the default */ //如果第二步查找也没找到，就用默认的default属性 if (hw_module_exists(path, sizeof(path), name, "default") == 0) &#123; goto found; &#125; return -ENOENT;found: /* load the module, if this fails, we're doomed, and we should not try * to load a different variant. */ //如果我们找到了相关so，则加载它 return load(class_id, path, module);&#125;/** * There are a set of variant filename for modules. The form of the filename * is "&lt;MODULE_ID&gt;.variant.so" so for the led module the Dream variants * of base "ro.product.board", "ro.board.platform" and "ro.arch" would be: * * led.trout.so * led.msm7k.so * led.ARMV6.so * led.default.so */static const char *variant_keys[] = &#123; "ro.hardware", /* This goes first so that it can pick up a different file on the emulator. */ "ro.product.board", "ro.board.platform", "ro.arch"&#125;; &#160; &#160; &#160; &#160;上面流程我们分步来看： 查询对应动态库&#160; &#160; &#160; &#160;1）先查找确定系统属性的值是否为空，property_get函数到系统属性，可以在shell环境下用getprop ro.hardware.%s找到ro.hardware.%s属性，最后一个字符串自己指定；然后调用hw_module_exists函数顺着/system/lib/hw和/vendor/lib/hw或者/system/lib64/hw和/vendor/lib64/hw查找相应so库是否存在，我们看看hw_module_exists函数：12345678910111213141516171819202122232425262728293031/* * Check if a HAL with given name and subname exists, if so return 0, otherwise * otherwise return negative. On success path will contain the path to the HAL. */ //检查HAL层给的文件名是否存在，如果存在返回0，否则返回负无穷 //如果成功了，会把这个路径包含进HALstatic int hw_module_exists(char *path, size_t path_len, const char *name, const char *subname)&#123; snprintf(path, path_len, "%s/%s.%s.so", HAL_LIBRARY_PATH2, name, subname); if (access(path, R_OK) == 0) return 0; snprintf(path, path_len, "%s/%s.%s.so", HAL_LIBRARY_PATH1, name, subname); if (access(path, R_OK) == 0) return 0; return -ENOENT;&#125;/** Base path of the hal modules *///主要在下面目录下找相关so库#if defined(__LP64__)#define HAL_LIBRARY_PATH1 "/system/lib64/hw"#define HAL_LIBRARY_PATH2 "/vendor/lib64/hw"#else#define HAL_LIBRARY_PATH1 "/system/lib/hw"#define HAL_LIBRARY_PATH2 "/vendor/lib/hw"#endif &#160; &#160; &#160; &#160;2）如果第一步GG了，就进入第二步，则循环遍历指定数组里的属性，然后找到第一个存在的so并跳出。&#160; &#160; &#160; &#160;函数hw_get_module依次在目录/system/lib/hw和/vendor/lib/hw中(或64位对应目录)查找一个名称为”.variant.so”的文件，其中，&lt; MODULE_ID &gt;是一个模块ID，而variant表示”ro.hardware”、”ro.product.board”、”ro.board.platform”和”ro.arch”四个系统属性值之一。例如，对于Gralloc模块来说，函数hw_get_module依次在目录/system/lib/hw和/vendor/lib/hw中检查是否存在以下四个文件： gralloc.&lt; ro.hardware &gt;.so gralloc.&lt; ro.product.board &gt;.so gralloc.&lt; ro.board.platform &gt;.so gralloc.&lt; ro.arch &gt;.so &#160; &#160; &#160; &#160;只要其中的一个文件存在， 函数hw_get_module就会停止查找过程，并且调用另外一个函数load来将这个文件加载到内存中来。 &#160; &#160; &#160; &#160;3）如果第二步也GG了，就进入第三步，如果第二步查找也没找到，就用默认的default属性。如果在/system/lib/hw和/vendor/lib/hw中均不存这些文件，那么函数hw_get_module就会在目录/system/lib/hw中查找是否存在一个名称为gralloc.default.so的文件。如果存在的话，那么也会调用函数load将它加载到内存中来。 &#160; &#160; &#160; &#160;4）找到对应的so库后，就要调用load函数将他加载到内存中。 加载对应动态库&#160; &#160; &#160; &#160;函数load也是实现在文件hardware/libhardware/hardware.c文件中，如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * Load the file defined by the variant and if successful * return the dlopen handle and the hmi. * @return 0 = success, !0 = failure. */static int load(const char *id, const char *path, const struct hw_module_t **pHmi)&#123; int status; void *handle; struct hw_module_t *hmi; /* * load the symbols resolving undefined symbols before * dlopen returns. Since RTLD_GLOBAL is not or'd in with * RTLD_NOW the external symbols will not be global */ handle = dlopen(path, RTLD_NOW);//用dlopen函数将对应so库加载到内存中 if (handle == NULL) &#123; char const *err_str = dlerror(); ALOGE("load: module=%s\n%s", path, err_str?err_str:"unknown"); status = -EINVAL; goto done; &#125; /* Get the address of the struct hal_module_info. */ //调用函数dlsym来获得它所导出的符号HMI const char *sym = HAL_MODULE_INFO_SYM_AS_STR; hmi = (struct hw_module_t *)dlsym(handle, sym); if (hmi == NULL) &#123; ALOGE("load: couldn't find symbol %s", sym); status = -EINVAL; goto done; &#125; /* Check that the id matches */ //检查id是否匹配 if (strcmp(id, hmi-&gt;id) != 0) &#123; ALOGE("load: id=%s != hmi-&gt;id=%s", id, hmi-&gt;id); status = -EINVAL; goto done; &#125; hmi-&gt;dso = handle; /* success */ status = 0; done: if (status != 0) &#123; hmi = NULL; if (handle != NULL) &#123; dlclose(handle); handle = NULL; &#125; &#125; else &#123; ALOGV("loaded HAL id=%s path=%s hmi=%p handle=%p", id, path, *pHmi, handle); &#125; *pHmi = hmi; return status;&#125; &#160; &#160; &#160; &#160;在Linux系统中，后缀名为”so”的文件为动态链接库文件，可能通过函数dlopen来加载到内存中。硬件抽象层模块编写规范规定每一个硬件抽象层模块都必须导出一个符号名称为HAL_MODULE_INFO_SYM_AS_STR的符号，而且这个符号必须是用来描述一个类型为hw_module_t的结构体的。&#160; &#160; &#160; &#160;HAL_MODULE_INFO_SYM_AS_STR是一个宏，定义在文件hardware/libhardware/include/hardware/hardware.h文件中，如下所示：1234/** * Name of the hal_module_info as a string */#define HAL_MODULE_INFO_SYM_AS_STR "HMI" &#160; &#160; &#160; &#160;将Gralloc模块加载到内存中来之后，就可以调用函数dlsym来获得它所导出的符号HMI。由于这个符号指向的是一个hw_module_t结构体，因此，最后函数load就可以强制地将这个符号转换为一个hw_module_t结构体指针，并且保存在输出参数pHmi中返回给调用者。调用者获得了这个hw_module_t结构体指针之后，就可以创建一个gralloc设备或者一个fb设备。 数据结构定义&#160; &#160; &#160; &#160;模块Gralloc实现在目录hardware/libhardware/modules/gralloc中，它导出的符号HMI定义在文件hardware/libhardware/modules/gralloc/gralloc.cpp文件中，如下所示：123456789101112131415161718192021222324252627static struct hw_module_methods_t gralloc_module_methods = &#123; .open = gralloc_device_open&#125;;struct private_module_t HAL_MODULE_INFO_SYM = &#123; .base = &#123; .common = &#123; .tag = HARDWARE_MODULE_TAG, .version_major = 1, .version_minor = 0, .id = GRALLOC_HARDWARE_MODULE_ID, .name = "Graphics Memory Allocator Module", .author = "The Android Open Source Project", .methods = &amp;gralloc_module_methods &#125;, .registerBuffer = gralloc_register_buffer, .unregisterBuffer = gralloc_unregister_buffer, .lock = gralloc_lock, .unlock = gralloc_unlock, &#125;, .framebuffer = 0, .flags = 0, .numBuffers = 0, .bufferMask = 0, .lock = PTHREAD_MUTEX_INITIALIZER, .currentBuffer = 0,&#125;; &#160; &#160; &#160; &#160;HAL_MODULE_INFO_SYM也是一个宏，它的值是与宏HAL_MODULE_INFO_SYM_AS_STR对应的，它也是定义在文件hardware/libhardware/include/hardware/hardware.h文件中，如下所示：1234/** * Name of the hal_module_info */#define HAL_MODULE_INFO_SYM HMI &#160; &#160; &#160; &#160;符号HAL_MODULE_INFO_SYM的类型为private_module_t。前面提到，符号HAL_MODULE_INFO_SYM必须指向一个hw_module_t结构体，但是这里它指向的却是一个private_module_t结构体，是不是有问题呢？为了弄清楚这个问题，我们首先了解一下结构体private_module_t的定义，如下图： &#160; &#160; &#160; &#160;结构体private_module_t的第一个成员变量base指向一个gralloc_module_t结构体，而gralloc_module_t结构体的第一个成员变量common又指向了一个hw_module_t结构体，这意味着，指向一个private_module_t结构体的指针同时可以用作一个gralloc_module_t或者hw_module_t结构体提针来使用。事实上，这是使用C语言来实现的一种继承关系，等价于结构体private_module_t继承结构体gralloc_module_t，而结构体gralloc_module_t继承hw_module_t结构体。这样，我们就可以把在Gralloc模块中定义的符号HAL_MODULE_INFO_SYM看作是一个hw_module_t结构体。 &#160; &#160; &#160; &#160;hw_module_t结构体有一个重要的成员变量methods，它的类型为hw_module_methods_t，它用来描述一个HAL模块的操作方法列表。结构体hw_module_methods_t只定义有一个操作方法open，用来打开一个指定的设备。在Gralloc模块中，用来打开指定设备的函数被指定为gralloc_device_open，通过这个函数就可以打开Gralloc模块中的gralloc或者fb设备，后面我们再详细分析。 &#160; &#160; &#160; &#160;1）结构体gralloc_module_t定义在文件hardware/libhardware/include/hardware/gralloc.h中，它主要是定义了四个用来操作图形缓冲区的成员函数，如下所示：1234567891011121314typedef struct gralloc_module_t &#123; struct hw_module_t common; //映射一块图形缓冲区到一个进程的地址空间去 int (*registerBuffer)(struct gralloc_module_t const* module,buffer_handle_t handle); //取消映射一块图形缓冲区到一个进程的地址空间去 int (*unregisterBuffer)(struct gralloc_module_t const* module,buffer_handle_t handle); //锁定一个指定的图形缓冲区 int (*lock)(struct gralloc_module_t const* module,buffer_handle_t handle, int usage, int l, int t, int w, int h,void** vaddr); //解锁一个指定的图形缓冲区 int (*unlock)(struct gralloc_module_t const* module,buffer_handle_t handle); int (*perform)(struct gralloc_module_t const* module,int operation, ... ); void* reserved_proc[3]; &#125; gralloc_module_t; 成员函数registerBuffer和unregisterBuffer分别用来注册和注销一个指定的图形缓冲区，这个指定的图形缓冲区使用一个buffer_handle_t句柄来描述。所谓注册图形缓冲区，实际上就是将一块图形缓冲区映射到一个进程的地址空间去，而注销图形缓冲区就是执行相反的操作。 成员函数lock和unlock分别用来锁定和解锁一个指定的图形缓冲区，这个指定的图形缓冲区同样是使用一个buffer_handle_t句柄来描述。在访问一块图形缓冲区的时候，例如，向一块图形缓冲写入内容的时候，需要将该图形缓冲区锁定，用来避免访问冲突。在锁定一块图形缓冲区的时候，可以指定要锁定的图形绘冲区的位置以及大小，这是通过参数l、t、w和h来指定的，其中，参数l和t指定的是要访问的图形缓冲区的左上角位置，而参数w和h指定的是要访问的图形缓冲区的宽度和长度。锁定之后，就可以获得由参数参数l、t、w和h所圈定的一块缓冲区的起始地址，保存在输出参数vaddr中。另一方面，在访问完成一块图形缓冲区之后，需要解除这块图形缓冲区的锁定。 在Gralloc模块中，符号HAL_MODULE_INFO_SYM指向的gralloc结构体的成员函数registerBuffer、unregisterBuffer、lock和unlock分别被指定为函数gralloc_register_buffer、gralloc_unregister_buffer、gralloc_lock和gralloc_unlock，后面我们再详细分析它们的实现。 &#160; &#160; &#160; &#160;2）结构体private_module_t定义在文件hardware/libhardware/modules/gralloc/gralloc_priv.h中，它主要是用来描述帧缓冲区的属性，如下所示：12345678910111213141516struct private_module_t &#123; gralloc_module_t base; private_handle_t* framebuffer; //指向系统帧缓冲区的句柄 uint32_t flags; //用来标志系统帧缓冲区是否支持双缓冲 uint32_t numBuffers;//表示系统帧缓冲区包含有多少个图形缓冲区 uint32_t bufferMask; //记录系统帧缓冲区中的图形缓冲区的使用情况 pthread_mutex_t lock; //一个互斥锁，用来保护结构体private_module_t的并行访问 buffer_handle_t currentBuffer; //用来描述当前正在被渲染的图形缓冲区 int pmem_master; void* pmem_master_base; struct fb_var_screeninfo info; //保存设备显示屏的动态属性信息 struct fb_fix_screeninfo finfo; ////保存设备显示屏的固定属性信息 float xdpi; //描述设备显示屏在宽度 float ydpi; //描述设备显示屏在高度 float fps; //用来描述显示屏的刷新频率 &#125;; 成员变量framebuffer的类型为private_handle_t，它是一个指向系统帧缓冲区的句柄，后面我们再分析结构体private_handle_t的定义。 成员变量flags用来标志系统帧缓冲区是否支持双缓冲。如果支持的话，那么它的PAGE_FLIP位就等于1，否则的话，就等于0。 成员变量numBuffers表示系统帧缓冲区包含有多少个图形缓冲区。一个帧缓冲区包含有多少个图形缓冲区是与它的可视分辨率以及虚拟分辨率的大小有关的。例如，如果一个帧缓冲区的可视分辨率为800 x 600，而虚拟分辨率为1600 x 600，那么这个帧缓冲区就可以包含有两个图形缓冲区。 成员变量bufferMask用来记录系统帧缓冲区中的图形缓冲区的使用情况。例如，假设系统帧缓冲区有两个图形缓冲区，这时候成员变量bufferMask就有四种取值，分别是二进制的00、01、10和11，其中，00分别表示两个图缓冲区都是空闲的，01表示第1个图形缓冲区已经分配出去，而第2个图形缓冲区是空闲的，10表示第1个图形缓冲区是空闲的，而第2个图形缓冲区已经分配出去，11表示两个图缓冲区都已经分配出去。 成员变量lock是一个互斥锁，用来保护结构体private_module_t的并行访问。 成员变量currentBuffer的类型为buffer_handle_t，用来描述当前正在被渲染的图形缓冲区，后面我们再分析它的定义。 成员变量pmem_master和pmem_master_base目前没有使用。 成员变量info和finfo的类型分别为fb_var_screeninfo和fb_fix_screeninfo，它们用来保存设备显示屏的属性信息，其中，成员变量info保存的属性信息是可以动态设置的，而成员变量finfo保存的属性信息是只读的。这两个成员变量的值可以通过IO控制命令FBIOGET_VSCREENINFO和FBIOGET_FSCREENINFO来从帧缓冲区驱动模块中获得。 成员变量xdpi和ydpi分别用来描述设备显示屏在宽度和高度上的密度，即每英寸有多少个像素点。 成员变量fps用来描述显示屏的刷新频率，它的单位的fps，即每秒帧数。 &#160; &#160; &#160; &#160;3）接下来， 我们再分析结构体buffer_handle_t的定义。结构体buffer_handle_t定义在文件hardware/libhardware/include/hardware/gralloc.h文件中，如下所示： 1typedef const native_handle* buffer_handle_t; &#160; &#160; &#160; &#160;它是一个类型为native_handle_t的指针，而结构体native_handle_t用来描述一个本地句柄值，它定义在系统运行时层的文件system/core/include/cutils/native_handle.h文件中，如下所示：1234567typedef struct native_handle &#123; int version; //设置为结构体native_handle_t的大小，用来标识结构体native_handle_t的版本 int numFds; //表示结构体native_handle_t所包含的文件描述符的个数，这些文件描述符保存在成员变量data所指向的一块缓冲区中。 int numInts; //表示结构体native_handle_t所包含的整数值的个数，这些整数保存在成员变量data所指向的一块缓冲区中。 int data[0]; //指向的一块缓冲区中 &#125; native_handle_t; 成员变量version的大小被设置为结构体native_handle_t的大小，用来标识结构体native_handle_t的版本。 成员变量numFds和numInts表示结构体native_handle_t所包含的文件描述符以及整数值的个数，这些文件描述符和整数保存在成员变量data所指向的一块缓冲区中。 &#160; &#160; &#160; &#160;4）我们一般不直接使用native_handle_t结构体来描述一个本地句柄值，而是通过它的子类来描述一个具体的本地句柄值。接下来我们就通过结构体private_handle_t的定义来说明native_handle_t结构体的用法。&#160; &#160; &#160; &#160;结构体private_handle_t用来描述一块图形缓冲区，这块图形缓冲区可能是在帧缓冲区中分配的，也可能是在内存中分配的，视具体情况而定，它定义在文件hardware/libhardware/modules/gralloc/gralloc_priv.h文件中，如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#ifdef __cplusplus struct private_handle_t : public native_handle &#123; #else struct private_handle_t &#123; struct native_handle nativeHandle; #endif enum &#123; PRIV_FLAGS_FRAMEBUFFER = 0x00000001 &#125;; // file-descriptors int fd; ////指向一个文件描述符，这个文件描述符要么指向帧缓冲区设备，要么指向一块匿名共享内存 // ints int magic; //指向一个魔数，它的值由静态成员变量sMagic来指定，用来标识一个private_handle_t结构 int flags; //用来描述一个缓冲区的标志，它的值要么等于0，要么等于PRIV_FLAGS_FRAMEBUFFER int size; //用来描述一个缓冲区的大小。 int offset; //用来描述一个缓冲区的偏移地址。 // FIXME: the attributes below should be out-of-line int base; //用来描述一个缓冲区的实际地址，它是通过成员变量offset来计算得到的。 int pid; //用来描述一个缓冲区的创建者的PID。 #ifdef __cplusplus static const int sNumInts = 6; //包含有6个整数 static const int sNumFds = 1; //包含有1个文件描述符 static const int sMagic = 0x3141592; private_handle_t(int fd, int size, int flags) : fd(fd), magic(sMagic), flags(flags), size(size), offset(0), base(0), pid(getpid()) &#123; version = sizeof(native_handle); numInts = sNumInts; numFds = sNumFds; &#125; ~private_handle_t() &#123; magic = 0; &#125; static int validate(const native_handle* h) &#123; const private_handle_t* hnd = (const private_handle_t*)h; if (!h || h-&gt;version != sizeof(native_handle) || h-&gt;numInts != sNumInts || h-&gt;numFds != sNumFds || hnd-&gt;magic != sMagic) &#123; LOGE("invalid gralloc handle (at %p)", h); return -EINVAL; &#125; return 0; &#125; #endif &#125;; &#160; &#160; &#160; &#160;为了方便描述，我们假设我们是在C++环境中编译文件gralloc_priv.h，即编译环境定义有宏__cplusplus。这样，结构体private_handle_t就是从结构体native_handle_t继承下来的，它包含有1个文件描述符以及6个整数，以及三个静态成员变量。 成员变量fd指向一个文件描述符，这个文件描述符要么指向帧缓冲区设备，要么指向一块匿名共享内存，取决于它的宿主结构体private_handle_t描述的一个图形缓冲区是在帧缓冲区分配的，还是在内存中分配的。 成员变量magic指向一个魔数，它的值由静态成员变量sMagic来指定，用来标识一个private_handle_t结构体。 成员变量flags用来描述一个图形缓冲区的标志，它的值要么等于0，要么等于PRIV_FLAGS_FRAMEBUFFER。当一个图形缓冲区的标志值等于PRIV_FLAGS_FRAMEBUFFER的时候，就表示它是在帧缓冲区中分配的。 成员变量size用来描述一个图形缓冲区的大小。 成员变量offset用来描述一个图形缓冲区的偏移地址。例如，当一个图形缓冲区是在一块内存中分块的时候，假设这块内存的地址为start，那么这个图形缓冲区的起始地址就为start + offset。 成员变量base用来描述一个图形缓冲区的实际地址，它是通过成员变量offset来计算得到的。例如，上面计算得到的start + offset的值就保存在成员变量base中。 成员变量pid用来描述一个图形缓冲区的创建者的PID。例如，如果一个图形缓冲区是在ID值为1000的进程中创建的，那么用来描述这个图形缓冲区的private_handle_t结构体的成员变量pid的值就等于1000。 &#160; &#160; &#160; &#160;结构体private_handle_t的静态成员变量sMagic前面已经描述过了，另外两个静态成员变量sNumInts和sNumFds的值分别等于1和6，表示结构体private_handle_t包含有1个文件描述符和6个整数，它们是用来初始化结构体private_handle_t的父类native_handle_t的成员变量numInts和numFds的，如结构体private_handle_t的构造函数所示。从这里就可以看出，结构体private_handle_t的父类native_handle_t的成员变量data所指向的缓冲区就是由结构体private_handle_t的成员变量fds、magic、flags、size、offset、base和pid所占用的连续内存块来组成的，一共包含有7个整数。&#160; &#160; &#160; &#160;结构体private_handle_t还定义了一个静态成员函数validate，用来验证一个native_handle_t指针是否指向了一个private_handle_t结构体。 &#160; &#160; &#160; &#160;至此，Gralloc模块的加载过程以及相关的数据结构体就介绍到这里，接下来我们分别分析定义在Gralloc模块中的gralloc和fb设备的打开过程。 Gralloc设备的打开过程&#160; &#160; &#160; &#160;在Gralloc模块中，gralloc设备的ID值定义为GRALLOC_HARDWARE_GPU0。GRALLOC_HARDWARE_GPU0是一个宏，定义在文件hardware/libhardware/include/hardware/gralloc.h中， 如下所示：12345/** * Name of the graphics device to open */#define GRALLOC_HARDWARE_GPU0 "gpu0" &#160; &#160; &#160; &#160;gralloc设备使用结构体alloc_device_t 来描述。结构体alloc_device_t有两个成员函数alloc和free，分别用来分配和释放图形缓冲区。&#160; &#160; &#160; &#160;结构体alloc_device_t 也是定义在文件hardware/libhardware/include/hardware/gralloc.h中， 如下所示：1234567891011typedef struct alloc_device_t &#123; struct hw_device_t common; int (*alloc)(struct alloc_device_t* dev, int w, int h, int format, int usage, buffer_handle_t* handle, int* stride); int (*free)(struct alloc_device_t* dev, buffer_handle_t handle); &#125; alloc_device_t; &#160; &#160; &#160; &#160;Gralloc模块在在文件hardware/libhardware/include/hardware/gralloc.h中定义了一个帮助函数gralloc_open，用来打开gralloc设备，如下所示：1234567/** convenience API for opening and closing a supported device */static inline int gralloc_open(const struct hw_module_t* module, struct alloc_device_t** device) &#123; return module-&gt;methods-&gt;open(module, GRALLOC_HARDWARE_GPU0, (struct hw_device_t**)device);&#125; &#160; &#160; &#160; &#160;参数module指向的是一个用来描述Gralloc模块的hw_module_t结构体，它的成员变量methods所指向的一个hw_module_methods_t结构体的成员函数open指向了Gralloc模块中的函数gralloc_device_open。 &#160; &#160; &#160; &#160;函数gralloc_device_open定义在文件hardware/libhardware/modules/gralloc/gralloc.cpp文件中，如下所示：12345678910111213141516171819202122232425262728293031323334struct gralloc_context_t &#123; alloc_device_t device; /* our private data here */ &#125;; ...... int gralloc_device_open(const hw_module_t* module, const char* name, hw_device_t** device) &#123; int status = -EINVAL; if (!strcmp(name, GRALLOC_HARDWARE_GPU0)) &#123; gralloc_context_t *dev; dev = (gralloc_context_t*)malloc(sizeof(*dev)); /* initialize our state here */ memset(dev, 0, sizeof(*dev)); /* initialize the procs */ dev-&gt;device.common.tag = HARDWARE_DEVICE_TAG; dev-&gt;device.common.version = 0; dev-&gt;device.common.module = const_cast&lt;hw_module_t*&gt;(module); dev-&gt;device.common.close = gralloc_close; dev-&gt;device.alloc = gralloc_alloc; dev-&gt;device.free = gralloc_free; *device = &amp;dev-&gt;device.common; status = 0; &#125; ...... return status; &#125; &#160; &#160; &#160; &#160;这个函数主要是用来创建一个gralloc_context_t结构体，并且对它的成员变量device进行初始化。结构体gralloc_context_t的成员变量device的类型为gralloc_device_t，它用来描述一个gralloc设备。前面提到，gralloc设备是用来分配和释放图形缓冲区的，这是通过调用它的成员函数alloc和free来实现的。从这里可以看出，函数gralloc_device_open所打开的gralloc设备的成员函数alloc和free分别被设置为Gralloc模块中的函数gralloc_alloc和gralloc_free，后面我们再详细分析它们的实现。 &#160; &#160; &#160; &#160;至此，gralloc设备的打开过程就分析完成了，接下来我们继续分析fb设备的打开过程。 fb设备的打开过程&#160; &#160; &#160; &#160;在Gralloc模块中，fb设备的ID值定义为GRALLOC_HARDWARE_FB0。GRALLOC_HARDWARE_FB0是一个宏，定义在文件hardware/libhardware/include/hardware/fb.h中， 如下所示：1#define GRALLOC_HARDWARE_FB0 "fb0" &#160; &#160; &#160; &#160;fb设备使用结构体framebuffer_device_t 来描述。结构体framebuffer_device_t是用来描述系统帧缓冲区的信息，它定义在文件hardware/libhardware/include/hardware/fb.h中， 如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344typedef struct framebuffer_device_t &#123; struct hw_device_t common; /* flags describing some attributes of the framebuffer */ const uint32_t flags; /* dimensions of the framebuffer in pixels */ const uint32_t width; const uint32_t height; /* frambuffer stride in pixels */ const int stride; /* framebuffer pixel format */ const int format; /* resolution of the framebuffer's display panel in pixel per inch*/ const float xdpi; const float ydpi; /* framebuffer's display panel refresh rate in frames per second */ const float fps; /* min swap interval supported by this framebuffer */ const int minSwapInterval; /* max swap interval supported by this framebuffer */ const int maxSwapInterval; int reserved[8]; int (*setSwapInterval)(struct framebuffer_device_t* window, int interval); int (*setUpdateRect)(struct framebuffer_device_t* window, int left, int top, int width, int height); int (*post)(struct framebuffer_device_t* dev, buffer_handle_t buffer); int (*compositionComplete)(struct framebuffer_device_t* dev); void* reserved_proc[8]; &#125; framebuffer_device_t; 成员变量flags用来记录系统帧缓冲区的标志，目前没有使用这成员变量，它的值被设置为0。 成员变量width和height分别用来描述设备显示屏的宽度和高度，它们是以像素为单位的。 成员变量stride用来描述设备显示屏的一行有多少个像素点。 成员变量format用来描述系统帧缓冲区的像素格式，支持的像素格式主要有HAL_PIXEL_FORMAT_RGBX_8888和HAL_PIXEL_FORMAT_RGB_565两种。HAL_PIXEL_FORMAT_RGBX_8888表示一个像素使用32位来描述，R、G和B分别占8位，另外8位未使用。HAL_PIXEL_FORMAT_RGB_565表示一个像素使用16位来描述，R、G和B分别占5、6和5位。 成员变量xdpi和ydpi分别用来描述设备显示屏在宽度和高度上的密度，即每英寸有多少个像素点。 成员变量fps用来描述设备显示屏的刷新频率，它的单位是帧每秒。 成员变量minSwapInterval和maxSwapInterval用来描述帧缓冲区交换前后两个图形缓冲区的最小和最大时间间隔。 成员变量reserved是保留给将来使用的。 成员函数setSwapInterval用来设置帧缓冲区交换前后两个图形缓冲区的最小和最大时间间隔。 成员函数setUpdateRect用来设置帧缓冲区的更新区域。 成员函数post用来将图形缓冲区buffer的内容渲染到帧缓冲区中去，即显示在设备的显示屏中去。 成员函数compositionComplete用来通知fb设备device，图形缓冲区的组合工作已经完成，目前没有使用这个成员函数。 成员变量reserved是一个函数指针数组，它们是保留给将来使用的。 &#160; &#160; &#160; &#160;在结构体framebuffer_device_t的一系列成员函数中，post是最重要的一个成员函数，用户空间的应用程序通过调用这个成员函数就可以在设备的显示屏中渲染指定的画面，后面我们将详细讲这个函数的实现。 Gralloc模块在在文件hardware/libhardware/include/hardware/fb.h中定义了一个帮助函数framebuffer_open，用来打开fb设备，如下所示：1234567/** convenience API for opening and closing a supported device */static inline int framebuffer_open(const struct hw_module_t* module, struct framebuffer_device_t** device) &#123; return module-&gt;methods-&gt;open(module, GRALLOC_HARDWARE_FB0, (struct hw_device_t**)device);&#125; &#160; &#160; &#160; &#160;参数module指向的是一个用来描述Gralloc模块的hw_module_t结构体，前面提到，它的成员变量methods所指向的一个hw_module_methods_t结构体的成员函数open指向了Gralloc模块中的函数gralloc_device_open，这个函数打开fb设备的代码段如下所示：1234567891011int gralloc_device_open(const hw_module_t* module, const char* name, hw_device_t** device) &#123; int status = -EINVAL; if (!strcmp(name, GRALLOC_HARDWARE_GPU0)) &#123; ...... &#125; else &#123; status = fb_device_open(module, name, device); &#125; return status; &#125; &#160; &#160; &#160; &#160;参数name的值等于GRALLOC_HARDWARE_FB0，因此，函数gralloc_device_open接下来会调用另外一个函数fb_device_open来执行打开fb设备的操作。 &#160; &#160; &#160; &#160;函数fb_device_open定义在文件hardware/libhardware/modules/gralloc/framebuffer.cpp中，如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960struct fb_context_t &#123; framebuffer_device_t device; &#125;; ...... int fb_device_open(hw_module_t const* module, const char* name, hw_device_t** device) &#123; int status = -EINVAL; //判断打开的是fb设备 if (!strcmp(name, GRALLOC_HARDWARE_FB0)) &#123; alloc_device_t* gralloc_device; //打开gpu设备 status = gralloc_open(module, &amp;gralloc_device); if (status &lt; 0) return status; /* initialize our state here */ //创建一个fb_context_t对象，用来描述fb设备上下文 fb_context_t *dev = (fb_context_t*)malloc(sizeof(*dev)); memset(dev, 0, sizeof(*dev)); /* initialize the procs */ //初始化fb_context_t对象 dev-&gt;device.common.tag = HARDWARE_DEVICE_TAG; dev-&gt;device.common.version = 0; dev-&gt;device.common.module = const_cast&lt;hw_module_t*&gt;(module); //注册fb设备的操作函数 dev-&gt;device.common.close = fb_close; dev-&gt;device.setSwapInterval = fb_setSwapInterval; dev-&gt;device.post = fb_post; dev-&gt;device.setUpdateRect = 0; private_module_t* m = (private_module_t*)module; //将fb映射到当前进程地址空间 status = mapFrameBuffer(m); if (status &gt;= 0) &#123; int stride = m-&gt;finfo.line_length / (m-&gt;info.bits_per_pixel &gt;&gt; 3); int format = (m-&gt;info.bits_per_pixel == 32) ? HAL_PIXEL_FORMAT_RGBX_8888 : HAL_PIXEL_FORMAT_RGB_565; #ifdef NO_32BPP format = HAL_PIXEL_FORMAT_RGB_565; #endif const_cast&lt;uint32_t&amp;&gt;(dev-&gt;device.flags) = 0; const_cast&lt;uint32_t&amp;&gt;(dev-&gt;device.width) = m-&gt;info.xres; const_cast&lt;uint32_t&amp;&gt;(dev-&gt;device.height) = m-&gt;info.yres; const_cast&lt;int&amp;&gt;(dev-&gt;device.stride) = stride; const_cast&lt;int&amp;&gt;(dev-&gt;device.format) = format; const_cast&lt;float&amp;&gt;(dev-&gt;device.xdpi) = m-&gt;xdpi; const_cast&lt;float&amp;&gt;(dev-&gt;device.ydpi) = m-&gt;ydpi; const_cast&lt;float&amp;&gt;(dev-&gt;device.fps) = m-&gt;fps; const_cast&lt;int&amp;&gt;(dev-&gt;device.minSwapInterval) = 1; const_cast&lt;int&amp;&gt;(dev-&gt;device.maxSwapInterval) = 1; *device = &amp;dev-&gt;device.common; &#125; &#125; return status; &#125; &#160; &#160; &#160; &#160;1）这个函数主要是用来创建一个fb_context_t结构体，并且对它的成员变量device进行初始化。结构体fb_context_t的成员变量device的类型为framebuffer_device_t，前面提到，它是用来描述fb设备的。fb设备主要是用来渲染图形缓冲区的，这是通过调用它的成员函数post来实现的。从这里可以看出，函数fb_device_open所打开的fb设备的成员函数post被设置为Gralloc模块中的函数fb_post，后面我们再详细分析它的实现。 &#160; &#160; &#160; &#160;2）函数fb_device_open在打开fb设备的过程中，会调用另外一个函数mapFrameBuffer来获得系统帧缓冲区的信息，并且将这些信息保存在参数module所描述的一个private_module_t结构体的各个成员变量中。有了系统帧缓冲区的信息之后，函数fb_device_open接下来就可以对前面所打开的一个fb设备的各个成员变量进行初始化。这些成员变量的含义可以参考前面对结构体framebuffer_device_t的介绍。接下来我们只简单介绍一下结构体framebuffer_device_t的成员变量stride和format的初始化过程。 &#160; &#160; &#160; &#160;3）变量m的成员变量finfo的类型为fb_fix_screeninfo，它是在函数mapFrameBuffer中被始化的。fb_fix_screeninfo是在内核中定义的一个结构体，用来描述设备显示屏的固定属性信息，其中，它的成员变量line_length用来描述显示屏一行像素总共所占用的字节数。&#160; &#160; &#160; &#160;变量m的另外一个成员变量info的类型为fb_var_screeninfo，它也是在函数mapFrameBuffer中被始化的。fb_var_screeninfo也是内核中定义的一个结构体，用来描述可以动态设置的显示屏属性信息，其中，它的成员变量bits_per_pixel用来描述显示屏每一个像素所占用的位数。&#160; &#160; &#160; &#160;这样，我们将m-&gt;info.bits_per_pixel的值向右移3位，就可以得到显示屏每一个像素所占用的字节数。用显示屏每一个像素所占用的字节数去除显示屏一行像素总共所占用的字节数m-&gt;finfo.line_length，就可以得到显示屏一行有多少个像素点。这个值最终就可以保存在前面所打开的fb设备的成员变量stride中。&#160; &#160; &#160; &#160;当显示屏每一个像素所占用的位数等于32的时候，那么前面所打开的fb设备的像素格式format就会被设置为HAL_PIXEL_FORMAT_RGBX_8888，否则的话，就会被设置为HAL_PIXEL_FORMAT_RGB_565。另一方面，如果在编译的时候定义了NO_32BPP宏，即不要使用32位来描述一个像素，那么函数fb_device_open就会强制将前面所打开的fb设备的像素格式format设置为HAL_PIXEL_FORMAT_RGB_565。 &#160; &#160; &#160; &#160;3）函数mapFrameBuffer除了用来获得系统帧缓冲区的信息之外，还会将系统帧缓冲区映射到当前进程的地址空间来。在Android系统中，Gralloc模块中的fb设备是由SurfaceFlinger服务来负责打开和管理的，而SurfaceFlinger服是运行System进程中的，因此，系统帧缓冲区实际上是映射到System进程的地址空间中的。 &#160; &#160; &#160; &#160;函数mapFrameBuffer实现在文件hardware/libhardware/modules/gralloc/framebuffer.cpp，如下所示：1234567static int mapFrameBuffer(struct private_module_t* module)&#123; pthread_mutex_lock(&amp;module-&gt;lock); int err = mapFrameBufferLocked(module); pthread_mutex_unlock(&amp;module-&gt;lock); return err;&#125; &#160; &#160; &#160; &#160;这个函数调用了同一个文件中的另外一个函数mapFrameBufferLocked来初始化参数module以及将系统帧缓冲区映射到当前进程的地址空间来。 映射内存过程&#160; &#160; &#160; &#160;函数mapFrameBufferLocked的实现比较长，我们分段来阅读：&#160; &#160; &#160; &#160;Part.1：1234567891011121314151617181920212223242526int mapFrameBufferLocked(struct private_module_t* module) &#123; // already initialized... if (module-&gt;framebuffer) &#123; return 0; &#125; char const * const device_template[] = &#123; "/dev/graphics/fb%u", "/dev/fb%u", 0 &#125;; int fd = -1; int i=0; char name[64]; while ((fd==-1) &amp;&amp; device_template[i]) &#123; snprintf(name, 64, device_template[i], 0); fd = open(name, O_RDWR, 0); i++; &#125; if (fd &lt; 0) return -errno; ...... &#125; &#160; &#160; &#160; &#160;这段代码在首先在系统中检查是否存在设备文件/dev/graphics/fb0或者/dev/fb0。如果存在的话，那么就调用函数open来打开它，并且将得到的文件描述符保存在变量fd中。这样，接下来函数mapFrameBufferLocked就可以通过文件描述符fd来与内核中的帧缓冲区驱动程序交互。 &#160; &#160; &#160; &#160;Part.2：1234567891011121314int mapFrameBufferLocked(struct private_module_t* module)&#123; ...Part.1... struct fb_fix_screeninfo finfo; if (ioctl(fd, FBIOGET_FSCREENINFO, &amp;finfo) == -1) return -errno; struct fb_var_screeninfo info; if (ioctl(fd, FBIOGET_VSCREENINFO, &amp;info) == -1) return -errno; ......&#125; &#160; &#160; &#160; &#160;这几行代码分别通过IO控制命令FBIOGET_FSCREENINFO和FBIOGET_VSCREENINFO来获得系统帧缓冲区的信息，分别保存在fb_fix_screeninfo结构体finfo和fb_var_screeninfo结构体info中。 &#160; &#160; &#160; &#160;Part.3：12345678910111213141516171819202122232425262728293031323334353637383940int mapFrameBufferLocked(struct private_module_t* module)&#123; ...Part.1... ...Part.2... info.reserved[0] = 0; info.reserved[1] = 0; info.reserved[2] = 0; info.xoffset = 0; info.yoffset = 0; info.activate = FB_ACTIVATE_NOW; /* * Request NUM_BUFFERS screens (at lest 2 for page flipping) */ info.yres_virtual = info.yres * NUM_BUFFERS; uint32_t flags = PAGE_FLIP;#if USE_PAN_DISPLAY // 0 if (ioctl(fd, FBIOPAN_DISPLAY, &amp;info) == -1) &#123; ALOGW("FBIOPAN_DISPLAY failed, page flipping not supported");#else if (ioctl(fd, FBIOPUT_VSCREENINFO, &amp;info) == -1) &#123; ALOGW("FBIOPUT_VSCREENINFO failed, page flipping not supported");#endif info.yres_virtual = info.yres; flags &amp;= ~PAGE_FLIP; &#125; if (info.yres_virtual &lt; info.yres * 2) &#123; // we need at least 2 for page-flipping info.yres_virtual = info.yres; flags &amp;= ~PAGE_FLIP; ALOGW("page flipping not supported (yres_virtual=%d, requested=%d)", info.yres_virtual, info.yres*2); &#125; ......&#125; &#160; &#160; &#160; &#160;这段代码主要是用来设置设备显示屏的虚拟分辨率。结构体fb_var_screeninfo的成员变量xres和yres用来描述显示屏的可视分辨率，而成员变量xres_virtual和yres_virtual用来描述显示屏的虚拟分辨率。这里保持可视分辨率以及虚拟分辨率的宽度值不变，而将虚拟分辨率的高度值设置为可视分辨率的高度值的NUM_BUFFERS倍。NUM_BUFFERS是一个宏，它的值被定义为2。这样，我们就可以将系统帧缓冲区划分为两个图形缓冲区来使用，即可以通过硬件来实现双缓冲技术。&#160; &#160; &#160; &#160;在结构体fb_var_screeninfo中，与显示屏的可视分辨率和虚拟分辨率相关的另外两个成员变量是xoffset和yoffset，它们用来告诉帧缓冲区当前要渲染的图形缓冲区是哪一个。 &#160; &#160; &#160; &#160;这段代码最终是通过IO控制命令FBIOPUT_VSCREENINFO来设置设备显示屏的虚拟分辨率以及像素格式的。如果设置失败，即调用函数ioctl的返回值等于-1，那么很可能是因为系统帧缓冲区在硬件上不支持双缓冲，因此，接下来的代码就会重新将显示屏的虚拟分辨率的高度值设置为可视分辨率的高度值，并且将变量flags的PAGE_FLIP位置为0。&#160; &#160; &#160; &#160;另一方面，如果调用函数ioctl成功，但是最终获得的显示屏的虚拟分辨率的高度值小于可视分辨率的高度值的2倍，那么也说明系统帧缓冲区在硬件上不支持双缓冲。在这种情况下，接下来的代码也会重新将显示屏的虚拟分辨率的高度值设置为可视分辨率的高度值，并且将变量flags的PAGE_FLIP位置为0。 &#160; &#160; &#160; &#160;Part.4：123456789101112131415161718192021222324252627int mapFrameBufferLocked(struct private_module_t* module)&#123; ...Part.1... ...Part.2... ...Part.3... if (ioctl(fd, FBIOGET_VSCREENINFO, &amp;info) == -1) return -errno; uint64_t refreshQuotient = ( uint64_t( info.upper_margin + info.lower_margin + info.yres ) * ( info.left_margin + info.right_margin + info.xres ) * info.pixclock ); /* Beware, info.pixclock might be 0 under emulation, so avoid a * division-by-0 here (SIGFPE on ARM) */ int refreshRate = refreshQuotient &gt; 0 ? (int)(1000000000000000LLU / refreshQuotient) : 0; if (refreshRate == 0) &#123; // bleagh, bad info from the driver refreshRate = 60*1000; // 60 Hz &#125; ......&#125; &#160; &#160; &#160; &#160;这段代码再次通过IO控制命令FBIOGET_VSCREENINFO来获得系统帧缓冲区的可变属性信息，并且保存在fb_var_screeninfo结构体info中，接下来再计算设备显示屏的刷新频率。 &#160; &#160; &#160; &#160;显示屏的刷新频率与显示屏的扫描时序相关。显示屏的扫描时序可以参考Kernel代码目录下的Documentation/fb/framebuffer.txt文件。我们结合这个txt文件中的一幅图来简单说明上述代码是如何计算显示屏的刷新频率的： &#160; &#160; &#160; &#160;中间由xres和yres组成的区域即为显示屏的图形绘制区，在绘制区的上、下、左和右分别有四个边距upper_margin、lower_margin、left_margin和right_margin。此外，在显示屏的最右边以及最下边还有一个水平同步区域hsync_len和一个垂直同步区域vsync_len。电子枪按照从左到右、从上到下的顺序来显示屏中打点，从而可以将要渲染的图形显示在屏幕中。前面所提到的区域信息分别保存在fb_var_screnninfo结构体info的成员变量xres、yres、upper_margin、lower_margin、left_margin、right_margin、hsync_len和vsync_len。 &#160; &#160; &#160; &#160;电子枪每在xres和yres所组成的区域中打一个点所花费的时间记录在fb_var_screnninfo结构体info的成员变量pixclock，单位为pico seconds，即10E-12秒。 &#160; &#160; &#160; &#160;电子枪从左到右扫描完成一行之后，都会处理关闭状态，并且会重新折回到左边去。由于电子枪在从右到左折回的过程中不需要打点，因此，这个过程会比从左到右扫描屏幕的过程要快，这个折回的时间大概就等于在xres和yres所组成的区域扫描（left_margin+right_margin）个点的时间。这样，我们就可以认为每渲染一行需要的时间为（xres + left_margin + right_margin）* pixclock。 &#160; &#160; &#160; &#160;同样，电子枪从上到下扫描完成显示屏之后，需要从右下角折回到左上角去，折回的时间大概等于在xres和yres所组成的区域中扫描（upper_margin + lower_margin）行所需要的时间。这样，我们就可以认为每渲染一屏图形所需要的时间等于在xres和yres所组成的区域中扫描（yres + upper_margin + lower_margin）行所需要的时间。由于在xres和yres所组成的区域中扫描一行所需要的时间为（xres + left_margin + right_margin） pixclock，因此，每渲染一屏图形所需要的总时间就等于（yres + upper_margin + lower_margin） （xres + left_margin + right_margin）* pixclock。 &#160; &#160; &#160; &#160;每渲染一屏图形需要的总时间经过计算之后，就保存在变量refreshQuotient中。注意，变量refreshQuotient所描述的时间的单位为1E-12秒。这样，将变量refreshQuotient的值倒过来，就可以得到设备显示屏的刷新频率。将这个频率值乘以10E15次方之后，就得到一个单位为10E-3 HZ的刷新频率，保存在变量refreshRate中。 &#160; &#160; &#160; &#160;当Android系统在模拟器运行的时候，保存在fb_var_screnninfo结构体info的成员变量pixclock中的值可能等于0。在这种情况下，前面计算得到的变量refreshRate的值就会等于0。在这种情况下，接下来的代码会将变量refreshRate的值设置为60 1000 10E-3 HZ，即将显示屏的刷新频率设置为60HZ。 &#160; &#160; &#160; &#160;Part.5：1234567891011121314151617181920int mapFrameBufferLocked(struct private_module_t* module)&#123; ...Part.1... ...Part.2... ...Part.3... ...Part.4... if (int(info.width) &lt;= 0 || int(info.height) &lt;= 0) &#123; // the driver doesn't return that information // default to 160 dpi info.width = ((info.xres * 25.4f)/160.0f + 0.5f); info.height = ((info.yres * 25.4f)/160.0f + 0.5f); &#125; float xdpi = (info.xres * 25.4f) / info.width; float ydpi = (info.yres * 25.4f) / info.height; float fps = refreshRate / 1000.0f; ......&#125; &#160; &#160; &#160; &#160;这段代码首先计算显示屏的密度，即每英寸有多少个像素点，分别宽度和高度两个维度，分别保存在变量xdpi和ydpi中。注意，fb_var_screeninfo结构体info的成员变量width和height用来描述显示屏的宽度和高度，它们是以毫米（mm）为单位的。&#160; &#160; &#160; &#160;这段代码接着再将前面计算得到的显示屏刷新频率的单位由10E-3 HZ转换为HZ，即帧每秒，并且保存在变量fps中。 &#160; &#160; &#160; &#160;Part.6：123456789101112131415161718192021222324int mapFrameBufferLocked(struct private_module_t* module)&#123; ...Part.1... ...Part.2... ...Part.3... ...Part.4... ...Part.5... if (ioctl(fd, FBIOGET_FSCREENINFO, &amp;finfo) == -1) return -errno; if (finfo.smem_len &lt;= 0) return -errno; module-&gt;flags = flags; module-&gt;info = info; module-&gt;finfo = finfo; module-&gt;xdpi = xdpi; module-&gt;ydpi = ydpi; module-&gt;fps = fps; ......&#125; &#160; &#160; &#160; &#160;这段代码再次通过IO控制命令FBIOGET_FSCREENINFO来获得系统帧缓冲区的固定信息，并且保存在fb_fix_screeninfo结构体finfo中，接下来再使用fb_fix_screeninfo结构体finfo以及前面得到的系统帧缓冲区的其它信息来初始化参数module所描述的一个private_module_t结构体。 &#160; &#160; &#160; &#160;Part.7：1234567891011121314151617181920212223242526272829int mapFrameBufferLocked(struct private_module_t* module)&#123; ...Part.1... ...Part.2... ...Part.3... ...Part.4... ...Part.5... ...Part.6... /* * map the framebuffer */ int err; size_t fbSize = roundUpToPageSize(finfo.line_length * info.yres_virtual); module-&gt;framebuffer = new private_handle_t(dup(fd), fbSize, 0); module-&gt;numBuffers = info.yres_virtual / info.yres; module-&gt;bufferMask = 0; void* vaddr = mmap(0, fbSize, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0); if (vaddr == MAP_FAILED) &#123; LOGE("Error mapping the framebuffer (%s)", strerror(errno)); return -errno; &#125; module-&gt;framebuffer-&gt;base = intptr_t(vaddr); memset(vaddr, 0, fbSize); return 0; &#125; &#160; &#160; &#160; &#160;表达式finfo.line_length * info.yres_virtual计算的是整个系统帧缓冲区的大小，它的值等于显示屏行数（虚拟分辨率的高度值，info.yres_virtual）乘以每一行所占用的字节数（finfo.line_length）。函数roundUpToPageSize用来将整个系统帧缓冲区的大小对齐到页面边界。对齐后的大小保存在变量fbSize中。 &#160; &#160; &#160; &#160;表达式finfo.yres_virtual / info.yres计算的是整个系统帧缓冲区可以划分为多少个图形缓冲区来使用，这个数值保存在参数module所描述的一个private_module_t结构体的成员变量nmBuffers中。参数module所描述的一个private_module_t结构体的另外一个成员变量bufferMask的值接着被设置为0，表示系统帧缓冲区中的所有图形缓冲区都是处于空闲状态，即它们可以分配出去给应用程序使用。 &#160; &#160; &#160; &#160;系统帧缓冲区是通过调用函数mmap来映射到当前进程的地址空间来的。映射后得到的地址空间使用一个private_handle_t结构体来描述，这个结构体的成员变量base保存的即为系统帧缓冲区在当前进程的地址空间中的起始地址。这样，Gralloc模块以后就可以从这块地址空间中分配图形缓冲区给当前进程使用。 &#160; &#160; &#160; &#160;至此，fb设备的打开过程就分析完成了。在打开fb设备的过程中，Gralloc模块还完成了对系统帧缓冲区的初始化工作。接下来我们继续分析Gralloc模块是如何分配图形缓冲区给用户空间的应用程序使用的。 分配图形缓冲区&#160; &#160; &#160; &#160;前面提到，用户空间的应用程序用到的图形缓冲区是由Gralloc模块中的函数gralloc_alloc来分配的，这个函数实现在文件hardware/libhardware/modules/gralloc/gralloc.cpp中，如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445static int gralloc_alloc(alloc_device_t* dev, int w, int h, int format, int usage, buffer_handle_t* pHandle, int* pStride)&#123; if (!pHandle || !pStride) return -EINVAL; size_t size, stride; int align = 4; int bpp = 0; switch (format) &#123; case HAL_PIXEL_FORMAT_RGBA_8888: case HAL_PIXEL_FORMAT_RGBX_8888: case HAL_PIXEL_FORMAT_BGRA_8888: bpp = 4; break; case HAL_PIXEL_FORMAT_RGB_888: bpp = 3; break; case HAL_PIXEL_FORMAT_RGB_565: case HAL_PIXEL_FORMAT_RAW_SENSOR: bpp = 2; break; default: return -EINVAL; &#125; size_t bpr = (w*bpp + (align-1)) &amp; ~(align-1); size = bpr * h; stride = bpr / bpp; int err; if (usage &amp; GRALLOC_USAGE_HW_FB) &#123; err = gralloc_alloc_framebuffer(dev, size, usage, pHandle); &#125; else &#123; err = gralloc_alloc_buffer(dev, size, usage, pHandle); &#125; if (err &lt; 0) &#123; return err; &#125; *pStride = stride; return 0;&#125; 参数format用来描述要分配的图形缓冲区的颜色格式。当format值等于HAL_PIXEL_FORMAT_RGBA_8888、HAL_PIXEL_FORMAT_RGBX_8888或者HAL_PIXEL_FORMAT_BGRA_8888的时候，一个像素需要使用32位来表示，即4个字节。当format值等于HAL_PIXEL_FORMAT_RGB_888的时候，一个像素需要使用24位来描述，即3个字节。当format值等于HAL_PIXEL_FORMAT_RGB_565、HAL_PIXEL_FORMAT_RGBA_5551或者HAL_PIXEL_FORMAT_RGBA_4444的时候，一个像需要使用16位来描述，即2个字节。最终一个像素需要使用的字节数保存在变量bpp中。 参数w表示要分配的图形缓冲区所保存的图像的宽度，将它乘以bpp，就可以得到保存一行像素所需要使用的字节数。我们需要将这个字节数对齐到4个字节边界，最后得到一行像素所需要的字节数就保存在变量bpr中。 参数h表示要分配的图形缓冲区所保存的图像的高度，将它乘以bpr，就可以得到保存整个图像所需要使用的字节数。 将变量bpr的值除以变量bpp的值，就得到要分配的图形缓冲区一行包含有多少个像素点，这个结果需要保存在输出参数pStride中，以便可以返回给调用者。 参数usage用来描述要分配的图形缓冲区的用途。如果是用来在系统帧缓冲区中渲染的，即参数usage的GRALLOC_USAGE_HW_FB位等于1，那么就必须要系统帧缓冲区中分配，否则的话，就在内存中分配。注意，在内存中分配的图形缓冲区，最终是需要拷贝到系统帧缓冲区去的，以便可以将它所描述的图形渲染出来。 函数gralloc_alloc_framebuffer用来在系统帧缓冲区中分配图形缓冲区，而函数gralloc_alloc_buffer用来在内存在分配图形缓冲区，接下来我们就分别分析这两个函数的实现。 系统帧缓冲区中分配图形缓冲区&#160; &#160; &#160; &#160;函数gralloc_alloc_framebuffer实现在文件hardware/libhardware/modules/gralloc/gralloc.cpp中，如下所示：12345678910static int gralloc_alloc_framebuffer(alloc_device_t* dev, size_t size, int usage, buffer_handle_t* pHandle)&#123; private_module_t* m = reinterpret_cast&lt;private_module_t*&gt;( dev-&gt;common.module); pthread_mutex_lock(&amp;m-&gt;lock); int err = gralloc_alloc_framebuffer_locked(dev, size, usage, pHandle); pthread_mutex_unlock(&amp;m-&gt;lock); return err;&#125; &#160; &#160; &#160; &#160;这个函数调用了另外一个函数gralloc_alloc_framebuffer_locked来分配图形缓冲区。函数gralloc_alloc_framebuffer_locked也是实现在文件hardware/libhardware/modules/gralloc/gralloc.cpp中，如下所示：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static int gralloc_alloc_framebuffer_locked(alloc_device_t* dev, size_t size, int usage, buffer_handle_t* pHandle)&#123; private_module_t* m = reinterpret_cast&lt;private_module_t*&gt;( dev-&gt;common.module); // allocate the framebuffer if (m-&gt;framebuffer == NULL) &#123; // initialize the framebuffer, the framebuffer is mapped once // and forever. int err = mapFrameBufferLocked(m); if (err &lt; 0) &#123; return err; &#125; &#125; const uint32_t bufferMask = m-&gt;bufferMask; const uint32_t numBuffers = m-&gt;numBuffers; const size_t bufferSize = m-&gt;finfo.line_length * m-&gt;info.yres; if (numBuffers == 1) &#123; // If we have only one buffer, we never use page-flipping. Instead, // we return a regular buffer which will be memcpy'ed to the main // screen when post is called. int newUsage = (usage &amp; ~GRALLOC_USAGE_HW_FB) | GRALLOC_USAGE_HW_2D; return gralloc_alloc_buffer(dev, bufferSize, newUsage, pHandle); &#125; if (bufferMask &gt;= ((1LU&lt;&lt;numBuffers)-1)) &#123; // We ran out of buffers. return -ENOMEM; &#125; // create a "fake" handles for it intptr_t vaddr = intptr_t(m-&gt;framebuffer-&gt;base); private_handle_t* hnd = new private_handle_t(dup(m-&gt;framebuffer-&gt;fd), size, private_handle_t::PRIV_FLAGS_FRAMEBUFFER); // find a free slot for (uint32_t i=0 ; i&lt;numBuffers ; i++) &#123; if ((bufferMask &amp; (1LU&lt;&lt;i)) == 0) &#123; m-&gt;bufferMask |= (1LU&lt;&lt;i); break; &#125; vaddr += bufferSize; &#125; hnd-&gt;base = vaddr; hnd-&gt;offset = vaddr - intptr_t(m-&gt;framebuffer-&gt;base); *pHandle = hnd; return 0;&#125; &#160; &#160; &#160; &#160;在系统帧缓冲区分配图形缓冲区之前，首先要对系统帧缓冲区进行过初始化，即这里的变量m所指向的一个private_module_t结构体的成员变量framebuffer的值不能等于NULL。如果等于NULL的话，那么就必须要调用另外一个函数mapFrameBufferLocked来初始化系统帧缓冲区。初始化系统帧缓冲区的过程可以参考前面的内容。 &#160; &#160; &#160; &#160;变量bufferMask用来描述系统帧缓冲区的使用情况，而变量numBuffers用来描述系统帧缓冲区可以划分为多少个图形缓冲区来使用，另外一个变量bufferSize用来描述设备显示屏一屏内容所占用的内存的大小。 &#160; &#160; &#160; &#160;如果系统帧缓冲区只有一个图形缓冲区大小，即变量numBuffers的值等于1，那么这个图形缓冲区就始终用作系统主图形缓冲区来使用。在这种情况下，我们就不能够在系统帧缓冲区中分配图形缓冲区来给用户空间的应用程序使用，因此，这时候就会转向内存中来分配图形缓冲区，即调用函数gralloc_alloc_buffer来分配图形缓冲区。注意，这时候分配的图形缓冲区的大小为一屏内容的大小，即bufferSize。 &#160; &#160; &#160; &#160;如果bufferMask的值大于等于((1LU&lt;&lt;numBuffers)-1)的值，那么就说明系统帧缓冲区中的图形缓冲区全部都分配出去了，这时候分配图形缓冲区就失败了。例如，假设图形缓冲区的个数为2，那么((1LU&lt;&lt;numBuffers)-1)的值就等于3，即二制制0x11。如果这时候bufferMask的值也等于0x11，那么就表示第一个和第二个图形缓冲区都已经分配出去了。因此，这时候就不能再在系统帧缓冲区中分配图形缓冲区。 &#160; &#160; &#160; &#160;假设此时系统帧缓冲区中尚有空闲的图形缓冲区的，接下来函数就会创建一个private_handle_t结构体hnd来描述这个即将要分配出去的图形缓冲区。注意，这个图形缓冲区的标志值等于PRIV_FLAGS_FRAMEBUFFER，即表示这是一块在系统帧缓冲区中分配的图形缓冲区。 &#160; &#160; &#160; &#160;接下来的for循环从低位到高位检查变量bufferMask的值，并且找到第一个值等于0的位，这样就可以知道在系统帧缓冲区中，第几个图形缓冲区的是空闲的。注意，变量vadrr的值开始的时候指向系统帧缓冲区的基地址，在下面的for循环中，每循环一次它的值都会增加bufferSize。从这里就可以看出，每次从系统帧缓冲区中分配出去的图形缓冲区的大小都是刚好等于显示屏一屏内容大小的。 &#160; &#160; &#160; &#160;最后分配出去的图形缓冲区的开始地址就保存在前面所创建的private_handle_t结构体hnd的成员变量base中，这样，用户空间的应用程序就可以直接将要渲染的图形内容拷贝到这个地址上去，这就相当于是直接将图形渲染到系统帧缓冲区中去。 &#160; &#160; &#160; &#160;在将private_handle_t结构体hnd返回给调用者之前，还需要设置它的成员变量offset，以便可以知道它所描述的图形缓冲区的起始地址相对于系统帧缓冲区的基地址的偏移量。 &#160; &#160; &#160; &#160;至此，在系统帧缓冲区中分配图形缓冲区的过程就分析完成了，接下来我们再分析在内存在分析图形缓冲区的过程，即分析函数gralloc_alloc_buffer的实现。 内存中分配图形缓冲区 &#160; &#160; &#160; &#160;函数gralloc_alloc_buffer也是实现在文件hardware/libhardware/modules/gralloc/gralloc.cpp中，如下所示：12345678910111213141516171819202122232425262728 static int gralloc_alloc_buffer(alloc_device_t* dev, size_t size, int usage, buffer_handle_t* pHandle) &#123; int err = 0; int fd = -1; size = roundUpToPageSize(size); fd = ashmem_create_region("gralloc-buffer", size); if (fd &lt; 0) &#123; LOGE("couldn't create ashmem (%s)", strerror(-errno)); err = -errno; &#125; if (err == 0) &#123; private_handle_t* hnd = new private_handle_t(fd, size, 0); gralloc_module_t* module = reinterpret_cast&lt;gralloc_module_t*&gt;( dev-&gt;common.module); err = mapBuffer(module, hnd); if (err == 0) &#123; *pHandle = hnd; &#125; &#125; LOGE_IF(err, "gralloc failed err=%s", strerror(-err)); return err; &#125; &#160; &#160; &#160; &#160;这个函数的实现很简单，它首先调用函数ashmem_create_region来创建一块匿名共享内存，接着再在这块匿名共享内存上分配一个图形缓冲区。注意，这个图形缓冲区也是使用一个private_handle_t结构体来描述的，不过这个图形缓冲区的标志值等于0，以区别于在系统帧缓冲区中分配的图形缓冲区。匿名共享内存的知识还没有入坑，以后再看。。。。。。 &#160; &#160; &#160; &#160;从匿名共享内存中分配的图形缓冲区还需要映射到进程的地址空间来，然后才可以使用，这是通过调用函数mapBuffer来实现的。 &#160; &#160; &#160; &#160;函数mapBuffer实现在文件hardware/libhardware/modules/gralloc/mapper.cpp中，如下所示：123456int mapBuffer(gralloc_module_t const* module, private_handle_t* hnd)&#123; void* vaddr; return gralloc_map(module, hnd, &amp;vaddr);&#125; &#160; &#160; &#160; &#160;它通过调用另外一个函数gralloc_map来将参数hnd所描述的一个图形缓冲区映射到当前进程的地址空间来。后面在分析图形缓冲区的注册过程时，我们再分析函数gralloc_map的实现。 &#160; &#160; &#160; &#160;注意，在Android系统中，在系统帧缓冲区中分配的图形缓冲区是在SurfaceFlinger服务中使用的，而在内存中分配的图形缓冲区既可以在SurfaceFlinger服务中使用，也可以在其它的应用程序中使用。当其它的应用程序需要使用图形缓冲区的时候，它们就会请求SurfaceFlinger服务为它们分配，因此，对于其它的应用程序来说，它们只需要将SurfaceFlinger服务返回来的图形缓冲区映射到自己的进程地址空间来使用就可以了，这就是后面我们所要分析的图形缓冲区的注册过程。 &#160; &#160; &#160; &#160;至此，图形缓冲区的分配过程就分析完成了，接下来我们继续分析图形缓冲区的释放过程。 图形缓冲区的释放过程&#160; &#160; &#160; &#160;前面提到，用户空间的应用程序用到的图形缓冲区是由Gralloc模块中的函数gralloc_free来释放的，这个函数实现在文件hardware/libhardware/modules/gralloc/gralloc.cpp中，如下所示：123456789101112131415161718192021222324static int gralloc_free(alloc_device_t* dev, buffer_handle_t handle)&#123; if (private_handle_t::validate(handle) &lt; 0) return -EINVAL; private_handle_t const* hnd = reinterpret_cast&lt;private_handle_t const*&gt;(handle); if (hnd-&gt;flags &amp; private_handle_t::PRIV_FLAGS_FRAMEBUFFER) &#123; // free this buffer private_module_t* m = reinterpret_cast&lt;private_module_t*&gt;( dev-&gt;common.module); const size_t bufferSize = m-&gt;finfo.line_length * m-&gt;info.yres; int index = (hnd-&gt;base - m-&gt;framebuffer-&gt;base) / bufferSize; m-&gt;bufferMask &amp;= ~(1&lt;&lt;index); &#125; else &#123; gralloc_module_t* module = reinterpret_cast&lt;gralloc_module_t*&gt;( dev-&gt;common.module); terminateBuffer(module, const_cast&lt;private_handle_t*&gt;(hnd)); &#125; close(hnd-&gt;fd); delete hnd; return 0;&#125; &#160; &#160; &#160; &#160;要释放的图形缓冲区使用参数handle来描述。前面提到，从Gralloc模块中分配的图形缓冲区是使用private_handle_t结构体来描述的，因此，这里的参数handle应该指向一个private_handle_t结构体，这是通过调用private_handle_t类的静态成员函数validate来验证的。private_handle_t类的静态成员函数validate的实现可以参考前面的内容。 &#160; &#160; &#160; &#160;要释放的图形缓冲区有可能是在系统帧缓冲区分配的，也有可能是在内存中分配的，这可以通过检查它的标志值flags的PRIV_FLAGS_FRAMEBUFFER位是否等于1来确认。 &#160; &#160; &#160; &#160;如果要释放的图形缓冲区是在系统帧缓冲区中分配的，那么首先要知道这个图形缓冲区是系统帧缓冲区的第index个位置，接着再将变量m所描述的一个private_module_t结构体的成员变量bufferMask的第index位重置为0即可。我们只需要将要释放的图形缓冲区的开始地址减去系统帧缓冲区的基地址，再除以一个图形缓冲区的大小，就可以知道要释放的图形缓冲区是系统帧缓冲区的第几个位置。这个过程刚好是在系统帧缓冲区中分配图形缓冲区的逆操作。 &#160; &#160; &#160; &#160;如果要释放的图形缓冲区是内存中分配的，那么只需要调用另外一个函数terminateBuffer来解除要释放的图形缓冲区在当前进程的地址空间中的映射。 &#160; &#160; &#160; &#160;最后，这个函数还会将用来描述要释放的图形缓冲区的private_handle_t结构体所占用的内存释放掉，并且将要要释放的图形缓冲区所在的系统帧缓冲区或者匿名共享内存的文件描述符关闭掉。 &#160; &#160; &#160; &#160;函数terminateBuffer实现在文件hardware/libhardware/modules/gralloc/mapper.cpp中，如下所示：12345678910int terminateBuffer(gralloc_module_t const* module, private_handle_t* hnd) &#123; if (hnd-&gt;base) &#123; // this buffer was mapped, unmap it now gralloc_unmap(module, hnd); &#125; return 0; &#125; &#160; &#160; &#160; &#160;它通过调用另外一个函数gralloc_unmap来解除参数hnd所描述的一个图形缓冲区在当前进程的地址空间中的映射。后面在分析图形缓冲区的注销过程时，我们再详细分析函数gralloc_unmap的实现。 &#160; &#160; &#160; &#160;至此，图形缓冲区的释放过程就分析完成了，接下来我们继续分析图形缓冲区的注册过程。 图形缓冲区的注册过程&#160; &#160; &#160; &#160;前面提到，在Android系统中，所有的图形缓冲区都是由SurfaceFlinger服务分配的，而当一个图形缓冲区被分配的时候，它会同时被映射到请求分配的进程的地址空间去，即分配的过程同时也包含了注册的过程。但是对用户空间的其它的应用程序来说，它们所需要的图形缓冲区是在由SurfaceFlinger服务分配的，因此，当它们得到SurfaceFlinger服务分配的图形缓冲区之后，还需要将这块图形缓冲区映射到自己的地址空间来，以便可以使用这块图形缓冲区。这个映射的过程即为我们接下来要分析的图形缓冲区注册过程。 &#160; &#160; &#160; &#160;前面还提到，注册图形缓冲区的操作是由Gralloc模块中的函数gralloc_register_buffer来实现的，这个函数实现在文件hardware/libhardware/modules/gralloc/mapper.cpp中，如下所示：123456789101112131415int gralloc_register_buffer(gralloc_module_t const* module, buffer_handle_t handle) &#123; if (private_handle_t::validate(handle) &lt; 0) return -EINVAL; // if this handle was created in this process, then we keep it as is. int err = 0; private_handle_t* hnd = (private_handle_t*)handle; if (hnd-&gt;pid != getpid()) &#123; void *vaddr; err = gralloc_map(module, handle, &amp;vaddr); &#125; return err; &#125; &#160; &#160; &#160; &#160;这个函数首先验证参数handle指向的一块图形缓冲区的确是由Gralloc模块分配的，方法是调用private_handle_t类的静态成员函数validate来验证，即如果参数handle指向的是一个private_handle_t结构体，那么它所指向的一块图形缓冲区就是由Gralloc模块分配的。 &#160; &#160; &#160; &#160;通过了上面的检查之后，函数gralloc_register_buffer还需要检查当前进程是否就是请求Gralloc模块分配图形缓冲区hnd的进程。如果是的话，那么当前进程在请求Gralloc模块分配图形缓冲区hnd的时候，就已经将图形缓冲区hnd映射进自己的地址空间来了，因此，这时候就不需要重复在当前进程中注册这个图形缓冲区。 &#160; &#160; &#160; &#160;真正执行注册图形缓冲区的操作是由函数gralloc_map来实现的，这个函数也是实现文件hardware/libhardware/modules/gralloc/mapper.cpp中，如下所示：1234567891011121314151617181920static int gralloc_map(gralloc_module_t const* module, buffer_handle_t handle, void** vaddr) &#123; private_handle_t* hnd = (private_handle_t*)handle; if (!(hnd-&gt;flags &amp; private_handle_t::PRIV_FLAGS_FRAMEBUFFER)) &#123; size_t size = hnd-&gt;size; void* mappedAddress = mmap(0, size, PROT_READ|PROT_WRITE, MAP_SHARED, hnd-&gt;fd, 0); if (mappedAddress == MAP_FAILED) &#123; LOGE("Could not mmap %s", strerror(errno)); return -errno; &#125; hnd-&gt;base = intptr_t(mappedAddress) + hnd-&gt;offset; //LOGD("gralloc_map() succeeded fd=%d, off=%d, size=%d, vaddr=%p", // hnd-&gt;fd, hnd-&gt;offset, hnd-&gt;size, mappedAddress); &#125; *vaddr = (void*)hnd-&gt;base; return 0; &#125; &#160; &#160; &#160; &#160;由于在系统帧缓冲区中分配的图形缓冲区只在SurfaceFlinger服务中使用，而SurfaceFlinger服务在初始化系统帧缓冲区的时候，已经将系统帧缓冲区映射到自己所在的进程中来了，因此，函数gralloc_map如果发现要注册的图形缓冲区是在系统帧缓冲区分配的时候，那么就不需要再执行映射图形缓冲区的操作了。 &#160; &#160; &#160; &#160;如果要注册的图形缓冲区是在内存中分配的，即它的标志值flags的PRIV_FLAGS_FRAMEBUFFER位等于1，那么接下来就需要将它映射到当前进程的地址空间来了。由于要注册的图形缓冲区是在文件描述符hnd-&gt;fd所描述的一块匿名共享内存中分配的，因此，我们只需要将文件描述符hnd-&gt;fd所描述的一块匿名共享内存映射到当前进程的地址空间来，就可以将参数hnd所描述的一个图形缓冲区映射到当前进程的地址空间来。 &#160; &#160; &#160; &#160;由于映射文件描述符hnd-&gt;fd得到的是一整块匿名共享内存在当前进程地址空间的基地址，而要注册的图形缓冲区可能只占据这块匿名共享内存的某一小部分，因此，我们还需要将要注册的图形缓冲区的在被映射的匿名共享内存中的偏移量hnd-&gt;offset加上被映射的匿名共享内存的基地址hnd-&gt;base，才可以得到要注册的图形缓冲区在当前进程中的访问地址，这个地址最终又被写入到hnd-&gt;base中去。 &#160; &#160; &#160; &#160;注册图形缓冲区的过程就是这么简单，接下来我们再分析图形缓冲区的注销过程。 图形缓冲区的注销过程&#160; &#160; &#160; &#160;图形缓冲区使用完成之后，就需要从当前进程中注销。前面提到，注销图形缓冲区是由Gralloc模块中的函数gralloc_unregister_buffer来实现的，这个函数实现在文件hardware/libhardware/modules/gralloc/mapper.cpp中，如下所示：123456789101112131415int gralloc_unregister_buffer(gralloc_module_t const* module, buffer_handle_t handle) &#123; if (private_handle_t::validate(handle) &lt; 0) return -EINVAL; // never unmap buffers that were created in this process private_handle_t* hnd = (private_handle_t*)handle; if (hnd-&gt;pid != getpid()) &#123; if (hnd-&gt;base) &#123; gralloc_unmap(module, handle); &#125; &#125; return 0; &#125; &#160; &#160; &#160; &#160;这个函数同样是首先调用private_handle_t类的静态成员函数validate来验证参数handle指向的一块图形缓冲区的确是由Gralloc模块分配的，接着再将将参数handle指向的一块图形缓冲区转换为一个private_handle_t结构体hnd来访问。 &#160; &#160; &#160; &#160;一块图形缓冲区只有被注册过，即被Gralloc模块中的函数gralloc_register_buffer注册过，才需要注销，而由函数gralloc_register_buffer注册的图形缓冲区都不是由当前进程分配的，因此，当前进程在注销一个图形缓冲区的时候，会检查要注销的图形缓冲区是否是由自己分配的。如果是由自己分配的话，那么它什么也不做就返回了。 &#160; &#160; &#160; &#160;假设要注销的图形缓冲区hnd不是由当前进程分配的，那么接下来就会调用另外一个函数galloc_unmap来注销图形缓冲区hnd。函数galloc_unmap也是实现在文件hardware/libhardware/modules/gralloc/mapper.cpp中，如下所示：123456789101112131415static int gralloc_unmap(gralloc_module_t const* module, buffer_handle_t handle) &#123; private_handle_t* hnd = (private_handle_t*)handle; if (!(hnd-&gt;flags &amp; private_handle_t::PRIV_FLAGS_FRAMEBUFFER)) &#123; void* base = (void*)hnd-&gt;base; size_t size = hnd-&gt;size; //LOGD("unmapping from %p, size=%d", base, size); if (munmap(base, size) &lt; 0) &#123; LOGE("Could not unmap %s", strerror(errno)); &#125; &#125; hnd-&gt;base = 0; return 0; &#125; &#160; &#160; &#160; &#160;这个函数的实现与前面所分析的函数gralloc_map的实现是类似的，只不过它执行的是相反的操作，即将解除一个指定的图形缓冲区在当前进程的地址空间中的映射，从而完成对这个图形缓冲区的注销工作。 &#160; &#160; &#160; &#160;这样，图形缓冲区的注销过程就分析完成了，接下来我们再继续分析一个图形缓冲区是如何被渲染到系统帧缓冲区去的，即它的内容是如何绘制在设备显示屏中的。 图形缓冲区的渲染过程&#160; &#160; &#160; &#160;用户空间的应用程序将画面内容写入到图形缓冲区中去之后，还需要将图形缓冲区渲染到系统帧缓冲区中去，这样才可以把画面绘制到设备显示屏中去。前面提到，渲染图形缓冲区是由Gralloc模块中的函数fb_post来实现的，这个函数实现在文件hardware/libhardware/modules/gralloc/framebuffer.cpp中，如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static int fb_post(struct framebuffer_device_t* dev, buffer_handle_t buffer) &#123; if (private_handle_t::validate(buffer) &lt; 0) return -EINVAL; fb_context_t* ctx = (fb_context_t*)dev; private_handle_t const* hnd = reinterpret_cast&lt;private_handle_t const*&gt;(buffer); private_module_t* m = reinterpret_cast&lt;private_module_t*&gt;( dev-&gt;common.module); if (hnd-&gt;flags &amp; private_handle_t::PRIV_FLAGS_FRAMEBUFFER) &#123; const size_t offset = hnd-&gt;base - m-&gt;framebuffer-&gt;base; m-&gt;info.activate = FB_ACTIVATE_VBL; m-&gt;info.yoffset = offset / m-&gt;finfo.line_length; if (ioctl(m-&gt;framebuffer-&gt;fd, FBIOPUT_VSCREENINFO, &amp;m-&gt;info) == -1) &#123; LOGE("FBIOPUT_VSCREENINFO failed"); m-&gt;base.unlock(&amp;m-&gt;base, buffer); return -errno; &#125; m-&gt;currentBuffer = buffer; &#125; else &#123; // If we can't do the page_flip, just copy the buffer to the front // FIXME: use copybit HAL instead of memcpy void* fb_vaddr; void* buffer_vaddr; m-&gt;base.lock(&amp;m-&gt;base, m-&gt;framebuffer, GRALLOC_USAGE_SW_WRITE_RARELY, 0, 0, m-&gt;info.xres, m-&gt;info.yres, &amp;fb_vaddr); m-&gt;base.lock(&amp;m-&gt;base, buffer, GRALLOC_USAGE_SW_READ_RARELY, 0, 0, m-&gt;info.xres, m-&gt;info.yres, &amp;buffer_vaddr); memcpy(fb_vaddr, buffer_vaddr, m-&gt;finfo.line_length * m-&gt;info.yres); m-&gt;base.unlock(&amp;m-&gt;base, buffer); m-&gt;base.unlock(&amp;m-&gt;base, m-&gt;framebuffer); &#125; return 0; &#125; &#160; &#160; &#160; &#160;1）参数buffer用来描述要渲染的图形缓冲区，它指向的必须要是一个private_handle_t结构体，这是通过调用private_handle_t类的静态成员函数validate来验证的。验证通过之后，就可以将参数buffer所描述的一个buffer_handle_t结构体转换成一个private_handle_t结构体hnd。 &#160; &#160; &#160; &#160;参数*dev用来描述在Gralloc模块中的一个fb设备。从前面的内容可以知道，在打开fb设备的时候，Gralloc模块返回给调用者的实际上是一个fb_context_t结构体，因此，这里就可以将参数dev所描述的一个framebuffer_device_t结构体转换成一个fb_context_t结构体ctx。 &#160; &#160; &#160; &#160;参数dev的成员变量common指向了一个hw_device_t结构体，这个结构体的成员变量module指向了一个Gralloc模块。从前面的内容可以知道，一个Gralloc模块是使用一个private_module_t结构体来描述的，因此，我们可以将dev-&gt;common.moudle转换成一个private_module_t结构体m。 &#160; &#160; &#160; &#160;2）由于private_handle_t结构体hnd所描述的图形缓冲区可能是在系统帧缓冲区分配的，也有可能是内存中分配的，因此，我们分两种情况来讨论图形缓冲区渲染过程。 &#160; &#160; &#160; &#160;当private_handle_t结构体hnd所描述的图形缓冲区是在系统帧缓冲区中分配的时候，即这个图形缓冲区的标志值flags的PRIV_FLAGS_FRAMEBUFFER位等于1的时候，我们是不需要将图形缓冲区的内容拷贝到系统帧缓冲区去的，因为我们将内容写入到图形缓冲区的时候，已经相当于是将内容写入到了系统帧缓冲区中去了。虽然在这种情况下，我们不需要将图形缓冲区的内容拷贝到系统帧缓冲区去，但是我们需要告诉系统帧缓冲区设备将要渲染的图形缓冲区作为系统当前的输出图形缓冲区，这样才可以将要渲染的图形缓冲区的内容绘制到设备显示屏来。例如，假设系统帧缓冲区有2个图形缓冲区，当前是以第1个图形缓冲区作为输出图形缓冲区的，这时候如果我们需要渲染第2个图形缓冲区，那么就必须告诉系统帧绘冲区设备，将第2个图形缓冲区作为输出图形缓冲区。 &#160; &#160; &#160; &#160;设置系统帧缓冲区的当前输出图形缓冲区是通过IO控制命令FBIOPUT_VSCREENINFO来进行的。IO控制命令FBIOPUT_VSCREENINFO需要一个fb_var_screeninfo结构体作为参数。从前面的内容可以知道，private_module_t结构体m的成员变量info正好保存在我们所需要的这个fb_var_screeninfo结构体。有了个m-&gt;info这个fb_var_screeninfo结构体之后，我们只需要设置好它的成员变量yoffset的值（不用设置成员变量xoffset的值是因为所有的图形缓冲区的宽度是相等的），就可以将要渲染的图形缓冲区设置为系统帧缓冲区的当前输出图形缓冲区。fb_var_screeninfo结构体的成员变量yoffset保存的是当前输出图形缓冲区在整个系统帧缓冲区的纵向偏移量，即Y偏移量。我们只需要将要渲染的图形缓冲区的开始地址hnd-&gt;base的值减去系统帧缓冲区的基地址m-&gt;framebuffer-&gt;base的值，再除以图形缓冲区一行所占据的字节数m-&gt;finfo.line_length，就可以得到所需要的Y偏移量。 &#160; &#160; &#160; &#160;在执行IO控制命令FBIOPUT_VSCREENINFO之前，还会将作为参数的fb_var_screeninfo结构体的成员变量activate的值设置FB_ACTIVATE_VBL，表示要等到下一个垂直同步事件出现时，再将当前要渲染的图形缓冲区的内容绘制出来。这样做的目的是避免出现屏幕闪烁，即避免前后两个图形缓冲区的内容各有一部分同时出现屏幕中。 &#160; &#160; &#160; &#160;成功地执行完成IO控制命令FBIOPUT_VSCREENINFO之后，函数还会将当前被渲染的图形缓冲区保存在private_module_t结构体m的成员变量currentBuffer中，以便可以记录当前被渲染的图形缓冲区是哪一个。 &#160; &#160; &#160; &#160;当private_handle_t结构体hnd所描述的图形缓冲区是在内存中分配的时候，即这个图形缓冲区的标志值flags的PRIV_FLAGS_FRAMEBUFFER位等于0的时候，我们就需要将它的内容拷贝到系统帧缓冲区中去了。这个拷贝的工作是通过调用函数memcpy来完成的。在拷贝之前，我们需要三个参数。第一个参数是要渲染的图形缓冲区的起址地址，这个地址保存在参数buffer所指向的一个private_handle_t结构体中。第二个参数是要系统帧缓冲区的基地址，这个地址保存在private_module_t结构体m的成员变量framebuffer所指向的一个private_handle_t结构体中。第三个参数是要拷贝的内容的大小，这个大小就刚好是一个屏幕像素所占据的内存的大小。屏幕高度由m-&gt;info.yres来描述，而一行屏幕像素所占用的字节数由m-&gt;finfo.line_length来描述，将这两者相乘，就可以得到一个屏幕像素所占据的内存的大小。 &#160; &#160; &#160; &#160;3）在将一块内存缓冲区的内容拷贝到系统帧缓冲区中去之前，需要对这两块缓冲区进行锁定，以保证在拷贝的过程中，这两块缓冲区的内容不会被修改。这个锁定的工作是由Gralloc模块中的函数gralloc_lock来实现的。从前面第1部分的内容可以知道，Gralloc模块中的函数gralloc_lock的地址正好就保存在private_module_t结构体m的成员变量base所描述的一个gralloc_module_t结构体的成员函数lock中。 &#160; &#160; &#160; &#160;在调用函数gralloc_lock来锁定一块缓冲区之后，还可以通过最后一个输出参数来获得被锁定的缓冲区的开始地址，因此，通过调用函数gralloc_lock来锁定要渲染的图形缓冲区以及系统帧缓冲区，就可以得到前面所需要的第一个和第二个参数。 &#160; &#160; &#160; &#160;将要渲染的图形缓冲区的内容拷贝到系统帧缓冲区之后，就可以解除前面对它们的锁定了，这个解锁的工作是由Gralloc模块中的函数gralloc_unlock来实现的。从前面第1部分的内容可以知道，Gralloc模块中的函数gralloc_unlock的地址正好就保存在private_module_t结构体m的成员变量base所描述的一个gralloc_module_t结构体的成员函数unlock中。 &#160; &#160; &#160; &#160;这样，一个图形缓冲区的渲染过程就分析完成了。 图形缓冲区的锁定和解锁&#160; &#160; &#160; &#160;为了完整性起见，最后我们再简要分析函数gralloc_lock和gralloc_unlock的实现，以便可以了解一个图形缓冲区的锁定和解锁操作是如何实现的。 &#160; &#160; &#160; &#160;1）图形缓冲区的锁定：&#160; &#160; &#160; &#160;函数gralloc_lock实现在文件hardware/libhardware/modules/gralloc/mapper.cpp文件中，如下所示：1234567891011121314151617181920int gralloc_lock(gralloc_module_t const* module, buffer_handle_t handle, int usage, int l, int t, int w, int h, void** vaddr) &#123; // this is called when a buffer is being locked for software // access. in thin implementation we have nothing to do since // not synchronization with the h/w is needed. // typically this is used to wait for the h/w to finish with // this buffer if relevant. the data cache may need to be // flushed or invalidated depending on the usage bits and the // hardware. if (private_handle_t::validate(handle) &lt; 0) return -EINVAL; private_handle_t* hnd = (private_handle_t*)handle; *vaddr = (void*)hnd-&gt;base; return 0; &#125; &#160; &#160; &#160; &#160;从这里可以看出，函数gralloc_lock其实并没有执行锁定参数handle所描述的一个缓冲区的操作，它只简单地将要锁定的缓冲区的开始地址返回给调用者。 &#160; &#160; &#160; &#160;理论上来说，函数gralloc_lock应该检查参数handle所描述的一个缓冲区是否正在被其进程或者线程使用。如果是的话，那么函数gralloc_lock就必须要等待，直到要锁定的缓冲区被其它进程或者线程使用结束为止，以便接下来可以独占它。由于函数gralloc_lock实际上并没有作这些操作，因此，就必须要由调用者来保证要锁定的缓冲区当前是没有被其它进程或者线程使用的。 &#160; &#160; &#160; &#160;2）图形缓冲区的解锁：&#160; &#160; &#160; &#160;函数gralloc_unlock也是实现在文件hardware/libhardware/modules/gralloc/mapper.cpp文件中，如下所示：12345678910int gralloc_unlock(gralloc_module_t const* module, buffer_handle_t handle) &#123; // we're done with a software buffer. nothing to do in this // implementation. typically this is used to flush the data cache. if (private_handle_t::validate(handle) &lt; 0) return -EINVAL; return 0; &#125; &#160; &#160; &#160; &#160;函数gralloc_unlock执行的操作本来是刚好与函数gralloc_lock相反的，但是由于函数gralloc_lock并没有真实地锁定参数handle所描述的一个缓冲区的，因此，函数gralloc_unlock是不需要执行实际的解锁工作的。 小结&#160; &#160; &#160; &#160;至此，我们就分析完成Android帧缓冲区硬件抽象层模块Gralloc的实现原理了。从分析的过程可以知道，为了在屏幕中绘制一个指定的画面，我们需要： 分配一个匹配屏幕大小的图形缓冲区 将分配好的图形缓冲区注册（映射）到当前进程的地址空间来 将要绘制的画面的内容写入到已经注册好的图形缓冲区中去，并且渲染（拷贝）到系统帧缓冲区中去&#160; &#160; &#160; &#160;为了实现以上三个操作，我们还需要： 加载Gralloc模块 打开Gralloc模块中的gralloc设备和fb设备 &#160; &#160; &#160; &#160;其中，gralloc设备负责分配图形缓冲区，Gralloc模块负责注册图形缓冲区，而fb设备负责渲染图形缓冲区。 &#160; &#160; &#160; &#160;这个系列内容还是太多了，我估计坚持不了多久。。。。。另外，求HR或者大神带我入坑，妹子图敬上~]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>SurfaceFlinger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android多媒体开发(九)----Video Buffer传输与Audio Playback流程]]></title>
    <url>%2F2017%2F01%2F13%2FAndroid%E5%A4%9A%E5%AA%92%E4%BD%93%E5%BC%80%E5%8F%91-%E4%B9%9D-Video-Buffer%E4%BC%A0%E8%BE%93%E4%B8%8EAudio-Playback%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;上一篇我们简要分析了一下播放流程，主要讲了音频、视频播放和音视频同步的问题。但是对于视频读取buffer还有音频start之后发生的流程都没有分析。本节我们就这两点再分析一番。 Video Buffer传输流程 &#160; &#160; &#160; &#160;上一节的play流程中，OMXCodec会在一开始的时候透过read函数来传送未解码的data给decoder，并要求decoder将解码后的data传回来。&#160; &#160; &#160; &#160;我们看看OMXCodec的read方法，位于framework/av/media/libstagefright/OMXCodec.cpp：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687status_t OMXCodec::read( MediaBuffer **buffer, const ReadOptions *options) &#123; status_t err = OK; *buffer = NULL; Mutex::Autolock autoLock(mLock); //状态已经置为EXECUTING if (mState != EXECUTING &amp;&amp; mState != RECONFIGURING) &#123; return UNKNOWN_ERROR; &#125; bool seeking = false; int64_t seekTimeUs; ReadOptions::SeekMode seekMode; //如果有seek，则要设置一下seek相关参数 if (options &amp;&amp; options-&gt;getSeekTo(&amp;seekTimeUs, &amp;seekMode)) &#123; seeking = true; &#125; //这个mInitialBufferSubmit默认为true,第一次会进来 if (mInitialBufferSubmit) &#123; mInitialBufferSubmit = false; if (seeking) &#123; CHECK(seekTimeUs &gt;= 0); mSeekTimeUs = seekTimeUs; mSeekMode = seekMode; // There's no reason to trigger the code below, there's // nothing to flush yet. seeking = false; mPaused = false; &#125; //这个很重要：读取需要解码的data，并送往omx解码 drainInputBuffers(); if (mState == EXECUTING) &#123; // Otherwise mState == RECONFIGURING and this code will trigger // after the output port is reenabled. //这个也很重要：从输入端口读取解码好的data fillOutputBuffers(); &#125; &#125; if (seeking) &#123; ...省略seek相关处理，这个不重要... &#125; //当输出缓冲区mFilledBuffers为空时会等待解码器解码并填充数据，如果有数据，则直接取走数据 while (mState != ERROR &amp;&amp; !mNoMoreOutputData &amp;&amp; mFilledBuffers.empty()) &#123; if ((err = waitForBufferFilled_l()) != OK) &#123; return err; &#125; &#125; //状态出错 if (mState == ERROR) &#123; return UNKNOWN_ERROR; &#125; //如果输出缓冲区为空，判断是否是文件结尾 if (mFilledBuffers.empty()) &#123; return mSignalledEOS ? mFinalStatus : ERROR_END_OF_STREAM; &#125; //判断输入端口设置是否发生了变化 if (mOutputPortSettingsHaveChanged) &#123; mOutputPortSettingsHaveChanged = false; return INFO_FORMAT_CHANGED; &#125; /*这里我们将输出缓冲区中的bufferinfo取出来，并将其中的mediabuffer赋值给传递进来的参数buffer*/ /*当decoder解码出来数据后会将存放数据的buffer放在mFilledBuffers中，因此audioplayer每次从omxcodec读取数据时，会从mFilledBuffers中取*/ size_t index = *mFilledBuffers.begin(); mFilledBuffers.erase(mFilledBuffers.begin()); BufferInfo *info = &amp;mPortBuffers[kPortIndexOutput].editItemAt(index); CHECK_EQ((int)info-&gt;mStatus, (int)OWNED_BY_US); //说明此info归client所有，client释放后会归还的 info-&gt;mStatus = OWNED_BY_CLIENT; //info-&gt;mMediaBuffer-&gt;add_ref();是增加一个引用，估计release的时候用~~ info-&gt;mMediaBuffer-&gt;add_ref(); if (mSkipCutBuffer != NULL) &#123; mSkipCutBuffer-&gt;submit(info-&gt;mMediaBuffer); &#125; *buffer = info-&gt;mMediaBuffer; return OK;&#125; &#160; &#160; &#160; &#160;这个流程比较长，我们分步来看：&#160; &#160; &#160; &#160;1）设置相关参数。前面设置好参数mState 后，会经过几次回调将状态设置成EXECUTING，不会return；获取seek的拖动进度，拿到时间戳等，更改拖动参数变量。 &#160; &#160; &#160; &#160;2）解封装读取数据，送往omx解码组件解码，并返回解码后的数据。（这一部分是整个流程的核心，我们接下来会讲到）123456//省略后相关代码如下 if (mInitialBufferSubmit) &#123; mInitialBufferSubmit = false; drainInputBuffers(); fillOutputBuffers(); &#125; &#160; &#160; &#160; &#160;这里需要注意的是mInitialBufferSubmit默认是true。drainInputBuffers可以认为从extractor读取一包数据。fillOutputBuffers是解码一包数据并放在输出buffer中。 &#160; &#160; &#160; &#160;3）将输出缓冲区中的bufferinfo取出来，并将其中的mediabuffer赋值给传递进来的参数buffer。123456789101112//省略后相关代码如下size_t index = *mFilledBuffers.begin();mFilledBuffers.erase(mFilledBuffers.begin()); BufferInfo *info = &amp;mPortBuffers[kPortIndexOutput].editItemAt(index); CHECK_EQ((int)info-&gt;mStatus, (int)OWNED_BY_US); info-&gt;mStatus = OWNED_BY_CLIENT; info-&gt;mMediaBuffer-&gt;add_ref(); if (mSkipCutBuffer != NULL) &#123; mSkipCutBuffer-&gt;submit(info-&gt;mMediaBuffer); &#125; *buffer = info-&gt;mMediaBuffer; return OK; &#160; &#160; &#160; &#160;这里我们将输出缓冲区中的bufferinfo取出来，并将其中的mediabuffer赋值给传递进来的参数buffer，当decoder解码出来数据后会将存放数据的buffer放在mFilledBuffers中，因此每次从omxcodec读取数据时，会从mFilledBuffers中取。区别在于，当mFilledBuffers为空时会等待解码器解码并填充数据，如果有数据，则直接取走数据。 &#160; &#160; &#160; &#160;在读取这一步之前，将info-&gt;mStatus 已经设置为OWNED_BY_CLIENT，说明此info归client所有，client释放后会归还的。&#160; &#160; &#160; &#160;通过设置mStatus可以让这一块内存由不同的模块来支配，如其角色有如下几个：123456enum BufferStatus &#123; OWNED_BY_US, OWNED_BY_COMPONENT, OWNED_BY_NATIVE_WINDOW, OWNED_BY_CLIENT,&#125;; &#160; &#160; &#160; &#160;显然component是解码器的，client是外部的。 &#160; &#160; &#160; &#160;info-&gt;mMediaBuffer-&gt;add_ref();是增加一个引用，估计release的时候用~~ &#160; &#160; &#160; &#160;下面着重分析下如何从extractor读数据，和如何解码数据。 drainInputBuffers实现&#160; &#160; &#160; &#160;先找到这个方法：123456789101112131415161718192021222324252627282930313233void OMXCodec::drainInputBuffers() &#123; CHECK(mState == EXECUTING || mState == RECONFIGURING); //DRM相关，忽略 if (mFlags &amp; kUseSecureInputBuffers) &#123; Vector&lt;BufferInfo&gt; *buffers = &amp;mPortBuffers[kPortIndexInput]; for (size_t i = 0; i &lt; buffers-&gt;size(); ++i) &#123; if (!drainAnyInputBuffer() || (mFlags &amp; kOnlySubmitOneInputBufferAtOneTime)) &#123; break; &#125; &#125; &#125; else &#123; //kPortIndexInput为0，kPortIndexOutput为1，一个输入一个输出 Vector&lt;BufferInfo&gt; *buffers = &amp;mPortBuffers[kPortIndexInput]; //我们可能申请了多个输入缓冲区，因此是一个循环 for (size_t i = 0; i &lt; buffers-&gt;size(); ++i) &#123; BufferInfo *info = &amp;buffers-&gt;editItemAt(i); //先检查我们有没有权限使用即OWNED_BY_US if (info-&gt;mStatus != OWNED_BY_US) &#123; continue; &#125; if (!drainInputBuffer(info)) &#123; break; &#125; //kOnlySubmitOneInputBufferAtOneTime即每次只允许读一个包，否则循环都读满 if (mFlags &amp; kOnlySubmitOneInputBufferAtOneTime) &#123; break; &#125; &#125; &#125;&#125; &#160; &#160; &#160; &#160;这里解释下，我们可能申请了多个输入缓冲区，因此是一个循环，先检查我们有没有权限使用即OWNED_BY_US，这一缓冲区获取完数据后会检测。&#160; &#160; &#160; &#160;kOnlySubmitOneInputBufferAtOneTime即每次只允许读一个包，否则循环都读满。 &#160; &#160; &#160; &#160;我们继续看drainInputBuffer实现。这一段代码很长，我们只能分部贴出分析： Part.1：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657bool OMXCodec::drainInputBuffer(BufferInfo *info) &#123; if (info != NULL) &#123; CHECK_EQ((int)info-&gt;mStatus, (int)OWNED_BY_US); &#125; if (mSignalledEOS) &#123; return false; &#125; //如果有未处理的mCodecSpecificData则先mOMX-&gt;emptyBuffer(info-&gt;Buffer,OMX_BUFFERFLAG_CODECCONFIG)处理这些配置数据 if (mCodecSpecificDataIndex &lt; mCodecSpecificData.size()) &#123; CHECK(!(mFlags &amp; kUseSecureInputBuffers)); const CodecSpecificData *specific = mCodecSpecificData[mCodecSpecificDataIndex]; size_t size = specific-&gt;mSize; //如果是avc/h264或者hevc/h265，则要处理NAL头部 if ((!strcasecmp(MEDIA_MIMETYPE_VIDEO_AVC, mMIME) || !strcasecmp(MEDIA_MIMETYPE_VIDEO_HEVC, mMIME)) &amp;&amp; !(mQuirks &amp; kWantsNALFragments)) &#123; static const uint8_t kNALStartCode[4] = &#123; 0x00, 0x00, 0x00, 0x01 &#125;; CHECK(info-&gt;mSize &gt;= specific-&gt;mSize + 4); size += 4; memcpy(info-&gt;mData, kNALStartCode, 4); memcpy((uint8_t *)info-&gt;mData + 4, specific-&gt;mData, specific-&gt;mSize); &#125; else &#123; CHECK(info-&gt;mSize &gt;= specific-&gt;mSize); memcpy(info-&gt;mData, specific-&gt;mData, specific-&gt;mSize); &#125; mNoMoreOutputData = false; CODEC_LOGV("calling emptyBuffer with codec specific data"); //处理mCodecSpecificData，mOMX-&gt;emptyBuffer我们后面会讲到 status_t err = mOMX-&gt;emptyBuffer( mNode, info-&gt;mBuffer, 0, size, OMX_BUFFERFLAG_ENDOFFRAME | OMX_BUFFERFLAG_CODECCONFIG, 0); CHECK_EQ(err, (status_t)OK); info-&gt;mStatus = OWNED_BY_COMPONENT; ++mCodecSpecificDataIndex; return true; &#125; if (mPaused) &#123; return false; &#125; ...未完，待续...&#125; &#160; &#160; &#160; &#160;如果有未处理的mCodecSpecificData则先mOMX-&gt;emptyBuffer(info-&gt;Buffer,OMX_BUFFERFLAG_CODECCONFIG)处理这些配置数据。（mOMX-&gt;emptyBuffer我们后边会讲到）&#160; &#160; &#160; &#160;如果是avc/h264或者hevc/h265，则要处理NAL头部。 &#160; &#160; &#160; &#160;在H.264/AVC视频编码标准中，整个系统框架被分为了两个层面：视频编码层面（VCL）和网络抽象层面（NAL）。其中，前者负责有效表示视频数据的内容，而后者则负责格式化数据并提供头信息，以保证数据适合各种信道和存储介质上的传输。因此我们平时的每帧数据就是一个NAL单元（SPS与PPS除外）。在实际的H264数据帧中，往往帧前面带有00 00 00 01 或 00 00 01分隔符，一般来说编码器编出的首帧数据为PPS与SPS，接着为I帧…… Part.2：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152bool OMXCodec::drainInputBuffer(BufferInfo *info) &#123; ...Part.1... status_t err; bool signalEOS = false; int64_t timestampUs = 0; size_t offset = 0; int32_t n = 0; for (;;) &#123; MediaBuffer *srcBuffer; //如果有seek，处理seek相关 if (mSeekTimeUs &gt;= 0) &#123; if (mLeftOverBuffer) &#123; mLeftOverBuffer-&gt;release(); mLeftOverBuffer = NULL; &#125; MediaSource::ReadOptions options; options.setSeekTo(mSeekTimeUs, mSeekMode); mSeekTimeUs = -1; mSeekMode = ReadOptions::SEEK_CLOSEST_SYNC; mBufferFilled.signal(); //这里的mSource是在AwesomePlayer里面设置的mVideoTrack，从extractor读取数据,用于不同数据源的解封装 err = mSource-&gt;read(&amp;srcBuffer, &amp;options); if (err == OK) &#123; int64_t targetTimeUs; if (srcBuffer-&gt;meta_data()-&gt;findInt64( kKeyTargetTime, &amp;targetTimeUs) &amp;&amp; targetTimeUs &gt;= 0) &#123; CODEC_LOGV("targetTimeUs = %lld us", targetTimeUs); mTargetTimeUs = targetTimeUs; &#125; else &#123; mTargetTimeUs = -1; &#125; &#125; &#125; else if (mLeftOverBuffer) &#123;//如果是读取溢出了，下面代码会有判断逻辑 srcBuffer = mLeftOverBuffer; mLeftOverBuffer = NULL; err = OK; &#125; else &#123; //同上，只不过没有seek的进度 err = mSource-&gt;read(&amp;srcBuffer); &#125; if (err != OK) &#123; signalEOS = true; mFinalStatus = err; mSignalledEOS = true; mBufferFilled.signal(); break; &#125; //DRM相关，忽略 if (mFlags &amp; kUseSecureInputBuffers) &#123; info = findInputBufferByDataPointer(srcBuffer-&gt;data()); CHECK(info != NULL); &#125; /*下面是判断从extractor读取到的数据是不是超过了总大小*/ //计算输入缓冲区剩余容量大小 size_t remainingBytes = info-&gt;mSize - offset; //如果剩余容量小于解封装后读取的data大小 if (srcBuffer-&gt;range_length() &gt; remainingBytes) &#123; //如果是每次读取的开始 if (offset == 0) &#123; CODEC_LOGE( "Codec's input buffers are too small to accomodate " "buffer read from source (info-&gt;mSize = %d, srcLength = %d)", info-&gt;mSize, srcBuffer-&gt;range_length()); //解码申请的输入缓冲区大小太小了，无法容纳解封装读取的数据大小 //释放读取的data srcBuffer-&gt;release(); srcBuffer = NULL; //设置错误状态 setState(ERROR); return false; &#125; mLeftOverBuffer = srcBuffer; break; &#125; bool releaseBuffer = true; if (mFlags &amp; kStoreMetaDataInVideoBuffers) &#123; releaseBuffer = false; info-&gt;mMediaBuffer = srcBuffer; &#125; if (mFlags &amp; kUseSecureInputBuffers) &#123;//DRM，忽略 // Data in "info" is already provided at this time. releaseBuffer = false; CHECK(info-&gt;mMediaBuffer == NULL); info-&gt;mMediaBuffer = srcBuffer; &#125; else &#123;//将读取到的数据拷贝到申请的读取缓冲区中 CHECK(srcBuffer-&gt;data() != NULL) ; memcpy((uint8_t *)info-&gt;mData + offset, (const uint8_t *)srcBuffer-&gt;data() + srcBuffer-&gt;range_offset(), srcBuffer-&gt;range_length()); &#125; ...... //读取，拷贝后，将读取位置偏移量加上读取大小 offset += srcBuffer-&gt;range_length(); //如果是OGG Vobis格式的音频 if (!strcasecmp(MEDIA_MIMETYPE_AUDIO_VORBIS, mMIME)) &#123; CHECK(!(mQuirks &amp; kSupportsMultipleFramesPerInputBuffer)); CHECK_GE(info-&gt;mSize, offset + sizeof(int32_t)); int32_t numPageSamples; if (!srcBuffer-&gt;meta_data()-&gt;findInt32( kKeyValidSamples, &amp;numPageSamples)) &#123; numPageSamples = -1; &#125; memcpy((uint8_t *)info-&gt;mData + offset, &amp;numPageSamples, sizeof(numPageSamples)); offset += sizeof(numPageSamples); &#125; if (releaseBuffer) &#123; srcBuffer-&gt;release(); srcBuffer = NULL; &#125; //读取次数，读了几帧 ++n; //如果不支持每次读取多帧，则一次就直接跳出 if (!(mQuirks &amp; kSupportsMultipleFramesPerInputBuffer)) &#123; break; &#125; //处理本次读取时间 int64_t coalescedDurationUs = lastBufferTimeUs - timestampUs; //如果时间超过250毫秒则舍弃 if (coalescedDurationUs &gt; 250000ll) &#123; // Don't coalesce more than 250ms worth of encoded data at once. break; &#125; &#125; ...未完，待续...&#125; &#160; &#160; &#160; &#160;Part2的代码虽然多，但核心不多。&#160; &#160; &#160; &#160;1）从extractor读取数据，用于不同数据源的解封装。这里的mSource是在AwesomePlayer里面设置的mVideoTrack，我们可以回到以前设置数据源那里查看，如果不记得了可回顾一下以前的Android多媒体开发(四)—-AwesomePlayer数据源处理。我们假设是MEDIA_MIMETYPE_CONTAINER_MPEG2TS封装格式，取得它的视频流信息，我们要查看MPEG2TSExtractor这个类的getTrack函数，位于frameworks/av/media/libstagefright/mpeg2ts/MPEG2TSExtractor.cpp中：1234567891011121314151617181920sp&lt;MediaSource&gt; MPEG2TSExtractor::getTrack(size_t index) &#123; if (index &gt;= mSourceImpls.size()) &#123; return NULL; &#125; bool seekable = true; if (mSourceImpls.size() &gt; 1) &#123; CHECK_EQ(mSourceImpls.size(), 2u); sp&lt;MetaData&gt; meta = mSourceImpls.editItemAt(index)-&gt;getFormat(); const char *mime; CHECK(meta-&gt;findCString(kKeyMIMEType, &amp;mime)); if (!strncasecmp("audio/", mime, 6)) &#123; seekable = false; &#125; &#125; return new MPEG2TSSource(this, mSourceImpls.editItemAt(index), seekable);&#125; &#160; &#160; &#160; &#160;要解封装读取数据，是MPEG2TSSource的read方法：123456789101112131415161718192021222324status_t MPEG2TSSource::read( MediaBuffer **out, const ReadOptions *options) &#123; *out = NULL; int64_t seekTimeUs; ReadOptions::SeekMode seekMode; if (mSeekable &amp;&amp; options &amp;&amp; options-&gt;getSeekTo(&amp;seekTimeUs, &amp;seekMode)) &#123; return ERROR_UNSUPPORTED; &#125; status_t finalResult; while (!mImpl-&gt;hasBufferAvailable(&amp;finalResult)) &#123; if (finalResult != OK) &#123; return ERROR_END_OF_STREAM; &#125; status_t err = mExtractor-&gt;feedMore(); if (err != OK) &#123; mImpl-&gt;signalEOS(err); &#125; &#125; return mImpl-&gt;read(out, options);&#125; &#160; &#160; &#160; &#160;不过遗憾的是这里涉及MPEG-TS文件格式问题，并且上述read方法主要是使用mImpl来实现的。由于文件格式比较复杂，这里就止步了，有兴趣的同学可以下去自行研究。 &#160; &#160; &#160; &#160;2）判断从extractor读取到的数据是不是超过了总大小 。先计算申请的缓冲区剩余容量，然后根据上一步读取的大小和她进行比大小，从而判断是否读取溢出。 &#160; &#160; &#160; &#160;3）将读取的数据拷贝进申请的读入缓冲区中。这里读取完毕后将缓冲区的状态设置成OWNED_BY_COMPONENT 解码器就可以解码了。这里可以看出来读取数据时实现了一次拷贝~~，而不是用的同一块缓冲区(省略一些细节) Part.31234567891011121314151617181920212223242526272829303132bool OMXCodec::drainInputBuffer(BufferInfo *info) &#123; ...Part.1... ...Part.2...if (n &gt; 1) &#123; ALOGV("coalesced %d frames into one input buffer", n); &#125; OMX_U32 flags = OMX_BUFFERFLAG_ENDOFFRAME; if (signalEOS) &#123; flags |= OMX_BUFFERFLAG_EOS; &#125; else &#123; mNoMoreOutputData = false; &#125; ...... //将解封装的数据送往OMX去解码 err = mOMX-&gt;emptyBuffer( mNode, info-&gt;mBuffer, 0, offset, flags, timestampUs); if (err != OK) &#123; setState(ERROR); return false; &#125; info-&gt;mStatus = OWNED_BY_COMPONENT; return true;&#125; &#160; &#160; &#160; &#160;这里读取完毕后将缓冲区的状态设置成OWNED_BY_COMPONENT 解码器就可以解码了。&#160; &#160; &#160; &#160;下面看读取数据完毕后调用mOMX-&gt;emptyBuffer都干了些啥。位于frameworks/av/media/libstagefright/omx/OMX.cpp中12345678status_t OMX::emptyBuffer( node_id node, buffer_id buffer, OMX_U32 range_offset, OMX_U32 range_length, OMX_U32 flags, OMX_TICKS timestamp) &#123; return findInstance(node)-&gt;emptyBuffer( buffer, range_offset, range_length, flags, timestamp);&#125; &#160; &#160; &#160; &#160;然后根绝node节点找到自己的OMXNodeInstance，找到emptyBuffer方法，位于frameworks/av/media/libstagefright/omx/OMXNodeInstance.cpp中：1234567891011121314151617181920status_t OMXNodeInstance::emptyBuffer( OMX::buffer_id buffer, OMX_U32 rangeOffset, OMX_U32 rangeLength, OMX_U32 flags, OMX_TICKS timestamp) &#123; Mutex::Autolock autoLock(mLock); OMX_BUFFERHEADERTYPE *header = findBufferHeader(buffer); header-&gt;nFilledLen = rangeLength; header-&gt;nOffset = rangeOffset; header-&gt;nFlags = flags; header-&gt;nTimeStamp = timestamp; BufferMeta *buffer_meta = static_cast&lt;BufferMeta *&gt;(header-&gt;pAppPrivate); buffer_meta-&gt;CopyToOMX(header); //此处mHandle对应相应解码组件的 OMX_ERRORTYPE err = OMX_EmptyThisBuffer(mHandle, header); return StatusFromOMXError(err);&#125; &#160; &#160; &#160; &#160;这时候就是调用OMX的方法了，我们在Android多媒体开发(七)—-Android中OpenMax的实现 讲过，OMX适配层会去查找匹配的相应解码组件，如果忘掉的可以查看这这一节。 &#160; &#160; &#160; &#160;以前我们分析的都是硬解，比如高通、TI的平台，但这里我们为了更清晰分析这个流程，我们选择软解组件，及OMX.google.XX.XX.Decoder。&#160; &#160; &#160; &#160;我们假设视频编码格式hevc/h265的，因此找到对应的软解组件SoftHEVC，位于frameworks/av/media/libstagefright/codecs/hevcdec/SoftHEVC.cpp。但是我们没有找到emptyThisBuffer方法，所以得去它父类的父类SimpleSoftOMXComponent中去查找，位于frameworks/av/media/libstagefright/omx/SimpleSoftOMXComponent.cpp中：12345678OMX_ERRORTYPE SimpleSoftOMXComponent::emptyThisBuffer( OMX_BUFFERHEADERTYPE *buffer) &#123; sp&lt;AMessage&gt; msg = new AMessage(kWhatEmptyThisBuffer, mHandler-&gt;id()); msg-&gt;setPointer("header", buffer); msg-&gt;post(); return OMX_ErrorNone;&#125; &#160; &#160; &#160; &#160;可以看到就是发了一条命令kWhatEmptyThisBuffer，通过handler-&gt;id确定了自己发的还得自己收，处理函数如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950void SimpleSoftOMXComponent::onMessageReceived(const sp&lt;AMessage&gt; &amp;msg) &#123; Mutex::Autolock autoLock(mLock); uint32_t msgType = msg-&gt;what(); ALOGV("msgType = %d", msgType); switch (msgType) &#123; ...... case kWhatEmptyThisBuffer: case kWhatFillThisBuffer: &#123; OMX_BUFFERHEADERTYPE *header; CHECK(msg-&gt;findPointer("header", (void **)&amp;header)); CHECK(mState == OMX_StateExecuting &amp;&amp; mTargetState == mState); bool found = false; size_t portIndex = (kWhatEmptyThisBuffer == msgType)? header-&gt;nInputPortIndex: header-&gt;nOutputPortIndex; PortInfo *port = &amp;mPorts.editItemAt(portIndex); for (size_t j = 0; j &lt; port-&gt;mBuffers.size(); ++j) &#123; BufferInfo *buffer = &amp;port-&gt;mBuffers.editItemAt(j); if (buffer-&gt;mHeader == header) &#123; CHECK(!buffer-&gt;mOwnedByUs); buffer-&gt;mOwnedByUs = true; CHECK((msgType == kWhatEmptyThisBuffer &amp;&amp; port-&gt;mDef.eDir == OMX_DirInput) || (port-&gt;mDef.eDir == OMX_DirOutput)); port-&gt;mQueue.push_back(buffer); onQueueFilled(portIndex); found = true; break; &#125; &#125; CHECK(found); break; &#125; default: TRESPASS(); break; &#125;&#125; &#160; &#160; &#160; &#160;从代码这里来看这两个case都走同一套代码，而且都是通过onQueueFilled来处理，这样我们就引出了实际的处理函数，也就是onQueueFilled。此时我们就得去子类SoftHEVC.cpp中查找：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394void SoftHEVC::onQueueFilled(OMX_U32 portIndex) &#123; UNUSED(portIndex); if (mOutputPortSettingsChange != NONE) &#123; return; &#125; //获取输入输出链表 List&lt;BufferInfo *&gt; &amp;inQueue = getPortQueue(kInputPortIndex); List&lt;BufferInfo *&gt; &amp;outQueue = getPortQueue(kOutputPortIndex); ...... while (!outQueue.empty()) &#123; //输入缓冲区 BufferInfo *inInfo; OMX_BUFFERHEADERTYPE *inHeader; //输出缓冲区 BufferInfo *outInfo; OMX_BUFFERHEADERTYPE *outHeader; size_t timeStampIx; inInfo = NULL; inHeader = NULL; //各自取输入输出缓冲区中的第一个缓冲区 outInfo = *outQueue.begin(); outHeader = outInfo-&gt;mHeader; outHeader-&gt;nFlags = 0; outHeader-&gt;nTimeStamp = 0; outHeader-&gt;nOffset = 0; //判断缓冲区是不是没有数据，若果第一个都没有那就是没有 if (inHeader != NULL &amp;&amp; (inHeader-&gt;nFlags &amp; OMX_BUFFERFLAG_EOS)) &#123; ALOGD("EOS seen on input"); mReceivedEOS = true; if (inHeader-&gt;nFilledLen == 0) &#123; inQueue.erase(inQueue.begin()); inInfo-&gt;mOwnedByUs = false; //如果输入缓冲区数据没有了，则调用notifyEmptyBufferDone notifyEmptyBufferDone(inHeader); inHeader = NULL; setFlushMode(); &#125; &#125; ...... /***************************省略解码相关细节******************************/ if (s_dec_op.u4_output_present) &#123; outHeader-&gt;nFilledLen = (mWidth * mHeight * 3) / 2; outHeader-&gt;nTimeStamp = mTimeStamps[s_dec_op.u4_ts]; mTimeStampsValid[s_dec_op.u4_ts] = false; outInfo-&gt;mOwnedByUs = false; outQueue.erase(outQueue.begin()); outInfo = NULL; //这是将解码出来的数据告诉外部，通过调用notifyFillBufferDone notifyFillBufferDone(outHeader); outHeader = NULL; &#125; else &#123; /* If in flush mode and no output is returned by the codec, * then come out of flush mode */ mIsInFlush = false; /* If EOS was recieved on input port and there is no output * from the codec, then signal EOS on output port */ if (mReceivedEOS) &#123; outHeader-&gt;nFilledLen = 0; outHeader-&gt;nFlags |= OMX_BUFFERFLAG_EOS; outInfo-&gt;mOwnedByUs = false; outQueue.erase(outQueue.begin()); outInfo = NULL; //这是将解码出来的数据告诉外部，通过调用notifyFillBufferDone notifyFillBufferDone(outHeader); outHeader = NULL; resetPlugin(); &#125; &#125; &#125; // TODO: Handle more than one picture data if (inHeader != NULL) &#123; inInfo-&gt;mOwnedByUs = false; inQueue.erase(inQueue.begin()); inInfo = NULL; // 如果输入缓冲区数据都解码完了，则调用notifyEmptyBufferDone notifyEmptyBufferDone(inHeader); inHeader = NULL; &#125; &#125;&#125; &#160; &#160; &#160; &#160;以上就是输入缓冲区数据解码再从输出缓冲区传递出去部分。流程大概是：&#160; &#160; &#160; &#160;1）读取输入缓冲区数据，如果空了则表示结束了，则调用notifyEmptyBufferDone，输入部分清空；&#160; &#160; &#160; &#160;2）如果读取到了数据，则送去解码（因为这里设计编码格式等等，因此省略解码细节）；&#160; &#160; &#160; &#160;3）然后将解码出来的数据告诉外部，通过调用notifyFillBufferDone ；&#160; &#160; &#160; &#160;4）循环上述过程。 &#160; &#160; &#160; &#160;所以上述过程的重点就是notifyEmptyBufferDone 和notifyFillBufferDone ，如何将输入缓冲区释放和将输出缓冲区中的数据传递出去。接下来我们分析这两个过程。 输入部分的清空notifyEmptyBufferDone&#160; &#160; &#160; &#160;notifyEmptyBufferDone位于它父类的父类的父类，位于frameworks/av/media/libstagefright/omx/SoftOMXComponent.cpp中：1234void SoftOMXComponent::notifyEmptyBufferDone(OMX_BUFFERHEADERTYPE *header) &#123; (*mCallbacks-&gt;EmptyBufferDone)( mComponent, mComponent-&gt;pApplicationPrivate, header);&#125; &#160; &#160; &#160; &#160;我们在Android多媒体开发(七)—-Android中OpenMax的实现 讲过，这里最后还是会回到OMXNodeInstance。通知外面我们emptythisbuffer完工了，具体回调的是OMXNodeInstance中的方法OnEmptyBufferDone，所以看看它的实现：123456789101112// staticOMX_ERRORTYPE OMXNodeInstance::OnEmptyBufferDone( OMX_IN OMX_HANDLETYPE /* hComponent */, OMX_IN OMX_PTR pAppData, OMX_IN OMX_BUFFERHEADERTYPE* pBuffer) &#123; OMXNodeInstance *instance = static_cast&lt;OMXNodeInstance *&gt;(pAppData); if (instance-&gt;mDying) &#123; return OMX_ErrorNone; &#125; return instance-&gt;owner()-&gt;OnEmptyBufferDone(instance-&gt;nodeID(), instance-&gt;findBufferID(pBuffer), pBuffer);&#125; &#160; &#160; &#160; &#160;OMXNodeInstance的ownner是OMX，因此代码在OMX.cpp中：12345678910111213OMX_ERRORTYPE OMX::OnEmptyBufferDone( node_id node, buffer_id buffer, OMX_IN OMX_BUFFERHEADERTYPE *pBuffer) &#123; ALOGV("OnEmptyBufferDone buffer=%p", pBuffer); omx_message msg; msg.type = omx_message::EMPTY_BUFFER_DONE; msg.node = node; msg.u.buffer_data.buffer = buffer; findDispatcher(node)-&gt;post(msg); return OMX_ErrorNone;&#125; &#160; &#160; &#160; &#160;其中findDispatcher定义如下:1234567sp&lt;OMX::CallbackDispatcher&gt; OMX::findDispatcher(node_id node) &#123; Mutex::Autolock autoLock(mLock); ssize_t index = mDispatchers.indexOfKey(node); return index &lt; 0 ? NULL : mDispatchers.valueAt(index);&#125; &#160; &#160; &#160; &#160;这里mDispatcher在之前allocateNode中通过mDispatchers.add(*node, new CallbackDispatcher(instance)); 创建的,看下实际的实现可知道，CallbackDispatcher的post方法最终会调用dispatch：1234567void OMX::CallbackDispatcher::dispatch(const omx_message &amp;msg) &#123; if (mOwner == NULL) &#123; ALOGV("Would have dispatched a message to a node that's already gone."); return; &#125; mOwner-&gt;onMessage(msg);&#125; &#160; &#160; &#160; &#160;而owner是OMXNodeInstance，因此消息饶了一圈还是到了OMXNodeInstance的OnMessage方法接收了:1234567891011121314151617181920212223void OMXNodeInstance::onMessage(const omx_message &amp;msg) &#123; const sp&lt;GraphicBufferSource&gt;&amp; bufferSource(getGraphicBufferSource()); if (msg.type == omx_message::FILL_BUFFER_DONE) &#123; ...... &#125; else if (msg.type == omx_message::EMPTY_BUFFER_DONE) &#123; if (bufferSource != NULL) &#123; // This is one of the buffers used exclusively by // GraphicBufferSource. // Don't dispatch a message back to ACodec, since it doesn't // know that anyone asked to have the buffer emptied and will // be very confused. OMX_BUFFERHEADERTYPE *buffer = findBufferHeader(msg.u.buffer_data.buffer); bufferSource-&gt;codecBufferEmptied(buffer); return; &#125; &#125; mObserver-&gt;onMessage(msg);&#125; &#160; &#160; &#160; &#160;而onMessage又将消息传递到 mObserver中，也就是在OMXCodec::Create中构造的OMXCodecObserver对象，其OnMessage实现如下:12345678910// from IOMXObservervirtual void onMessage(const omx_message &amp;msg) &#123; sp&lt;OMXCodec&gt; codec = mTarget.promote(); if (codec.get() != NULL) &#123; Mutex::Autolock autoLock(codec-&gt;mLock); codec-&gt;on_message(msg); codec.clear(); &#125;&#125; &#160; &#160; &#160; &#160;最终还是传递给了OMXCodec里，具体看下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364void OMXCodec::on_message(const omx_message &amp;msg) &#123; if (mState == ERROR) &#123; /* * only drop EVENT messages, EBD and FBD are still * processed for bookkeeping purposes */ if (msg.type == omx_message::EVENT) &#123; ALOGW("Dropping OMX EVENT message - we're in ERROR state."); return; &#125; &#125; switch (msg.type) &#123; ...... case omx_message::EMPTY_BUFFER_DONE: &#123; IOMX::buffer_id buffer = msg.u.extended_buffer_data.buffer; CODEC_LOGV("EMPTY_BUFFER_DONE(buffer: %u)", buffer); Vector&lt;BufferInfo&gt; *buffers = &amp;mPortBuffers[kPortIndexInput]; size_t i = 0; while (i &lt; buffers-&gt;size() &amp;&amp; (*buffers)[i].mBuffer != buffer) &#123; ++i; &#125; CHECK(i &lt; buffers-&gt;size()); if ((*buffers)[i].mStatus != OWNED_BY_COMPONENT) &#123; ALOGW("We already own input buffer %u, yet received " "an EMPTY_BUFFER_DONE.", buffer); &#125; BufferInfo* info = &amp;buffers-&gt;editItemAt(i); info-&gt;mStatus = OWNED_BY_US; // Buffer could not be released until empty buffer done is called. if (info-&gt;mMediaBuffer != NULL) &#123; //此处虽然调用了info-&gt;mMediaBuffer-&gt;release();但是由于其引用始终大于0，因此不会真正的release info-&gt;mMediaBuffer-&gt;release(); info-&gt;mMediaBuffer = NULL; &#125; if (mPortStatus[kPortIndexInput] == DISABLING) &#123; CODEC_LOGV("Port is disabled, freeing buffer %u", buffer); status_t err = freeBuffer(kPortIndexInput, i); CHECK_EQ(err, (status_t)OK); &#125; else if (mState != ERROR &amp;&amp; mPortStatus[kPortIndexInput] != SHUTTING_DOWN) &#123; CHECK_EQ((int)mPortStatus[kPortIndexInput], (int)ENABLED); if (mFlags &amp; kUseSecureInputBuffers) &#123; drainAnyInputBuffer(); &#125; else &#123; //重点在这里，会调用drainInputBuffer(&amp;buffers-&gt;editItemAt(i));来填充数据 //也就是说当我们启动一次解码播放后，会在此处循环读取数和据解码数据。而输出数据在后面的filloutbuffer中。 drainInputBuffer(&amp;buffers-&gt;editItemAt(i)); &#125; &#125; break; &#125; ...... &#125;&#125; &#160; &#160; &#160; &#160;此处虽然调用了info-&gt;mMediaBuffer-&gt;release();但是由于其引用始终大于0，因此不会真正的release。&#160; &#160; &#160; &#160;二是当release完毕后，会调用drainInputBuffer(&amp;buffers-&gt;editItemAt(i));来填充数据。也就是说当我们启动一次解码播放后，会在此处循环读取数和据解码数据。而输出数据在后面的filloutbuffer中。 &#160; &#160; &#160; &#160;输入部分的清空notifyEmptyBufferDone就分析完了这部分很绕，但搞清楚就好了，请大家仔细阅读。接着我们分析输出数据的清空notifyFillBufferDone(outHeader)。 输出数据的清空notifyFillBufferDone(outHeader)&#160; &#160; &#160; &#160;notifyFillBufferDone同样位于SoftOMXComponent.cpp中：1234void SoftOMXComponent::notifyFillBufferDone(OMX_BUFFERHEADERTYPE *header) &#123; (*mCallbacks-&gt;FillBufferDone)( mComponent, mComponent-&gt;pApplicationPrivate, header);&#125; &#160; &#160; &#160; &#160;这个和上一步流程分析方法一样，最后回到OMX的OnFillBufferDone方法：123456789101112131415161718OMX_ERRORTYPE OMX::OnFillBufferDone( node_id node, buffer_id buffer, OMX_IN OMX_BUFFERHEADERTYPE *pBuffer) &#123; ALOGV("OnFillBufferDone buffer=%p", pBuffer); omx_message msg; msg.type = omx_message::FILL_BUFFER_DONE; msg.node = node; msg.u.extended_buffer_data.buffer = buffer; msg.u.extended_buffer_data.range_offset = pBuffer-&gt;nOffset; msg.u.extended_buffer_data.range_length = pBuffer-&gt;nFilledLen; msg.u.extended_buffer_data.flags = pBuffer-&gt;nFlags; msg.u.extended_buffer_data.timestamp = pBuffer-&gt;nTimeStamp; findDispatcher(node)-&gt;post(msg); return OMX_ErrorNone;&#125; &#160; &#160; &#160; &#160;最终处理在OMXCodec.cpp中:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596void OMXCodec::on_message(const omx_message &amp;msg) &#123; if (mState == ERROR) &#123; /* * only drop EVENT messages, EBD and FBD are still * processed for bookkeeping purposes */ if (msg.type == omx_message::EVENT) &#123; ALOGW("Dropping OMX EVENT message - we're in ERROR state."); return; &#125; &#125; switch (msg.type) &#123; ...... case omx_message::FILL_BUFFER_DONE: &#123; IOMX::buffer_id buffer = msg.u.extended_buffer_data.buffer; OMX_U32 flags = msg.u.extended_buffer_data.flags; ...... Vector&lt;BufferInfo&gt; *buffers = &amp;mPortBuffers[kPortIndexOutput]; size_t i = 0; while (i &lt; buffers-&gt;size() &amp;&amp; (*buffers)[i].mBuffer != buffer) &#123; ++i; &#125; CHECK(i &lt; buffers-&gt;size()); BufferInfo *info = &amp;buffers-&gt;editItemAt(i); if (info-&gt;mStatus != OWNED_BY_COMPONENT) &#123; ALOGW("We already own output buffer %u, yet received " "a FILL_BUFFER_DONE.", buffer); &#125; //先将mStatus设置成OWNED_BY_US，这样component便不能操作了 info-&gt;mStatus = OWNED_BY_US; if (mPortStatus[kPortIndexOutput] == DISABLING) &#123; ...... &#125; else if (mPortStatus[kPortIndexOutput] != SHUTTING_DOWN) &#123; CHECK_EQ((int)mPortStatus[kPortIndexOutput], (int)ENABLED); MediaBuffer *buffer = info-&gt;mMediaBuffer; bool isGraphicBuffer = buffer-&gt;graphicBuffer() != NULL; ...... /*buffer相关参数设置，可以忽略*/ buffer-&gt;set_range( msg.u.extended_buffer_data.range_offset, msg.u.extended_buffer_data.range_length); buffer-&gt;meta_data()-&gt;clear(); buffer-&gt;meta_data()-&gt;setInt64( kKeyTime, msg.u.extended_buffer_data.timestamp); if (msg.u.extended_buffer_data.flags &amp; OMX_BUFFERFLAG_SYNCFRAME) &#123; buffer-&gt;meta_data()-&gt;setInt32(kKeyIsSyncFrame, true); &#125; bool isCodecSpecific = false; if (msg.u.extended_buffer_data.flags &amp; OMX_BUFFERFLAG_CODECCONFIG) &#123; buffer-&gt;meta_data()-&gt;setInt32(kKeyIsCodecConfig, true); isCodecSpecific = true; &#125; if (isGraphicBuffer || mQuirks &amp; kOutputBuffersAreUnreadable) &#123; buffer-&gt;meta_data()-&gt;setInt32(kKeyIsUnreadable, true); &#125; buffer-&gt;meta_data()-&gt;setInt32( kKeyBufferID, msg.u.extended_buffer_data.buffer); if (msg.u.extended_buffer_data.flags &amp; OMX_BUFFERFLAG_EOS) &#123; CODEC_LOGV("No more output data."); mNoMoreOutputData = true; &#125; if (mIsEncoder &amp;&amp; mIsVideo) &#123; int64_t decodingTimeUs = isCodecSpecific? 0: getDecodingTimeUs(); buffer-&gt;meta_data()-&gt;setInt64(kKeyDecodingTime, decodingTimeUs); &#125; ...... //核心是下面几句，将这个buffer push到mFilledBuffers中。 mFilledBuffers.push_back(i); mBufferFilled.signal(); if (mIsEncoder) &#123; sched_yield(); &#125; &#125; break; &#125; ...... &#125;&#125; &#160; &#160; &#160; &#160;上述代码主体也不多：&#160; &#160; &#160; &#160;1）先将mStatus设置成OWNED_BY_US，这样component便不能操作了；&#160; &#160; &#160; &#160;2）对于解码好的buffer进行相关参数设置；&#160; &#160; &#160; &#160;2）后面将这个buffer push到mFilledBuffers中。 &#160; &#160; &#160; &#160;输出数据的清空notifyFillBufferDone就分析完了，这里我们的mFilledBuffers就有数据了，就能为Video Buffer传输流程的下一步fillOutputBuffers做准备了。 fillOutputBuffers实现 &#160; &#160; &#160; &#160;回到开始的步骤，OMXCodec的read第一步drainInputBuffers实现完成了数据的解封装和送往OMX去解码，完成后返回给mFilledBuffers。 &#160; &#160; &#160; &#160;所以我们这一步fillOutputBuffers就是读取这些返回解码数据。我们先看看fillOutputBuffers函数：123456789101112131415 void OMXCodec::fillOutputBuffers() &#123; CHECK_EQ((int)mState, (int)EXECUTING); ...... Vector&lt;BufferInfo&gt; *buffers = &amp;mPortBuffers[kPortIndexOutput]; for (size_t i = 0; i &lt; buffers-&gt;size(); ++i) &#123; BufferInfo *info = &amp;buffers-&gt;editItemAt(i); if (info-&gt;mStatus == OWNED_BY_US) &#123; //找到一个输出缓冲区bufferinfo，启动输出 fillOutputBuffer(&amp;buffers-&gt;editItemAt(i)); &#125; &#125;&#125; &#160; &#160; &#160; &#160;找到一个输出缓冲区bufferinfo，启动输出:1234567891011121314151617 void OMXCodec::fillOutputBuffer(BufferInfo *info) &#123; CHECK_EQ((int)info-&gt;mStatus, (int)OWNED_BY_US); ...... CODEC_LOGV("Calling fillBuffer on buffer %p", info-&gt;mBuffer); //依旧可以参考上一步的步骤，最后进入解码器组件里面 status_t err = mOMX-&gt;fillBuffer(mNode, info-&gt;mBuffer); if (err != OK) &#123; CODEC_LOGE("fillBuffer failed w/ error 0x%08x", err); setState(ERROR); return; &#125; info-&gt;mStatus = OWNED_BY_COMPONENT;&#125; &#160; &#160; &#160; &#160;这一步和上面分析过的步骤相同，最后还是会进入解码器的组件内部，依然以hevc/h265为例子。同样还是如下步骤：&#160; &#160; &#160; &#160;OMXNodeInstance::fillBuffer—-&gt;SimpleSoftOMXComponent::fillThisBuffer—-&gt;SimpleSoftOMXComponent::onMessageReceived—-&gt;SoftHEVC::onQueueFilled。然后通过notifyEmptyBufferDone(inHeader);和notifyFillBufferDone(outHeader);两个函数来推进播放进度。只不过这次我们最后回到了OMXCodec的on_message回到函数，走了FILL_BUFFER_DONE这个case里的notifyFillBufferDone部分，这个我们上一步最后也分析过这个流程了。 &#160; &#160; &#160; &#160;到这里我们fillOutputBuffers实现部分也分析完了，主要就是传输输出缓冲区的数据。 Video Buffer传输流程总结 &#160; &#160; &#160; &#160;如果把这一流程总结一下，详细流程如下： 1 调用OMXCodec:read() 1.1 先读取ReadOptions看看是不是seek 1.2 如果不是seek，则当前mState必须是EXECUTING或者RECONFIGURING 1.3 如果是提交的第一帧，先调用drainInputBuffers() 1.3.1 执行drainInputBuffers()必须是在mState为EXECUTING，RECONFIGURING和FLUSHING的状态 1.3.2 从mPortBuffers[input]里循环取出每个BufferInfo,并对每个info-&gt;mStatus等于OWNED_BY_US的buffer调用drainInputBuffer(info) 1.3.2.1 drainInputBuffer(BufferInfo)要求buffer必须是OWNED_BY_US 1.3.2.2 如果有未处理的mCodecSpecificData则先mOMX-&gt;emptyBuffer(info-&gt;Buffer,OMX_BUFFERFLAG_CODECCONFIG)处理这些配置数据 1.3.2.3 如果mSignalledEOS或者mPaused为true则停止drain并return false 1.3.2.4 循环调用mSource-&gt;read()读取压缩源srcBuffer，读取失败的话则设置mSignalledEOS为true，如果成功则将其copy进info-&gt;mData里去 1.3.2.5 结果是info-&gt;mData里装着从mSource中多次读取出来的数据总和，并将timestampUs设为第一块数据的kKeyTime值 1.3.2.6 设置flags为OMX_BUFFERFLAG_ENDOFFRAME，如果刚才遇到EOS则再并一个OMX_BUFFERFLAG_EOS 1.3.2.7 调用mOMX-&gt;emptyBuffer(mNode, info-&gt;mBuffer, 0, offset,flags, timestampUs); 1.3.2.8 如果emptyBuffer返回OK，则设置info-&gt;mStatus为OWNED_BY_COMPONENT并return true，否则设置mState为ERROR并返回false 1.4 再调用fillOutputBuffers() 1.4.1 执行fillOutputBuffers()必须是在mState为EXECUTING，FLUSHING的状态 1.4.2 从mPortBuffers[output]里循环取出每个BufferInfo,并对每个info-&gt;mStatus等于OWNED_BY_US的buffer调用fillOutputBuffer(info) 1.4.2.1 如果mNoMoreOutputData为true则return 1.4.2.2 如果info-&gt;mMediaBuffer不为空，则取出其中的GraphicBuffer，并调用mNativeWindow-&gt;lockBuffer(mNativeWindow,graphicBuffer)来锁定该buffer，出错则设置mState为ERROR 1.4.2.3 调用mOMX-&gt;fillBuffer(mNode, info-&gt;mBuffer) 1.4.2.4 如果emptyBuffer返回OK，则设置info-&gt;mStatus为OWNED_BY_COMPONENT，否则设置mState为ERROR，最后return 1.5 如果mState不为ERROR，并且mNoMoreOutputData为false,并且mFilledBuffers为空，并且mOutputPortSettingsChangedPending为false的情况下，则调用waitForBufferFilled_l()，让mBufferFilled去wait lock 1.6 从waitForBufferFilled_l()释放出来后，判断mFilledBuffers为空的话，如果mOutputPortSettingsChangedPending为true则去调用之前延迟执行的onPortSettingsChanged（），否则return EOS 1.7 取出mFilledBuffers的第一个buffer，该buffer的mStatus此时必须为OWNED_BY_US 1.8 然后设置其为OWNED_BY_CLIENT，给该buffer的mMediaBuffer-&gt;add_ref()增加一个引用，并把该mMediaBuffer赋值给buffer并返回给AwesomePlayer Audio Playback 流程（简要分析） &#160; &#160; &#160; &#160;从上一篇可以看到，Audio播放是从AudioPlayer的start函数开始，位于frameworks/av/media/libstagefright/AudioPlayer.cpp中：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889status_t AudioPlayer::start(bool sourceAlreadyStarted) &#123; ...... //先处理seek操作 MediaSource::ReadOptions options; if (mSeeking) &#123; options.setSeekTo(mSeekTimeUs); mSeeking = false; &#125; //从mAudioTrack中读取音频数据，read方法我们在将Video Buffer时分析过了 mFirstBufferResult = mSource-&gt;read(&amp;mFirstBuffer, &amp;options); if (mFirstBufferResult == INFO_FORMAT_CHANGED) &#123; ALOGV("INFO_FORMAT_CHANGED!!!"); CHECK(mFirstBuffer == NULL); mFirstBufferResult = OK; mIsFirstBuffer = false; &#125; else &#123; mIsFirstBuffer = true; &#125; ...省略一些判断处理... //如果mAudioSink存在，这个我们在setDataSource已经设置过了，位于MediaPlayerService的AudioOutput if (mAudioSink.get() != NULL) &#123; ...... //调用AudioOutput的open函数，注意参数里有AudioPlayer::AudioSinkCallback，我们会用到这个回调 status_t err = mAudioSink-&gt;open( mSampleRate, numChannels, channelMask, audioFormat, DEFAULT_AUDIOSINK_BUFFERCOUNT, &amp;AudioPlayer::AudioSinkCallback, this, (audio_output_flags_t)flags, useOffload() ? &amp;offloadInfo : NULL); if (err == OK) &#123; mLatencyUs = (int64_t)mAudioSink-&gt;latency() * 1000; mFrameSize = mAudioSink-&gt;frameSize(); if (useOffload()) &#123; // If the playback is offloaded to h/w we pass the // HAL some metadata information // We don't want to do this for PCM because it will be going // through the AudioFlinger mixer before reaching the hardware sendMetaDataToHal(mAudioSink, format); &#125; err = mAudioSink-&gt;start(); // do not alter behavior for non offloaded tracks: ignore start status. if (!useOffload()) &#123; err = OK; &#125; &#125; if (err != OK) &#123; if (mFirstBuffer != NULL) &#123; mFirstBuffer-&gt;release(); mFirstBuffer = NULL; &#125; if (!sourceAlreadyStarted) &#123; mSource-&gt;stop(); &#125; return err; &#125; &#125; else &#123;//如果mAudioSink不存在 ...... //需要创建一个AudioTrack mAudioTrack = new AudioTrack( AUDIO_STREAM_MUSIC, mSampleRate, AUDIO_FORMAT_PCM_16_BIT, audioMask, 0 /*frameCount*/, AUDIO_OUTPUT_FLAG_NONE, &amp;AudioCallback, this, 0 /*notificationFrames*/); ...... mLatencyUs = (int64_t)mAudioTrack-&gt;latency() * 1000; mFrameSize = mAudioTrack-&gt;frameSize(); //然后调用AudioTrack的start函数 mAudioTrack-&gt;start(); &#125; mStarted = true; mPlaying = true; mPinnedTimeUs = -1ll; return OK;&#125; &#160; &#160; &#160; &#160;依然是分部查看：&#160; &#160; &#160; &#160;1）从mAudioTrack中读取音频数据，read方法我们在将Video Buffer时分析过了；&#160; &#160; &#160; &#160;2）如果mAudioSink存在，这个我们在setDataSource已经设置过了，位于MediaPlayerService的AudioOutput，如果忘记了可以查看上一篇的设置同步时钟部分。&#160; &#160; &#160; &#160;3）如果mAudioSink不存在，则创建一个AudioTrack，然后start（这一步我们忽略，因为mAudioSink我们在setDataSource已经创建了）。 &#160; &#160; &#160; &#160;AudioPlayer在启动过程中会先去取第一帧解码的资料，并且开启audio output。这个过程和video buffer很相似。 &#160; &#160; &#160; &#160;开启audio output的同时，AudioPlayer会将callback回调函数指针设给他，之后每次callback函数都被回调，AudioPlayer便去audio decoder读取解码后的数据。我们可以看看这个回调函数：123456789101112131415161718192021222324252627282930// staticvoid AudioPlayer::AudioCallback(int event, void *user, void *info) &#123; static_cast&lt;AudioPlayer *&gt;(user)-&gt;AudioCallback(event, info);&#125;// staticsize_t AudioPlayer::AudioSinkCallback( MediaPlayerBase::AudioSink * /* audioSink */, void *buffer, size_t size, void *cookie, MediaPlayerBase::AudioSink::cb_event_t event) &#123; AudioPlayer *me = (AudioPlayer *)cookie; switch(event) &#123; case MediaPlayerBase::AudioSink::CB_EVENT_FILL_BUFFER: return me-&gt;fillBuffer(buffer, size); case MediaPlayerBase::AudioSink::CB_EVENT_STREAM_END: ALOGV("AudioSinkCallback: stream end"); me-&gt;mReachedEOS = true; me-&gt;notifyAudioEOS(); break; case MediaPlayerBase::AudioSink::CB_EVENT_TEAR_DOWN: ALOGV("AudioSinkCallback: Tear down event"); me-&gt;mObserver-&gt;postAudioTearDown(); break; &#125; return 0;&#125; &#160; &#160; &#160; &#160;上面两个回调分别对应上一步的mAudioSink存在和不存在的情况。但是最后都会调用到自己的fillBuffer方法，我们继续查看(省略大部分逻辑)：12345678size_t AudioPlayer::fillBuffer(void *data, size_t size) &#123; ...省略大部分逻辑... err = mSource-&gt;read(&amp;mInputBuffer, &amp;options); ...省略大部分逻辑... memcpy((char *)data + size_done, (const char *)mInputBuffer-&gt;data() + mInputBuffer-&gt;range_offset(), copy);&#125; &#160; &#160; &#160; &#160;解码后audio数据的读取就是有callback回调函数所驱动。另一方面，fillBuffer数据(mInputBuffer)复制到data之后，audio output会去取这些data。 &#160; &#160; &#160; &#160;至于audio decoder的工作流程和video decoder相同，可以参照上面的video buffer流程。 结语&#160; &#160; &#160; &#160;本节的重点其实是Video Buffer传输流程，它重点是输入缓冲区释放和将输出缓冲区中的数据传递出去。也算马马虎虎的，请恕我对多媒体的了解也是个未入门的渣渣，还正在自学途中，如有错误麻烦大家及时指出，我会在第一时间修正。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android多媒体开发(八)----播放流程]]></title>
    <url>%2F2017%2F01%2F06%2FAndroid%E5%A4%9A%E5%AA%92%E4%BD%93%E5%BC%80%E5%8F%91-%E5%85%AB-%E6%92%AD%E6%94%BE%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;前面几篇都是视频文件播放前的准备工作，比如设置数据源，初始化解码器等等，本节我们分析MediaPlayer播放器start之后的流程。 播放流程基本流程&#160; &#160; &#160; &#160;当MediaPlayer prepared成功之后，调用start就开始播放视频了。从前几篇我们知道底层会调用到StageFrightPlayer，最后到AwesomePlayer，我们继续跟踪下去：位于frameworks/av/media/libmediaplayerservice/StageFrightPlayer.cpp：123456789101112status_t StagefrightPlayer::start() &#123; ALOGV("start"); return mPlayer-&gt;play();&#125;StagefrightPlayer::StagefrightPlayer() : mPlayer(new AwesomePlayer) &#123; ALOGV("StagefrightPlayer"); mPlayer-&gt;setListener(this);&#125; &#160; &#160; &#160; &#160;接着会调用AwesomePlayer的paly函数，位于frameworks/av/media/libstagefright/AwesomePlayer.cpp中：123456789status_t AwesomePlayer::play() &#123; ATRACE_CALL(); Mutex::Autolock autoLock(mLock); modifyFlags(CACHE_UNDERRUN, CLEAR); return play_l();&#125; &#160; &#160; &#160; &#160;play函数里有调用了play_l函数，表示是一个内部显示方法：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970status_t AwesomePlayer::play_l() &#123; modifyFlags(SEEK_PREVIEW, CLEAR); //如果在播放，则直接返回 if (mFlags &amp; PLAYING) &#123; return OK; &#125; mMediaRenderingStartGeneration = ++mStartGeneration; //如果还没有prepare，则先prepare if (!(mFlags &amp; PREPARED)) &#123; status_t err = prepare_l(); if (err != OK) &#123; return err; &#125; &#125; //设置标志位PLAYING modifyFlags(PLAYING, SET); //设置标志位FIRST_FRAME，表示第一帧 modifyFlags(FIRST_FRAME, SET); ...... //音频解码器不为空 if (mAudioSource != NULL) &#123; //创建一个音频播放器，这一步还会设置播放同步时钟，即音频/视频/字幕播放的参考时间，下来我们会分析 if (mAudioPlayer == NULL) &#123; createAudioPlayer_l(); &#125; CHECK(!(mFlags &amp; AUDIO_RUNNING)); //如果只有Audio没有Video，省略这一部分 if (mVideoSource == NULL) &#123; ...忽略... &#125; &#125; //如果没有设置同步时钟，则用系统的UTC时间作为播放同步时钟 //在createAudioPlayer_l里面已经设置了Audio时间轴为同步时钟，下面会讲到 if (mTimeSource == NULL &amp;&amp; mAudioPlayer == NULL) &#123; mTimeSource = &amp;mSystemTimeSource; &#125; //递归消息处理 if (mVideoSource != NULL) &#123; // Kick off video playback //开球，开播 postVideoEvent_l(); //解码状态回调，不重要 if (mAudioSource != NULL &amp;&amp; mVideoSource != NULL) &#123; postVideoLagEvent_l(); &#125; &#125; //如果播放完了，则seek到开头 if (mFlags &amp; AT_EOS) &#123; // Legacy behaviour, if a stream finishes playing and then // is started again, we play from the start... seekTo_l(0); &#125; ...... //如果是网络视频，则要一直发送bufferUpdate if (isStreamingHTTP()) &#123; postBufferingEvent_l(); &#125; return OK;&#125; &#160; &#160; &#160; &#160;上面代码虽然多，但核心只有两件事： 创建一个音频播放器，这一步还会设置播放同步时钟，即音频/视频/字幕播放的参考时间 递归消息处理，这一步就是处理音频/视频/字幕的播放核心 &#160; &#160; &#160; &#160;我们一个一个分析：&#160; &#160; &#160; &#160;1）创建音频播放器，设置同步时钟 设置同步时钟&#160; &#160; &#160; &#160;我们先看看createAudioPlayer_l这个函数：123456789101112131415161718192021222324252627282930313233void AwesomePlayer::createAudioPlayer_l()&#123; uint32_t flags = 0; int64_t cachedDurationUs; bool eos; //如果是offload模式，即音频分载，音频由dsp解码，不走omx框架 if (mOffloadAudio) &#123; flags |= AudioPlayer::USE_OFFLOAD; &#125; else if (mVideoSource == NULL &amp;&amp; (mDurationUs &gt; AUDIO_SINK_MIN_DEEP_BUFFER_DURATION_US || (getCachedDuration_l(&amp;cachedDurationUs, &amp;eos) &amp;&amp; cachedDurationUs &gt; AUDIO_SINK_MIN_DEEP_BUFFER_DURATION_US))) &#123; flags |= AudioPlayer::ALLOW_DEEP_BUFFERING; &#125; if (isStreamingHTTP()) &#123; flags |= AudioPlayer::IS_STREAMING; &#125; if (mVideoSource != NULL) &#123; flags |= AudioPlayer::HAS_VIDEO; &#125; //创建一个AudioPlayer，构造方法里传入了AudioSink，并设置进入音频解码器mAudioSource mAudioPlayer = new AudioPlayer(mAudioSink, flags, this); mAudioPlayer-&gt;setSource(mAudioSource); //这就是设置同步时钟，是以音频时间轴为参考时间的 mTimeSource = mAudioPlayer; // If there was a seek request before we ever started, // honor the request now. // Make sure to do this before starting the audio player // to avoid a race condition. //如果又一个seek请求在播放之前，我们要优先处理这个seek操作 seekAudioIfNecessary_l();&#125; &#160; &#160; &#160; &#160;上述函数先查看是不是offload模式，即音频分载，音频由dsp解码，不走omx框架。android L以后会支持音频的的offload播放。就高通的8994平台来看，即会把音频的编解码和后处理放到QDSP里面处理。但其实很多芯片不支持，这里我们这个不是重点，所以往下看。 &#160; &#160; &#160; &#160;接着就是构造AudioPlayer了，这里的构造方法传入了AudioSink，即用来输出声音。这个mAudioSink传入还要追溯到之前setDataSource那里，如果忘记了可以查看Android多媒体开发(四)—-AwesomePlayer数据源处理。在最初的mediaplayerservice中，位于frameworks/av/media/libmediaplayerservice/MediaPlayerSrevice.cpp，调用setdatasource操作代码如下：1234567891011121314151617181920sp&lt;MediaPlayerBase&gt; MediaPlayerService::Client::setDataSource_pre( player_type playerType)&#123; ALOGV("player type = %d", playerType); // create the right type of player sp&lt;MediaPlayerBase&gt; p = createPlayer(playerType); if (p == NULL) &#123; return p; &#125; //核心在这里 if (!p-&gt;hardwareOutput()) &#123; mAudioOutput = new AudioOutput(mAudioSessionId, IPCThreadState::self()-&gt;getCallingUid(), mPid, mAudioAttributes); //这里会构造一个AudioOutput对象传入作为mAudioSink static_cast&lt;MediaPlayerInterface*&gt;(p.get())-&gt;setAudioSink(mAudioOutput); &#125; return p;&#125; &#160; &#160; &#160; &#160;这里会构造一个AudioOutput对象传入作为mAudioSink，实际的播放顺序是 mAudioPlayer -&gt;mAudioSink -&gt;mAudioTrack（不要混淆，此处类是AudioTrack）。 &#160; &#160; &#160; &#160;最后就是设置同步时钟了，将mAudioPlayer赋值给mTimeSource 。我们可以看看AudioPlayer的继承关系，它也是继承与TimeSource类的，所以需要实现父类的getRealTimeUs这个虚函数定义位于frameworks/av/include/media/stagefright/AudioPlayer.h中，实现位于frameworks/av/media/libstagefright/AudioPlayer.cpp：123456class AudioPlayer : public TimeSource &#123; int64_t AudioPlayer::getRealTimeUs() &#123; ...... &#125;&#125; &#160; &#160; &#160; &#160;关于getRealTimeUs函数我们后面会分析到。&#160; &#160; &#160; &#160;往下走play_l方法里还有这样的逻辑：123if (mTimeSource == NULL &amp;&amp; mAudioPlayer == NULL) &#123; mTimeSource = &amp;mSystemTimeSource;&#125; &#160; &#160; &#160; &#160;设置同步时钟的，这里如果mAudioPlayer存在的话，以audio为基准进行播放，否则以系统时钟为基准控制播放。但我们的mAudioPlayer存在，所以已AudioPlayer作为参考基准控制播放。 &#160; &#160; &#160; &#160;注意：这里有一个重点，就是播放的同步时钟是以音频为参考时间的，后面的视频和字幕也是参考这个同步时钟来渲染和展示的。 &#160; &#160; &#160; &#160;2）递归消息处理，这一步就是处理音频/视频/字幕的播放核心&#160; &#160; &#160; &#160;然后就是递归postVideoEvent_l()消息处理，我们看看这个方法：1234567891011void AwesomePlayer::postVideoEvent_l(int64_t delayUs) &#123;//缺省参数，int64_t delayUs = -1 ATRACE_CALL(); if (mVideoEventPending) &#123; return; &#125; mVideoEventPending = true; //因为delayUs 缺省为-1，所以每10ms触发一下回调 mQueue.postEventWithDelay(mVideoEvent, delayUs &lt; 0 ? 10000 : delayUs);&#125; &#160; &#160; &#160; &#160;主要就是每10ms触发mVideoEvent事件，其响应函数是AwesomePlayer::onVideoEvent。这个方法很长，所以要一步一步查看： Part.1：123456789101112131415161718192021222324252627282930313233343536373839void AwesomePlayer::onVideoEvent() &#123; ATRACE_CALL(); Mutex::Autolock autoLock(mLock); if (!mVideoEventPending) &#123; // The event has been cancelled in reset_l() but had already // been scheduled for execution at that time. return; &#125; mVideoEventPending = false; //如果有seek操作 if (mSeeking != NO_SEEK) &#123; //先清空VideoBuffer视频数据缓冲区 if (mVideoBuffer) &#123; mVideoBuffer-&gt;release(); mVideoBuffer = NULL; &#125; //如果是seek操作中，且是网络数据流 if (mSeeking == SEEK &amp;&amp; isStreamingHTTP() &amp;&amp; mAudioSource != NULL &amp;&amp; !(mFlags &amp; SEEK_PREVIEW)) &#123; // We're going to seek the video source first, followed by // the audio source. // In order to avoid jumps in the DataSource offset caused by // the audio codec prefetching data from the old locations // while the video codec is already reading data from the new // locations, we'll "pause" the audio source, causing it to // stop reading input data until a subsequent seek. //我们将要seek视频的数据，然后跟着去seek音频数据 //为了避免当音频预加载数据是旧的位置取到的而视频却早就取了新的位置的数据，所以我们要先pause住audio，先完成video的seek，后面再seek audio if (mAudioPlayer != NULL &amp;&amp; (mFlags &amp; AUDIO_RUNNING)) &#123; mAudioPlayer-&gt;pause(); modifyFlags(AUDIO_RUNNING, CLEAR); &#125; mAudioSource-&gt;pause(); &#125; &#125; ...未完，待续...&#125; &#160; &#160; &#160; &#160;首先是判断是否需要seek，若需要seek，则先pause住audio，先完成video的seek，后面再seek audio。 Part.2：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778void AwesomePlayer::onVideoEvent() &#123; ...Part.1...if (!mVideoBuffer) &#123;//已经清空了VideoBuffer MediaSource::ReadOptions options; //如果需要拖动 if (mSeeking != NO_SEEK) &#123; ALOGV("seeking to %" PRId64 " us (%.2f secs)", mSeekTimeUs, mSeekTimeUs / 1E6); //设置拖动标志 options.setSeekTo( mSeekTimeUs, mSeeking == SEEK_VIDEO_ONLY ? MediaSource::ReadOptions::SEEK_NEXT_SYNC : MediaSource::ReadOptions::SEEK_CLOSEST_SYNC); &#125; for (;;) &#123; //读取一个视频帧的画面，这个是调用omx解码后的数据，传入VideoBuffer，这个我们以后再讲 status_t err = mVideoSource-&gt;read(&amp;mVideoBuffer, &amp;options); options.clearSeekTo(); //如果读取失败的处理，ignore if (err != OK) &#123; CHECK(mVideoBuffer == NULL); if (err == INFO_FORMAT_CHANGED) &#123; ALOGV("VideoSource signalled format change."); //检查宽度高度是否发生变化 notifyVideoSize_l(); if (mVideoRenderer != NULL) &#123; mVideoRendererIsPreview = false; initRenderer_l(); &#125; continue; &#125; // So video playback is complete, but we may still have // a seek request pending that needs to be applied // to the audio track. if (mSeeking != NO_SEEK) &#123; ALOGV("video stream ended while seeking!"); &#125; finishSeekIfNecessary(-1); if (mAudioPlayer != NULL &amp;&amp; !(mFlags &amp; (AUDIO_RUNNING | SEEK_PREVIEW))) &#123; startAudioPlayer_l(); &#125; //否则即video播放完毕了，设置EOF标记 modifyFlags(VIDEO_AT_EOS, SET); //并触发mStreamDoneEvent消息 postStreamDoneEvent_l(err); return; &#125; /*如果读取一个视频帧成功，go on*/ //检查读取到mVideoBuffer长度，如果长度小于0，则继续读取 if (mVideoBuffer-&gt;range_length() == 0) &#123; // Some decoders, notably the PV AVC software decoder // return spurious empty buffers that we just want to ignore. //一些解码器，尤其是MEPG4 h264的返回假的空buffers，我们需要忽略这些 mVideoBuffer-&gt;release(); mVideoBuffer = NULL; continue; &#125; //如果读取一个视频帧成功了，则跳出for(;;)循环 break; &#125; &#123; Mutex::Autolock autoLock(mStatsLock); //将解码的视频帧数+1 ++mStats.mNumVideoFramesDecoded; &#125; &#125; ...未完，待续... &#125; &#160; &#160; &#160; &#160;上述代码分两部分：&#160; &#160; &#160; &#160;①第一部分判断是否需要seek，若需要则设置option。&#160; &#160; &#160; &#160;②第二部分是从mVideoSource中读取一帧画面，这里读取的时候会将option传入，如果需要seek，则读取出的数据直接就是seek后的解码数据。（中间还有些小细节：如果数据读取失败，则检查宽度高度是否发生变化，否则即video播放完毕了，设置EOF标记，并触发mStreamDoneEvent消息。） &#160; &#160; &#160; &#160;mVideoSource-&gt;read是经过omx调用底层解码器后，读取返回的已经解码过的视频帧数据。关于和omx的调用关系我们以后会讲到。 音频播放Part.3：123456789101112131415161718192021222324252627282930313233343536373839404142434445void AwesomePlayer::onVideoEvent() &#123; ...Part.1... ...Part.2... int64_t timeUs;//从上一步读取到的视频解码帧数据中取出它的时间戳 CHECK(mVideoBuffer-&gt;meta_data()-&gt;findInt64(kKeyTime, &amp;timeUs)); //因为这一帧即将播放，所以赋给mLastVideoTimeUs mLastVideoTimeUs = timeUs; //如果仅仅seek视频，ignore if (mSeeking == SEEK_VIDEO_ONLY) &#123; if (mSeekTimeUs &gt; timeUs) &#123; ALOGI("XXX mSeekTimeUs = %" PRId64 " us, timeUs = %" PRId64 " us", mSeekTimeUs, timeUs); &#125; &#125; &#123; Mutex::Autolock autoLock(mMiscStateLock); //将视频帧时间戳赋值 mVideoTimeUs = timeUs; &#125; SeekType wasSeeking = mSeeking; ////完成audio的seek。之前说如果有seek请求，则先pause住audio，读取seek的video数据，拿到第一帧数据后，以此数据为标准，来seek audio，此处finishSeekIfNecessary便是完成此功能 finishSeekIfNecessary(timeUs); //如果音频没有开始播放，则开始播放音频 if (mAudioPlayer != NULL &amp;&amp; !(mFlags &amp; (AUDIO_RUNNING | SEEK_PREVIEW))) &#123; //播放音频 status_t err = startAudioPlayer_l(); if (err != OK) &#123; ALOGE("Starting the audio player failed w/ err %d", err); return; &#125; &#125; //如果字幕没有开播，则播放字幕 if ((mFlags &amp; TEXTPLAYER_INITIALIZED) &amp;&amp; !(mFlags &amp; (TEXT_RUNNING | SEEK_PREVIEW))) &#123; //播放字幕 mTextDriver-&gt;start(); modifyFlags(TEXT_RUNNING, SET); &#125; ...未完，待续...&#125; &#160; &#160; &#160; &#160;上述代码也分步完：&#160; &#160; &#160; &#160;①从上一步读取到的视频解码帧数据中取出它的时间戳。&#160; &#160; &#160; &#160;②完成audio的seek。之前说如果有seek请求，则先pause住audio，读取seek的video数据，拿到第一帧数据后，以此数据为标准，来seek audio，此处finishSeekIfNecessary便是完成此功能。我们可以看看finishSeekIfNecessary方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445void AwesomePlayer::finishSeekIfNecessary(int64_t videoTimeUs) &#123; ATRACE_CALL(); //如果是仅仅seek视频，则return if (mSeeking == SEEK_VIDEO_ONLY) &#123; mSeeking = NO_SEEK; return; &#125; //如果没有拖动或者是预览时候 if (mSeeking == NO_SEEK || (mFlags &amp; SEEK_PREVIEW)) &#123; return; &#125; // If we paused, then seeked, then resumed, it is possible that we have // signaled SEEK_COMPLETE at a copmletely different media time than where // we are now resuming. Signal new position to media time provider. // Cannot signal another SEEK_COMPLETE, as existing clients may not expect // multiple SEEK_COMPLETE responses to a single seek() request. //一个seek请求在恢复拖动后可能会发送多个SEEK_COMPLETE消息，客户端当然不希望这样，所以需要处理掉多余的SEEK_COMPLETE if (mSeekNotificationSent &amp;&amp; abs(mSeekTimeUs - videoTimeUs) &gt; 10000) &#123; // notify if we are resuming more than 10ms away from desired seek time notifyListener_l(MEDIA_SKIPPED); &#125; if (mAudioPlayer != NULL) &#123; ALOGV("seeking audio to %" PRId64 " us (%.2f secs).", videoTimeUs, videoTimeUs / 1E6); // If we don't have a video time, seek audio to the originally // requested seek time instead. //如果指定了拖动时间点mSeekTimeUs且它大于0，则拖到这个位置；如果没有则拖到原始请求拖动时候的时间点 mAudioPlayer-&gt;seekTo(videoTimeUs &lt; 0 ? mSeekTimeUs : videoTimeUs); mWatchForAudioSeekComplete = true; mWatchForAudioEOS = true; &#125; else if (!mSeekNotificationSent) &#123; // If we're playing video only, report seek complete now, // otherwise audio player will notify us later. //若果仅仅是播放视频，则不用拖动音频 notifyListener_l(MEDIA_SEEK_COMPLETE); mSeekNotificationSent = true; &#125; //拖动完后要设置第一帧标志位FIRST_FRAME modifyFlags(FIRST_FRAME, SET); mSeeking = NO_SEEK; ......&#125; &#160; &#160; &#160; &#160;③seek完audio后，然后开始播放音频。我们看看startAudioPlayer_l：1234567891011121314151617181920212223242526272829303132333435363738394041424344status_t AwesomePlayer::startAudioPlayer_l(bool sendErrorNotification) &#123; ...... if (!(mFlags &amp; AUDIOPLAYER_STARTED)) &#123;//如果还没有开始播放 bool wasSeeking = mAudioPlayer-&gt;isSeeking(); // We've already started the MediaSource in order to enable // the prefetcher to read its data. //开始播放 err = mAudioPlayer-&gt;start( true /* sourceAlreadyStarted */); //错误处理 if (err != OK) &#123; if (sendErrorNotification) &#123; notifyListener_l(MEDIA_ERROR, MEDIA_ERROR_UNKNOWN, err); &#125; return err; &#125; //修改标志位 modifyFlags(AUDIOPLAYER_STARTED, SET); //如果是拖动 if (wasSeeking) &#123; CHECK(!mAudioPlayer-&gt;isSeeking()); // We will have finished the seek while starting the audio player. //发送完成拖动回调 postAudioSeekComplete(); &#125; else &#123; notifyIfMediaStarted_l(); &#125; &#125; else &#123;//如果已经开始了，则恢复播放 err = mAudioPlayer-&gt;resume(); &#125; //播放成功，设置标志位 if (err == OK) &#123; modifyFlags(AUDIO_RUNNING, SET); mWatchForAudioEOS = true; &#125; return err;&#125; &#160; &#160; &#160; &#160;这样就开始播放音频了，关于AudioPlayer的playback流程我们以后再讲。 &#160; &#160; &#160; &#160;④然后播放字幕。 音视频同步Part.4：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647void AwesomePlayer::onVideoEvent() &#123; ...Part.1... ...Part.2... ...Part.3... //同步时钟，是mTimeSource，即AudioPlayer TimeSource *ts = ((mFlags &amp; AUDIO_AT_EOS) || !(mFlags &amp; AUDIOPLAYER_STARTED)) ? &amp;mSystemTimeSource : mTimeSource; int64_t systemTimeUs = mSystemTimeSource.getRealTimeUs(); int64_t looperTimeUs = ALooper::GetNowUs(); //如果是第一帧 if (mFlags &amp; FIRST_FRAME) &#123; modifyFlags(FIRST_FRAME, CLEAR); mSinceLastDropped = 0; mClockEstimator-&gt;reset(); //在4.4以及往前的版本是 mTimeSourceDeltaUs = ts-&gt;getRealTimeUs() - timeUs; //5.0只是更精准了而已，使用了一个WindowedLinearFitEstimator工具类，有兴趣的可以研究。其实和4.4结果差别不大 mTimeSourceDeltaUs = estimateRealTimeUs(ts, systemTimeUs) - timeUs; &#125; int64_t realTimeUs, mediaTimeUs; //如果不是第一帧，mAudioPlayer-&gt;getMediaTimeMapping(&amp;realTimeUs, &amp;mediaTimeUs)返回true，而在第一帧返回false if (!(mFlags &amp; AUDIO_AT_EOS) &amp;&amp; mAudioPlayer != NULL &amp;&amp; mAudioPlayer-&gt;getMediaTimeMapping(&amp;realTimeUs, &amp;mediaTimeUs)) &#123; ALOGV("updating TSdelta (%" PRId64 " =&gt; %" PRId64 " change %" PRId64 ")", mTimeSourceDeltaUs, realTimeUs - mediaTimeUs, mTimeSourceDeltaUs - (realTimeUs - mediaTimeUs)); ATRACE_INT("TS delta change (ms)", (mTimeSourceDeltaUs - (realTimeUs - mediaTimeUs)) / 1E3); mTimeSourceDeltaUs = realTimeUs - mediaTimeUs; &#125; //如果是仅仅拖动视频 if (wasSeeking == SEEK_VIDEO_ONLY) &#123; int64_t nowUs = estimateRealTimeUs(ts, systemTimeUs) - mTimeSourceDeltaUs; int64_t latenessUs = nowUs - timeUs; ATRACE_INT("Video Lateness (ms)", latenessUs / 1E3); if (latenessUs &gt; 0) &#123; ALOGI("after SEEK_VIDEO_ONLY we're late by %.2f secs", latenessUs / 1E6); &#125; &#125; ...未完，待续...&#125; &#160; &#160; &#160; &#160;上面是更新时间信息：&#160; &#160; &#160; &#160;①首先获取时钟源，系统时钟或者audio时钟。我们这里是audio时钟，即AudioPlayer。&#160; &#160; &#160; &#160;②先要说明几个变量的意义： timeUs：这是下一视频帧画面的时间戳 ts-&gt;getRealTimeUs()：这是通过计算播放了多少audio帧换算出来的实际时间。这里我们可以看看AudioPlayer的getRealTimeUs函数的实现，填上上面的坑： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364int64_t AudioPlayer::getRealTimeUs() &#123; Mutex::Autolock autoLock(mLock); if (useOffload()) &#123;//如果支持音频分载 if (mSeeking) &#123; return mSeekTimeUs; &#125; //则从mAudioSink或者mAudioTrack处获取已经播放帧数消耗时间 mPositionTimeRealUs = getOutputPlayPositionUs_l(); return mPositionTimeRealUs; &#125; //直接计算播放了多少帧消耗时间 return getRealTimeUsLocked();&#125;int64_t AudioPlayer::getRealTimeUsLocked() const &#123; CHECK(mStarted); CHECK_NE(mSampleRate, 0); //已经播放的帧数 * 1s / 帧率 = 已经播放帧数消耗时间 int64_t result = -mLatencyUs + (mNumFramesPlayed * 1000000) / mSampleRate; // Compensate for large audio buffers, updates of mNumFramesPlayed // are less frequent, therefore to get a "smoother" notion of time we // compensate using system time. int64_t diffUs; if (mPinnedTimeUs &gt;= 0ll) &#123;//-1ll,小于0 diffUs = mPinnedTimeUs; &#125; else &#123; diffUs = ALooper::GetNowUs(); &#125; //mNumFramesPlayedSysTimeUs = ALooper::GetNowUs() diffUs -= mNumFramesPlayedSysTimeUs; //上面都是障眼法 return result + diffUs;&#125;int64_t AudioPlayer::getOutputPlayPositionUs_l()&#123; uint32_t playedSamples = 0; uint32_t sampleRate; if (mAudioSink != NULL) &#123;//mAudioSink 不为空，setDataSource时已经设置了 mAudioSink-&gt;getPosition(&amp;playedSamples); sampleRate = mAudioSink-&gt;getSampleRate(); &#125; else &#123;//如果为空 mAudioTrack-&gt;getPosition(&amp;playedSamples); sampleRate = mAudioTrack-&gt;getSampleRate(); &#125; if (sampleRate != 0) &#123; mSampleRate = sampleRate; &#125; int64_t playedUs; //一共播放时间 if (mSampleRate != 0) &#123; playedUs = (static_cast&lt;int64_t&gt;(playedSamples) * 1000000 ) / mSampleRate; &#125; else &#123; playedUs = 0; &#125; // HAL position is relative to the first buffer we sent at mStartPosUs //计算出总的渲染时间 const int64_t renderedDuration = mStartPosUs + playedUs; ALOGV("getOutputPlayPositionUs_l %" PRId64, renderedDuration); return renderedDuration;&#125; realTimeUs： 这是从mAudioplayer中获取的信息（如果有audio的话），是当前播放位置的时间 mediaTimeUs：下一音频帧的时间戳 &#160; &#160; &#160; &#160;附注：音频跟视频很不一样，视频每一帧就是一张图像，而从上面的正玄波可以看出，音频数据是流式的，本身没有明确的一帧帧的概念，在实际的应用中，为了音频算法处理/传输的方便，一般约定俗成取2.5ms~60ms为单位的数据量为一帧音频。&#160; &#160; &#160; &#160;这个时间被称之为“采样时间”，其长度没有特别的标准，它是根据编解码器和具体应用的需求来决定的，我们可以计算一下一帧音频帧的大小：&#160; &#160; &#160; &#160;假设某通道的音频信号是采样率为8kHz，位宽为16bit，20ms一帧，双通道，则一帧音频数据的大小为：&#160; &#160; &#160; &#160;int size = 8000 x 16bit x 0.02s x 2 = 5120 bit = 640 byte &#160; &#160; &#160; &#160;这两个变量都是根据mAudioPlayer-&gt;getMediaTimeMapping(&amp;realTimeUs, &amp;mediaTimeUs)方法获取的，我们进入其中看看：12345678910111213141516171819 /*在构造函数里这两个变量初始赋值为-1 mPositionTimeMediaUs(-1), //这是从mAudioplayer中获取的信息（如果有audio的话），是当前播放位置的时间 mPositionTimeRealUs(-1), //下一音频帧的时间戳 */bool AudioPlayer::getMediaTimeMapping( int64_t *realtime_us, int64_t *mediatime_us) &#123; Mutex::Autolock autoLock(mLock); if (useOffload()) &#123; mPositionTimeRealUs = getOutputPlayPositionUs_l(); *realtime_us = mPositionTimeRealUs; *mediatime_us = mPositionTimeRealUs; &#125; else &#123; *realtime_us = mPositionTimeRealUs; *mediatime_us = mPositionTimeMediaUs; &#125; //如果是第一帧，则两个变量都是-1，则返回false；如果不是第一帧，则返回true return mPositionTimeRealUs != -1 &amp;&amp; mPositionTimeMediaUs != -1;&#125; &#160; &#160; &#160; &#160;③这正好应对了Part.4的逻辑： 如果是第一帧画面则mTimeSourceDeltaUs=ts-&gt;getRealTimeUs() - timeUs; 如果不是第一帧画面则：mTimeSourceDeltaUs = realTimeUs - mediaTimeUs; &#160; &#160; &#160; &#160; 对于第一帧的处理，在4.4以及往前的版本是 mTimeSourceDeltaUs = ts-&gt;getRealTimeUs() - timeUs; 5.0只是更精准了而已，使用了一个WindowedLinearFitEstimator工具类，有兴趣的可以研究。其实和4.4结果差别不大，我为了方便分析，所以取这个值。 &#160; &#160; &#160; &#160;下面wasSeeking == SEEK_VIDEO_ONLY先忽略掉,我们继续往下： Part.5：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778void AwesomePlayer::onVideoEvent() &#123; ...Part.1... ...Part.2... ...Part.3... ...Part.4...int64_t latenessUs = 0;//时间偏差latency //播放视频 if (wasSeeking == NO_SEEK) &#123; // Let's display the first frame after seeking right away. //同4.4处理, int64_t nowUs = ts-&gt;getRealTimeUs() - mTimeSourceDeltaUs; int64_t nowUs = estimateRealTimeUs(ts, systemTimeUs) - mTimeSourceDeltaUs; latenessUs = nowUs - timeUs; ATRACE_INT("Video Lateness (ms)", latenessUs / 1E3); //如果latency过大，则要跳帧，seek到固定为止 if (latenessUs &gt; 500000ll &amp;&amp; mAudioPlayer != NULL &amp;&amp; mAudioPlayer-&gt;getMediaTimeMapping( &amp;realTimeUs, &amp;mediaTimeUs)) &#123; if (mWVMExtractor == NULL) &#123;//如果不是google的私货 ALOGI("we're much too late (%.2f secs), video skipping ahead", latenessUs / 1E6); //清空VideoBuffer，为下一次读取解码帧做准备 mVideoBuffer-&gt;release(); mVideoBuffer = NULL; //跳帧仅仅拖动视频 mSeeking = SEEK_VIDEO_ONLY; mSeekTimeUs = mediaTimeUs; //postVideoEvent_l函数参数缺省为-1，所以10ms后下一次onVideoEvent postVideoEvent_l(); return; &#125; else &#123;//是google的私货，good bye // The widevine extractor doesn't deal well with seeking // audio and video independently. We'll just have to wait // until the decoder catches up, which won't be long at all. ALOGI("we're very late (%.2f secs)", latenessUs / 1E6); &#125; &#125; //如果latency稍大，则要丢帧 if (latenessUs &gt; 40000) &#123; // We're more than 40ms late. ALOGV("we're late by %" PRId64 " us (%.2f secs)", latenessUs, latenessUs / 1E6); if (!(mFlags &amp; SLOW_DECODER_HACK) || mSinceLastDropped &gt; FRAME_DROP_FREQ) &#123; ALOGV("we're late by %" PRId64 " us (%.2f secs) dropping " "one after %d frames", latenessUs, latenessUs / 1E6, mSinceLastDropped); mSinceLastDropped = 0; mVideoBuffer-&gt;release(); mVideoBuffer = NULL; &#123; Mutex::Autolock autoLock(mStatsLock); //丢帧数+1 ++mStats.mNumVideoFramesDropped; &#125; //丢帧后立刻下一次onVideoEvent postVideoEvent_l(0); return; &#125; &#125; //如果latency超前了，则要等待延时下一次onVideoEvent if (latenessUs &lt; -30000) &#123; // We're more than 30ms early, schedule at most 20 ms before time due postVideoEvent_l(latenessUs &lt; -60000 ? 30000 : -latenessUs - 20000); return; &#125; &#125; ...未完，待续...&#125; &#160; &#160; &#160; &#160;上面代码和之前的时间处理要结合起来看，计算出来mTimeSourceDeltaUs之后，就可以分析播放信息如： 当前播放进度，即int64_t nowUs = ts-&gt;getRealTimeUs() - mTimeSourceDeltaUs; 播放的latency： int64_t latenessUs = nowUs - timeUs; （这里timeUs是下一帧的时间戳） &#160; &#160; &#160; &#160;下面是处理latency过大的情况：这里比对参考是audio或者系统时钟，即与音视频同步的处理： 超过500000ll US，则seek到对应位置，跳帧 超过40000 则丢帧处理 当比参考时钟早了30ms，则通过postVideoEvent_l(latenessUs &lt; -60000 ? 30000 : -latenessUs - 20000);延迟触发下一次的mVideoEvent &#160; &#160; &#160; &#160;音视频同步就分析到这里，自我感觉挺重要的。 视频播放Part.6：123456789101112131415161718192021222324252627282930313233343536373839void AwesomePlayer::onVideoEvent() &#123; ...Part.1... ...Part.2... ...Part.3... ...Part.4... ...Part.5... if ((mNativeWindow != NULL) &amp;&amp; (mVideoRendererIsPreview || mVideoRenderer == NULL)) &#123; mVideoRendererIsPreview = false; //初始化视窗 initRenderer_l(); &#125; if (mVideoRenderer != NULL) &#123; mSinceLastDropped++; mVideoBuffer-&gt;meta_data()-&gt;setInt64(kKeyTime, looperTimeUs - latenessUs); //开始渲染画面 mVideoRenderer-&gt;render(mVideoBuffer); if (!mVideoRenderingStarted) &#123; mVideoRenderingStarted = true; notifyListener_l(MEDIA_INFO, MEDIA_INFO_RENDERING_START); &#125; if (mFlags &amp; PLAYING) &#123; notifyIfMediaStarted_l(); &#125; &#125; mVideoBuffer-&gt;release(); mVideoBuffer = NULL; if (wasSeeking != NO_SEEK &amp;&amp; (mFlags &amp; SEEK_PREVIEW)) &#123; modifyFlags(SEEK_PREVIEW, CLEAR); return; &#125; ...未完，待续...&#125; &#160; &#160; &#160; &#160;最后就是显示此帧画面了，当播放过程一切正常时，则显示此帧画面。我们可以看看初始化Renderer的方法initRenderer_l：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950void AwesomePlayer::initRenderer_l() &#123; ATRACE_CALL(); if (mNativeWindow == NULL) &#123; return; &#125; //先获取视频元数据 sp&lt;MetaData&gt; meta = mVideoSource-&gt;getFormat(); int32_t format; const char *component; int32_t decodedWidth, decodedHeight; CHECK(meta-&gt;findInt32(kKeyColorFormat, &amp;format));//颜色编码 CHECK(meta-&gt;findCString(kKeyDecoderComponent, &amp;component));//解码组件 CHECK(meta-&gt;findInt32(kKeyWidth, &amp;decodedWidth));//视频宽度 CHECK(meta-&gt;findInt32(kKeyHeight, &amp;decodedHeight));//视频高度 int32_t rotationDegrees;//获取旋转角度 if (!mVideoTrack-&gt;getFormat()-&gt;findInt32( kKeyRotation, &amp;rotationDegrees)) &#123; rotationDegrees = 0; &#125; mVideoRenderer.clear(); // Must ensure that mVideoRenderer's destructor is actually executed // before creating a new one. IPCThreadState::self()-&gt;flushCommands(); // Even if set scaling mode fails, we will continue anyway setVideoScalingMode_l(mVideoScalingMode); //如果是硬解 if (USE_SURFACE_ALLOC &amp;&amp; !strncmp(component, "OMX.", 4) &amp;&amp; strncmp(component, "OMX.google.", 11)) &#123; // Hardware decoders avoid the CPU color conversion by decoding // directly to ANativeBuffers, so we must use a renderer that // just pushes those buffers to the ANativeWindow. mVideoRenderer = new AwesomeNativeWindowRenderer(mNativeWindow, rotationDegrees); &#125; else &#123;//如果是软解 // Other decoders are instantiated locally and as a consequence // allocate their buffers in local address space. This renderer // then performs a color conversion and copy to get the data // into the ANativeBuffer. sp&lt;AMessage&gt; format; convertMetaDataToMessage(meta, &amp;format); mVideoRenderer = new AwesomeLocalRenderer(mNativeWindow, format); &#125;&#125; &#160; &#160; &#160; &#160;就是对硬解和软解选择不同的Renderer。 Part.7：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354void AwesomePlayer::onVideoEvent() &#123; ...Part.1... ...Part.2... ...Part.3... ...Part.4... ...Part.5... ...Part.6... /* get next frame time 获取下一视频帧的时间*/ if (wasSeeking == NO_SEEK) &#123; MediaSource::ReadOptions options; for (;;) &#123; //同上，读取下一视频帧数据 status_t err = mVideoSource-&gt;read(&amp;mVideoBuffer, &amp;options); if (err != OK) &#123; // deal with any errors next time CHECK(mVideoBuffer == NULL); postVideoEvent_l(0); return; &#125; if (mVideoBuffer-&gt;range_length() != 0) &#123; break; &#125; // Some decoders, notably the PV AVC software decoder // return spurious empty buffers that we just want to ignore. mVideoBuffer-&gt;release(); mVideoBuffer = NULL; &#125; &#123; Mutex::Autolock autoLock(mStatsLock); ++mStats.mNumVideoFramesDecoded; &#125; int64_t nextTimeUs; //下一视频帧时间戳 CHECK(mVideoBuffer-&gt;meta_data()-&gt;findInt64(kKeyTime, &amp;nextTimeUs)); systemTimeUs = mSystemTimeSource.getRealTimeUs(); //延时时间 int64_t delayUs = nextTimeUs - estimateRealTimeUs(ts, systemTimeUs) + mTimeSourceDeltaUs; ATRACE_INT("Frame delta (ms)", (nextTimeUs - timeUs) / 1E3); ALOGV("next frame in %" PRId64, delayUs); // try to schedule 30ms before time due //延时处理 postVideoEvent_l(delayUs &gt; 60000 ? 30000 : (delayUs &lt; 30000 ? 0 : delayUs - 30000)); return; &#125; //如果在get next frame time上述逻辑return了，就不会触发这个了 postVideoEvent_l();&#125; &#160; &#160; &#160; &#160;上述逻辑主要是获取下一视频帧的先关时间信息，然后通过计算延时处理下一次onVideEvent回调。如果在get next frame time上述逻辑return了，就不会触发末尾的postVideoEvent_l了。 &#160; &#160; &#160; &#160;分析到这里大家应该明白，awesoemplayer的播放驱动机制即通过递归的调用postVideoEvent_l(); 来完成。而且由于postVideoEvent_l(); 里有延迟触发消息机制，因此也不会阻塞。 &#160; &#160; &#160; &#160;现在应该回到play_l函数中，处理消息递归处理最后一步：12345678if (mVideoSource != NULL) &#123; // Kick off video playback postVideoEvent_l();//这个已经处理过了 if (mAudioSource != NULL &amp;&amp; mVideoSource != NULL) &#123; postVideoLagEvent_l();//然后是这个消息处理 &#125;&#125; &#160; &#160; &#160; &#160;postVideoLagEvent看下此事件的处理方法：12345678910111213141516171819202122void AwesomePlayer::onVideoLagUpdate() &#123; Mutex::Autolock autoLock(mLock); if (!mVideoLagEventPending) &#123; return; &#125; mVideoLagEventPending = false; int64_t audioTimeUs = mAudioPlayer-&gt;getMediaTimeUs(); int64_t videoLateByUs = audioTimeUs - mVideoTimeUs; if (!(mFlags &amp; VIDEO_AT_EOS) &amp;&amp; videoLateByUs &gt; 300000ll) &#123; ALOGV("video late by %lld ms.", videoLateByUs / 1000ll); notifyListener_l( MEDIA_INFO, MEDIA_INFO_VIDEO_TRACK_LAGGING, videoLateByUs / 1000ll); &#125; postVideoLagEvent_l();&#125; &#160; &#160; &#160; &#160;这一段其实没什么多大意义，主要是为了更新信息。我们可以看看MEDIA_INFO_VIDEO_TRACK_LAGGING 这个标志位的意义：123// The video is too complex for the decoder: it can't decode frames fast// enough. Possibly only the audio plays fine at this stage.MEDIA_INFO_VIDEO_TRACK_LAGGING = 700, &#160; &#160; &#160; &#160;可以知道当视频解码速度不够时，会通知上层，decoder不给力。 &#160; &#160; &#160; &#160;上述就是播放流程的大概过程。 其他回调事件分析mStreamDoneEvent&#160; &#160; &#160; &#160;这里是当vidoe播放结束后会触发，在onVideoEvent中当读取帧数据失败时。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859void AwesomePlayer::onStreamDone() &#123; // Posted whenever any stream finishes playing. ATRACE_CALL(); Mutex::Autolock autoLock(mLock); //mStreamDoneEvent 事件是否存在 if (!mStreamDoneEventPending) &#123; return; &#125; mStreamDoneEventPending = false; //错误处理 if (mStreamDoneStatus != ERROR_END_OF_STREAM) &#123; ALOGV("MEDIA_ERROR %d", mStreamDoneStatus); notifyListener_l( MEDIA_ERROR, MEDIA_ERROR_UNKNOWN, mStreamDoneStatus); pause_l(true /* at eos */); modifyFlags(AT_EOS, SET); return; &#125; //是否播放完了 const bool allDone = (mVideoSource == NULL || (mFlags &amp; VIDEO_AT_EOS)) &amp;&amp; (mAudioSource == NULL || (mFlags &amp; AUDIO_AT_EOS)); if (!allDone) &#123; return; &#125; //如果是循环播放 if ((mFlags &amp; LOOPING) || ((mFlags &amp; AUTO_LOOPING) &amp;&amp; (mAudioSink == NULL || mAudioSink-&gt;realtime()))) &#123; // Don't AUTO_LOOP if we're being recorded, since that cannot be // turned off and recording would go on indefinitely. //seek到起始位置 seekTo_l(0); if (mVideoSource != NULL) &#123; postVideoEvent_l(); &#125; &#125; else &#123; ALOGV("MEDIA_PLAYBACK_COMPLETE"); //通知上层播放完毕 notifyListener_l(MEDIA_PLAYBACK_COMPLETE); //播放完毕，暂停 pause_l(true /* at eos */); // If audio hasn't completed MEDIA_SEEK_COMPLETE yet, // notify MEDIA_SEEK_COMPLETE to observer immediately for state persistence. if (mWatchForAudioSeekComplete) &#123; notifyListener_l(MEDIA_SEEK_COMPLETE); mWatchForAudioSeekComplete = false; &#125; modifyFlags(AT_EOS, SET); &#125;&#125; &#160; &#160; &#160; &#160;主要做了这几件事： 判断是否真的播放完毕了 若播放完毕了，是否需要循环，若需要则调用seekTo_l（0）继续播放 否则，通知上层本次播放结束，发送MEDIA_PLAYBACK_COMPLETE给调用者 mBufferingEvent&#160; &#160; &#160; &#160;awesomeplayer中通过调用postBufferingEvent_l来触发此事件，作用是缓冲数据。调用的位置有：12345678910void AwesomePlayer::onPrepareAsyncEvent() &#123; ...... if(isStreamingHTTP()) &#123; postBufferingEvent_l(); &#125;else&#123; finishAsyncPrepare_l(); &#125; &#125; &#160; &#160; &#160; &#160;当时网络流的时候，先缓冲一部分数据，看下具体实现:1234567void AwesomePlayer::postBufferingEvent_l() &#123; if(mBufferingEventPending) &#123; return; &#125; mBufferingEventPending =true; mQueue.postEventWithDelay(mBufferingEvent, 1000000ll); &#125; &#160; &#160; &#160; &#160;首先修改标志位mBufferingEventPending，之后触发消息。&#160; &#160; &#160; &#160;这里就不贴代码了，说说原理：当需要cache数据的时候，在onPrepareAsyncEvent调用postBufferingEvent_l 后onPrepareAsyncEvent 就结束了。由于此时解码器已经开始解码，即数据链路已经建立。因此会不断的进行 读数据-解码的操作，而在onBufferingUpdate响应函数中，会先pause住输出，等数据缓存足够了之后，调用finishAsyncPrepare_l等完成prepareAsync的操作。 结语&#160; &#160; &#160; &#160;到这里播放流程就分析完了，最复杂的还是递归消息处理那一段。流程中对于Video Buffer传入流程和Audio Playback流程还没分析，这个我们以后再讲。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android多媒体开发(七)----Android中OpenMax的实现]]></title>
    <url>%2F2016%2F12%2F29%2FAndroid%E5%A4%9A%E5%AA%92%E4%BD%93%E5%BC%80%E5%8F%91-%E4%B8%83-Android%E4%B8%ADOpenMax%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;preview篇是android对openmax的接口实现的一些规则，还有一些厂商的方案。本节就顺着上上篇的流程，分析一下从AwesomePlayer到OpenMax的调用流程。 AwesomePlayer中openmax的入口&#160; &#160; &#160; &#160;android中很多模块都是C/S架构的，这里AwesomePlayer中获取openmax的入口也不例外。如果要获取OMX服务，AwesomePlayer是作为Client端的。&#160; &#160; &#160; &#160;AwesomePlayer 中有个变量，声明位于framework/av/media/libstagefright/include/AwesomePlayer.h中 ：1OMXClient mClient; 我们看看让我们看看 OMXClient 这个类，位于framework/av/include/media/stagefright/OMXClient.h：1234567891011121314151617class OMXClient &#123;public: OMXClient(); status_t connect(); void disconnect(); sp&lt;IOMX&gt; interface() &#123; return mOMX; &#125;private: sp&lt;IOMX&gt; mOMX; OMXClient(const OMXClient &amp;); OMXClient &amp;operator=(const OMXClient &amp;);&#125;; &#160; &#160; &#160; &#160;OMXClient 有个IOMX 的变量 mOMX ，这个就是和OMX服务进行binder通讯的。&#160; &#160; &#160; &#160;上一篇 的Android中OpenMax的适配层中讲到，这个IOMX就是OpenMax的适配层接口。IOMX表示OpenMax的一个组件，根据Android的Binder IPC机制，BnOMX继承IOMX，实现者需要继承实现BnOMX。 &#160; &#160; &#160; &#160;在 AwesomePlayer 的构造函数中会调用如下代码，位于framework/av/media/libstagefright/AwesomePlayer.cpp中：1234567AwesomePlayer::AwesomePlayer() : ...... &#123; ...... CHECK_EQ(mClient.connect(), (status_t)OK); ...... &#125; &#160; &#160; &#160; &#160;构造方法中会调用OMXClient的connect方法，位于framework/av/media/libstagefright/OMXClient.cpp：1234567891011121314151617181920status_t OMXClient::connect() &#123; /*获取MediaPlayerService服务*/ sp&lt;IServiceManager&gt; sm = defaultServiceManager(); sp&lt;IBinder&gt; binder = sm-&gt;getService(String16("media.player")); sp&lt;IMediaPlayerService&gt; service = interface_cast&lt;IMediaPlayerService&gt;(binder); CHECK(service.get() != NULL); //然后通过MediaPlayerService来创建OMX的实例 mOMX = service-&gt;getOMX(); CHECK(mOMX.get() != NULL); //什么情况会有不是在本地，貌似OMX是在MediaPlayerService当中new出来的， // OMXClient是在AwesomePlayer当中new出来的，而AwesomePlayer又是在MediaPlayerService当中new出来 // 所以什么情况下会走到如下的这个using client-side OMX mux当中去(MuxOMX位于OMXClient.cpp当中)？ if (!mOMX-&gt;livesLocally(0 /* node */, getpid())) &#123; ALOGI("Using client-side OMX mux."); mOMX = new MuxOMX(mOMX); &#125; return OK;&#125; &#160; &#160; &#160; &#160;OMXClient::connect函数是通过binder机制 获得到MediaPlayerService，然后通过MediaPlayerService来创建OMX的实例。这样OMXClient就获得到了OMX的入口，接下来就可以通过binder机制来获得OMX提供的服务：123456789sp&lt;IOMX&gt; MediaPlayerService::getOMX() &#123; Mutex::Autolock autoLock(mLock); if (mOMX.get() == NULL) &#123; mOMX = new OMX; &#125; return mOMX;&#125; &#160; &#160; &#160; &#160;（什么情况会有不是在本地，貌似OMX是在MediaPlayerService当中new出来的，OMXClient是在AwesomePlayer当中new出来的，而AwesomePlayer又是在MediaPlayerService当中new出来， 所以什么情况下会走到如下的这个using client-side OMX mux当中去(MuxOMX位于OMXClient.cpp当中)？） &#160; &#160; &#160; &#160;OMX的定义位于framework/av/media/libstagefright/include/OMX.h中，我们可以看到OMX继承于BnOMX，所以这里我们就获取了OpenMax的入口了。 &#160; &#160; &#160; &#160;在创建音视频解码mVideoSource、mAudioSource的时候会把OMXClient中的sp&lt; IOMX &gt; mOMX的实例 传给mVideoSource、mAudioSource来共享使用这个OMX的入口。&#160; &#160; &#160; &#160;也就是说一个AwesomePlayer对应着 一个IOMX 变量，AwesomePlayer中的音视频解码器共用这个IOMX变量来获得OMX服务。123456789101112131415//OMXClient.h中的interface函数sp&lt;IOMX&gt; interface() &#123; return mOMX; &#125;//AwesomePlayer.cpp中initAudioDecoder函数片段，创建音频解码器 mAudioSource = OMXCodec::Create( mClient.interface(), mAudioTrack-&gt;getFormat(), false, // createEncoder mAudioTrack); //AwesomePlayer.cpp中initVideoDecoder函数片段，创建视频解码器 mVideoSource = OMXCodec::Create( mClient.interface(), mVideoTrack-&gt;getFormat(), false, // createEncoder mVideoTrack, NULL, flags, USE_SURFACE_ALLOC ? mNativeWindow : NULL); OMX中的成员&#160; &#160; &#160; &#160;每个AwesomePlayer 只有一个OMX服务的入口，但是AwesomePlayer不一定就只需要1种解码器。有可能音视频都有，或者有很多种。这个时候这些解码器都需要OMX的服务，也就是OMX那头需要建立不同的解码器的组件来对应着AwesomePlayer中不同的code。OMX中非常重要的2个成员就是 OMXMaster 和 OMXNodeInstance。OMX通过这俩个成员来创建和维护不同的openmax 解码器组件，为AwesomePlayer中不同解码提供服务。让我们看看他们是怎么实现这些工作的。 OMX中 OMXNodeInstance 负责创建并维护不同的实例，这些实例是根据上面需求创建的，以node作为唯一标识。这样播放器中每个OMXCodec在OMX服务端都对应有了自己的OMXNodeInstance实例。 OMXMaster 维护底层软硬件解码库，根据OMXNodeInstance中想要的解码器来创建解码实体组件。 &#160; &#160; &#160; &#160;接下来我们来看看解码器创建的流程。 准备工作初始化OMXMaster&#160; &#160; &#160; &#160;OMX构造函数中会进行初始化OMXMaster，位于framework/av/media/libstagefright/omx/OMX.cpp中：1234567//这个mMaster全部变量声明位于OMX.h中OMXMaster *mMaster; OMX::OMX() : mMaster(new OMXMaster), mNodeCounter(0) &#123;&#125; &#160; &#160; &#160; &#160;接着看OMXMaster的构造方法，位于framework/av/media/libstagefright/omx/OMXMaster.cpp中：12345OMXMaster::OMXMaster() : mVendorLibHandle(NULL) &#123; addVendorPlugin();//添加芯片商编解码插件(硬解) addPlugin(new SoftOMXPlugin);//添加软解插件&#125; &#160; &#160; &#160; &#160;OMXMaster 负责OMX中编解码器插件管理，软件解码和硬件解码都是使用OMX标准，挂载plugins的方式来进行管理。接下来我们分析这两个流程： 添加硬解插件&#160; &#160; &#160; &#160;硬解插件查看addVendorPlugin函数：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758void OMXMaster::addVendorPlugin() &#123; addPlugin("libstagefrighthw.so");//添加libstagefrighthw.so&#125;void OMXMaster::addPlugin(const char *libname) &#123; mVendorLibHandle = dlopen(libname, RTLD_NOW);//动态打开libstagefrighthw.so if (mVendorLibHandle == NULL) &#123; return; &#125; //定义一个OMXPluginBase 函数指针 typedef OMXPluginBase *(*CreateOMXPluginFunc)(); //调用插件类中的createOMXPlugin函数，创建一个编解码plugin CreateOMXPluginFunc createOMXPlugin = (CreateOMXPluginFunc)dlsym( mVendorLibHandle, "createOMXPlugin"); //度过调用上述方法失败，则调用如下函数(一般不会失败) if (!createOMXPlugin) createOMXPlugin = (CreateOMXPluginFunc)dlsym( mVendorLibHandle, "_ZN7android15createOMXPluginEv"); //如果创建编解码插件成功，则调用如下重载函数 if (createOMXPlugin) &#123; addPlugin((*createOMXPlugin)()); &#125;&#125; //保存软解和硬解的插件List List&lt;OMXPluginBase *&gt; mPlugins; //保存所有编解码组件的map KeyedVector&lt;String8, OMXPluginBase *&gt; mPluginByComponentName; void OMXMaster::addPlugin(OMXPluginBase *plugin) &#123; Mutex::Autolock autoLock(mLock); //将硬解插件加入到list中 mPlugins.push_back(plugin); OMX_U32 index = 0; char name[128]; OMX_ERRORTYPE err; //循环遍历所有的编解码组件 while ((err = plugin-&gt;enumerateComponents( name, sizeof(name), index++)) == OMX_ErrorNone) &#123; String8 name8(name); //如果这个组件已经加载过了，则忽略，continue if (mPluginByComponentName.indexOfKey(name8) &gt;= 0) &#123; ALOGE("A component of name '%s' already exists, ignoring this one.", name8.string()); continue; &#125; //然后根据每一个组件的名字和硬解插件对象指针，保存进map mPluginByComponentName.add(name8, plugin); &#125; if (err != OMX_ErrorNoMore) &#123; ALOGE("OMX plugin failed w/ error 0x%08x after registering %zu " "components", err, mPluginByComponentName.size()); &#125; &#160; &#160; &#160; &#160;这里主要分两步：&#160; &#160; &#160; &#160;（1）动态打开libstagefrighthw.so库，然后获取硬解插件对象指针。&#160; &#160; &#160; &#160;这个libstagefrighthw.so是和硬件相关的，每个厂商内部实现都不一样，编译目录都在hardware/[厂商]/{一些目录}/libstagefrighthw/ 下面，以Qualcomm(高通)为例，就是hardware/qcom/media/libstagefrighthw/，这个目录下的文件都会被编译进入libstagefrighthw.so。 &#160; &#160; &#160; &#160;（2）先保存上一步获取的硬解码插件。&#160; &#160; &#160; &#160;循环遍历所有的编解码组件，然后根据每一个组件的名字和硬解插件对象指针，保存进mPluginByComponentName这个类似map的变量。 &#160; &#160; &#160; &#160;循环遍历硬解插件的流程我们就得进入第一步提到的编译硬解插件的libstagefrighthw.so的源文件看看了。依然以高通为例，步入hardware/qcom/media/libstagefrighthw/QComOMXPlugin：12345678910111213141516171819202122232425262728293031323334353637383940OMX_ERRORTYPE QComOMXPlugin::enumerateComponents( OMX_STRING name, size_t size, OMX_U32 index) &#123; if (mLibHandle == NULL) &#123; return OMX_ErrorUndefined; &#125; //会调用mComponentNameEnum函数指针，赋值位于构造函数 return (*mComponentNameEnum)(name, size, index);&#125;/*我们第一步创建硬解插件的对象，然后将指针传递给上面保存*/OMXPluginBase *createOMXPlugin() &#123; return new QComOMXPlugin;&#125;//QComOMXPlugin的构造函数QComOMXPlugin::QComOMXPlugin() : mLibHandle(dlopen("libOmxCore.so", RTLD_NOW)), mInit(NULL), mDeinit(NULL), mComponentNameEnum(NULL), mGetHandle(NULL), mFreeHandle(NULL), mGetRolesOfComponentHandle(NULL) &#123; if (mLibHandle != NULL) &#123; mInit = (InitFunc)dlsym(mLibHandle, "OMX_Init"); mDeinit = (DeinitFunc)dlsym(mLibHandle, "OMX_Deinit"); //这个是我们需要的，在libOmxCore.so里面，调用OMX_ComponentNameEnum函数 mComponentNameEnum = (ComponentNameEnumFunc)dlsym(mLibHandle, "OMX_ComponentNameEnum"); mGetHandle = (GetHandleFunc)dlsym(mLibHandle, "OMX_GetHandle"); mFreeHandle = (FreeHandleFunc)dlsym(mLibHandle, "OMX_FreeHandle"); mGetRolesOfComponentHandle = (GetRolesOfComponentFunc)dlsym( mLibHandle, "OMX_GetRolesOfComponent"); (*mInit)(); &#125; &#160; &#160; &#160; &#160;然后我们需要的，在libOmxCore.so里面，调用OMX_ComponentNameEnum函数。这时候我们就得查看上一篇文章，Android多媒体开发(六)—-Android中OpenMax的实现(preview) ，看看高通对于这一部分的而实现。&#160; &#160; &#160; &#160;找到后进入hardware/qcom/media/mm-core/omxcore/src/common/qc_omx_core.cpp中：12345678910111213141516171819202122232425OMX_API OMX_ERRORTYPE OMX_APIENTRYOMX_ComponentNameEnum(OMX_OUT OMX_STRING componentName, OMX_IN OMX_U32 nameLen, OMX_IN OMX_U32 index)&#123; OMX_ERRORTYPE eRet = OMX_ErrorNone; DEBUG_PRINT("OMXCORE API - OMX_ComponentNameEnum %x %d %d\n",(unsigned) componentName ,(unsigned)nameLen ,(unsigned)index); if(index &lt; SIZE_OF_CORE) &#123; #ifdef _ANDROID_ //其实就是这个core数组里面所有的组件名，这个core我们上一节也分析过omx_core_cb_type core[] //是不同型号中注册的编解码组件，在hardware/qcom/media/mm-core/src下面 strlcpy(componentName, core[index].name,nameLen); #else strncpy(componentName, core[index].name,nameLen); #endif &#125; else &#123; eRet = OMX_ErrorNoMore; &#125; return eRet;&#125; &#160; &#160; &#160; &#160;因为要找到所有的硬件编解码组件，我们找到了上一盘文章分析到的数组core。其实就是这个core数组里面所有的组件名，这个core我们上一节也分析过omx_core_cb_type core[]，是不同型号中注册的编解码组件，在hardware/qcom/media/mm-core/src下面。会看到有许多型号，7627A、7630、8084、8226、8610、8660等等。比如这个8974的，位于hardware/qcom/media/mm-core/src/8974/qc_registry_table_android.c。里面有非常多的硬件编解码组件。 &#160; &#160; &#160; &#160;然后根据每一个组件的名字和硬解插件对象指针，保存进mPluginByComponentName这个类似map的变量。&#160; &#160; &#160; &#160;硬解码插件添加就到这儿了。 添加软解插件&#160; &#160; &#160; &#160;软解插件添加就简单了，addPlugin(new SoftOMXPlugin)。所以我们看看SoftOMXPlugin的enumerateComponents函数，位于framework/av/media/libstagefright/omx/SoftOMXPlugin.cpp中：123456789101112OMX_ERRORTYPE SoftOMXPlugin::enumerateComponents( OMX_STRING name, size_t /* size */, OMX_U32 index) &#123; if (index &gt;= kNumComponents) &#123; return OMX_ErrorNoMore; &#125; //软解查找所有的组件就是在kComponents这个数组里 strcpy(name, kComponents[index].mName); return OMX_ErrorNone;&#125; &#160; &#160; &#160; &#160;软解查找所有的组件就是在kComponents这个数组里，我们看看这个数组：12345678910111213141516171819202122232425262728293031static const struct &#123; const char *mName; const char *mLibNameSuffix; const char *mRole;&#125; kComponents[] = &#123; &#123; "OMX.google.aac.decoder", "aacdec", "audio_decoder.aac" &#125;, &#123; "OMX.google.aac.encoder", "aacenc", "audio_encoder.aac" &#125;, &#123; "OMX.google.amrnb.decoder", "amrdec", "audio_decoder.amrnb" &#125;, &#123; "OMX.google.amrnb.encoder", "amrnbenc", "audio_encoder.amrnb" &#125;, &#123; "OMX.google.amrwb.decoder", "amrdec", "audio_decoder.amrwb" &#125;, &#123; "OMX.google.amrwb.encoder", "amrwbenc", "audio_encoder.amrwb" &#125;, &#123; "OMX.google.h264.decoder", "h264dec", "video_decoder.avc" &#125;, &#123; "OMX.google.h264.encoder", "h264enc", "video_encoder.avc" &#125;, &#123; "OMX.google.hevc.decoder", "hevcdec", "video_decoder.hevc" &#125;, &#123; "OMX.google.g711.alaw.decoder", "g711dec", "audio_decoder.g711alaw" &#125;, &#123; "OMX.google.g711.mlaw.decoder", "g711dec", "audio_decoder.g711mlaw" &#125;, &#123; "OMX.google.h263.decoder", "mpeg4dec", "video_decoder.h263" &#125;, &#123; "OMX.google.h263.encoder", "mpeg4enc", "video_encoder.h263" &#125;, &#123; "OMX.google.mpeg4.decoder", "mpeg4dec", "video_decoder.mpeg4" &#125;, &#123; "OMX.google.mpeg4.encoder", "mpeg4enc", "video_encoder.mpeg4" &#125;, &#123; "OMX.google.mp3.decoder", "mp3dec", "audio_decoder.mp3" &#125;, &#123; "OMX.google.vorbis.decoder", "vorbisdec", "audio_decoder.vorbis" &#125;, &#123; "OMX.google.opus.decoder", "opusdec", "audio_decoder.opus" &#125;, &#123; "OMX.google.vp8.decoder", "vpxdec", "video_decoder.vp8" &#125;, &#123; "OMX.google.vp9.decoder", "vpxdec", "video_decoder.vp9" &#125;, &#123; "OMX.google.vp8.encoder", "vpxenc", "video_encoder.vp8" &#125;, &#123; "OMX.google.raw.decoder", "rawdec", "audio_decoder.raw" &#125;, &#123; "OMX.google.flac.encoder", "flacenc", "audio_encoder.flac" &#125;, &#123; "OMX.google.gsm.decoder", "gsmdec", "audio_decoder.gsm" &#125;,&#125;; &#160; &#160; &#160; &#160;android 默认会提供一系列的软件解码器，我们会发现这些软解组件名都是以OMX.google开头的。 &#160; &#160; &#160; &#160;然后根据每一个组件的名字和软解插件对象指针，同样保存进mPluginByComponentName这个类似map的变量。 创建mVideoSource/mAudioSource&#160; &#160; &#160; &#160;有了上面的OMX，接下来会在AwesomePlayer::initVideoDecoder中创建mVideoSource，AwesomePlayer::initAudioDecoder中创建mAudioSource。 创建mVideoSource&#160; &#160; &#160; &#160;创建视频解码器，依然位于AwesomePlayer中，省略部分代码如下：123456789101112status_t AwesomePlayer::initVideoDecoder(uint32_t flags) &#123; ATRACE_CALL(); ...... mVideoSource = OMXCodec::Create( mClient.interface(), mVideoTrack-&gt;getFormat(), false, // createEncoder mVideoTrack, NULL, flags, USE_SURFACE_ALLOC ? mNativeWindow : NULL); status_t err = mVideoSource-&gt;start(); ...... return mVideoSource != NULL ? OK : UNKNOWN_ERROR; &#125; &#160; &#160; &#160; &#160;保留主要部分，然后查看OMXCodec的Create方法，位于framework/av/media/libstagefright/OMXCode.cpp中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899// staticsp&lt;MediaSource&gt; OMXCodec::Create( const sp&lt;IOMX&gt; &amp;omx, //OMX const sp&lt;MetaData&gt; &amp;meta, //视频源的元数据 bool createEncoder, //false const sp&lt;MediaSource&gt; &amp;source, //mVideoTrack视频源 const char *matchComponentName,//NULl uint32_t flags,//缺省，0 const sp&lt;ANativeWindow&gt; &amp;nativeWindow) &#123; int32_t requiresSecureBuffers; //如果是安全的数据源，需要DRM处理 if (source-&gt;getFormat()-&gt;findInt32( kKeyRequiresSecureBuffers, &amp;requiresSecureBuffers) &amp;&amp; requiresSecureBuffers) &#123; flags |= kIgnoreCodecSpecificData; flags |= kUseSecureInputBuffers; &#125; const char *mime; //获取视频源的mime bool success = meta-&gt;findCString(kKeyMIMEType, &amp;mime); CHECK(success); //匹配的解码器组件 Vector&lt;CodecNameAndQuirks&gt; matchingCodecs; //查找和视频源mime匹配的所有解码器组件 findMatchingCodecs( mime, createEncoder, matchComponentName, flags, &amp;matchingCodecs); //如果没找到解码器返回NULL if (matchingCodecs.isEmpty()) &#123; ALOGV("No matching codecs! (mime: %s, createEncoder: %s, " "matchComponentName: %s, flags: 0x%x)", mime, createEncoder ? "true" : "false", matchComponentName, flags); return NULL; &#125; //创建一个OMXCodecObserver消息观察器，用于处理消息回调 sp&lt;OMXCodecObserver&gt; observer = new OMXCodecObserver; IOMX::node_id node = 0; //遍历和视频源mime匹配的所有解码器组件 for (size_t i = 0; i &lt; matchingCodecs.size(); ++i) &#123; //解码组件名 const char *componentNameBase = matchingCodecs[i].mName.string(); //解码组件的Quirk uint32_t quirks = matchingCodecs[i].mQuirks; const char *componentName = componentNameBase; ...... //如果是编码器，false if (createEncoder) &#123; sp&lt;MediaSource&gt; softwareCodec = InstantiateSoftwareEncoder(componentName, source, meta); if (softwareCodec != NULL) &#123; ALOGV("Successfully allocated software codec '%s'", componentName); return softwareCodec; &#125; &#125; ALOGV("Attempting to allocate OMX node '%s'", componentName); //如果是安全的数据源，则需要OMX.SEC开头的解码器 if (!createEncoder &amp;&amp; (quirks &amp; kOutputBuffersAreUnreadable) &amp;&amp; (flags &amp; kClientNeedsFramebuffer)) &#123; if (strncmp(componentName, "OMX.SEC.", 8)) &#123; // For OMX.SEC.* decoders we can enable a special mode that // gives the client access to the framebuffer contents. ALOGW("Component '%s' does not give the client access to " "the framebuffer contents. Skipping.", componentName); continue; &#125; &#125; //这个allocateNode 就是文章最开始讲的，在OMX那头创建一个和mVideoSource相匹配的解码实例。用node值作为唯一标识。 status_t err = omx-&gt;allocateNode(componentName, observer, &amp;node); if (err == OK) &#123; ALOGV("Successfully allocated OMX node '%s'", componentName); //创建一个OMXCodec实例 sp&lt;OMXCodec&gt; codec = new OMXCodec( omx, node, quirks, flags, createEncoder, mime, componentName, source, nativeWindow); //然后将这个OMXCodec实例设置给OMXCodecObserver，用于回调消息 observer-&gt;setCodec(codec); //根据元数据设置相关内容 err = codec-&gt;configureCodec(meta); if (err == OK) &#123; return codec; &#125; ALOGV("Failed to configure codec '%s'", componentName); &#125; &#125; return NULL;&#125; &#160; &#160; &#160; &#160;主要流程我们分步查看： 查找所有匹配解码组件&#160; &#160; &#160; &#160;根据mVideoTrack传进来的视频信息mime，查找相匹配的解码器组件。查看findMatchingCodecs方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// staticvoid OMXCodec::findMatchingCodecs( const char *mime, //视频源mime bool createEncoder, //false const char *matchComponentName,//NULL uint32_t flags,//0 Vector&lt;CodecNameAndQuirks&gt; *matchingCodecs) &#123;//外部刚创建的空的Vector matchingCodecs-&gt;clear(); //获取本地配置的编解码器组件列表 const sp&lt;IMediaCodecList&gt; list = MediaCodecList::getInstance(); if (list == NULL) &#123; return; &#125; size_t index = 0; for (;;) &#123; //查找匹配的解码器在list中的index ssize_t matchIndex = list-&gt;findCodecByType(mime, createEncoder, index); if (matchIndex &lt; 0) &#123; break; &#125; index = matchIndex + 1; //获取匹配解码器的MediaCodecInfo const sp&lt;MediaCodecInfo&gt; info = list-&gt;getCodecInfo(matchIndex); CHECK(info != NULL); const char *componentName = info-&gt;getCodecName(); // If a specific codec is requested, skip the non-matching ones. //如果已经确定了解码器组件名（即外部传入的那个matchComponentName，为NULL）, //并且本地配置组件列表中的匹配解码器名字和外部传入的解码器组件名不行等，则跳过这个列表中的组件，继续往下 if (matchComponentName &amp;&amp; strcmp(componentName, matchComponentName)) &#123; continue; &#125; // When requesting software-only codecs, only push software codecs 如果仅仅需要软解 // When requesting hardware-only codecs, only push hardware codecs 如果仅仅需要硬解 // When there is request neither for software-only nor for 如果不是上面两种情况 // hardware-only codecs, push all codecs if (((flags &amp; kSoftwareCodecsOnly) &amp;&amp; IsSoftwareCodec(componentName)) || ((flags &amp; kHardwareCodecsOnly) &amp;&amp; !IsSoftwareCodec(componentName)) || (!(flags &amp; (kSoftwareCodecsOnly | kHardwareCodecsOnly)))) &#123; //将匹配到的的解码器加入到外部传入的Vector当中 ssize_t index = matchingCodecs-&gt;add(); CodecNameAndQuirks *entry = &amp;matchingCodecs-&gt;editItemAt(index); entry-&gt;mName = String8(componentName); entry-&gt;mQuirks = getComponentQuirks(info); ALOGV("matching '%s' quirks 0x%08x", entry-&gt;mName.string(), entry-&gt;mQuirks); &#125; &#125; if (flags &amp; kPreferSoftwareCodecs) &#123;//如果是优先软解，则需要将软解组件排序到前面 matchingCodecs-&gt;sort(CompareSoftwareCodecsFirst); &#125;&#125; &#160; &#160; &#160; &#160;这里主要也是做了两件事： 取出获取本地配置的编解码器组件列表 根据视频的mime类型在本地配置的编解码器组件列表中选取所有匹配的解码器 &#160; &#160; &#160; &#160;我们依然查看： &#160; &#160; &#160; &#160;（1） 获取本地配置编解码器列表 &#160; &#160; &#160; &#160;const sp&lt; IMediaCodecList &gt; list = MediaCodecList::getInstance();我们进入MediaCodecList的getInstance()函数看看，位于framework/av/media/libstagefright/MediaCodecList.cpp中：1234567891011121314151617181920// staticsp&lt;IMediaCodecList&gt; MediaCodecList::getInstance() &#123; Mutex::Autolock _l(sRemoteInitMutex); if (sRemoteList == NULL) &#123; //获取MediaPlayerService服务 sp&lt;IBinder&gt; binder = defaultServiceManager()-&gt;getService(String16("media.player")); sp&lt;IMediaPlayerService&gt; service = interface_cast&lt;IMediaPlayerService&gt;(binder); if (service.get() != NULL) &#123;//MediaPlayerService的getCodecList函数 sRemoteList = service-&gt;getCodecList(); &#125; if (sRemoteList == NULL) &#123; // if failed to get remote list, create local list sRemoteList = getLocalInstance(); &#125; &#125; return sRemoteList;&#125; &#160; &#160; &#160; &#160;然后进入MediaPlayerService查看getCodecList函数：123sp&lt;IMediaCodecList&gt; MediaPlayerService::getCodecList() const &#123; return MediaCodecList::getLocalInstance();//这特么又回去了&#125; &#160; &#160; &#160; &#160;这特么又回去了，继续查看MediaCodecList::getLocalInstance()：1234MediaCodecList::MediaCodecList() : mInitCheck(NO_INIT) &#123; parseTopLevelXMLFile("/etc/media_codecs.xml");&#125; &#160; &#160; &#160; &#160;后续其实就是解析/etc/media_codecs.xml，然后将里面的编解码器相关信息保存起来，每一个编解码器就是一个MediaCodecInfo。&#160; &#160; &#160; &#160;解析xml的函数都在MediaCodecList.cpp中，只不过用C++的函数处理的，看了下先关函数，和java层的SAX解析xml很像。有兴趣的童鞋可以看看，这里限于篇幅就不贴代码了。 &#160; &#160; &#160; &#160;实际上系统中存在的解码器可以很多，但能够被应用使用的解码器是根据配置来的，即/etc/media_codecc.xml。这个文件一般由硬件或者系统的生产厂家在build整个系统的时候提供，一般是保存在代码的device/[company]/[codename]目录下的，例如device/samsung/manta/media_codecs.xml。这个文件配置了系统中有哪些可用的codec以及，这些codec对应的媒体文件类型。在这个文件里面，系统里面提供的软硬codec都需要被列出来。 &#160; &#160; &#160; &#160;我借了同事的nexus 5，刷的是android的原生系统，也是高通的芯片。然后取出media_codecc.xml文件，如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111&lt;MediaCodecs&gt; &lt;Include href="media_codecs_google_audio.xml" /&gt; &lt;!-- audio codec相关，在另一个xml --&gt; &lt;Include href="media_codecs_google_telephony.xml" /&gt;&lt;!-- telephony codec相关 --&gt; &lt;Settings&gt; &lt;Setting name="max-video-encoder-input-buffers" value="9" /&gt; &lt;/Settings&gt; &lt;Encoders&gt;&lt;!-- 一些视频的encoder --&gt; &lt;MediaCodec name="OMX.qcom.video.encoder.mpeg4" type="video/mp4v-es" &gt; &lt;Quirk name="requires-allocate-on-input-ports" /&gt; &lt;Quirk name="requires-allocate-on-output-ports"/&gt; &lt;Quirk name="requires-loaded-to-idle-after-allocation"/&gt; &lt;Limit name="size" min="96x64" max="1920x1088" /&gt; &lt;Limit name="alignment" value="2x2" /&gt; &lt;Limit name="block-size" value="16x16" /&gt; &lt;Limit name="blocks-per-second" min="1" max="489600" /&gt; &lt;Limit name="bitrate" range="1-60000000" /&gt; &lt;Limit name="concurrent-instances" max="13" /&gt; &lt;/MediaCodec&gt; &lt;MediaCodec name="OMX.qcom.video.encoder.h263" type="video/3gpp" &gt; &lt;Quirk name="requires-allocate-on-input-ports" /&gt; &lt;Quirk name="requires-allocate-on-output-ports"/&gt; &lt;Quirk name="requires-loaded-to-idle-after-allocation"/&gt; &lt;Limit name="size" min="96x64" max="720x576" /&gt; &lt;Limit name="alignment" value="2x2" /&gt; &lt;Limit name="concurrent-instances" max="13" /&gt; &lt;/MediaCodec&gt; &lt;MediaCodec name="OMX.qcom.video.encoder.avc" type="video/avc" &gt; &lt;Quirk name="requires-allocate-on-input-ports" /&gt; &lt;Quirk name="requires-allocate-on-output-ports"/&gt; &lt;Quirk name="requires-loaded-to-idle-after-allocation"/&gt; &lt;Limit name="size" min="96x64" max="3840x2160" /&gt; &lt;Limit name="alignment" value="2x2" /&gt; &lt;Limit name="block-size" value="16x16" /&gt; &lt;Limit name="blocks-per-second" min="1" max="972000" /&gt; &lt;Limit name="bitrate" range="1-100000000" /&gt; &lt;Limit name="concurrent-instances" max="13" /&gt; &lt;/MediaCodec&gt; &lt;MediaCodec name="OMX.qcom.video.encoder.vp8" type="video/x-vnd.on2.vp8" &gt; &lt;Quirk name="requires-allocate-on-input-ports" /&gt; &lt;Quirk name="requires-allocate-on-output-ports"/&gt; &lt;Quirk name="requires-loaded-to-idle-after-allocation"/&gt; &lt;Limit name="size" min="96x64" max="3840x2160" /&gt; &lt;Limit name="alignment" value="2x2" /&gt; &lt;Limit name="block-size" value="16x16" /&gt; &lt;Limit name="blocks-per-second" min="1" max="777600" /&gt; &lt;Limit name="bitrate" range="1-20000000" /&gt; &lt;Limit name="concurrent-instances" max="13" /&gt; &lt;/MediaCodec&gt; &lt;/Encoders&gt; &lt;Decoders&gt;&lt;!-- 一些视频的decoder --&gt; &lt;MediaCodec name="OMX.qcom.video.decoder.avc" type="video/avc" &gt; &lt;Quirk name="requires-allocate-on-input-ports" /&gt; &lt;Quirk name="requires-allocate-on-output-ports"/&gt; &lt;Quirk name="defers-output-buffer-allocation"/&gt; &lt;Limit name="size" min="64x64" max="3840x2160" /&gt; &lt;Limit name="alignment" value="2x2" /&gt; &lt;Limit name="block-size" value="16x16" /&gt; &lt;Limit name="blocks-per-second" min="1" max="972000" /&gt; &lt;Limit name="bitrate" range="1-100000000" /&gt; &lt;Feature name="adaptive-playback" /&gt; &lt;Limit name="concurrent-instances" max="13" /&gt; &lt;/MediaCodec&gt; &lt;MediaCodec name="OMX.qcom.video.decoder.avc.secure" type="video/avc" &gt; &lt;Quirk name="requires-allocate-on-input-ports" /&gt; &lt;Quirk name="requires-allocate-on-output-ports"/&gt; &lt;Quirk name="defers-output-buffer-allocation"/&gt; &lt;Limit name="size" min="64x64" max="3840x2160" /&gt; &lt;Limit name="alignment" value="2x2" /&gt; &lt;Limit name="block-size" value="16x16" /&gt; &lt;Limit name="blocks-per-second" min="1" max="972000" /&gt; &lt;Limit name="bitrate" range="1-100000000" /&gt; &lt;Feature name="adaptive-playback" /&gt; &lt;Feature name="secure-playback" required="true" /&gt; &lt;Limit name="concurrent-instances" max="6" /&gt; &lt;/MediaCodec&gt; &lt;MediaCodec name="OMX.qcom.video.decoder.mpeg4" type="video/mp4v-es" &gt; &lt;Quirk name="requires-allocate-on-input-ports" /&gt; &lt;Quirk name="requires-allocate-on-output-ports"/&gt; &lt;Quirk name="defers-output-buffer-allocation"/&gt; &lt;Limit name="size" min="64x64" max="1920x1088" /&gt; &lt;Limit name="alignment" value="2x2" /&gt; &lt;Limit name="block-size" value="16x16" /&gt; &lt;Limit name="blocks-per-second" min="1" max="489600" /&gt; &lt;Limit name="bitrate" range="1-60000000" /&gt; &lt;Feature name="adaptive-playback" /&gt; &lt;Limit name="concurrent-instances" max="13" /&gt; &lt;/MediaCodec&gt; &lt;MediaCodec name="OMX.qcom.video.decoder.h263" type="video/3gpp" &gt; &lt;Quirk name="requires-allocate-on-input-ports" /&gt; &lt;Quirk name="requires-allocate-on-output-ports"/&gt; &lt;Quirk name="defers-output-buffer-allocation"/&gt; &lt;Limit name="size" min="64x64" max="720x576" /&gt; &lt;Limit name="alignment" value="2x2" /&gt; &lt;Feature name="adaptive-playback" /&gt; &lt;Limit name="concurrent-instances" max="13" /&gt; &lt;/MediaCodec&gt; &lt;MediaCodec name="OMX.qcom.video.decoder.vp8" type="video/x-vnd.on2.vp8" &gt; &lt;Quirk name="requires-allocate-on-input-ports" /&gt; &lt;Quirk name="requires-allocate-on-output-ports" /&gt; &lt;Quirk name="defers-output-buffer-allocation" /&gt; &lt;Limit name="size" min="64x64" max="3840x2160" /&gt; &lt;Limit name="alignment" value="2x2" /&gt; &lt;Limit name="block-size" value="16x16" /&gt; &lt;Limit name="blocks-per-second" min="1" max="777600" /&gt; &lt;Limit name="bitrate" range="1-20000000" /&gt; &lt;Feature name="adaptive-playback" /&gt; &lt;Limit name="concurrent-instances" max="13" /&gt; &lt;/MediaCodec&gt; &lt;/Decoders&gt; &lt;Include href="media_codecs_google_video.xml" /&gt;&lt;/MediaCodecs&gt; &#160; &#160; &#160; &#160;也就是说，如果系统里面实际上包含了某个codec，但是并没有被配置在这个文件里，那么应用程序也无法使用到！&#160; &#160; &#160; &#160;上面获取配置xml中的解码器，因为解析xml和SAX解析很像，是逐行扫描，所以保存的list也是按照xml中的顺序保存的。 &#160; &#160; &#160; &#160;（2）取出所有匹配的解码器列表&#160; &#160; &#160; &#160;获取本地配置的编解码器组件列表后，查找匹配的解码器在list中的index。我们查看ssize_t matchIndex = list-&gt;findCodecByType(mime, createEncoder, index) ：123456789101112131415161718192021222324252627282930313233343536373839404142434445// legacy method for non-advanced codecsssize_t MediaCodecList::findCodecByType( const char *type, //视频源mime bool encoder, //false size_t startIndex //从编解码器开始遍历的位置 ) const &#123; static const char *advancedFeatures[] = &#123;//高级播放模式 "feature-secure-playback", //DRM加密 "feature-tunneled-playback", //“隧道”播放模式，为了低功耗 &#125;; //mCodecInfos就是我们刚刚解析media_codecs.xml中配置的编解码器信息 size_t numCodecs = mCodecInfos.size(); //选混遍历所有配置的解码器 for (; startIndex &lt; numCodecs; ++startIndex) &#123; const MediaCodecInfo &amp;info = *mCodecInfos.itemAt(startIndex).get(); if (info.isEncoder() != encoder) &#123; continue; &#125; //如果这个视频源mime类型，这个遍历的解码器有能力播放，则ok sp&lt;MediaCodecInfo::Capabilities&gt; capabilities = info.getCapabilitiesFor(type); if (capabilities == NULL) &#123; continue; &#125; const sp&lt;AMessage&gt; &amp;details = capabilities-&gt;getDetails(); int32_t required; bool isAdvanced = false; //如果是高级播放模式，则不能播 for (size_t ix = 0; ix &lt; ARRAY_SIZE(advancedFeatures); ix++) &#123; if (details-&gt;findInt32(advancedFeatures[ix], &amp;required) &amp;&amp; required != 0) &#123; isAdvanced = true; break; &#125; &#125; if (!isAdvanced) &#123;//然后返回有能力播放这个类型视频的解码器index return startIndex; &#125; &#125; return -ENOENT;&#125; &#160; &#160; &#160; &#160;上面内容就是从配置的本地编解码列表中选取支持视频源mime类型的解码器，然后返回它位于列表中的index。&#160; &#160; &#160; &#160;然后回到findMatchingCodecs函数中，就是根据所有匹配的index去一一获取编解码器的MediaCodecInfo，然后再做先关设置，最后加入到matchingCodecs这个Vector当中。 选取并初始化第一个解码器&#160; &#160; &#160; &#160;接着会创建OMXCodecObserver 实例，OMXCodecObserver功能后续会详细介绍。创建一个node 并初始化为0。12sp&lt;OMXCodecObserver&gt; observer = new OMXCodecObserver; IOMX::node_id node = 0; &#160; &#160; &#160; &#160;然后通过omx入口 依靠binder 机制调用OMX服务中的allocateNode()，这一步把匹配得到的解码器组件名、OMXCodecObserver实例和初始化为0的node一并传入。1status_t err = omx-&gt;allocateNode(componentName, observer, &amp;node); &#160; &#160; &#160; &#160;这个allocateNode 就是文章最开始讲的，在OMX那头创建一个和mVideoSource相匹配的解码实例。用node值作为唯一标识。&#160; &#160; &#160; &#160;让我们来看看真正的omx中allocateNode做了啥，位于framework/av/media/libstagefright/omx/OMX.cpp中：1234567891011121314151617181920212223242526272829303132status_t OMX::allocateNode( const char *name, const sp&lt;IOMXObserver&gt; &amp;observer, node_id *node) &#123; Mutex::Autolock autoLock(mLock); *node = 0; //创建一个OMXNodeInstance实例 OMXNodeInstance *instance = new OMXNodeInstance(this, observer); OMX_COMPONENTTYPE *handle; //通过mMaster-&gt;makeComponentInstance创建真正解码器的组件，并通过handle与OMXNodeInstance关联。 OMX_ERRORTYPE err = mMaster-&gt;makeComponentInstance( name, &amp;OMXNodeInstance::kCallbacks, instance, &amp;handle); if (err != OMX_ErrorNone) &#123; ALOGE("FAILED to allocate omx component '%s'", name); instance-&gt;onGetHandleFailed(); return UNKNOWN_ERROR; &#125; *node = makeNodeID(instance); mDispatchers.add(*node, new CallbackDispatcher(instance)); instance-&gt;setHandle(*node, handle); mLiveNodes.add(observer-&gt;asBinder(), instance); observer-&gt;asBinder()-&gt;linkToDeath(this); return OK;&#125; &#160; &#160; &#160; &#160;创建一个OMXNodeInstance实例。&#160; &#160; &#160; &#160;通过mMaster-&gt;makeComponentInstance创建真正解码器的组件，并通过handle与OMXNodeInstance关联。&#160; &#160; &#160; &#160;所以说mMaster-&gt;makeComponentInstance这里是建立解码器组件的核心。会把mVideoSource需要的解码器name一直传递下去。&#160; &#160; &#160; &#160;我们查看OMXMaster的makeComponentInstance函数：12345678910111213141516171819202122232425262728293031//mPluginByInstance位于OMXMaster.h中KeyedVector&lt;OMX_COMPONENTTYPE *, OMXPluginBase *&gt; mPluginByInstance;OMX_ERRORTYPE OMXMaster::makeComponentInstance( const char *name, const OMX_CALLBACKTYPE *callbacks, OMX_PTR appData, OMX_COMPONENTTYPE **component) &#123; Mutex::Autolock autoLock(mLock); *component = NULL; //mPluginByComponentName就是我们第一步准备工作中保存所有编解码组件的map，然后找到匹配解码器组件名在其中的位置 ssize_t index = mPluginByComponentName.indexOfKey(String8(name)); if (index &lt; 0) &#123; return OMX_ErrorInvalidComponentName; &#125; //然后根据解码器在其中的位置找到队形的插件，即软解或者硬解插件之一，二选一 OMXPluginBase *plugin = mPluginByComponentName.valueAt(index); //然后调用注册插件的makeComponentInstance函数 OMX_ERRORTYPE err = plugin-&gt;makeComponentInstance(name, callbacks, appData, component); if (err != OMX_ErrorNone) &#123; return err; &#125; //将组件和对应插件加入到这个map中 mPluginByInstance.add(*component, plugin); // return err;&#125; &#160; &#160; &#160; &#160;mPluginByComponentName就是我们第一步准备工作中保存所有编解码组件的map，然后找到匹配解码器组件名在其中的位置，然后根据解码器在其中的位置找到队形的插件，即软解或者硬解插件之一，二选一，最后调用注册插件的makeComponentInstance函数。 &#160; &#160; &#160; &#160;我们依然以高通平台为例，查看步入hardware/qcom/media/libstagefrighthw/QComOMXPlugin.cpp，查看makeComponentInstance函数：12345678910111213141516171819202122232425262728293031323334353637383940OMX_ERRORTYPE QComOMXPlugin::makeComponentInstance( const char *name, const OMX_CALLBACKTYPE *callbacks, OMX_PTR appData, OMX_COMPONENTTYPE **component) &#123; if (mLibHandle == NULL) &#123; return OMX_ErrorUndefined; &#125; //会调用mGetHandle函数函数指针，在构造函数中又定义 return (*mGetHandle)( reinterpret_cast&lt;OMX_HANDLETYPE *&gt;(component), const_cast&lt;char *&gt;(name), appData, const_cast&lt;OMX_CALLBACKTYPE *&gt;(callbacks));&#125;QComOMXPlugin::QComOMXPlugin() : mLibHandle(dlopen("libOmxCore.so", RTLD_NOW)), mInit(NULL), mDeinit(NULL), mComponentNameEnum(NULL), mGetHandle(NULL), mFreeHandle(NULL), mGetRolesOfComponentHandle(NULL) &#123; if (mLibHandle != NULL) &#123; mInit = (InitFunc)dlsym(mLibHandle, "OMX_Init"); mDeinit = (DeinitFunc)dlsym(mLibHandle, "OMX_Deinit"); mComponentNameEnum = (ComponentNameEnumFunc)dlsym(mLibHandle, "OMX_ComponentNameEnum"); //重点在这里，会调用hardware/qcom/media/mm-core/omxcore/src/common/qc_omx_core.c的OMX_GetHandle方法 mGetHandle = (GetHandleFunc)dlsym(mLibHandle, "OMX_GetHandle"); mFreeHandle = (FreeHandleFunc)dlsym(mLibHandle, "OMX_FreeHandle"); mGetRolesOfComponentHandle = (GetRolesOfComponentFunc)dlsym( mLibHandle, "OMX_GetRolesOfComponent"); (*mInit)(); &#125;&#125; &#160; &#160; &#160; &#160;最后会调用qcom OpenMax IL的核心和公共内容。 其中qc_omx_core为主要文件，位于hardware/qcom/media/mm-core/omxcore/src/common/qc_omx_core.c，调用OMX_GetHandle()函数，用户获取各个组件的句柄。这一部分上一篇已经讲过了，可以查看Android多媒体开发(六)—-Android中OpenMax的实现(preview) ，看看高通对于这一部分的而实现。&#160; &#160; &#160; &#160;假设是hevc/h265类型编码的视频，OMX解码组件会加载对应的libOmxVdec.so或libOmxVdecHevc.so库。高通芯片确实犀利呀，我在同事的nexus手机上查找视频解码库就一个libOmxVdec.so，而别的一些平台放了一大堆so，比如amlogic或者mstar的，不仅夹了很多私货，其实性能也很差。。。。。。 &#160; &#160; &#160; &#160;经过这一路下来，终于完成了解码器的创建工作。简单总结一下： AwesomePlayer中通过initVideoDecoder 来创建video解码器mVideoSource。 mVideoSource 中通过上部分demux后的视频流 mVideoTrack来获得解码器的类型，通过类型调用omx-&gt;allocateNode 创建omx node实例与自己对应。以后都是通过node实例来操作解码器。 在 omx-&gt;allocateNode中 通过mMaster-&gt;makeComponentInstance 来创建真正对应的解码器组件。这个解码器组件是完成之后解码实际工作的。 在创建mMaster-&gt;makeComponentInstance过程中，也是通过上面mVideoTrack过来的解码器类型名，找到相对应的解码器的库，然后实例化。 消息处理&#160; &#160; &#160; &#160;OMXCodec，OMXCodecObserver和OMXNodeInstance是一一对应的，简单的可以理解它们3个构成了OpenMAX IL的一个Component，每一个node就是一个codec在OMX服务端的标识。&#160; &#160; &#160; &#160;当然还有CallbackDispatcher，用于处理codec过来的消息，通过它的post/loop/dispatch来发起接收，最终透过IOMX::onMessage -&gt; OMXNodeInstance::onMessage -&gt; OMXCodecObserver::onMessage -&gt; OMXCodec::on_message一路往上，当然消息的来源是因为我们有向codec注册OMXNodeInstance::kCallbacks，请看：123456789status_t OMX::allocateNode( const char *name, const sp&lt;IOMXObserver&gt; &amp;observer, node_id *node) &#123; ... OMX_ERRORTYPE err = mMaster-&gt;makeComponentInstance( name, &amp;OMXNodeInstance::kCallbacks, instance, &amp;handle); ... return OK;&#125; &#160; &#160; &#160; &#160;kCallbacks包含3种事件:123OMX_CALLBACKTYPE OMXNodeInstance::kCallbacks = &#123; &amp;OnEvent, &amp;OnEmptyBufferDone, &amp;OnFillBufferDone&#125;; &#160; &#160; &#160; &#160;它们分别都会调用到自己owner的OnEvent/OnEmptyBufferDone/OnFillBufferDone123456789101112131415// staticOMX_ERRORTYPE OMXNodeInstance::OnEvent( OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_PTR pAppData, OMX_IN OMX_EVENTTYPE eEvent, OMX_IN OMX_U32 nData1, OMX_IN OMX_U32 nData2, OMX_IN OMX_PTR pEventData) &#123; OMXNodeInstance *instance = static_cast&lt;OMXNodeInstance *&gt;(pAppData); if (instance-&gt;mDying) &#123; return OMX_ErrorNone; &#125; return instance-&gt;owner()-&gt;OnEvent( instance-&gt;nodeID(), eEvent, nData1, nData2, pEventData);&#125; &#160; &#160; &#160; &#160;而owner相应的又会调用自己dispatcher的post方法，如下：12345678910111213141516171819OMX_ERRORTYPE OMX::OnEvent( node_id node, OMX_IN OMX_EVENTTYPE eEvent, OMX_IN OMX_U32 nData1, OMX_IN OMX_U32 nData2, OMX_IN OMX_PTR pEventData) &#123; ALOGV("OnEvent(%d, %ld, %ld)", eEvent, nData1, nData2); omx_message msg; msg.type = omx_message::EVENT; msg.node = node; msg.u.event_data.event = eEvent; msg.u.event_data.data1 = nData1; msg.u.event_data.data2 = nData2; findDispatcher(node)-&gt;post(msg); return OMX_ErrorNone;&#125; &#160; &#160; &#160; &#160;这样所有的事件都串起来了，消息有来源，有最终的去处！&#160; &#160; &#160; &#160;结合这些信息所以我们就可以认为在这里是创建出了一个Component出来。下面贴个图： 硬解还是软解&#160; &#160; &#160; &#160;上面加载解码组件的时候有一段很重要的话，为了加深记忆我要再贴一遍： &#160; &#160; &#160; &#160;实际上系统中存在的解码器可以很多，但能够被应用使用的解码器是根据配置来的，即/etc/media_codecc.xml。这个文件一般由硬件或者系统的生产厂家在build整个系统的时候提供，一般是保存在代码的device/[company]/[codename]目录下的，例如device/samsung/manta/media_codecs.xml。这个文件配置了系统中有哪些可用的codec以及，这些codec对应的媒体文件类型。在这个文件里面，系统里面提供的软硬codec都需要被列出来。&#160; &#160; &#160; &#160;也就是说，如果系统里面实际上包含了某个codec，但是并没有被配置在这个文件里，那么应用程序也无法使用到！ &#160; &#160; &#160; &#160;在这里配置文件里面，如果出现多个codec对应同样类型的媒体格式的时候，这些codec都会被保留起来。当系统使用的时候，将后选择第一个匹配的codec。除非是指明了要软解码还是硬解码，但是Android的framework层为上层提供服务的AwesomePlayer中在处理音频和视频的时候，对到底是选择软解还是硬解的参数没有设置。所以虽然底层是支持选择的，但是对于上层使用MediaPlayer的Java程序来说，还是只能接受默认的codec选取规则。 &#160; &#160; &#160; &#160;一般来说，如果系统里面有对应媒体的硬件解码器的话，系统开发人员应该是会配置在media_codecs.xml中，所以大多数情况下，如果有硬件解码器，那么我们总是会使用到硬件解码器。 &#160; &#160; &#160; &#160;所以对于这种情况，经常听到APP开发人员骂google的MediaPlayer支持格式太少，兼容性太差，然后将锅强行甩给google。。。。。。这事也不能怨google，要怪就怪芯片商或者方案商吧！ &#160; &#160; &#160; &#160;google估计是听到的骂声太多了，所以推出了ExoPlayer，可以避免这种芯片商这种匹配第一个codec规则的弊端。 结语&#160; &#160; &#160; &#160;本篇算是对Android中OpenMax的实现做了一个差不多详细的分析。我水平有限，这几天看源码看的也很头疼，如有错误欢迎指正，我会在第一时间修改的。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android多媒体开发(六)----Android中OpenMax的实现(preview)]]></title>
    <url>%2F2016%2F12%2F26%2FAndroid%E5%A4%9A%E5%AA%92%E4%BD%93%E5%BC%80%E5%8F%91-%E5%85%AD-Android%E4%B8%ADOpenMax%E7%9A%84%E5%AE%9E%E7%8E%B0-preview%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;上一篇对OpenMax有了简单介绍，本篇就讲讲Android上对OpenMax IL层的实现。（可以忽略，下一篇会分析流程） OpenMax的接口与实现&#160; &#160; &#160; &#160;在Android中实现OpenMax IL层和标准的OpenMax IL层的方式基本，一般需要实现以下两个环节。 编解码驱动程序：位于Linux内核空间，需要通过Linux内核调用驱动程序，通常使用非标准的驱动程序。 OpenMax IL层：根据OpenMax IL层的标准头文件实现不同功能的组件。 &#160; &#160; &#160; &#160;Android中还提供了OpenMax的适配层接口（对OpenMax IL的标准组件进行封装适配），它作为Android本地层的接口，可以被Android的多媒体引擎调用。 OpenMax IL层接口&#160; &#160; &#160; &#160;OpenMax IL层的接口定义由若干个头文件组成，这也是实现它需要实现的内容，位于frameworks/native/include/media/openmax下，它们的基本描述如下所示： OMX_Types.h：OpenMax Il的数据类型定义OMX_Core.h：OpenMax IL核心的APIOMX_Component.h：OpenMax IL 组件相关的 APIOMX_Audio.h：音频相关的常量和数据结构OMX_IVCommon.h：图像和视频公共的常量和数据结构OMX_Image.h：图像相关的常量和数据结构OMX_Video.h：视频相关的常量和数据结构OMX_Other.h：其他数据结构（包括A/V 同步）OMX_Index.h：OpenMax IL定义的数据结构索引OMX_ContentPipe.h：内容的管道定义 &#160; &#160; &#160; &#160;提示：OpenMax标准只有头文件，没有标准的库，设置没有定义函数接口。对于实现者，需要实现的主要是包含函数指针的结构体。 &#160; &#160; &#160; &#160;其中，OMX_Component.h中定义的OMX_COMPONENTTYPE结构体是OpenMax IL层的核心内容，表示一个组件，其内容如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889typedef struct OMX_COMPONENTTYPE &#123; OMX_U32 nSize; /* 这个结构体的大小 */ OMX_VERSIONTYPE nVersion; /* 版本号 */ OMX_PTR pComponentPrivate; /* 这个组件的私有数据指针. */ /* 调用者（IL client）设置的指针，用于保存它的私有数据，传回给所有的回调函数 */ OMX_PTR pApplicationPrivate; /* 以下的函数指针返回OMX_core.h中的对应内容 */ OMX_ERRORTYPE (*GetComponentVersion)(/* 获得组件的版本*/ OMX_IN OMX_HANDLETYPE hComponent, OMX_OUT OMX_STRING pComponentName, OMX_OUT OMX_VERSIONTYPE* pComponentVersion, OMX_OUT OMX_VERSIONTYPE* pSpecVersion, OMX_OUT OMX_UUIDTYPE* pComponentUUID); OMX_ERRORTYPE (*SendCommand)(/* 发送命令 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_COMMANDTYPE Cmd, OMX_IN OMX_U32 nParam1, OMX_IN OMX_PTR pCmdData); OMX_ERRORTYPE (*GetParameter)(/* 获得参数 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_INDEXTYPE nParamIndex, OMX_INOUT OMX_PTR pComponentParameterStructure); OMX_ERRORTYPE (*SetParameter)(/* 设置参数 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_INDEXTYPE nIndex, OMX_IN OMX_PTR pComponentParameterStructure); OMX_ERRORTYPE (*GetConfig)(/* 获得配置 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_INDEXTYPE nIndex, OMX_INOUT OMX_PTR pComponentConfigStructure); OMX_ERRORTYPE (*SetConfig)(/* 设置配置 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_INDEXTYPE nIndex, OMX_IN OMX_PTR pComponentConfigStructure); OMX_ERRORTYPE (*GetExtensionIndex)(/* 转换成OMX结构的索引 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_STRING cParameterName, OMX_OUT OMX_INDEXTYPE* pIndexType); OMX_ERRORTYPE (*GetState)(/* 获得组件当前的状态 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_OUT OMX_STATETYPE* pState); OMX_ERRORTYPE (*ComponentTunnelRequest)(/* 用于连接到另一个组件*/ OMX_IN OMX_HANDLETYPE hComp, OMX_IN OMX_U32 nPort, OMX_IN OMX_HANDLETYPE hTunneledComp, OMX_IN OMX_U32 nTunneledPort, OMX_INOUT OMX_TUNNELSETUPTYPE* pTunnelSetup); OMX_ERRORTYPE (*UseBuffer)(/* 为某个端口使用Buffer */ OMX_IN OMX_HANDLETYPE hComponent, OMX_INOUT OMX_BUFFERHEADERTYPE** ppBufferHdr, OMX_IN OMX_U32 nPortIndex, OMX_IN OMX_PTR pAppPrivate, OMX_IN OMX_U32 nSizeBytes, OMX_IN OMX_U8* pBuffer); OMX_ERRORTYPE (*AllocateBuffer)(/* 在某个端口分配Buffer */ OMX_IN OMX_HANDLETYPE hComponent, OMX_INOUT OMX_BUFFERHEADERTYPE** ppBuffer, OMX_IN OMX_U32 nPortIndex, OMX_IN OMX_PTR pAppPrivate, OMX_IN OMX_U32 nSizeBytes); OMX_ERRORTYPE (*FreeBuffer)(/*将某个端口Buffer释放*/ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_U32 nPortIndex, OMX_IN OMX_BUFFERHEADERTYPE* pBuffer); OMX_ERRORTYPE (*EmptyThisBuffer)(/* 让组件消耗这个Buffer */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_BUFFERHEADERTYPE* pBuffer); OMX_ERRORTYPE (*FillThisBuffer)(/* 让组件填充这个Buffer */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_BUFFERHEADERTYPE* pBuffer); OMX_ERRORTYPE (*SetCallbacks)(/* 设置回调函数 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_CALLBACKTYPE* pCallbacks, OMX_IN OMX_PTR pAppData); OMX_ERRORTYPE (*ComponentDeInit)(/* 反初始化组件 */ OMX_IN OMX_HANDLETYPE hComponent); OMX_ERRORTYPE (*UseEGLImage)( OMX_IN OMX_HANDLETYPE hComponent, OMX_INOUT OMX_BUFFERHEADERTYPE** ppBufferHdr, OMX_IN OMX_U32 nPortIndex, OMX_IN OMX_PTR pAppPrivate, OMX_IN void* eglImage); OMX_ERRORTYPE (*ComponentRoleEnum)( OMX_IN OMX_HANDLETYPE hComponent, OMX_OUT OMX_U8 *cRole, OMX_IN OMX_U32 nIndex); &#125; OMX_COMPONENTTYPE; &#160; &#160; &#160; &#160;1）EmptyThisBuffer和FillThisBuffer是驱动组件运行的基本的机制，前者表示让组件消耗缓冲区，表示对应组件输入的内容；后者表示让组件填充缓冲区，表示对应组件输出的内容。&#160; &#160; &#160; &#160;2）UseBuffer，AllocateBuffer，FreeBuffer为和端口相关的缓冲区管理函数，对于组件的端口有些可以自己分配缓冲区，有些可以使用外部的缓冲区，因此有不同的接口对其进行操作。&#160; &#160; &#160; &#160;3）SendCommand表示向组件发送控制类的命令。GetParameter，SetParameter，GetConfig，SetConfig几个接口用于辅助的参数和配置的设置和获取。&#160; &#160; &#160; &#160;4）ComponentTunnelRequest用于组件之间的隧道化连接，其中需要制定两个组件及其相连的端口。&#160; &#160; &#160; &#160;5）ComponentDeInit用于组件的反初始化。 &#160; &#160; &#160; &#160;OMX_COMPONENTTYPE结构体实现后，其中的各个函数指针就是调用者可以使用的内容。各个函数指针和OMX_core.h中定义的内容相对应。&#160; &#160; &#160; &#160;提示：OpenMax函数的参数中，经常包含OMX_IN和OMX_OUT等宏，它们的实际内容为空，只是为了标记参数的方向是输入还是输出。 &#160; &#160; &#160; &#160;OMX_Component.h中端口类型的定义为OMX_PORTDOMAINTYPE枚举类型，内容如下所示：123456789typedef enum OMX_PORTDOMAINTYPE &#123; OMX_PortDomainAudio, /* 音频类型端口 */ OMX_PortDomainVideo, /* 视频类型端口 */ OMX_PortDomainImage, /* 图像类型端口 */ OMX_PortDomainOther, /* 其他类型端口 */ OMX_PortDomainKhronosExtensions = 0x6F000000, //为Khronos标准预留宽展 OMX_PortDomainVendorStartUnused = 0x7F000000 //为厂商预留扩展 OMX_PortDomainMax = 0x7ffffff &#125; OMX_PORTDOMAINTYPE; &#160; &#160; &#160; &#160;音频类型，视频类型，图像类型，其他类型是OpenMax IL层此所定义的四种端口的类型。 端口具体内容的定义使用OMX_PARAM_PORTDEFINITIONTYPE类（也在OMX_Component.h中定义）来表示，其内容如下所示：1234567891011121314151617181920typedef struct OMX_PARAM_PORTDEFINITIONTYPE &#123; OMX_U32 nSize; /* 结构体大小 */ OMX_VERSIONTYPE nVersion; /* 版本*/ OMX_U32 nPortIndex; /* 端口号 */ OMX_DIRTYPE eDir; /* 端口的方向 */ OMX_U32 nBufferCountActual; /* 为这个端口实际分配的Buffer的数目 */ OMX_U32 nBufferCountMin; /* 这个端口最小Buffer的数目*/ OMX_U32 nBufferSize; /* 缓冲区的字节数 */ OMX_BOOL bEnabled; /* 是否使能 */ OMX_BOOL bPopulated; /* 是否在填充 */ OMX_PORTDOMAINTYPE eDomain; /* 端口的类型 */ union &#123; /* 端口实际的内容，由类型确定具体结构 */ OMX_AUDIO_PORTDEFINITIONTYPE audio; OMX_VIDEO_PORTDEFINITIONTYPE video; OMX_IMAGE_PORTDEFINITIONTYPE image; OMX_OTHER_PORTDEFINITIONTYPE other; &#125; format; OMX_BOOL bBuffersContiguous; OMX_U32 nBufferAlignment; &#125; OMX_PARAM_PORTDEFINITIONTYPE; &#160; &#160; &#160; &#160;对于一个端口，其重点的内容如下: 端口的方向（OMX_DIRTYPE）：包含OMX_DirInput（输入）和OMX_DirOutput（输出）两种 端口分配的缓冲区数目和最小缓冲区数目 端口的类型（OMX_PORTDOMAINTYPE）：可以是四种类型 端口格式的数据结构：使用format联合体来表示，具体由四种不同类型来表示，与端口的类型相对应OMX_AUDIO_PORTDEFINITIONTYPE，OMX_VIDEO_PORTDEFINITIONTYPE，OMX_IMAGE_PORTDEFINITIONTYPE和OMX_OTHER_PORTDEFINITIONTYPE等几个具体的格式类型，分别在OMX_Audio.h，OMX_Video.h，OMX_Image.h和OMX_Other.h这四个头文件中定义。 &#160; &#160; &#160; &#160;OMX_Core.h中定义的枚举类型OMX_STATETYPE命令表示OpenMax的状态机，内容如下所示：123456789101112typedef enum OMX_STATETYPE &#123; OMX_StateInvalid, /* 组件监测到内部的数据结构被破坏 */ OMX_StateLoaded, /* 组件被加载但是没有完成初始化 */ OMX_StateIdle, /* 组件初始化完成，准备开始 */ OMX_StateExecuting, /* 组件接受了开始命令，正在树立数据 */ OMX_StatePause, /* 组件接受暂停命令*/ OMX_StateWaitForResources, /* 组件正在等待资源 */ OMX_StateKhronosExtensions = 0x6F000000, /* 保留for Khronos */ OMX_StateVendorStartUnused = 0x7F000000, /* 保留for厂商 */ OMX_StateMax = 0X7FFFFFFF &#125; OMX_STATETYPE; &#160; &#160; &#160; &#160;OpenMax组件的状态机可以由外部的命令改变，也可以由内部发生的情况改变。OpenMax IL组件的状态机的迁移关系如图所示： &#160; &#160; &#160; &#160;OMX_Core.h中定义的枚举类型OMX_COMMANDTYPE表示对组件的命令类型，内容如下所示：1234567891011typedef enum OMX_COMMANDTYPE &#123; OMX_CommandStateSet, /* 改变状态机器 */ OMX_CommandFlush, /* 刷新数据队列 */ OMX_CommandPortDisable, /* 禁止端口 */ OMX_CommandPortEnable, /* 使能端口 */ OMX_CommandMarkBuffer, /* 标记组件或Buffer用于观察 */ OMX_CommandKhronosExtensions = 0x6F000000, /* 保留for Khronos */ OMX_CommandVendorStartUnused = 0x7F000000, /* 保留for厂商 */ OMX_CommandMax = 0X7FFFFFFF &#125; OMX_COMMANDTYPE; &#160; &#160; &#160; &#160;OMX_COMMANDTYPE类型在SendCommand调用中作为参数被使用，其中OMX_CommandStateSet就是改变状态机的命令。 OpenMax IL实现的内容&#160; &#160; &#160; &#160;对于OpenMax IL层的实现，一般的方式并不调用OpenMax DL层。具体实现的内容就是各个不同的组件。&#160; &#160; &#160; &#160;OpenMax IL组件的实现包含以下两个步骤： 组件的初始化函数：硬件和OpenMax数据结构的初始化，一般分成函数指针初始化、私有数据结构的初始化、端口的初始化等几个步骤，使用OMX_Component.h其中的pComponentPrivate成员保留本组件的私有数据为上下文，最后获得填充完成OMX_COMPONENTTYPE类型的结构体。 OMX_COMPONENTTYPE类型结构体的各个指针：实现其中的各个函数指针，需要使用私有数据的时候，从其中的pComponentPrivate得到指针，转化成实际的数据结构使用。 &#160; &#160; &#160; &#160;端口的定义是OpenMax IL组件对外部的接口。OpenMax IL常用的组件大都是输入和输出端口各一个。对于最常用的编解码（Codec）组件，通常需要在每个组件的实现过程中，调用硬件的编解码接口来实现。在组件的内部处理中，可以建立线程来处理。OpenMax的组件的端口有默认参数，但也可以在运行时设置，因此一个端口也可以支持不同的编码格式。音频编码组件的输出和音频编码组件的输入通常是原始数据格式（PCM格式），视频编码组件的输出和视频编码组件的输入通常是原始数据格式（YUV格式）。&#160; &#160; &#160; &#160;提示：在一种特定的硬件实现中，编解码部分具有相似性，因此通常可以构建一个OpenMax组件的”基类”或者公共函数，来完成公共性的操作。 Android中OpenMax的适配层&#160; &#160; &#160; &#160;Android中的OpenMax适配层的接口在frameworks/av/include/media/IOMX.h文件定义，其内容如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class IOMX : public IInterface &#123; public: DECLARE_META_INTERFACE(OMX); typedef void *buffer_id; typedef void *node_id; virtual bool livesLocally(pid_t pid) = 0; struct ComponentInfo &#123;// 组件的信息 String8 mName; List&lt;String8&gt; mRoles; &#125;; virtual status_t listNodes(List&lt;ComponentInfo&gt; *list) = 0; // 节点列表 virtual status_t allocateNode( const char *name, const sp&lt;IOMXObserver&gt; &amp;observer, // 分配节点 node_id *node) = 0; virtual status_t freeNode(node_id node) = 0; // 找到节点 virtual status_t sendCommand(// 发送命令 node_id node, OMX_COMMANDTYPE cmd, OMX_S32 param) = 0; virtual status_t getParameter(// 获得参数 node_id node, OMX_INDEXTYPE index, void *params, size_t size) = 0; virtual status_t setParameter(// 设置参数 node_id node, OMX_INDEXTYPE index, const void *params, size_t size) = 0; virtual status_t getConfig(// 获得配置 node_id node, OMX_INDEXTYPE index, void *params, size_t size) = 0; virtual status_t setConfig(// 设置配置 node_id node, OMX_INDEXTYPE index, const void *params, size_t size) = 0; virtual status_t useBuffer(// 使用缓冲区 node_id node, OMX_U32 port_index, const sp&lt;IMemory&gt; ¶ms, buffer_id *buffer) = 0; virtual status_t allocateBuffer(// 分配缓冲区 node_id node, OMX_U32 port_index, size_t size, buffer_id *buffer, void **buffer_data) = 0; virtual status_t allocateBufferWithBackup(// 分配带后备缓冲区 node_id node, OMX_U32 port_index, const sp&lt;IMemory&gt; ¶ms, buffer_id *buffer) = 0; virtual status_t freeBuffer(// 释放缓冲区 node_id node, OMX_U32 port_index, buffer_id buffer) = 0; virtual status_t fillBuffer(node_id node, buffer_id buffer) = 0; // 填充缓冲区 virtual status_t emptyBuffer(// 消耗缓冲区 node_id node, buffer_id buffer, OMX_U32 range_offset, OMX_U32 range_length, OMX_U32 flags, OMX_TICKS timestamp) = 0; virtual status_t getExtensionIndex( node_id node, const char *parameter_name, OMX_INDEXTYPE *index) = 0; virtual sp&lt;IOMXRenderer&gt; createRenderer(// 创建渲染器（从ISurface） const sp&lt;ISurface&gt; &amp;surface, const char *componentName, OMX_COLOR_FORMATTYPE colorFormat, size_t encodedWidth, size_t encodedHeight, size_t displayWidth, size_t displayHeight) = 0; ...... &#125;; &#160; &#160; &#160; &#160;IOMX表示的是OpenMax的一个组件，根据Android的Binder IPC机制，BnOMX继承IOMX，实现者需要继承实现BnOMX。IOMX类中，有标准的OpenMax的GetParameter，SetParameter，GetConfig，SetConfig，SendCommand，UseBuffer，AllocateBuffer，FreeBuffer，FillThisBuffer和EmptyThisBuffer等接口。&#160; &#160; &#160; &#160;在IOMX.h文件中，另有表示观察器类的IOMXObserver，这个类表示OpenMax的观察者，其中只包含一个onMessage()函数，其参数为omx_message接口体，其中包含Event事件类型、FillThisBuffer完成和EmptyThisBuffer完成几种类型。&#160; &#160; &#160; &#160;提示：Android中OpenMax的适配层是OpenMAX IL层至上的封装层，在Android系统中被StageFright调用，也可以被其他部分调用。 TI(Texas Instruments 德州仪器) OpenMax IL的硬件实现TI OpenMax IL实现的结构和机制&#160; &#160; &#160; &#160;Android的开源代码中，已经包含了TI的OpenMax IL层的实现代码，其路径如hardware/ti/omap3/omx下。其中包含的主要目录如下所示： system：OpenMax核心和公共部分 audio：音频处理部分的OpenMax IL组件 video：视频处理部分OpenMax IL组件 image：图像处理部分OpenMax IL组件 &#160; &#160; &#160; &#160;TI OpenMax IL实现的结构如图所示: &#160; &#160; &#160; &#160;在TI OpenMax IL实现中，最上面的内容是OpenMax的管理者用于管理和初始化，中间层是各个编解码单元的OpenMax IL标准组件，下层是LCML层，供各个OpenMax IL标准组件所调用。&#160; &#160; &#160; &#160;（1）TI OpenMax IL实现的公共部分在system/src/openmax_il/目录中，主要的内容如下所示。 omx_core/src：OpenMax IL的核心，生成动态库libOMX_Core.so lcml/：LCML的工具库，生成动态库libLCML.so &#160; &#160; &#160; &#160;（2）I OpenMax IL的视频（Video）相关的组件在video/src/openmax_il/目录中，主要的内容如下所示。 prepost_processor：Video数据的前处理和后处理，生成动态库libOMX.TI.VPP.so video_decode：Video解码器，生成动态库libOMX.TI.Video.Decoder.so video_encode：Video编码器，生成动态库libOMX.TI.Video.encoder.so &#160; &#160; &#160; &#160;（3）TI OpenMax IL的音频（Audio）相关的组件在audio/src/openmax_il/目录中，主要的内容如下所示。 g711_dec：G711解码器，生成动态库libOMX.TI.G711.decode.so g711_enc：G711编码器，生成动态库libOMX.TI.G711.encode.so g722_dec：G722解码器，生成动态库libOMX.TI.G722.decode.so g722_enc：G722编码器，生成动态库libOMX.TI.G722.encode.so g726_dec：G726解码器，生成动态库libOMX.TI.G726.decode.so g726_enc：G726编码器，生成动态库libOMX.TI.G726.encode.so g729_dec：G729解码器，生成动态库libOMX.TI.G729.decode.so g729_enc：G720编码器，生成动态库libOMX.TI.G729.encode.so nbamr_dec：AMR窄带解码器，生成动态库libOMX.TI.AMR.decode.so nbamr_enc：AMR窄带编码器，生成动态库libOMX.TI.AMR.encode.so wbamr_dec：AMR宽带解码器，生成动态库libOMX.TI.WBAMR.decode.so wbamr_enc：AMR宽带编码器，生成动态库libOMX.TI.WBAMR.encode.so mp3_dec：MP3解码器，生成动态库libOMX.TI.MP3.decode.so aac_dec：AAC解码器，生成动态库libOMX.TI.AAC.decode.so aac_enc：AAC编码器，生成动态库libOMX.TI.AAC.encode.so wma_dec：WMA解码器，生成动态库libOMX.TI.WMA.decode.so &#160; &#160; &#160; &#160;（4）TI OpenMax IL的图像（Image）相关的组件在image/src/openmax_il/目录中，主要的内容如下所示。 jpeg_enc：JPEG编码器，生成动态库libOMX.TI.JPEG.Encoder.so jpeg_dec：JPEG解码器，生成动态库libOMX.TI.JPEG.decoder.so TI OpenMax IL的核心和公共内容&#160; &#160; &#160; &#160;LCML的全称是”Linux Common Multimedia Layer“，是TI的Linux公共多媒体层。在OpenMax IL的实现中，这个内容在system/src/openmax_il/lcml/目录中，主要文件是子目录src中的LCML_DspCodec.c文件。通过调用DSPBridge的内容， 让ARM和DSP进行通信，然DSP进行编解码方面的处理。DSP的运行还需要固件的支持。&#160; &#160; &#160; &#160;TI OpenMax IL的核心实现在system/src/openmax_il/omx_core/目录中，生成TI OpenMax IL的核心库libOMX_Core.so。&#160; &#160; &#160; &#160;其中子目录src中的OMX_Core.c为主要文件，其中定义了编解码器的名称等，其片断如下所示：12345678910111213141516171819202122char *tComponentName[MAXCOMP][2] = &#123; &#123;"OMX.TI.JPEG.decoder", "image_decoder.jpeg"&#125;,/* 图像和视频编解码器 */ &#123;"OMX.TI.JPEG.Encoder", "image_encoder.jpeg"&#125;, &#123;"OMX.TI.Video.Decoder", "video_decoder.avc"&#125;, &#123;"OMX.TI.Video.Decoder", "video_decoder.mpeg4"&#125;, &#123;"OMX.TI.Video.Decoder", "video_decoder.wmv"&#125;, &#123;"OMX.TI.Video.encoder", "video_encoder.mpeg4"&#125;, &#123;"OMX.TI.Video.encoder", "video_encoder.h263"&#125;, &#123;"OMX.TI.Video.encoder", "video_encoder.avc"&#125;, /* ......省略 ，语音相关组件*/ #ifdef BUILD_WITH_TI_AUDIO /* 音频编解码器 */ &#123;"OMX.TI.MP3.decode", "audio_decoder.mp3"&#125;, &#123;"OMX.TI.AAC.encode", "audio_encoder.aac"&#125;, &#123;"OMX.TI.AAC.decode", "audio_decoder.aac"&#125;, &#123;"OMX.TI.WMA.decode", "audio_decoder.wma"&#125;, &#123;"OMX.TI.WBAMR.decode", "audio_decoder.amrwb"&#125;, &#123;"OMX.TI.AMR.decode", "audio_decoder.amrnb"&#125;, &#123;"OMX.TI.AMR.encode", "audio_encoder.amrnb"&#125;, &#123;"OMX.TI.WBAMR.encode", "audio_encoder.amrwb"&#125;, #endif &#123;NULL, NULL&#125;, &#125;; &#160; &#160; &#160; &#160;tComponentName数组的各个项中，第一个表示编解码库内容，第二个表示库所实现的功能。&#160; &#160; &#160; &#160;其中，TIOMX_GetHandle()函数用于获得各个组件的句柄，其实现的主要片断如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647OMX_ERRORTYPE TIOMX_GetHandle( OMX_HANDLETYPE* pHandle, OMX_STRING cComponentName, OMX_PTR pAppData, OMX_CALLBACKTYPE* pCallBacks) &#123; static const char prefix[] = "lib"; static const char postfix[] = ".so"; OMX_ERRORTYPE (*pComponentInit)(OMX_HANDLETYPE*); OMX_ERRORTYPE err = OMX_ErrorNone; OMX_COMPONENTTYPE *componentType; const char* pErr = dlerror(); // ...... 省略错误处理内容 int i = 0; for(i=0; i&lt; COUNTOF(pModules); i++) &#123; // 循环查找 if(pModules[i] == NULL) break; &#125; // ...... 省略错误处理内容 int refIndex = 0; for (refIndex=0; refIndex &lt; MAX_TABLE_SIZE; refIndex++) &#123; // 循环查找组件列表 if (strcmp(componentTable[refIndex].name, cComponentName) == 0) &#123; if (componentTable[refIndex].refCount&gt;= MAX_CONCURRENT_INSTANCES) &#123; // ...... 省略错误处理内容 &#125; else &#123; char buf[sizeof(prefix) + MAXNAMESIZE+ sizeof(postfix)]; strcpy(buf, prefix); strcat(buf, cComponentName); strcat(buf, postfix); pModules[i] = dlopen(buf, RTLD_LAZY | RTLD_GLOBAL); // ...... 省略错误处理内容 // 动态取出初始化的符号 pComponentInit = dlsym(pModules[i], "OMX_ComponentInit"); pErr = dlerror(); // ...... 省略错误处理内容 *pHandle = malloc(sizeof(OMX_COMPONENTTYPE)); // ...... 省略错误处理内容 pComponents[i] = *pHandle; componentType = (OMX_COMPONENTTYPE*) *pHandle; componentType-&gt;nSize = sizeof(OMX_COMPONENTTYPE); err = (*pComponentInit)(*pHandle); // 执行初始化工作 // ...... 省略部分内容 &#125; &#125; &#125; err = OMX_ErrorComponentNotFound; goto UNLOCK_MUTEX; // ...... 省略部分内容 return (err); &#125; &#160; &#160; &#160; &#160;在TIOMX_GetHandle()函数中，根据tComponentName数组中动态库的名称，动态打开各个编解码实现的动态库，取出其中的OMX_ComponentInit符号来执行各个组件的初始化。 一个TI OpenMax IL组件的实现&#160; &#160; &#160; &#160;TI OpenMax IL中各个组件都是通过调用LCML来实现的，实现的方式基本类似。主要都是实现了名称为OMX_ComponentInit的初始化函数，实现OMX_COMPONENTTYPE类型的结构体中的各个成员。各个组件其目录结构和文件结构也类似。 &#160; &#160; &#160; &#160;以MP3解码器的实现为例，在audio/src/openmax_il/mp3_dec/src目录中，主要包含以下文件： OMX_Mp3Decoder.c：MP3解码器组件实现 OMX_Mp3Dec_CompThread.c：MP3解码器组件的线程循环 OMX_Mp3Dec_Utils.c：MP3解码器的相关工具，调用LCML实现真正的MP3解码的功能 &#160; &#160; &#160; &#160;OMX_Mp3Decoder.c中的OMX_ComponentInit()函数负责组件的初始化，返回的内容再从参数中得到，这个函数的主要片断如下所示：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667OMX_ERRORTYPE OMX_ComponentInit (OMX_HANDLETYPE hComp) &#123; OMX_ERRORTYPE eError = OMX_ErrorNone; OMX_COMPONENTTYPE *pHandle = (OMX_COMPONENTTYPE*) hComp; OMX_PARAM_PORTDEFINITIONTYPE *pPortDef_ip = NULL, *pPortDef_op = NULL; OMX_AUDIO_PARAM_PORTFORMATTYPE *pPortFormat = NULL; OMX_AUDIO_PARAM_MP3TYPE *mp3_ip = NULL; OMX_AUDIO_PARAM_PCMMODETYPE *mp3_op = NULL; MP3DEC_COMPONENT_PRIVATE *pComponentPrivate = NULL; MP3D_AUDIODEC_PORT_TYPE *pCompPort = NULL; MP3D_BUFFERLIST *pTemp = NULL; int i=0; MP3D_OMX_CONF_CHECK_CMD(pHandle,1,1); /* ......省略，初始化OMX_COMPONENTTYPE类型的指针pHandle */ OMX_MALLOC_GENERIC(pHandle-&gt;pComponentPrivate, MP3DEC_COMPONENT_PRIVATE); pComponentPrivate = pHandle-&gt;pComponentPrivate; /* 私有指针互相指向 */ pComponentPrivate-&gt;pHandlepHandle = pHandle; /* ......略，初始化似有数据指针pComponentPrivate */ /* 设置输入端口（OMX_PARAM_PORTDEFINITIONTYPE类型）的默认值 */ pPortDef_ip-&gt;nSize = sizeof(OMX_PARAM_PORTDEFINITIONTYPE); pPortDef_ip-&gt;nPortIndex = MP3D_INPUT_PORT; pPortDef_ip-&gt;eDir = OMX_DirInput; pPortDef_ip-&gt;nBufferCountActual = MP3D_NUM_INPUT_BUFFERS; pPortDef_ip-&gt;nBufferCountMin = MP3D_NUM_INPUT_BUFFERS; pPortDef_ip-&gt;nBufferSize = MP3D_INPUT_BUFFER_SIZE; pPortDef_ip-&gt;nBufferAlignment = DSP_CACHE_ALIGNMENT; pPortDef_ip-&gt;bEnabled = OMX_TRUE; pPortDef_ip-&gt;bPopulated = OMX_FALSE; pPortDef_ip-&gt;eDomain = OMX_PortDomainAudio; pPortDef_ip-&gt;format.audio.eEncoding = OMX_AUDIO_CodingMP3; pPortDef_ip-&gt;format.audio.cMIMEType = NULL; pPortDef_ip-&gt;format.audio.pNativeRender = NULL; pPortDef_ip-&gt;format.audio.bFlagErrorConcealment = OMX_FALSE; /* 设置输出端口（OMX_PARAM_PORTDEFINITIONTYPE类型）的默认值 */ pPortDef_op-&gt;nSize = sizeof(OMX_PARAM_PORTDEFINITIONTYPE); pPortDef_op-&gt;nPortIndex = MP3D_OUTPUT_PORT; pPortDef_op-&gt;eDir = OMX_DirOutput; pPortDef_op-&gt;nBufferCountMin = MP3D_NUM_OUTPUT_BUFFERS; pPortDef_op-&gt;nBufferCountActual = MP3D_NUM_OUTPUT_BUFFERS; pPortDef_op-&gt;nBufferSize = MP3D_OUTPUT_BUFFER_SIZE; pPortDef_op-&gt;nBufferAlignment = DSP_CACHE_ALIGNMENT; pPortDef_op-&gt;bEnabled = OMX_TRUE; pPortDef_op-&gt;bPopulated = OMX_FALSE; pPortDef_op-&gt;eDomain = OMX_PortDomainAudio; pPortDef_op-&gt;format.audio.eEncoding = OMX_AUDIO_CodingPCM; pPortDef_op-&gt;format.audio.cMIMEType = NULL; pPortDef_op-&gt;format.audio.pNativeRender = NULL; pPortDef_op-&gt;format.audio.bFlagErrorConcealment = OMX_FALSE; /* ......省略，分配端口 */ /* 设置输入端口的默认格式 */ pPortFormat = pComponentPrivate-&gt;pCompPort[MP3D_INPUT_PORT]-&gt;pPortFormat; OMX_CONF_INIT_STRUCT(pPortFormat, OMX_AUDIO_PARAM_PORTFORMATTYPE); pPortFormat-&gt;nPortIndex = MP3D_INPUT_PORT; pPortFormat-&gt;nIndex = OMX_IndexParamAudioMp3; pPortFormat-&gt;eEncoding = OMX_AUDIO_CodingMP3; /* 设置输出端口的默认格式 */ pPortFormat = pComponentPrivate-&gt;pCompPort[MP3D_OUTPUT_PORT]-&gt;pPortFormat; OMX_CONF_INIT_STRUCT(pPortFormat, OMX_AUDIO_PARAM_PORTFORMATTYPE); pPortFormat-&gt;nPortIndex = MP3D_OUTPUT_PORT; pPortFormat-&gt;nIndex = OMX_IndexParamAudioPcm; pPortFormat-&gt;eEncoding = OMX_AUDIO_CodingPCM; /* ......省略部分内容 */ eError = Mp3Dec_StartCompThread(pHandle); // 启动MP3解码线程 /* ......省略部分内容 */ return eError; &#125; &#160; &#160; &#160; &#160;这个组件是OpenMax的标准实现方式，对外的接口的内容只有一个初始化函数。完成OMX_COMPONENTTYPE类型的初始化。输入端口的编号为MP3D_INPUT_PORT（==0），类型为OMX_PortDomainAudio，格式为OMX_AUDIO_CodingMP3。输出端口的编号是MP3D_OUTPUT_PORT（==1），类型为OMX_PortDomainAudio，格式为OMXAUDIO CodingPCM。 &#160; &#160; &#160; &#160;OMX_Mp3Dec_CompThread.c中定义了MP3DEC_ComponentThread()函数，用于创建MP3解码的线程的执行函数。&#160; &#160; &#160; &#160;OMX_Mp3Dec_Utils.c中的Mp3Dec_StartCompThread()函数，调用了POSIX的线程库建立MP3解码的线程，如下所示：12nRet = pthread_create (&amp;(pComponentPrivate-&gt;ComponentThread), NULL, MP3DEC_ComponentThread, pComponentPrivate); &#160; &#160; &#160; &#160;Mp3Dec_StartCompThread()函数就是在组件初始化函数OMX_ComponentInit()最后调用的内容。MP3线程的开始并不表示解码过程开始，线程需要等待通过pipe机制获得命令和数据（cmdPipe和dataPipe），在适当的时候开始工作。这个pipe在MP3解码组件的SendCommand等实现写操作，在线程中读取其内容。 Qualcomm(高通) OpenMax IL的硬件实现qcom OpenMax IL实现的结构和机制&#160; &#160; &#160; &#160;（1）在AOSP中依然有对高通平台的OpenMax IL层实现代码，位于hardware/qcom/media/mm-core下。这一部分是OpenMax核心和公共部分，主要编译为libOmxCore.so。&#160; &#160; &#160; &#160;（2）e.g. 继续在hardware/qcom/media下，选取mm-video-v4l2目录。即Video4linux2（简称V4L2),是linux中关于视频设备的内核驱动。再次进入vidc，（DivxDrmDecrypt为DRM数字版权相关）主要目录如下： vdec：视频解码处理，编译成libOmxVdec.so/libOmxVdecHevc.so venc：视频编码处理，编译成libOmxVenc.so qcom OpenMax IL的核心和公共内容&#160; &#160; &#160; &#160;类似于前面介绍的TI，高通平台在OpenMax IL实现也是大同小异，位于hardware/qcom/media/mm-core，生成libOmxCore.so库。&#160; &#160; &#160; &#160;其中qc_omx_core为主要文件，位于hardware/qcom/media/mm-core/omxcore/src/common/下面。和TI的差不多，OMX_GetHandle()函数用户获取各个组件的句柄，其实现的主要片断如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798//编解码器组件集合数组extern omx_core_cb_type core[]; OMX_API OMX_ERRORTYPE OMX_APIENTRYOMX_GetHandle(OMX_OUT OMX_HANDLETYPE* handle, OMX_IN OMX_STRING componentName, OMX_IN OMX_PTR appData, OMX_IN OMX_CALLBACKTYPE* callBacks)&#123; OMX_ERRORTYPE eRet = OMX_ErrorNone; int cmp_index = -1; int hnd_index = -1; DEBUG_PRINT("OMXCORE API : Get Handle %p %s %p\n", handle, componentName, appData); pthread_mutex_lock(&amp;lock_core); if(handle) &#123; struct stat sd; //组件句柄 *handle = NULL; //获取根据组件名获取相应index cmp_index = get_cmp_index(componentName); if(cmp_index &gt;= 0) &#123; DEBUG_PRINT("getting fn pointer\n"); // dynamically load the so 动态加载组件的so库 core[cmp_index].fn_ptr = omx_core_load_cmp_library(core[cmp_index].so_lib_name, &amp;core[cmp_index].so_lib_handle); if(core[cmp_index].fn_ptr) &#123; // Construct the component requested // Function returns the opaque handle //根据获取的组件句柄初始化它 void* pThis = (*(core[cmp_index].fn_ptr))(); if(pThis) &#123; //包装一层，忽略 void *hComp = NULL; hComp = qc_omx_create_component_wrapper((OMX_PTR)pThis); if((eRet = qc_omx_component_init(hComp, core[cmp_index].name)) != OMX_ErrorNone) &#123; DEBUG_PRINT("Component not created succesfully\n"); pthread_mutex_unlock(&amp;lock_core); return eRet; &#125; //设置回调 qc_omx_component_set_callbacks(hComp,callBacks,appData); hnd_index = get_comp_handle_index(componentName); if(hnd_index &gt;= 0) &#123; //保存这个组件句柄 core[cmp_index].inst[hnd_index]= *handle = (OMX_HANDLETYPE) hComp; &#125; else &#123; /*-------下面全是错误处理，忽略------*/ DEBUG_PRINT("OMX_GetHandle:NO free slot available to store Component Handle\n"); pthread_mutex_unlock(&amp;lock_core); return OMX_ErrorInsufficientResources; &#125; DEBUG_PRINT("Component %p Successfully created\n",*handle); &#125; else &#123; eRet = OMX_ErrorInsufficientResources; DEBUG_PRINT("Component Creation failed\n"); &#125; &#125; else &#123; eRet = OMX_ErrorNotImplemented; DEBUG_PRINT("library couldnt return create instance fn\n"); &#125; &#125; else &#123; eRet = OMX_ErrorNotImplemented; DEBUG_PRINT("ERROR: Already another instance active ;rejecting \n"); &#125; &#125; else &#123; eRet = OMX_ErrorBadParameter; DEBUG_PRINT("\n OMX_GetHandle: NULL handle \n"); &#125; pthread_mutex_unlock(&amp;lock_core); return eRet;&#125; &#160; &#160; &#160; &#160;这里的有个数组：extern omx_core_cb_type core[]，是从别的文件中声明过来的全局变量，其中包含了各种编解码器的名称和一些属性的结构体。结构体定义位于hardware/qcom/media/mm-core/omxcore/src/common/qc_omx_core.h：123456789typedef struct _omx_core_cb_type&#123; char* name;// Component name 组件名 create_qc_omx_component fn_ptr;// create instance fn ptr 创建实例函数指针 void* inst[OMX_COMP_MAX_INST];// Instance handle 实例句柄 void* so_lib_handle;// So Library handle so库句柄 char* so_lib_name;// so directory so名 char* roles[OMX_CORE_MAX_CMP_ROLES];// roles played 组件扮演的角色&#125;omx_core_cb_type; &#160; &#160; &#160; &#160;但是给omx_core_cb_type core[]这个结构体数组复制的地方要根据不同型号进行选取，我们进入hardware/qcom/media/mm-core/src下面，会看到有许多型号，7627A、7630、8084、8226、8610、8660等等。比如这个8974的，位于hardware/qcom/media/mm-core/src/8974/qc_registry_table_android.c中：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162omx_core_cb_type core[] =&#123; //avc/h264解码器 &#123; "OMX.qcom.video.decoder.avc", NULL, // Create instance function // Unique instance handle &#123; NULL &#125;, NULL, // Shared object library handle "libOmxVdec.so", &#123; "video_decoder.avc" &#125; &#125;, //mpeg4解码器&#123; "OMX.qcom.video.decoder.mpeg4", NULL, // Create instance function // Unique instance handle &#123; NULL &#125;, NULL, // Shared object library handle "libOmxVdec.so", &#123; "video_decoder.mpeg4" &#125; &#125;, //wmv解码器 &#123; "OMX.qcom.video.decoder.wmv", NULL, // Create instance function // Unique instance handle &#123; NULL &#125;, NULL, // Shared object library handle "libOmxVdec.so", &#123; "video_decoder.vc1" &#125; &#125;, //hevc/h265编码器 &#123; "OMX.qcom.video.encoder.hevc", NULL, // Create instance function // Unique instance handle &#123; NULL &#125;, NULL, // Shared object library handle "libOmxVencHevc.so", &#123; "video_encoder.hevc" &#125; &#125;, ...太多了，省略... &#125; &#160; &#160; &#160; &#160;上面就是对编解码器相关信息的注册。 &#160; &#160; &#160; &#160;在OMX_GetHandle()函数中，根据omx_core_cb_type core[]数组中动态库的名称，动态打开各个编解码实现的动态库,然后进行初始化。 一个qcom OpenMax IL组件的实现&#160; &#160; &#160; &#160;高通平台对于编解码组件的处理都比较集中，不像TI那么分散和细致。一个组件实现都要包含Qc_omx_component.h头文件，位于很多地方，如hardware/qcom/media/mm-core/inc，要实现里面相关纯虚函数。当一个组件被创建后要初始化，就要实现component_init(OMX_IN OMX_STRING componentName)方法。 &#160; &#160; &#160; &#160;举个例子，依然以Video4linux2平台，进入hardware/qcom/media/mm-video-v4l2/vidc/vdec/src查看视频解码相关组件。比如我们看看解码组件omx_vdec_hevc.cpp，查看component_init方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414#ifdef VENUS_HEVC#define DEVICE_NAME "/dev/video/venus_dec"#else#define DEVICE_NAME "/dev/video/q6_dec"#endif/* ====================================================================== FUNCTION omx_vdec::ComponentInit DESCRIPTION Initialize the component. PARAMETERS ctxt -- Context information related to the self. id -- Event identifier. This could be any of the following: 1. Command completion event 2. Buffer done callback event 3. Frame done callback event RETURN VALUE None. ========================================================================== */OMX_ERRORTYPE omx_vdec::component_init(OMX_STRING role)&#123; OMX_ERRORTYPE eRet = OMX_ErrorNone; struct v4l2_fmtdesc fdesc; struct v4l2_format fmt; struct v4l2_requestbuffers bufreq; struct v4l2_control control; unsigned int alignment = 0,buffer_size = 0; int fds[2]; int r,ret=0; bool codec_ambiguous = false; //打开设备文件"/dev/video/venus_dec"或"/dev/video/q6_dec" OMX_STRING device_name = (OMX_STRING)DEVICE_NAME; ...... drv_ctx.video_driver_fd = open(device_name, O_RDWR); ...... //如果是一个打开成功后，为什么要再次打开？？excuse me ？ if (drv_ctx.video_driver_fd == 0) &#123; drv_ctx.video_driver_fd = open(device_name, O_RDWR); &#125; //打开设备文件失败 if (drv_ctx.video_driver_fd &lt; 0) &#123; DEBUG_PRINT_ERROR("Omx_vdec::Comp Init Returning failure, errno %d", errno); return OMX_ErrorInsufficientResources; &#125; drv_ctx.frame_rate.fps_numerator = DEFAULT_FPS;//帧率分子 drv_ctx.frame_rate.fps_denominator = 1;//帧率分母 //创建一个异步线程，执行async_message_thread函数，对输入端进行设置 ret = pthread_create(&amp;async_thread_id,0,async_message_thread,this); //创建线程失败，则关闭设备文件 if (ret &lt; 0) &#123; close(drv_ctx.video_driver_fd); DEBUG_PRINT_ERROR("Failed to create async_message_thread"); return OMX_ErrorInsufficientResources; &#125; ...... // Copy the role information which provides the decoder kind //将组建角色名字copy进设备驱动上下文结构体的kind属性 strlcpy(drv_ctx.kind,role,128); //如果是mpeg4解码组件 if (!strncmp(drv_ctx.kind,"OMX.qcom.video.decoder.mpeg4",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.mpeg4",\ OMX_MAX_STRINGNAME_SIZE); drv_ctx.timestamp_adjust = true; drv_ctx.decoder_format = VDEC_CODECTYPE_MPEG4; eCompressionFormat = OMX_VIDEO_CodingMPEG4; output_capability=V4L2_PIX_FMT_MPEG4; /*Initialize Start Code for MPEG4*/ codec_type_parse = CODEC_TYPE_MPEG4; m_frame_parser.init_start_codes (codec_type_parse); ...... //如果是mpeg2解码组件 &#125; else if (!strncmp(drv_ctx.kind,"OMX.qcom.video.decoder.mpeg2",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.mpeg2",\ OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_MPEG2; output_capability = V4L2_PIX_FMT_MPEG2; eCompressionFormat = OMX_VIDEO_CodingMPEG2; /*Initialize Start Code for MPEG2*/ codec_type_parse = CODEC_TYPE_MPEG2; m_frame_parser.init_start_codes (codec_type_parse); ...... //如果是h263解码组件 &#125; else if (!strncmp(drv_ctx.kind, "OMX.qcom.video.decoder.h263",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.h263",OMX_MAX_STRINGNAME_SIZE); DEBUG_PRINT_LOW("H263 Decoder selected"); drv_ctx.decoder_format = VDEC_CODECTYPE_H263; eCompressionFormat = OMX_VIDEO_CodingH263; output_capability = V4L2_PIX_FMT_H263; codec_type_parse = CODEC_TYPE_H263; m_frame_parser.init_start_codes (codec_type_parse); ...... //如果是divx311... &#125; else if (!strncmp(drv_ctx.kind, "OMX.qcom.video.decoder.divx311",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.divx",OMX_MAX_STRINGNAME_SIZE); DEBUG_PRINT_LOW ("DIVX 311 Decoder selected"); drv_ctx.decoder_format = VDEC_CODECTYPE_DIVX_3; output_capability = V4L2_PIX_FMT_DIVX_311; eCompressionFormat = (OMX_VIDEO_CODINGTYPE)QOMX_VIDEO_CodingDivx; codec_type_parse = CODEC_TYPE_DIVX; m_frame_parser.init_start_codes (codec_type_parse); eRet = createDivxDrmContext(); if (eRet != OMX_ErrorNone) &#123; DEBUG_PRINT_ERROR("createDivxDrmContext Failed"); return eRet; &#125; //如果是divx4... &#125; else if (!strncmp(drv_ctx.kind, "OMX.qcom.video.decoder.divx4",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.divx",OMX_MAX_STRINGNAME_SIZE); DEBUG_PRINT_ERROR ("DIVX 4 Decoder selected"); drv_ctx.decoder_format = VDEC_CODECTYPE_DIVX_4; output_capability = V4L2_PIX_FMT_DIVX; eCompressionFormat = (OMX_VIDEO_CODINGTYPE)QOMX_VIDEO_CodingDivx; codec_type_parse = CODEC_TYPE_DIVX; codec_ambiguous = true; m_frame_parser.init_start_codes (codec_type_parse); eRet = createDivxDrmContext(); if (eRet != OMX_ErrorNone) &#123; DEBUG_PRINT_ERROR("createDivxDrmContext Failed"); return eRet; &#125; //如果是divx... &#125; else if (!strncmp(drv_ctx.kind, "OMX.qcom.video.decoder.divx",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.divx",OMX_MAX_STRINGNAME_SIZE); DEBUG_PRINT_ERROR ("DIVX 5/6 Decoder selected"); drv_ctx.decoder_format = VDEC_CODECTYPE_DIVX_6; output_capability = V4L2_PIX_FMT_DIVX; eCompressionFormat = (OMX_VIDEO_CODINGTYPE)QOMX_VIDEO_CodingDivx; codec_type_parse = CODEC_TYPE_DIVX; codec_ambiguous = true; m_frame_parser.init_start_codes (codec_type_parse); eRet = createDivxDrmContext(); if (eRet != OMX_ErrorNone) &#123; DEBUG_PRINT_ERROR("createDivxDrmContext Failed"); return eRet; &#125; //如果是avc/h264... &#125; else if (!strncmp(drv_ctx.kind, "OMX.qcom.video.decoder.avc",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.avc",OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_H264; output_capability=V4L2_PIX_FMT_H264; eCompressionFormat = OMX_VIDEO_CodingAVC; codec_type_parse = CODEC_TYPE_H264; m_frame_parser.init_start_codes (codec_type_parse); m_frame_parser.init_nal_length(nal_length); ...... //如果是hevc/h265... &#125; else if (!strncmp(drv_ctx.kind, "OMX.qcom.video.decoder.hevc",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.hevc",OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_HEVC; output_capability=V4L2_PIX_FMT_HEVC; eCompressionFormat = (OMX_VIDEO_CODINGTYPE)QOMX_VIDEO_CodingHevc; codec_type_parse = CODEC_TYPE_HEVC; m_frame_parser.init_start_codes (codec_type_parse); m_frame_parser.init_nal_length(nal_length); ... //如果是vc1... &#125; else if (!strncmp(drv_ctx.kind, "OMX.qcom.video.decoder.vc1",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.vc1",OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_VC1; eCompressionFormat = OMX_VIDEO_CodingWMV; codec_type_parse = CODEC_TYPE_VC1; output_capability = V4L2_PIX_FMT_VC1_ANNEX_G; m_frame_parser.init_start_codes (codec_type_parse); //如果是wmv... &#125; else if (!strncmp(drv_ctx.kind, "OMX.qcom.video.decoder.wmv",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.vc1",OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_VC1_RCV; eCompressionFormat = OMX_VIDEO_CodingWMV; codec_type_parse = CODEC_TYPE_VC1; output_capability = V4L2_PIX_FMT_VC1_ANNEX_L; m_frame_parser.init_start_codes (codec_type_parse); //如果是vp8... &#125; else if (!strncmp(drv_ctx.kind, "OMX.qcom.video.decoder.vp8",\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, "video_decoder.vp8",OMX_MAX_STRINGNAME_SIZE); output_capability=V4L2_PIX_FMT_VP8; eCompressionFormat = OMX_VIDEO_CodingVPX; codec_type_parse = CODEC_TYPE_VP8; arbitrary_bytes = false; // 如果是不认识的解码组件，则报错 &#125; else &#123; DEBUG_PRINT_ERROR("ERROR:Unknown Component"); eRet = OMX_ErrorInvalidComponentName; &#125; //如果错误 if (eRet == OMX_ErrorNone) &#123; //设置视频输出编码格式为YUV的一种 drv_ctx.output_format = VDEC_YUV_FORMAT_NV12; //设置颜色编码 OMX_COLOR_FORMATTYPE dest_color_format = (OMX_COLOR_FORMATTYPE) QOMX_COLOR_FORMATYUV420PackedSemiPlanar32m; if (!client_buffers.set_color_format(dest_color_format)) &#123; DEBUG_PRINT_ERROR("Setting color format failed"); eRet = OMX_ErrorInsufficientResources; &#125; //订阅事件 capture_capability= V4L2_PIX_FMT_NV12; ret = subscribe_to_events(drv_ctx.video_driver_fd); if (ret) &#123; DEBUG_PRINT_ERROR("Subscribe Event Failed"); return OMX_ErrorInsufficientResources; &#125; struct v4l2_capability cap; //设置查询能力标志位 ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_QUERYCAP, &amp;cap); if (ret) &#123; DEBUG_PRINT_ERROR("Failed to query capabilities"); /*TODO: How to handle this case */ &#125; else &#123; DEBUG_PRINT_HIGH("Capabilities: driver_name = %s, card = %s, bus_info = %s," " version = %d, capabilities = %x", cap.driver, cap.card, cap.bus_info, cap.version, cap.capabilities); &#125; ret=0; fdesc.type=V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE; fdesc.index=0; while (ioctl(drv_ctx.video_driver_fd, VIDIOC_ENUM_FMT, &amp;fdesc) == 0) &#123; DEBUG_PRINT_HIGH("fmt: description: %s, fmt: %x, flags = %x", fdesc.description, fdesc.pixelformat, fdesc.flags); fdesc.index++; &#125; fdesc.type=V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE; fdesc.index=0; while (ioctl(drv_ctx.video_driver_fd, VIDIOC_ENUM_FMT, &amp;fdesc) == 0) &#123; DEBUG_PRINT_HIGH("fmt: description: %s, fmt: %x, flags = %x", fdesc.description, fdesc.pixelformat, fdesc.flags); fdesc.index++; &#125; update_resolution(320, 240); fmt.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE; fmt.fmt.pix_mp.height = drv_ctx.video_resolution.frame_height; fmt.fmt.pix_mp.width = drv_ctx.video_resolution.frame_width; fmt.fmt.pix_mp.pixelformat = output_capability; ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_S_FMT, &amp;fmt); if (ret) &#123; /*TODO: How to handle this case */ DEBUG_PRINT_ERROR("Failed to set format on output port"); &#125; DEBUG_PRINT_HIGH("Set Format was successful"); //如果有歧义的解码组件 if (codec_ambiguous) &#123; if (output_capability == V4L2_PIX_FMT_DIVX) &#123; struct v4l2_control divx_ctrl; if (drv_ctx.decoder_format == VDEC_CODECTYPE_DIVX_4) &#123; divx_ctrl.value = V4L2_MPEG_VIDC_VIDEO_DIVX_FORMAT_4; &#125; else if (drv_ctx.decoder_format == VDEC_CODECTYPE_DIVX_5) &#123; divx_ctrl.value = V4L2_MPEG_VIDC_VIDEO_DIVX_FORMAT_5; &#125; else &#123; divx_ctrl.value = V4L2_MPEG_VIDC_VIDEO_DIVX_FORMAT_6; &#125; divx_ctrl.id = V4L2_CID_MPEG_VIDC_VIDEO_DIVX_FORMAT; ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_S_CTRL, &amp;divx_ctrl); if (ret) &#123; DEBUG_PRINT_ERROR("Failed to set divx version"); &#125; &#125; else &#123; DEBUG_PRINT_ERROR("Codec should not be ambiguous"); &#125; &#125; //解码相关参数设置 fmt.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE; fmt.fmt.pix_mp.height = drv_ctx.video_resolution.frame_height; fmt.fmt.pix_mp.width = drv_ctx.video_resolution.frame_width; fmt.fmt.pix_mp.pixelformat = capture_capability; ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_S_FMT, &amp;fmt); if (ret) &#123; /*TODO: How to handle this case */ DEBUG_PRINT_ERROR("Failed to set format on capture port"); &#125; DEBUG_PRINT_HIGH("Set Format was successful"); if (secure_mode) &#123; control.id = V4L2_CID_MPEG_VIDC_VIDEO_SECURE; control.value = 1; DEBUG_PRINT_LOW("Omx_vdec:: calling to open secure device %d", ret); ret=ioctl(drv_ctx.video_driver_fd, VIDIOC_S_CTRL,&amp;control); if (ret) &#123; DEBUG_PRINT_ERROR("Omx_vdec:: Unable to open secure device %d", ret); close(drv_ctx.video_driver_fd); return OMX_ErrorInsufficientResources; &#125; &#125; /*Get the Buffer requirements for input and output ports*/ //获得输入和输出的缓冲条件 drv_ctx.ip_buf.buffer_type = VDEC_BUFFER_TYPE_INPUT; drv_ctx.op_buf.buffer_type = VDEC_BUFFER_TYPE_OUTPUT; if (secure_mode) &#123; drv_ctx.op_buf.alignment=SZ_1M; drv_ctx.ip_buf.alignment=SZ_1M; &#125; else &#123; drv_ctx.op_buf.alignment=SZ_4K; drv_ctx.ip_buf.alignment=SZ_4K; &#125; drv_ctx.interlace = VDEC_InterlaceFrameProgressive; drv_ctx.extradata = 0; drv_ctx.picture_order = VDEC_ORDER_DISPLAY; control.id = V4L2_CID_MPEG_VIDC_VIDEO_OUTPUT_ORDER; control.value = V4L2_MPEG_VIDC_VIDEO_OUTPUT_ORDER_DISPLAY; ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_S_CTRL, &amp;control); drv_ctx.idr_only_decoding = 0; m_state = OMX_StateLoaded;#ifdef DEFAULT_EXTRADATA if (eRet == OMX_ErrorNone &amp;&amp; !secure_mode) enable_extradata(DEFAULT_EXTRADATA, true, true);#endif eRet=get_buffer_req(&amp;drv_ctx.ip_buf); DEBUG_PRINT_HIGH("Input Buffer Size =%d",drv_ctx.ip_buf.buffer_size); get_buffer_req(&amp;drv_ctx.op_buf); //如果解码器格式是h264或者hevc/h265 if (drv_ctx.decoder_format == VDEC_CODECTYPE_H264 || drv_ctx.decoder_format == VDEC_CODECTYPE_HEVC) &#123; h264_scratch.nAllocLen = drv_ctx.ip_buf.buffer_size; h264_scratch.pBuffer = (OMX_U8 *)malloc (drv_ctx.ip_buf.buffer_size); h264_scratch.nFilledLen = 0; h264_scratch.nOffset = 0; if (h264_scratch.pBuffer == NULL) &#123; DEBUG_PRINT_ERROR("h264_scratch.pBuffer Allocation failed "); return OMX_ErrorInsufficientResources; &#125; &#125; //如果解码器格式是h264 if (drv_ctx.decoder_format == VDEC_CODECTYPE_H264) &#123; if (m_frame_parser.mutils == NULL) &#123; m_frame_parser.mutils = new H264_Utils(); if (m_frame_parser.mutils == NULL) &#123; DEBUG_PRINT_ERROR("parser utils Allocation failed "); eRet = OMX_ErrorInsufficientResources; &#125; else &#123; m_frame_parser.mutils-&gt;initialize_frame_checking_environment(); m_frame_parser.mutils-&gt;allocate_rbsp_buffer (drv_ctx.ip_buf.buffer_size); &#125; &#125; //创建一个h264流的解析器 h264_parser = new h264_stream_parser(); if (!h264_parser) &#123; DEBUG_PRINT_ERROR("ERROR: H264 parser allocation failed!"); eRet = OMX_ErrorInsufficientResources; &#125; &#125; //打开一个管道，读写端保存进fds数组 if (pipe(fds)) &#123; DEBUG_PRINT_ERROR("pipe creation failed"); eRet = OMX_ErrorInsufficientResources; &#125; else &#123; int temp1[2]; if (fds[0] == 0 || fds[1] == 0) &#123; if (pipe (temp1)) &#123; DEBUG_PRINT_ERROR("pipe creation failed"); return OMX_ErrorInsufficientResources; &#125; //close (fds[0]); //close (fds[1]); fds[0] = temp1 [0]; fds[1] = temp1 [1]; &#125; //输入/读 m_pipe_in = fds[0]; //输出/写 m_pipe_out = fds[1]; //创建一个工作线程，调用omx开始处理解码，并进行i/o操作 r = pthread_create(&amp;msg_thread_id,0,message_thread,this); if (r &lt; 0) &#123; DEBUG_PRINT_ERROR("component_init(): message_thread creation failed"); eRet = OMX_ErrorInsufficientResources; &#125; &#125; &#125; //没有错误，然后收尾 if (eRet != OMX_ErrorNone) &#123; DEBUG_PRINT_ERROR("Component Init Failed"); DEBUG_PRINT_HIGH("Calling VDEC_IOCTL_STOP_NEXT_MSG"); (void)ioctl(drv_ctx.video_driver_fd, VDEC_IOCTL_STOP_NEXT_MSG, NULL); DEBUG_PRINT_HIGH("Calling close() on Video Driver"); close (drv_ctx.video_driver_fd); drv_ctx.video_driver_fd = -1; &#125; else &#123; DEBUG_PRINT_HIGH("omx_vdec::component_init() success"); &#125; //memset(&amp;h264_mv_buff,0,sizeof(struct h264_mv_buffer)); return eRet;&#125; &#160; &#160; &#160; &#160;这个是解码组件的初始化实现。我们能够看出和TI的差距挺大的。步骤大概如下（maybe wrong）： 打开media相关设备文件 创建一个异步线程，执行async_message_thread函数，对输入端进行设置 根据解码器role名配置相关属性 对视频解码相关基本配置进行设置 创建一个管道，然后再开一个个工作线程，调用omx开始处理解码，并进行i/o操作 结语&#160; &#160; &#160; &#160;本篇和上一篇都是科普一下OpenMax和它在android上的实现，可以忽略不用看。下一篇我们将承接上上一篇和上一篇结尾的部分，分析android平台Stagefright和codec的交互。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android多媒体开发(五)----OpenMax简介]]></title>
    <url>%2F2016%2F12%2F15%2FAndroid%E5%A4%9A%E5%AA%92%E4%BD%93%E5%BC%80%E5%8F%91-%E4%BA%94-OpenMax%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;android中的 AwesomePlayer就是用OpenMax来做(codec)编解码的，上一篇最后一步初始化解码器我们只是初窥了一下，以后会仔细分析。本节就主要科普一下OpenMax和它在Android系统中扮演的角色。 OpenMax系统的结构OpenMax总体层次结构&#160; &#160; &#160; &#160;OpenMax是一个多媒体应用程序的框架标准，由NVIDIA公司和Khronos在2006年推出。&#160; &#160; &#160; &#160;OpenMax是无授权费的，跨平台的应用程序接口API，通过使媒体加速组件能够在开发、集成和编程环节中实现跨多操作系统和处理器硬件平台，提供全面的流媒体编解码器和应用程序便携化。&#160; &#160; &#160; &#160;OpenMax的官方网站如下所示：&#160; &#160; &#160; &#160;http://www.khronos.org/openmax/ &#160; &#160; &#160; &#160;OpenMax实际上分成三个层次，自上而下分别是，OpenMax DL（开发层），OpenMax IL（集成层）和OpenMax AL（应用层）。三个层次的内容分别如下所示： &#160; &#160; &#160; &#160;第一层：OpenMax DL（Development Layer，开发层）&#160; &#160; &#160; &#160; OpenMax DL定义了一个API，它是音频、视频和图像功能的集合。供应商能够在一个新的处理器上实现并优化，然后编解码供应商使用它来编写更广泛的编解码器功能。它包括音频信号的处理功能，如FFT和filter，图像原始处理，如颜色空间转换、视频原始处理，以实现例如MPEG-4、H.264、MP3、AAC和JPEG等编解码器的优化。 &#160; &#160; &#160; &#160;第二层：OpenMax IL（Integration Layer，集成层）&#160; &#160; &#160; &#160;OpenMax IL作为音频、视频和图像编解码器能与多媒体编解码器交互，并以统一的行为支持组件（例如，资源和皮肤）。这些编解码器或许是软硬件的混合体，对用户是透明的底层接口应用于嵌入式、移动设备。它提供了应用程序和媒体框架，透明的。编解码器供应商必须写私有的或者封闭的接口，集成进移动设备。IL的主要目的是使用特征集合为编解码器提供一个系统抽象，为解决多个不同媒体系统之间轻便性的问题。 &#160; &#160; &#160; &#160;第三层：OpenMax AL（Appliction Layer，应用层）&#160; &#160; &#160; &#160;OpenMax AL API在应用程序和多媒体中间件之间提供了一个标准化接口，多媒体中间件提供服务以实现被期待的API功能。 &#160; &#160; &#160; &#160;OpenMax API将会与处理器一同提供，以使库和编解码器开发者能够高速有效地利用新器件的完整加速潜能，无须担心其底层的硬件结构。该标准是针对嵌入式设备和移动设备的多媒体软件架构。在架构底层上为多媒体的编解码和数据处理定义了一套统一的编程接口，对多媒体数据的处理功能进行系统级抽象，为用户屏蔽了底层的细节。因此，多媒体应用程序和多媒体框架通过OpenMax IL可以以一种统一的方式来使用编解码和其他多媒体数据处理功能，具有了跨越软硬件平台的移植性。 &#160; &#160; &#160; &#160;注：在实际的应用中，OpenMax的三个层次中使用较多的是OpenMax IL集成层，由于操作系统到硬件的差异和多媒体应用的差异，OpenMax的DL和AL层使用相对较少。 OpenMax IL简介&#160; &#160; &#160; &#160;OpenMax IL 处在中间层的位置，OpenMAX IL 作为音频，视频和图像编解码器 能与多媒体编解码器交互，并以统一的行为支持组件（例如资源和皮肤）。这些编解码器或许是软硬件的混合体，对用户是 的底层接口应用于嵌入式或 / 和移动设备。它提供了应用程序和媒体框架， 透明的。本质上不存在这种标准化的接口，编解码器供 应商必须写私有的或者封闭的接口，集成进移动设备。 IL 的主要目的 是使用特征集合为编解码器提供一个系统抽象，为解决多个不同媒体系统之间轻便性的问题。&#160; &#160; &#160; &#160;OpenMax IL 的目的就是为硬件平台的图形及音视频提供一个抽象层，可以为上层的应用提供一个可跨平台的支撑。这一点对于跨平台的媒体应用来说十分重要。本人也接触过几家高清解码芯片，这些芯片底层的音视频接口虽然功能上大致相同，但是接口设计及用法上各有不同，而且相差很多。你要想让自己开发的媒体应用完美的运行在不同的硬件厂商平台上，就得适应不同芯片的底层解码接口。这个对于应用开发来说十分繁琐。所以就需要类似于OpenMax IL 这种接口规范。应用假如涉及到音视频相关功能时，只需调用这些标准的接口，而不需要关心接口下方硬件相关的实现。假如换了硬件平台时，只需要把接口层与硬件适配好了就行了。上层应用不需要频繁改动。&#160; &#160; &#160; &#160;你可以把OpenMax IL 看作是中间件中的porting层接口，但是现在中间件大部分都是自家定义自己的。 &#160; &#160; &#160; &#160;OpenMax 想做的就是定义一个这样的行业标准，这样媒体应用、硬件厂商都遵循这种标准。硬件厂商将OpenMax 与处理器一并提供，上层的多媒体框架想要用到硬件音视频加速功能时，只需遵循openmax的接口就可以扩平台运行。&#160; &#160; &#160; &#160;可喜的，现在越来越多的多媒体框架及多媒体应用正在遵循openmax标准，包括各种知名的媒体开源软件。越来越多的芯片厂商也在遵循openmax的标准。对于现在的音视频编解码来说，分辨率越来越高，需要芯片提供硬件加速功能是个大的趋势。我相信 接口的标准化是一定要走的。如下图所示， openmax IL在多媒体框架中的应用： &#160; &#160; &#160; &#160;OpenMax IL目前已经成为了事实上的多媒体框架标准。嵌入式处理器或者多媒体编解码模块的硬件生产者，通常提供标准的OpenMax IL层的软件接口，这样软件的开发者就可以基于这个层次的标准化接口进行多媒体程序的开发。&#160; &#160; &#160; &#160;OpenMax IL的接口层次结构适中，既不是硬件编解码的接口，也不是应用程序层的接口，因此比较容易实现标准化。OpenMax IL的层次结构如下： &#160; &#160; &#160; &#160;图中的虚线中的内容是OpenMax IL层的内容，其主要实现了OpenMax IL中的各个组件（Component）。对下层，OpenMax IL可以调用OpenMax DL层的接口，也可以直接调用各种Codec实现。对上层，OpenMax IL可以给OpenMax AL 层等框架层（Middleware）调用，也可以给应用程序直接调用。 OpenMax IL结构&#160; &#160; &#160; &#160;OpenMax IL主要内容如下所示。 客户端（Client）：OpenMax IL的调用者 组件（Component）：OpenMax IL的单元，每一个组件实现一种功能 端口（Port）：组件的输入输出接口 隧道化（Tunneled）：让两个组件直接连接的方式 &#160; &#160; &#160; &#160;OpenMax IL的基本运作过程如图所示： &#160; &#160; &#160; &#160;OpenMAL IL的客户端，通过调用四个OpenMAL IL组件，实现了一个功能。四个组件分别是Source组件、Host组件、Accelerator组件和Sink组件。Source组件只有一个输出端口；而Host组件有一个输入端口和一个输出端口；Accelerator组件具有一个输入端口，调用了硬件的编解码器，加速主要体现在这个环节上。Accelerator组件和Sink组件通过私有通讯方式在内部进行连接，没有经过明确的组件端口。&#160; &#160; &#160; &#160;OpenMAL IL在使用的时候，其数据流也有不同的处理方式：既可以经由客户端，也可以不经由客户端。图中Source组件到Host组件的数据流就是经过客户端的；而Host组件到Accelerator组件的数据流就没有经过客户端，使用了隧道化的方式；Accelerator组件和Sink组件甚至可以使用私有的通讯方式。 &#160; &#160; &#160; &#160;OpenMax Core是辅助各个组件运行的部分，它通常需要完成各个组件的初始化等工作，在真正运行过程中，重点是各个OpenMax IL的组件，OpenMax Core不是重点，也不是标准。 &#160; &#160; &#160; &#160;OpenMAL IL的组件是OpenMax IL实现的核心内容，一个组件以输入、输出端口为接口，端口可以被连接到另一个组件上。外部对组件可以发送命令，还进行设置/获取参数、配置等内容。组件的端口可以包含缓冲区（Buffer）的队列。&#160; &#160; &#160; &#160;组件的处理的核心内容是：通过输入端口消耗Buffer，通过输出端口填充Buffer，由此多组件相联接可以构成流式的处理。OpenMAL IL中一个组件的结构如下图所示： &#160; &#160; &#160; &#160;组件的功能和其定义的端口类型密切相关，通常情况下：只有一个输出端口的，为Source组件；只有一个输入端口的，为Sink组件；有多个输入端口，一个输出端口的为Mux组件；有一个输入端口，多个输出端口的为DeMux组件；输入输出端口各一个组件的为中间处理环节，这是最常见的组件。 &#160; &#160; &#160; &#160;端口具体支持的数据也有不同的类型。例如，对于一个输入、输出端口各一个组件，其输入端口使用MP3格式的数据，输出端口使用PCM格式的数据，那么这个组件就是一个MP3解码组件。 &#160; &#160; &#160; &#160;隧道化（Tunneled）是一个关于组件连接方式的概念。通过隧道化可以将不同的组件的一个输入端口和一个输出端口连接到一起，在这种情况下，两个组件的处理过程合并，共同处理。尤其对于单输入和单输出的组件，两个组件将作为类似一个使用。 Android中的OpenMaxOpenMax在Android中的使用情况&#160; &#160; &#160; &#160;在Android中，OpenMax IL层，通常可以用于多媒体引擎的插件，Android的多媒体引擎OpenCore和StageFright都可以使用OpenMax作为插件，主要用于编解码（Codec）处理。&#160; &#160; &#160; &#160;在Android的框架层，也定义了由Android封装的OpenMax接口，和标准的接口概念基本相同，但是使用C++类型的接口，并且使用了Android的Binder IPC机制。Android封装OpenMax的接口被StageFright使用，OpenCore没有使用这个接口，而是使用其他形式对OpenMax IL层接口进行封装。Android OpenMax的基本层次结构如图： &#160; &#160; &#160; &#160;Android系统的一些部分对OpenMax IL层进行使用，基本使用的是标准OpenMax IL层的接口，只是进行了简单的封装。标准的OpenMax IL实现很容易以插件的形式加入到Android系统中。&#160; &#160; &#160; &#160;Android的多媒体引擎OpenCore和StageFright都可以使用OpenMax作为多媒体编解码的插件，只是没有直接使用OpenMax IL层提供的纯C接口，而是对其进行了一定的封装(C++封装)。&#160; &#160; &#160; &#160;在Android2.x版本之后，Android的框架层也对OpenMax IL层的接口进行了封装定义，甚至使用Android中的Binder IPC机制。Stagefright使用了这个层次的接口，OpenCore没有使用。&#160; &#160; &#160; &#160;注：OpenCore使用OpenMax IL层作为编解码插件在前，Android框架层封装OpenMax接口在后面的版本中才引入。 Android OpenMax实现的内容&#160; &#160; &#160; &#160;android中的 AwesomePlayer就是用openmax来做(Codec)编解码,其实在openmax接口设计中，他不光能用来当编解码。通过他的组件可以组成一个完整的播放器，包括sourc、demux、decode、output。但是为什么android只用他来做code呢？应该有如下方面： 1.在整个播放器中，解码器不得不说是最重要的一部分，而且也是最耗资源的一块。如果全靠软解，直接通过cpu来运算，特别是高清视频。别的事你就可以啥都不干了。所以解码器是最需要硬件提供加速的部分。现在的高清解码芯片都是主芯片+DSP结构，解码的工作都是通过DSP来做，不会在过多的占用主芯片。所有将芯片中DSP硬件编解码的能力通过openmax标准接口呈现出来，提供上层播放器来用。我认为这块是openmax最重要的意义。 2.source 主要是和协议打交道，demux 分解容器部分，大多数的容器格式的分解是不需要通过硬件来支持。只是ts流这种格式最可能用到硬件的支持。因为ts格式比较特殊，单包的大小太小了，只有188字节。所以也是为什么现在常见的解码芯片都会提供硬件ts demux 的支持。 3.音视频输出部分video\audio output 这块和操作系统关系十分紧密。可以看看著名开源播放器vlc。vlc 在mac、linux、Windows都有，功能上差别也不大。所以说他是跨平台的，他跨平台跨在哪？主要的工作量还是在音视频解码完之后的输出模块。因为各个系统的图像渲染和音频输出实现方法不同，所以vlc需要针对每个平台实现不同的output。这部分内容放在openmax来显然不合适。 &#160; &#160; &#160; &#160;Android中使用的主要是OpenMax的编解码功能。虽然OpenMax也可以生成输入、输出、文件解析-构建等组件，但是在各个系统（不仅是Android）中使用的最多的还是编解码组件。媒体的输入、输出环节和系统的关系很大，引入OpenMax标准比较麻烦；文件解析-构建环节一般不需要使用硬件加速。编解码组件也是最能体现硬件加速的环节，因此最常使用。 初窥适配层接口&#160; &#160; &#160; &#160;在Android中实现OpenMax IL层和标准的OpenMax IL层的方式基本，一般需要实现以下两个环节： 编解码驱动程序：位于Linux内核空间，需要通过Linux内核调用驱动程序，通常使用非标准的驱动程序。 OpenMax IL层：根据OpenMax IL层的标准头文件实现不同功能的组件。 &#160; &#160; &#160; &#160;Android中还提供了OpenMax的适配层接口（对OpenMax IL的标准组件进行封装适配），它作为Android本地层的接口，可以被Android的多媒体引擎调用。上一篇文章末尾，初始化解码器核心调用的两个方法就是适配层的接口。 1.上面已经说过了，android系统中只用openmax来做Codec，所以android向上抽象了一层OMXCodec，提供给上层播放器用。播放器中音视频解码器mVideosource、mAudiosource都是OMXCodec的实例。 2.OMXCodec通过IOMX 依赖binder机制 获得 OMX服务，OMX服务 才是openmax 在android中 实现。 3.OMX把软编解码和硬件编解码统一看作插件的形式管理起来。 结语&#160; &#160; &#160; &#160;本届基本上是对OpenMax的介绍，科普一下知识。下一节我们承接上一节末尾，详细分析适配层代码的调用流程。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android多媒体开发(四)----AwesomePlayer数据源处理]]></title>
    <url>%2F2016%2F12%2F12%2FAndroid%E5%A4%9A%E5%AA%92%E4%BD%93%E5%BC%80%E5%8F%91-%E5%9B%9B-AwesomePlayer%E6%95%B0%E6%8D%AE%E6%BA%90%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;我们追着setDataSource都能挖出这么多内容，这次我们分析AwesomePlayer的setDataSource流程。 设置数据源&#160; &#160; &#160; &#160;通过前几篇的了解，我们知道StageFright框架在整个playback的位置： &#160; &#160; &#160; &#160;前一个章节我们了解了播放器的大概流程。如果要分析数据源设置，就要在前一章节的图上稍作改动。 &#160; &#160; &#160; &#160;通过setDataSource 指定播放器的数据源。可以是URI或者fd.可以是http:// 、rtsp://、本地地址或者本地文件描述符fd。其最终调用是将上层传递来的参数转化为DataSource，为下一步的demux提供数据支持。 &#160; &#160; &#160; &#160;继续跟进AwesomePlayer的setDataSource，我们依然选取Fd类型，方便分析。位于framework/av/media/libstagefright/AwesomePlayer.cpp：1234567891011121314151617181920212223status_t AwesomePlayer::setDataSource( int fd, int64_t offset, int64_t length) &#123; Mutex::Autolock autoLock(mLock); reset_l(); //创建了一个FileSource的类对象，主要提供一些文件操作的方法 sp&lt;DataSource&gt; dataSource = new FileSource(fd, offset, length); //检查文件是否ok status_t err = dataSource-&gt;initCheck(); if (err != OK) &#123; return err; &#125; //赋给全局变量 mFileSource = dataSource; &#123; Mutex::Autolock autoLock(mStatsLock); mStats.mFd = fd; mStats.mURI = String8(); &#125; //最后会调用setDataSource_l(dataSource)方法 return setDataSource_l(dataSource);&#125; &#160; &#160; &#160; &#160;这里主要讲外部传入的fd封装了一个FileSource的对象，FileSource.cpp主要实现了一些对文件的操作方法，位于framework/av/media/libstagefright/FileSource.cpp，有兴趣的可以看看。我们继续查看setDataSource_l(dataSource)方法：123456789101112131415status_t AwesomePlayer::setDataSource_l( const sp&lt;DataSource&gt; &amp;dataSource) &#123; //这里根据文件类型创建不同的MediaExtractor对象 sp&lt;MediaExtractor&gt; extractor = MediaExtractor::Create(dataSource); if (extractor == NULL) &#123; return UNKNOWN_ERROR; &#125; //检查是否是DRM加密文件 if (extractor-&gt;getDrmFlag()) &#123; checkDrmStatus(dataSource); &#125; //最后会调用到这里 return setDataSource_l(extractor);&#125; &#160; &#160; &#160; &#160;方法中又根据文件类型创建不同的MediaExtractor对象，我们看看它是怎么对文件类型进行分类的，位于framework/av/media/libstagefright/MediaExtractor.cpp中：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// static//这个Create方法第二个参数mime是缺省参数，缺省为NULLsp&lt;MediaExtractor&gt; MediaExtractor::Create( const sp&lt;DataSource&gt; &amp;source, const char *mime) &#123; sp&lt;AMessage&gt; meta; String8 tmp; if (mime == NULL) &#123;//如果外部没有传入文件的mime类型(缺省为NULL) float confidence; //从FileSource中读取文件的mime类型 if (!source-&gt;sniff(&amp;tmp, &amp;confidence, &amp;meta)) &#123; ALOGV("FAILED to autodetect media content."); return NULL; &#125; //将读取到的mime复制给局部变量 mime = tmp.string(); ALOGV("Autodetected media content as '%s' with confidence %.2f", mime, confidence); &#125; //是否为DRM加密文件 bool isDrm = false; // DRM MIME type syntax is "drm+type+original" where // type is "es_based" or "container_based" and // original is the content's cleartext MIME type //如果是DRM加密文件，则mime字符串有些不同，这里检查是否为"drm+type+original"这种格式 if (!strncmp(mime, "drm+", 4)) &#123; const char *originalMime = strchr(mime+4, '+'); if (originalMime == NULL) &#123; // second + not found return NULL; &#125; ++originalMime; if (!strncmp(mime, "drm+es_based+", 13)) &#123; // DRMExtractor sets container metadata kKeyIsDRM to 1 return new DRMExtractor(source, originalMime); &#125; else if (!strncmp(mime, "drm+container_based+", 20)) &#123; mime = originalMime; isDrm = true; &#125; else &#123; return NULL; &#125; &#125; //下面逻辑就是根据mime对文件类型的判断了，比如audio/wav wav,video/x-msvideo avi,然后选择创建哪一种Extractor数据解析器 MediaExtractor *ret = NULL; if (!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_MPEG4) || !strcasecmp(mime, "audio/mp4")) &#123; ret = new MPEG4Extractor(source); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_MPEG)) &#123; ret = new MP3Extractor(source, meta); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_AMR_NB) || !strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_AMR_WB)) &#123; ret = new AMRExtractor(source); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_FLAC)) &#123; ret = new FLACExtractor(source); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_WAV)) &#123; ret = new WAVExtractor(source); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_OGG)) &#123; ret = new OggExtractor(source); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_MATROSKA)) &#123; ret = new MatroskaExtractor(source); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_MPEG2TS)) &#123; ret = new MPEG2TSExtractor(source); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_WVM)) &#123; // Return now. WVExtractor should not have the DrmFlag set in the block below. return new WVMExtractor(source); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_AAC_ADTS)) &#123; ret = new AACExtractor(source, meta); &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_MPEG2PS)) &#123; ret = new MPEG2PSExtractor(source); &#125; //如果文件经过DRM处理，则设置一下标签 if (ret != NULL) &#123; if (isDrm) &#123; ret-&gt;setDrmFlag(true); &#125; else &#123; ret-&gt;setDrmFlag(false); &#125; &#125; return ret;&#125; &#160; &#160; &#160; &#160;上面逻辑只要先获取文件的mime，然后根绝mime的类型去创造不同的文件解析器Extractor。MIME(Multipurpose Internet Mail Extensions)多用途互联网邮件扩展类型，这个应该都很熟悉了吧。列一些常见的比如：(类型/子类型 扩展名) image/jpeg jpg application/msword doc audio/mpeg mp3 application/octet-stream flv //特殊处理 video/x-ms-wmv wmv video/mp4 mp4 //特殊处理 &#160; &#160; &#160; &#160;(如果不知道MIME类型 可以写通用的: application/octet-stream；还有一些规律是平台工具直接打开类型的，比如文本：text/扩展名，音频：audio/扩展名，视频：video/扩展名)&#160; &#160; &#160; &#160;常见的mime可以上w3school查询，可以点这里。 &#160; &#160; &#160; &#160;创建好了数据解析器，比如MPEG4Extractor，位于framework/av/media/libstagefright/MPEG4Extractor.cpp。&#160; &#160; &#160; &#160;然后我们继续往下看，进入setDataSource_l(extractor)方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129status_t AwesomePlayer::setDataSource_l(const sp&lt;MediaExtractor&gt; &amp;extractor) &#123; // Attempt to approximate overall stream bitrate by summing all // tracks' individual bitrates, if not all of them advertise bitrate, // we have to fail. //比特率为每一路音频/视频/字幕流各自比特率的累加，如果又哪一路数据流没有拿到它单个的比特率，则数据解析失败 int64_t totalBitRate = 0; mExtractor = extractor; //遍历每一路数据流 for (size_t i = 0; i &lt; extractor-&gt;countTracks(); ++i) &#123; //读取数据流的元数据，比如MPEG4Extractor中的获取方法，对文件头部信息的读取(也有一些类型文件元数据在尾部) sp&lt;MetaData&gt; meta = extractor-&gt;getTrackMetaData(i); //单独数据流的比特率 int32_t bitrate; if (!meta-&gt;findInt32(kKeyBitRate, &amp;bitrate)) &#123; const char *mime; CHECK(meta-&gt;findCString(kKeyMIMEType, &amp;mime)); ALOGV("track of type '%s' does not publish bitrate", mime); totalBitRate = -1; break; &#125; //将单个数据流比特率累加，比特率越大，码率越高，片子质量越清晰越好 totalBitRate += bitrate; &#125; sp&lt;MetaData&gt; fileMeta = mExtractor-&gt;getMetaData(); if (fileMeta != NULL) &#123; int64_t duration; if (fileMeta-&gt;findInt64(kKeyDuration, &amp;duration)) &#123; mDurationUs = duration; &#125; &#125; mBitrate = totalBitRate; ALOGV("mBitrate = %lld bits/sec", mBitrate); &#123; Mutex::Autolock autoLock(mStatsLock); mStats.mBitrate = mBitrate; mStats.mTracks.clear(); mStats.mAudioTrackIndex = -1; mStats.mVideoTrackIndex = -1; &#125; //是否有音频流 bool haveAudio = false; //是否有视频流 bool haveVideo = false; //开始遍历文件的每一路数据流 for (size_t i = 0; i &lt; extractor-&gt;countTracks(); ++i) &#123; sp&lt;MetaData&gt; meta = extractor-&gt;getTrackMetaData(i); const char *_mime; CHECK(meta-&gt;findCString(kKeyMIMEType, &amp;_mime)); String8 mime = String8(_mime); //根据每一路数据流的mime判断是音频还是视频，然后做轨道分离 if (!haveVideo &amp;&amp; !strncasecmp(mime.string(), "video/", 6)) &#123;//是否是视频流 //将视频流分离，生成mVideoTrack这个MediaSource，比如MPEG4Source：MediaSource setVideoSource(extractor-&gt;getTrack(i)); //有视频流标签置为true haveVideo = true; // Set the presentation/display size //显示宽高 int32_t displayWidth, displayHeight; bool success = meta-&gt;findInt32(kKeyDisplayWidth, &amp;displayWidth); if (success) &#123; success = meta-&gt;findInt32(kKeyDisplayHeight, &amp;displayHeight); &#125; if (success) &#123; mDisplayWidth = displayWidth; mDisplayHeight = displayHeight; &#125; &#123; Mutex::Autolock autoLock(mStatsLock); mStats.mVideoTrackIndex = mStats.mTracks.size(); mStats.mTracks.push(); TrackStat *stat = &amp;mStats.mTracks.editItemAt(mStats.mVideoTrackIndex); stat-&gt;mMIME = mime.string(); &#125; &#125; else if (!haveAudio &amp;&amp; !strncasecmp(mime.string(), "audio/", 6)) &#123;//音频流分离 //将视频流分离，生成mAudioTrack setAudioSource(extractor-&gt;getTrack(i)); //重置有音频流标签 haveAudio = true; mActiveAudioTrackIndex = i; &#123; Mutex::Autolock autoLock(mStatsLock); mStats.mAudioTrackIndex = mStats.mTracks.size(); mStats.mTracks.push(); TrackStat *stat = &amp;mStats.mTracks.editItemAt(mStats.mAudioTrackIndex); stat-&gt;mMIME = mime.string(); &#125; //对ogg类型音频的额外处理 if (!strcasecmp(mime.string(), MEDIA_MIMETYPE_AUDIO_VORBIS)) &#123; // Only do this for vorbis audio, none of the other audio // formats even support this ringtone specific hack and // retrieving the metadata on some extractors may turn out // to be very expensive. sp&lt;MetaData&gt; fileMeta = extractor-&gt;getMetaData(); int32_t loop; if (fileMeta != NULL &amp;&amp; fileMeta-&gt;findInt32(kKeyAutoLoop, &amp;loop) &amp;&amp; loop != 0) &#123; modifyFlags(AUTO_LOOPING, SET); &#125; &#125; &#125; else if (!strcasecmp(mime.string(), MEDIA_MIMETYPE_TEXT_3GPP)) &#123;//分离字幕 //对字幕流分离 addTextSource_l(i, extractor-&gt;getTrack(i)); &#125; &#125; //如果既没有视频也没有音频，则返回错误 if (!haveAudio &amp;&amp; !haveVideo) &#123; if (mWVMExtractor != NULL) &#123; return mWVMExtractor-&gt;getError(); &#125; else &#123; return UNKNOWN_ERROR; &#125; &#125; mExtractorFlags = extractor-&gt;flags(); return OK;&#125; &#160; &#160; &#160; &#160;上述方法比较长，总结如下： 根据MediaExtractor，读取数据源文件的元数据信息，将每一路数据流的比特率进行累加； 分离数据流，音频/视频/字幕分离：设置视频源mVideoTrack ；设置音频源mAudioTrack；分离字幕。mVideoTrack和mAudioTrack的做为创建的AwesomePlayer的成员函数，举个例子，比如其类型为MPEG4Source，继承了MediaSource。 &#160; &#160; &#160; &#160;注释已经写得比较清楚了。到这里文件类型的setDataSource就分析完了，这个自己挖的大坑也算填的差不多了。 网络类型数据源&#160; &#160; &#160; &#160;如果我们给MediaPlayer里面setDataSource设置的路径是网络路径，比如 http://xxxxxx......xx.mp4 ,那么在setDataSource时候是不能直接获取数据流的，要到prepareAsync才能得到数据信息。所以我们继续顺着思路，看看网络类型数据信息怎么获取。 &#160; &#160; &#160; &#160;其实这里涉及另一个知识点：MediaHTTPService ，这里给出模块图，具体细节有兴趣的同学可以自己研究，限于篇幅就不详细赘述。 java：\frameworks2\media\java\android\mediaIMediaHTTPService.aidlIMediaHTTPConnection.aidl jni：android_media_MediaHTTPConnection.cpp (.\framework\media\jni)android_media_MediaHTTPConnection.h (.\framework\media\jni) native：Lib: libstagefright_http_support.soframeworks2\av\media\libstagefright\httpIMediaHTTPConnection.aidl (.\framework\media\java\android\media)IMediaHTTPService.aidl (.\framework\media\java\android\media)MediaHTTPConnection.java (.\framework\media\java\android\media)MediaHTTPService.java (.\framework\media\java\android\media)IMediaHTTPConnection.h (.\framework\av\include\media)IMediaHTTPService.h (.\framework\av\include\media)IMediaHTTPConnection.aidl (.\framework\av\media\libstagefright)IMediaHTTPService.aidl (.\framework\av\media\libstagefright)MediaHTTP.cpp (.\framework\av\media\libstagefright\http)MediaHTTP.h (.\framework\av\include\media\stagefright)IMediaHTTPConnection.cpp (.\framework\av\media\libmedia)IMediaHTTPService.cpp (.\framework\av\media\libmedia) &#160; &#160; &#160; &#160;依然先看AwesomePlayer的另一个重载方法setDataSource：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647status_t AwesomePlayer::setDataSource( const sp&lt;IMediaHTTPService&gt; &amp;httpService, const char *uri, const KeyedVector&lt;String8, String8&gt; *headers) &#123; Mutex::Autolock autoLock(mLock); //会调用下一个方法 return setDataSource_l(httpService, uri, headers);&#125;status_t AwesomePlayer::setDataSource_l( const sp&lt;IMediaHTTPService&gt; &amp;httpService, const char *uri, const KeyedVector&lt;String8, String8&gt; *headers) &#123; reset_l(); //仅仅将httpservice保存到全局变量而已 mHTTPService = httpService; //保存了上层传入的uri mUri = uri; if (headers) &#123; mUriHeaders = *headers; ssize_t index = mUriHeaders.indexOfKey(String8("x-hide-urls-from-log")); if (index &gt;= 0) &#123; // Browser is in "incognito" mode, suppress logging URLs. // This isn't something that should be passed to the server. mUriHeaders.removeItemsAt(index); modifyFlags(INCOGNITO, SET); &#125; &#125; ALOGI("setDataSource_l(%s)", uriDebugString(mUri, mFlags &amp; INCOGNITO).c_str()); // The actual work will be done during preparation in the call to // ::finishSetDataSource_l to avoid blocking the calling thread in // setDataSource for any significant time. &#123; Mutex::Autolock autoLock(mStatsLock); mStats.mFd = -1; mStats.mURI = mUri; &#125; return OK;&#125; &#160; &#160; &#160; &#160;上面代码仅仅保存了httpService变量，这个用于请求网络；还有上层传入的uri。那么获取数据应该在prepareAsync中。我们知道prepare和prepareAsync不同在于一个是同步操作，一个是异步回调。那么我们先看看prepare：123456789101112131415161718192021222324252627282930status_t AwesomePlayer::prepare() &#123; ATRACE_CALL(); Mutex::Autolock autoLock(mLock); //会调用prepare_l方法 return prepare_l();&#125;status_t AwesomePlayer::prepare_l() &#123; if (mFlags &amp; PREPARED) &#123;//已经prepare完成了，则返回 return OK; &#125; if (mFlags &amp; PREPARING) &#123;//如果正在preparing，则返回错误 return UNKNOWN_ERROR; &#125; mIsAsyncPrepare = false;//将异步标签置为false //最终还是会调用prepareAsync_l，只不过在同一线程 status_t err = prepareAsync_l(); if (err != OK) &#123; return err; &#125; //因此prepare会等待在同步线程 while (mFlags &amp; PREPARING) &#123; mPreparedCondition.wait(mLock); &#125; return mPrepareResult;&#125; &#160; &#160; &#160; &#160;同步prepare很简单，执行方法最终还是会调用prepareAsync_l方法，只不过在同一线程等待。&#160; &#160; &#160; &#160;那么我们看看异步prepareAsync：1234567891011121314151617181920212223242526272829303132status_t AwesomePlayer::prepareAsync() &#123; ATRACE_CALL(); Mutex::Autolock autoLock(mLock); //如果正在preparing，则返回错误 if (mFlags &amp; PREPARING) &#123; return UNKNOWN_ERROR; // async prepare already pending &#125; mIsAsyncPrepare = true; //调用prepareAsync_l方法 return prepareAsync_l();&#125;status_t AwesomePlayer::prepareAsync_l() &#123; if (mFlags &amp; PREPARING) &#123;//同上 return UNKNOWN_ERROR; // async prepare already pending &#125; //这一步就是我们上一节分析的事件调度线程启动，可以参考上一节 if (!mQueueStarted) &#123; mQueue.start(); mQueueStarted = true; &#125; //修改标志位，加上PREPARING标志 modifyFlags(PREPARING, SET); //将封装onPrepareAsyncEvent方法的时间加入事件队列中 mAsyncPrepareEvent = new AwesomeEvent( this, &amp;AwesomePlayer::onPrepareAsyncEvent); mQueue.postEvent(mAsyncPrepareEvent); return OK;&#125; &#160; &#160; &#160; &#160;这里我们可以看到它启动了事件调度队列，这个我们上一节分析过了，如果不太明白可以回去看看：Android多媒体开发(三)—-从StageFright到AwesomePlayer 。&#160; &#160; &#160; &#160;然后给事件队列加入封装onPrepareAsyncEvent方法的事件，我们接着看这个方法：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152void AwesomePlayer::onPrepareAsyncEvent() &#123; Mutex::Autolock autoLock(mLock); //调用beginPrepareAsync_l beginPrepareAsync_l();&#125;void AwesomePlayer::beginPrepareAsync_l() &#123; if (mFlags &amp; PREPARE_CANCELLED) &#123; ALOGI("prepare was cancelled before doing anything"); abortPrepare(UNKNOWN_ERROR); return; &#125; //主要在这一步，设置网络数据源 if (mUri.size() &gt; 0) &#123; status_t err = finishSetDataSource_l(); if (err != OK) &#123; abortPrepare(err); return; &#125; &#125; //初始化视频解码器 if (mVideoTrack != NULL &amp;&amp; mVideoSource == NULL) &#123; status_t err = initVideoDecoder(); if (err != OK) &#123; abortPrepare(err); return; &#125; &#125; //初始化音频解码器 if (mAudioTrack != NULL &amp;&amp; mAudioSource == NULL) &#123; status_t err = initAudioDecoder(); if (err != OK) &#123; abortPrepare(err); return; &#125; &#125; modifyFlags(PREPARING_CONNECTED, SET); //是否是http的网络数据流 if (isStreamingHTTP()) &#123; postBufferingEvent_l();//开始onBufferingUpdate回调，开始缓冲视频 &#125; else &#123; finishAsyncPrepare_l();//完成prepare &#125;&#125;bool AwesomePlayer::isStreamingHTTP() const &#123; return mCachedSource != NULL || mWVMExtractor != NULL;&#125; &#160; &#160; &#160; &#160;我们这一步的重点是在设置网络数据源，调用finishSetDataSource_l方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152status_t AwesomePlayer::finishSetDataSource_l() &#123; ATRACE_CALL(); sp&lt;DataSource&gt; dataSource; bool isWidevineStreaming = false; //Widevine是google在ICS版本上新推出的一种DRM数字版权管理功能，有这个功能的话，就能从google指定的服务器上，下载经过google加密的版权文件，例如视频、应用等等。 if (!strncasecmp("widevine://", mUri.string(), 11)) &#123; isWidevineStreaming = true; //如果是google自家的私货，就要在uri加上http://的前缀 String8 newURI = String8("http://"); newURI.append(mUri.string() + 11); mUri = newURI; &#125; //mime AString sniffedMIME; //如果是http、https的网络数据流，或者是谷歌自家的私货widevine if (!strncasecmp("http://", mUri.string(), 7) || !strncasecmp("https://", mUri.string(), 8) || isWidevineStreaming) &#123; //这个就是我们在setDataSource里面取到的httpservice，用于发送http请求 if (mHTTPService == NULL) &#123; ALOGE("Attempt to play media from http URI without HTTP service."); return UNKNOWN_ERROR; &#125; //建立http链接 sp&lt;IMediaHTTPConnection&gt; conn = mHTTPService-&gt;makeHTTPConnection(); mConnectingDataSource = new MediaHTTP(conn); String8 cacheConfig; bool disconnectAtHighwatermark; NuCachedSource2::RemoveCacheSpecificHeaders( &amp;mUriHeaders, &amp;cacheConfig, &amp;disconnectAtHighwatermark); mLock.unlock(); //开始链接网络 status_t err = mConnectingDataSource-&gt;connect(mUri, &amp;mUriHeaders); // force connection at this point, to avoid a race condition between getMIMEType and the // caching datasource constructed below, which could result in multiple requests to the // server, and/or failed connections. String8 contentType = mConnectingDataSource-&gt;getMIMEType(); mLock.lock(); if (err != OK) &#123; mConnectingDataSource.clear(); ALOGI("mConnectingDataSource-&gt;connect() returned %d", err); return err; &#125; if (!isWidevineStreaming) &#123;//非google的私货，正常的http网络请求读取数据 // The widevine extractor does its own caching.#if 0 mCachedSource = new NuCachedSource2( new ThrottledSource( mConnectingDataSource, 50 * 1024 /* bytes/sec */));#else // NuCachedSource2，带缓存的DataSource，不包含媒体信息，只管理缓存以及调用底层的DataSource读取和缓存数据。可以获取缓存的信息以及操作缓存。 mCachedSource = new NuCachedSource2( mConnectingDataSource, cacheConfig.isEmpty() ? NULL : cacheConfig.string(), disconnectAtHighwatermark);#endif //获取缓存数据源 dataSource = mCachedSource; &#125; else &#123;//是google的私货 dataSource = mConnectingDataSource; &#125; mConnectingDataSource.clear(); //如果不是纯音频的文件类型 if (strncasecmp(contentType.string(), "audio/", 6)) &#123; // We're not doing this for streams that appear to be audio-only // streams to ensure that even low bandwidth streams start // playing back fairly instantly. ...省略一些代码... if (!dataSource-&gt;sniff(&amp;tmp, &amp;confidence, &amp;meta)) &#123; mLock.lock(); return UNKNOWN_ERROR; &#125; // We successfully identified the file's extractor to // be, remember this mime type so we don't have to // sniff it again when we call MediaExtractor::Create() // below. //获取mime sniffedMIME = tmp.string(); ...省略一些代码... &#125; &#125; else &#123;//如果是不http、https的网络数据流、谷歌自家的私货widevine //请求网络数据 dataSource = DataSource::CreateFromURI( mHTTPService, mUri.string(), &amp;mUriHeaders); &#125; if (dataSource == NULL) &#123; return UNKNOWN_ERROR; &#125; sp&lt;MediaExtractor&gt; extractor; //是google的私货 if (isWidevineStreaming) &#123; String8 mimeType; float confidence; sp&lt;AMessage&gt; dummy; bool success; // SniffWVM is potentially blocking since it may require network access. // Do not call it with mLock held. mLock.unlock(); //嗅探wvm类型资源 success = SniffWVM(dataSource, &amp;mimeType, &amp;confidence, &amp;dummy); mLock.lock(); if (!success || strcasecmp( mimeType.string(), MEDIA_MIMETYPE_CONTAINER_WVM)) &#123; return ERROR_UNSUPPORTED; &#125; //创建wvm的数据解析器 mWVMExtractor = new WVMExtractor(dataSource); mWVMExtractor-&gt;setAdaptiveStreamingMode(true); if (mUIDValid) mWVMExtractor-&gt;setUID(mUID); extractor = mWVMExtractor; &#125; else &#123;//如果不是google的私货，则根据MIME创建相应的数据解析器 extractor = MediaExtractor::Create( dataSource, sniffedMIME.empty() ? NULL : sniffedMIME.c_str()); if (extractor == NULL) &#123; return UNKNOWN_ERROR; &#125; &#125; //检查是否有DRM处理 if (extractor-&gt;getDrmFlag()) &#123; checkDrmStatus(dataSource); &#125; //setDataSource_l(extractor)我们本节分析，主要是分流音频/视频/字幕 status_t err = setDataSource_l(extractor); if (err != OK) &#123; mWVMExtractor.clear(); return err; &#125; return OK;&#125; &#160; &#160; &#160; &#160;上述就是网络获取数据源的过程：1）setDataSource获取一个网络连接服务；2）prepareAsync中请求网络，读取缓存，然后得到文件类型mime；3）根据mime创建MediaExtractor，然后分离音频/视频/字幕流数据。 初窥解码器&#160; &#160; &#160; &#160;上一步我们分析完了setDataSource，并且进入了prepare环节。走完了finishSetDataSource_l，但是beginPrepareAsync_l方法还没有走完。接下来应该是初始化音/视频解码器了。 视频解码器入口&#160; &#160; &#160; &#160;进入beginPrepareAsync_l方法，执行到initVideoDecoder方法位置，我们继续查看：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273status_t AwesomePlayer::initVideoDecoder(uint32_t flags) &#123; ATRACE_CALL(); // Either the application or the DRM system can independently say // that there must be a hardware-protected path to an external video sink. // For now we always require a hardware-protected path to external video sink // if content is DRMed, but eventually this could be optional per DRM agent. // When the application wants protection, then // (USE_SURFACE_ALLOC &amp;&amp; (mSurface != 0) &amp;&amp; // (mSurface-&gt;getFlags() &amp; ISurfaceComposer::eProtectedByApp)) // will be true, but that part is already handled by SurfaceFlinger. ...... if (mDecryptHandle != NULL) &#123; flags |= OMXCodec::kEnableGrallocUsageProtected; &#125; ...... ALOGV("initVideoDecoder flags=0x%x", flags); //和视频解码关，视频解码器 mVideoSource = OMXCodec::Create( mClient.interface(), mVideoTrack-&gt;getFormat(), false, // createEncoder mVideoTrack, NULL, flags, USE_SURFACE_ALLOC ? mNativeWindow : NULL); if (mVideoSource != NULL) &#123; int64_t durationUs; //获取视频轨道格式，然后读取时长 if (mVideoTrack-&gt;getFormat()-&gt;findInt64(kKeyDuration, &amp;durationUs)) &#123; Mutex::Autolock autoLock(mMiscStateLock); if (mDurationUs &lt; 0 || durationUs &gt; mDurationUs) &#123; mDurationUs = durationUs; &#125; &#125; //读取原始视频数据 status_t err = mVideoSource-&gt;start(); if (err != OK) &#123; ALOGE("failed to start video source"); mVideoSource.clear(); return err; &#125; &#125; //一些检查逻辑 if (mVideoSource != NULL) &#123; const char *componentName; CHECK(mVideoSource-&gt;getFormat() -&gt;findCString(kKeyDecoderComponent, &amp;componentName)); &#123; Mutex::Autolock autoLock(mStatsLock); TrackStat *stat = &amp;mStats.mTracks.editItemAt(mStats.mVideoTrackIndex); stat-&gt;mDecoderName = componentName; &#125; static const char *kPrefix = "OMX.Nvidia."; static const char *kSuffix = ".decode"; static const size_t kSuffixLength = strlen(kSuffix); size_t componentNameLength = strlen(componentName); if (!strncmp(componentName, kPrefix, strlen(kPrefix)) &amp;&amp; componentNameLength &gt;= kSuffixLength &amp;&amp; !strcmp(&amp;componentName[ componentNameLength - kSuffixLength], kSuffix)) &#123; modifyFlags(SLOW_DECODER_HACK, SET); &#125; &#125; return mVideoSource != NULL ? OK : UNKNOWN_ERROR;&#125; &#160; &#160; &#160; &#160;视频解码器的入口，核心是OMXCodec::Create()方法，创建解码器。这个我们下一节会自信分析，本节只是入口。 音频解码器入口&#160; &#160; &#160; &#160;然后我们看看音频解码器：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677status_t AwesomePlayer::initAudioDecoder() &#123; ATRACE_CALL(); //获取音频轨道跟踪格式 sp&lt;MetaData&gt; meta = mAudioTrack-&gt;getFormat(); const char *mime; CHECK(meta-&gt;findCString(kKeyMIMEType, &amp;mime)); // Check whether there is a hardware codec for this stream // This doesn't guarantee that the hardware has a free stream // but it avoids us attempting to open (and re-open) an offload // stream to hardware that doesn't have the necessary codec audio_stream_type_t streamType = AUDIO_STREAM_MUSIC; if (mAudioSink != NULL) &#123; //得到音频流类型 streamType = mAudioSink-&gt;getAudioStreamType(); &#125; mOffloadAudio = canOffloadStream(meta, (mVideoSource != NULL), isStreamingHTTP(), streamType); //如果是原始数据不用解码 if (!strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_RAW)) &#123; ALOGV("createAudioPlayer: bypass OMX (raw)"); mAudioSource = mAudioTrack; &#125; else &#123; // If offloading we still create a OMX decoder as a fall-back // but we don't start it //创建音频解码器。和上面的视频解码器一样，只不过参数由mVideoTrack变为mAudioTrack mOmxSource = OMXCodec::Create( mClient.interface(), mAudioTrack-&gt;getFormat(), false, // createEncoder mAudioTrack); if (mOffloadAudio) &#123; ALOGV("createAudioPlayer: bypass OMX (offload)"); mAudioSource = mAudioTrack; &#125; else &#123; mAudioSource = mOmxSource; &#125; &#125; if (mAudioSource != NULL) &#123; int64_t durationUs; if (mAudioTrack-&gt;getFormat()-&gt;findInt64(kKeyDuration, &amp;durationUs)) &#123; Mutex::Autolock autoLock(mMiscStateLock); if (mDurationUs &lt; 0 || durationUs &gt; mDurationUs) &#123; mDurationUs = durationUs; &#125; &#125; //读取音频原始数据 status_t err = mAudioSource-&gt;start(); if (err != OK) &#123; mAudioSource.clear(); mOmxSource.clear(); return err; &#125; &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_QCELP)) &#123; // For legacy reasons we're simply going to ignore the absence // of an audio decoder for QCELP instead of aborting playback // altogether. return OK; &#125; if (mAudioSource != NULL) &#123; Mutex::Autolock autoLock(mStatsLock); TrackStat *stat = &amp;mStats.mTracks.editItemAt(mStats.mAudioTrackIndex); const char *component; if (!mAudioSource-&gt;getFormat() -&gt;findCString(kKeyDecoderComponent, &amp;component)) &#123; component = "none"; &#125; stat-&gt;mDecoderName = component; &#125; return mAudioSource != NULL ? OK : UNKNOWN_ERROR;&#125; &#160; &#160; &#160; &#160;初始化音频解码和视频解码器代码部分大同小异，核心还是OMXCodec::Create()方法，只是参数不用。 &#160; &#160; &#160; &#160;总结一下上述流程： 通过setDataSource 指定播放器的数据源。可以是URI或者fd.可以是http:// 、rtsp://、本地地址或者本地文件描述符fd。其最终调用是将上层传递来的参数转化为DataSource，为下一步的demux提供数据支持。 在真正Prepare功能函数onPrepareAsyncEvent()会调用finishSetDataSource_l。通过第一步产生的DataSource来生成extractor，因为封装的格式很多，所以需要通过DataSource的信息，去创建不同的extractor。 得到extractor之后，通过setVideoSource() setAudioSource()产生独立的mVideoTrack(视频)、mAudioTrack(音频)数据流，分别为音视频解码器提供有各自需要的数据流。 接下来就是initVideoDecoder() initAudioDecoder().依赖上面产生的mVideoTrack(视频)、mAudioTrack(音频)数据流。生成了mVideoSource和mAudioSource这两个音视频解码器。 &#160; &#160; &#160; &#160;最后一步创建解码器都是调用同样的接口：12345678910mVideoSource = OMXCodec::Create( mClient.interface(), mVideoTrack-&gt;getFormat(), false, // createEncoder mVideoTrack, NULL, flags, USE_SURFACE_ALLOC ? mNativeWindow : NULL); mAudioSource = OMXCodec::Create( mClient.interface(), mAudioTrack-&gt;getFormat(), false, // createEncoder mAudioTrack); &#125; &#160; &#160; &#160; &#160;mVideoSource、mAudioSource组成了播放器模型中的decoder部分。 &#160; &#160; &#160; &#160;Android系统中的编解码器部分用的是openmax，以后会深入了解。openma x是一套标准接口，各家硬件厂商都可以遵循这个标准来做自己的实现，发挥自己芯片特性。然后提供给android系统来用。因为大部分的机顶盒芯片产品硬件的编解码是它的优势，可以把这种优势完全融入到android平台中。以后手机高清视频硬解码也会是个趋势。 &#160; &#160; &#160; &#160;解码完之后的数据就要输出了。AwesomePlayer分别用了mVideoRenderer做视频输出、mAudioPlayer做音频输出。他们分别调用android图像和音频的相关服务。mVideoRenderer和mAudioPlayer就组成了播放器中output的部分。这俩部分是android平台中十分重要的2块，以后会深入了解。 结语&#160; &#160; &#160; &#160;综上AwesomePlayer的整体框架和流程就清晰了，其实也脱离不了DataSource、demux、decoder、output这4大部分。接下来会分别了解每个部分是怎么实现的。&#160; &#160; &#160; &#160;下一节我们就从openmax开始。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android多媒体开发(三)----从StageFright到AwesomePlayer]]></title>
    <url>%2F2016%2F12%2F11%2FAndroid%E5%A4%9A%E5%AA%92%E4%BD%93%E5%BC%80%E5%8F%91-%E4%B8%89-%E4%BB%8EStageFright%E5%88%B0AwesomePlayer%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;上一节我们分析到了MediaPlayer的C/S架构最下层的StagefrightPlayer，今天我们继续往下挖，看看这个东西到底是个什么鬼。 皮包公司StagefrightPlayer&#160; &#160; &#160; &#160;上一节我们再次止步于C++层的setDateSource，但是我们初窥了StatefrightPlayer，简要了解android高版本对opencore的舍弃，和采用了Stagefright框架。所以我们来看看这个StagefrightPlayer是怎么实现的。位于framework/av/media/libmediaplayerservice/StagefrightPlayer.cpp：123456789101112131415161718192021222324252627282930313233343536373839......StagefrightPlayer::StagefrightPlayer() : mPlayer(new AwesomePlayer) &#123; //StagefrightPlayer构造方法默认创建了AwesomePlayer代替他完成职能 ALOGV("StagefrightPlayer"); mPlayer-&gt;setListener(this);&#125;StagefrightPlayer::~StagefrightPlayer() &#123; ALOGV("~StagefrightPlayer"); reset(); delete mPlayer; mPlayer = NULL;&#125;// Warning: The filedescriptor passed into this method will only be valid until// the method returns, if you want to keep it, dup it!status_t StagefrightPlayer::setDataSource(int fd, int64_t offset, int64_t length) &#123; ALOGV("setDataSource(%d, %lld, %lld)", fd, offset, length); return mPlayer-&gt;setDataSource(dup(fd), offset, length);&#125;status_t StagefrightPlayer::setVideoSurfaceTexture( const sp&lt;IGraphicBufferProducer&gt; &amp;bufferProducer) &#123; ALOGV("setVideoSurfaceTexture"); return mPlayer-&gt;setSurfaceTexture(bufferProducer);&#125;status_t StagefrightPlayer::prepare() &#123; return mPlayer-&gt;prepare();&#125;...省略诸多AwesomePlayer代替他完成职能的方法...... &#160; &#160; &#160; &#160;我们看到这个StagefrightPlayer其实是个皮包公司，对MediaPlayerInterface接口的实现方法都是由AwesomePlayer这个播放器来实现的。(听着名字都应该是装饰模式关系，StageFright叫舞台恐怖者，Awesome是令人畏惧者，一个前台耍花腔散播恐惧，一个后台实际操控恐怖主义)。&#160; &#160; &#160; &#160;前面一篇中，分析到mediaplayerservice会调到Stagefright中，进行编码解码操作在libsstagefright中,预设的多媒体解码是openCore，由于其过于庞大和复杂，需要成本较高，开始引进了另一个框架，也就是stagefright框架，以后默认情况Android选择stagefright，但是并没有完全抛弃opencore，做了一个OMX层，仅仅是对 opencore的omx-component部分做了引用。stagefright是和opencore是并列的。Stagefright在 Android中是以shared library的形式存在(libstagefright.so)，其中的module – AwesomePlayer可用来播放video/audio。 AwesomePlayer提供许多API，可以让上层的应用程序(Java/JNI)来调用。 播放器基本模型&#160; &#160; &#160; &#160;AwesomePlayer不管有多么神秘，说到底还是个播放器。在播放器的基本模型上，他与VCL、mplayer、ffmpeg等开源的结构是一致的。只是组织实现的方式不同。&#160; &#160; &#160; &#160;深入了解AwesomePlayer 之前，把播放器的基本模型总结一下，然后按照模型的各个部分来深入研究AwesomePlayer 的实现方式。 &#160; &#160; &#160; &#160;播放器大致分为4大部分：source、demux、decoder、output。 source数据源：数据源，数据的来源不一定都是本地file，也有可能是网路上的各种协议例如：http、rtsp、HLS等。source的任务就是把数据源抽象出来，为下一个demux模块提供它需要的稳定的数据流。demux不用关信数据到底是从什么地方来的。 demux解复用：视频文件一般情况下都是把音视频的ES流交织的通过某种规则放在一起。这种规则就是容器规则。现在有很多不同的容器格式。如ts、mp4、flv、mkv、avi、rmvb等等。demux的功能就是把音视频的ES流从容器中剥离出来，然后分别送到不同的解码器中。其实音频和视频本身就是2个独立的系统。容器把它们包在了一起。但是他们都是独立解码的，所以解码之前，需要把它分别 独立出来。demux就是干这活的，他为下一步decoder解码提供了数据流。 decoder解码：解码器—-播放器的核心模块。分为音频和视频解码器。影像在录制后, 原始的音视频都是占用大量空间, 而且是冗余度较高的数据. 因此, 通常会在制作的时候就会进行某种压缩 ( 压缩技术就是将数据中的冗余信息去除数据之间的相关性 ). 这就是我们熟知的音视频编码格式, 包括MPEG1（VCD）\ MPEG2（DVD）\ MPEG4 \ H.264 等等. 音视频解码器的作用就是把这些压缩了的数据还原成原始的音视频数据. 当然, 编码解码过程基本上都是有损的 .解码器的作用就是把编码后的数据还原成原始数据。 output输出：输出部分分为音频和视频输出。解码后的音频（pcm）和视频（yuv）的原始数据需要得到音视频的output模块的支持才能真正的让人的感官系统（眼和耳）辨识到。 &#160; &#160; &#160; &#160;所以，播放器大致分成上述4部分。怎么抽象的实现这4大部分、以及找到一种合理的方式将这几部分组织并运动起来。是每个播放器不同的实现方式而已。接下来就围绕这4大部分做深入学习，看看AwesomePlayer 是怎么玩的吧。 AwesomePlayer基础AwesomePlayer事件调度器————TimedEventQueue-event&#160; &#160; &#160; &#160;在分析AwesomePlayer的主要流程前，我们先了解一下它的时间调度机制，为以后分析主流程打好基础。&#160; &#160; &#160; &#160;视频处理过程中有很多都是十分耗时的，如果都放在一个大的线程空间中。用户体验的效果可想而知。所以通常都是做异步操作。 &#160; &#160; &#160; &#160;AwesomePlayer是通过event事件调度来实现这些功能之间的驱动和调用的，类似于上层的Handler-Looper。AwesomePlayer中的内部变量TimedEventQueue mQueue；位于framework/av/media/libstagefright/include/AwesomePlayer.h中：1TimedEventQueue mQueue; &#160; &#160; &#160; &#160;这个mQueue就是AwesomePlayer的事件队列，也是事件调度器。从他类型的名字上就能很清楚的看出他是以时间为基础事件队列。接下来看看它是怎么玩转的。 &#160; &#160; &#160; &#160;1. 我们看看TimedEventQueue的内部结构，位于framework/av/media/libstagefright/include/TimedEventQueue.h中。TimedEventQueue内部有一个List&lt; QueueItem &gt;，每个QueueItem包含enent和触发事件时间，还有一个是否持有唤醒锁的标志位。12345678910111213141516171819202122232425262728293031List&lt;QueueItem&gt; mQueue;//触发事件队列pthread_t mThread;//独立调度线程typedef int32_t event_id;//这里是个typedef，下面Event结构体的事件id会用到event_id mEventID; //事件队列单项结构体 struct QueueItem &#123; sp&lt;Event&gt; event;//事件 int64_t realtime_us;//触发时间 bool has_wakelock; &#125;; //封装事件结构体 struct Event : public RefBase &#123; Event() : mEventID(0) &#123; &#125; virtual ~Event() &#123;&#125; event_id eventID() &#123; return mEventID; &#125; protected: //发射事件，即执行事件 virtual void fire(TimedEventQueue *queue, int64_t now_us) = 0; private: friend class TimedEventQueue; event_id mEventID;//每个事件自动分配的id，初始值为1，每多一个事件id会+1 void setEventID(event_id id) &#123; mEventID = id; &#125; Event(const Event &amp;); Event &amp;operator=(const Event &amp;); &#160; &#160; &#160; &#160;这里有一个独立线程mThread，它主要负责事件调度。是在TimedEventQueue::start被创建，TimedEventQueue::stop被销毁的。位于framework/av/media/libstagefright/TimedEventQueue.cpp中：1234567891011121314151617181920212223242526272829303132333435363738void TimedEventQueue::start() &#123; if (mRunning) &#123; return; &#125; mStopped = false; pthread_attr_t attr; pthread_attr_init(&amp;attr); pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE); //创建调度线程，同时调用ThreadWrapper方法 pthread_create(&amp;mThread, &amp;attr, ThreadWrapper, this); pthread_attr_destroy(&amp;attr); mRunning = true;&#125;void TimedEventQueue::stop(bool flush) &#123; if (!mRunning) &#123; return; &#125; if (flush) &#123; postEventToBack(new StopEvent); &#125; else &#123; postTimedEvent(new StopEvent, INT64_MIN); &#125; void *dummy; pthread_join(mThread, &amp;dummy); // some events may be left in the queue if we did not flush and the wake lock // must be released. releaseWakeLock_l(true /*force*/); mQueue.clear(); mRunning = false; &#160; &#160; &#160; &#160;2. 这里创建了调度线程，然后执行了ThreadWrapper方法，我们继续看看这个方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122// staticvoid *TimedEventQueue::ThreadWrapper(void *me) &#123; androidSetThreadPriority(0, ANDROID_PRIORITY_FOREGROUND); //调用了threadEntry方法 static_cast&lt;TimedEventQueue *&gt;(me)-&gt;threadEntry(); return NULL;&#125;void TimedEventQueue::threadEntry() &#123; prctl(PR_SET_NAME, (unsigned long)"TimedEventQueue", 0, 0, 0); for (;;) &#123; int64_t now_us = 0; sp&lt;Event&gt; event; bool wakeLocked = false; &#123; Mutex::Autolock autoLock(mLock); //如果停止标志则退出轮询 if (mStopped) &#123; break; &#125; //事件队列为空，则线程等待 while (mQueue.empty()) &#123; mQueueNotEmptyCondition.wait(mLock); &#125; event_id eventID = 0; for (;;) &#123; //如果时间对垒为空，跳出内循环，进入外循环等待 if (mQueue.empty()) &#123; // The only event in the queue could have been cancelled // while we were waiting for its scheduled time. //当我们等着这个事件被执行，但是它被取消了，则跳出这次轮询 break; &#125; //遍历队列中的所有事件 List&lt;QueueItem&gt;::iterator it = mQueue.begin(); eventID = (*it).event-&gt;eventID(); //获取当前的系统时间(毫秒) now_us = ALooper::GetNowUs(); //事件触发的事件 int64_t when_us = (*it).realtime_us; //等待延时执行的时间 int64_t delay_us; if (when_us &lt; 0 || when_us == INT64_MAX) &#123;//如果等待时间小于0或者是int_64最大值，则将等待时间置为0 delay_us = 0; &#125; else &#123;//正常等待 delay_us = when_us - now_us; &#125; //等待时间小于等于0，则跳出这次事件轮询 if (delay_us &lt;= 0) &#123; break; &#125; //等待的最大超时时间为10秒 static int64_t kMaxTimeoutUs = 10000000ll; // 10 secs bool timeoutCapped = false; if (delay_us &gt; kMaxTimeoutUs) &#123; ALOGW("delay_us exceeds max timeout: %" PRId64 " us", delay_us); // We'll never block for more than 10 secs, instead // we will split up the full timeout into chunks of // 10 secs at a time. This will also avoid overflow // when converting from us to ns. //如果等待时长大于10秒，则强制让它10秒后执行。 //上面注释说这样避免了从毫秒到纳秒转化后的值溢出int_64最大范围。 //然后将超时标签置为true delay_us = kMaxTimeoutUs; timeoutCapped = true; &#125; //线程开始等待 status_t err = mQueueHeadChangedCondition.waitRelative( mLock, delay_us * 1000ll); //时间到了，调出内循环，到外层循环去执行事件 if (!timeoutCapped &amp;&amp; err == -ETIMEDOUT) &#123; // We finally hit the time this event is supposed to // trigger. now_us = ALooper::GetNowUs(); break; &#125; &#125; // The event w/ this id may have been cancelled while we're // waiting for its trigger-time, in that case // removeEventFromQueue_l will return NULL. // Otherwise, the QueueItem will be removed // from the queue and the referenced event returned. //如果在等待时间过程中，事件被移除了，则返回NULL //如果正常情况，则返回这个事件，同时从队列中移除这个事件 event = removeEventFromQueue_l(eventID, &amp;wakeLocked); &#125; if (event != NULL) &#123; // Fire event with the lock NOT held. //发射事件，执行事件方法 event-&gt;fire(this, now_us); if (wakeLocked) &#123; Mutex::Autolock autoLock(mLock); releaseWakeLock_l(); &#125; &#125; &#125;&#125; //如果在等待时间过程中，事件被移除了，则返回NULL //如果正常情况，则返回这个事件，同时从队列中移除这个事件sp&lt;TimedEventQueue::Event&gt; TimedEventQueue::removeEventFromQueue_l( event_id id, bool *wakeLocked) &#123; for (List&lt;QueueItem&gt;::iterator it = mQueue.begin(); it != mQueue.end(); ++it) &#123; if ((*it).event-&gt;eventID() == id) &#123; sp&lt;Event&gt; event = (*it).event; event-&gt;setEventID(0); *wakeLocked = (*it).has_wakelock; mQueue.erase(it); return event; &#125; &#125; ALOGW("Event %d was not found in the queue, already cancelled?", id); return NULL;&#125; &#160; &#160; &#160; &#160;这一段代码比较长，其实就是讲List目的就是按照延时时间维护一个event事件队列，threadEntry线程就是不断的从队列的头取出一个event，然后通过 event-&gt;fire(this, now_us); 回调到这个event事件提前注册好的相对应功能函数。这一部分和Handler-Looper很像，应该不难理解。发射执行事件比较简单，我们看看AwesomeEvent这个继承TimedEventQueue::Event的实现类，位于framework/av/media/libstagefright/AwesomePlayer.cpp：123456789101112131415161718192021struct AwesomeEvent : public TimedEventQueue::Event &#123; AwesomeEvent( AwesomePlayer *player, void (AwesomePlayer::*method)()) : mPlayer(player), mMethod(method) &#123; &#125;protected: virtual ~AwesomeEvent() &#123;&#125; //发射事件，调用AwesomePlayer的相关方法 virtual void fire(TimedEventQueue * /* queue */, int64_t /* now_us */) &#123; (mPlayer-&gt;*mMethod)(); &#125;private: AwesomePlayer *mPlayer; void (AwesomePlayer::*mMethod)(); AwesomeEvent(const AwesomeEvent &amp;); AwesomeEvent &amp;operator=(const AwesomeEvent &amp;); &#160; &#160; &#160; &#160;3. 然后看看AwesomePlayer是怎么用TimedEventQueue，AwesomePlayer会定义很多类型的event事件，并把和这些事件相关的功能函数一定绑定起来。比如AwesomePlayer的构造方法里有这么几个，位于framework/av/media/libstagefright/AwesomePlayer.cpp中：12345678910......mVideoEvent = new AwesomeEvent(this, &amp;AwesomePlayer::onVideoEvent);mVideoEventPending = false;mStreamDoneEvent = new AwesomeEvent(this, &amp;AwesomePlayer::onStreamDone);mStreamDoneEventPending = false;mBufferingEvent = new AwesomeEvent(this, &amp;AwesomePlayer::onBufferingUpdate);mBufferingEventPending = false;mVideoLagEvent = new AwesomeEvent(this, &amp;AwesomePlayer::onVideoLagUpdate);mVideoLagEventPending = false;...... &#160; &#160; &#160; &#160;当然还有一些Event是在方法内部构造的。 &#160; &#160; &#160; &#160;原因之前也说了，因为好多音视频处理的功能是十分耗时间的，假如AwesomePlayer 想用某个功能，他并不是直线去调用它，而是抽象成一种AwesomeEvent，将想要调用的功能函数与事件捆绑。通过TimedEventQueue::postTimedEvent(),按照延时的优先顺序把它放到TimedEventQueue的队列之中。然后AwesomePlayer就不管了。TimedEventQueue start之后，自己内部的线程会从队列中依次取出这些事件，然后通过event-&gt;fire回调事件的功能函数。这样就达到了AwesomePlayer的目的。 &#160; &#160; &#160; &#160;4. 我们举个栗子，比如prepareAsync过程，我们看看AwesomePlayer的prepareAsync过程：123456789101112131415161718192021222324status_t AwesomePlayer::prepareAsync() &#123; ...... return prepareAsync_l();&#125;status_t AwesomePlayer::prepareAsync_l() &#123; if (mFlags &amp; PREPARING) &#123; return UNKNOWN_ERROR; // async prepare already pending &#125; if (!mQueueStarted) &#123; mQueue.start(); mQueueStarted = true; &#125; modifyFlags(PREPARING, SET); //在这里new了一个AwesomeEvent给onPrepareAsyncEvent事件回调 mAsyncPrepareEvent = new AwesomeEvent( this, &amp;AwesomePlayer::onPrepareAsyncEvent); mQueue.postEvent(mAsyncPrepareEvent); return OK;&#125; &#160; &#160; &#160; &#160;他并没有实际的调用onPrepareAsyncEvent（）真正的功能函数，他只是把mQueue start之后，然后创建个mAsyncPrepareEvent事件，把它插入到mQueue之中就不管了，具体调用是由mQueue中的threadEntry线程来做。 结语&#160; &#160; &#160; &#160;本节我们只是做一个铺垫，在分析AwesomePlayer基本框架及播放流程之前的预备知识。这里我们依然止步于AwesomePlayer的setDataSource（这个setDataSource都可以贯穿这么篇幅了，果然不简单），所以下一节将仔细分析这些过程。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android多媒体开发(二)----MediaPlayer的C/S架构以及C++层调用步骤]]></title>
    <url>%2F2016%2F11%2F30%2FAndroid%E5%A4%9A%E5%AA%92%E4%BD%93%E5%BC%80%E5%8F%91-%E4%BA%8C-MediaPlayer%E7%9A%84C-S%E6%9E%B6%E6%9E%84%E4%BB%A5%E5%8F%8AC-%E5%B1%82%E8%B0%83%E7%94%A8%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;上一节主要分析了MediaPlayer从java层到jni层做的一些工作，并且setDataSource和后续流程还没有往下分析。这一节先介绍MediaPlayer的C/S架构，然后顺着架构往下深究，我们的思路会更清晰。 MediaPlayer的C/S架构&#160; &#160; &#160; &#160;整个MediaPlayer在运行的时候，可以分成Client和Server两个部分，它们分别在两个进程中运行，它们之间使用Binder机制实现IPC通信。架构图如下，我的viso用的不太好，但是还是能看清。 &#160; &#160; &#160; &#160;1）如果从功能从上往下看，最上层是java层MediaPlayer的API，然后是jni层到C++层之间的IPC通信，最下边就是player的具体实现了（如StageFrightPlayer、MstarPlayer）。&#160; &#160; &#160; &#160;2）C++层是比较重要的环节，这一块也是C/S架构的核心。主要围绕C++层MediaPlayer通过BpMediaPlayerService这个本地proxy对象，经过IPC与远程服务MediaPlayerService(BnMediaPlayerService)通信，完成C/S架构。（其实Android有很多模块设计都是C/S架构，都是通过BpXXX这个代理皮包和拥有干货的BnXXX通信。这里的p指的是proxy，n就是native）&#160; &#160; &#160; &#160;3）当Server端收到Client端的请求，MediaPlayerService会为每一个Client进程创建一个会话，这里就是new一个MediaPlayerService:Client对象和其交互。然后这个对象再根据Client端请求的资源类型去判断创建什么类型的Player，就是最下边那些。（其实这些Player有些是芯片商自己做的，每家做的都不一样） C++层MediaPlayer实现&#160; &#160; &#160; &#160;上一节我们只是分析了从java层到jni层的一些步骤，应该算是准备工作，正好我们入门。这次我们从jni层入手，应该位于架构图中客户端进程部分，C++层MediaPlayer往上，jni层往下的部分。继续往下，还得从setDataSource入手。 setDataSource C++实现&#160; &#160; &#160; &#160;通过JNI方式调用到framework层 android_media_MediaPlayer.cpp(\frameworks\base\media\jni\android_media_MediaPlayer.cpp)，继而调用mediaplayer.cpp(frameworks\av\media\libmedia\MediaPlayer.cpp)。我们找到setDataSource方法，上次我们分析的而是获取文件描述符的重载方法：123456789101112131415161718status_t MediaPlayer::setDataSource(int fd, int64_t offset, int64_t length)&#123; ALOGV("setDataSource(%d, %" PRId64 ", %" PRId64 ")", fd, offset, length); status_t err = UNKNOWN_ERROR; //获取BpMediaPlayerService这个代理MediaPlayerService的proxy对象 const sp&lt;IMediaPlayerService&gt;&amp; service(getMediaPlayerService()); if (service != 0) &#123; //从service manager中获得MediaPlayerService 服务，然后通过服务来创建player,这个player就是架构图最下层那些拥有真正干活本领的player sp&lt;IMediaPlayer&gt; player(service-&gt;create(this, mAudioSessionId)); if ((NO_ERROR != doSetRetransmitEndpoint(player)) || //会走到这里，调用那些能够干活的player的setDataSource方法 (NO_ERROR != player-&gt;setDataSource(fd, offset, length))) &#123; player.clear(); &#125; err = attachNewPlayer(player); &#125; return err;&#125; &#160; &#160; &#160; &#160;在整个应用程序的进程中，Mediaplayer.cpp 中 setDataSource会从service manager中获得MediaPlayerService 服务，然后通过服务来创建player,这个player就是播放器的真实实例。 &#160; &#160; &#160; &#160;分工步骤如下： 通过Binder获取远程服务&#160; &#160; &#160; &#160;通过 getMediaPlayerService 得到的service其实是 BpMediaPlayerService，这是和MediaPlayerService进程中的BnMediaPlayerService 相对应负责binder通讯。BpMediaPlayerService中的create其实通过binder机制将CREATE消息发送出去。位于framework/av/media/libmedia/IMediaPlayerService.cpp：1234567891011virtual sp&lt;IMediaPlayer&gt; create( const sp&lt;IMediaPlayerClient&gt;&amp; client, int audioSessionId) &#123; //发送数据data，相应数据reply Parcel data, reply; data.writeInterfaceToken(IMediaPlayerService::getInterfaceDescriptor()); data.writeStrongBinder(client-&gt;asBinder()); data.writeInt32(audioSessionId); //通过binder将CTEATE消息发送出去 remote()-&gt;transact(CREATE, data, &amp;reply); return interface_cast&lt;IMediaPlayer&gt;(reply.readStrongBinder()); &#125; &#160; &#160; &#160; &#160;在对面的BnMediaPlayerService中，通过onTransact()接受这些消息。并把结果返回。同样位于framework/av/media/libmedia/IMediaPlayerService.cpp：12345678910111213141516171819status_t BnMediaPlayerService::onTransact( uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags)&#123; switch (code) &#123; case CREATE: &#123; CHECK_INTERFACE(IMediaPlayerService, data, reply); sp&lt;IMediaPlayerClient&gt; client = interface_cast&lt;IMediaPlayerClient&gt;(data.readStrongBinder()); int audioSessionId = data.readInt32(); //BnMediaPlayerService的子类是MediaPlayerService，因此会调用MediaPlayerService的create方法 sp&lt;IMediaPlayer&gt; player = create(client, audioSessionId); //将create创建的player回执给客户端 reply-&gt;writeStrongBinder(player-&gt;asBinder()); return NO_ERROR; &#125; break; case DECODE_URL: ...... &#125;&#125; 服务端创建会话&#160; &#160; &#160; &#160;当发现是CREATE才真正调用了MediaPlayerService 中的create函数，位于framework/av/media/libmediaplayerservice/MediaPlayerService.cpp中：1234567891011121314151617181920sp&lt;IMediaPlayer&gt; MediaPlayerService::create(const sp&lt;IMediaPlayerClient&gt;&amp; client, int audioSessionId)&#123; pid_t pid = IPCThreadState::self()-&gt;getCallingPid(); int32_t connId = android_atomic_inc(&amp;mNextConnId); //在create函数中其实是创建了一个MediaPlayerService::Client的实例，也就是 //说MediaPlayerService会为每个client应用进程创建一个相应的MediaPlayerService::Client的实例，来提供服务 sp&lt;Client&gt; c = new Client( this, pid, connId, client, audioSessionId, IPCThreadState::self()-&gt;getCallingUid()); ALOGV("Create new client(%d) from pid %d, uid %d, ", connId, pid, IPCThreadState::self()-&gt;getCallingUid()); wp&lt;Client&gt; w = c; &#123; Mutex::Autolock lock(mLock); mClients.add(w); &#125; return c; &#160; &#160; &#160; &#160;在create函数中其实是创建了一个MediaPlayerService::Client的实例，也就是 说MediaPlayerService会为每个client应用进程创建一个相应的MediaPlayerService::Client的实例，来提供服务。&#160; &#160; &#160; &#160;这样Mediaplayer.cpp就得到了一个player的实例，对他来说这个实例和本地的其他类的实例没什么用法上的区别，殊不知其实这个实例是运行在另外一个进程中。实现这种假象的就是binder机制。&#160; &#160; &#160; &#160;如果对Binder机制不是很了解，可以看看这篇文章。 回执后setDataSource流程&#160; &#160; &#160; &#160;获得这个实例后继续player-&gt;setDataSource().在MediaPlayerService的进程中他的实际函数中，才会真正的创建框架图最下面那些具有干活能力的播放器具体实例。&#160; &#160; &#160; &#160;我们继续查看MediaPlayerService::Client::setDataSource方法：123456789101112131415161718status_t MediaPlayerService::Client::setDataSource(int fd, int64_t offset, int64_t length)&#123; ...... //获取播放器类型 player_type playerType = MediaPlayerFactory::getPlayerType(this, fd, offset, length); //这里会根据上面选择的播放器类型去创建相应的播放器 sp&lt;MediaPlayerBase&gt; p = setDataSource_pre(playerType); if (p == NULL) &#123; return NO_INIT; &#125; // now set data source //最后在这里在将数据源设置给播放器 setDataSource_post(p, p-&gt;setDataSource(fd, offset, length)); return mStatus; &#160; &#160; &#160; &#160;这也也要分步骤执行，我们一步一步去看，然后才能弄清楚每一步的职责。&#160; &#160; &#160; &#160;1）先是获取播放器的类型，我们进入MediaPlayerFactory类看看getPlayerType方法，位于framework/av/media/libmediaplayerservice/MediaPlayerFactory.cpp中：12345678910111213141516171819202122232425262728293031323334player_type MediaPlayerFactory::getPlayerType(const sp&lt;IMediaPlayer&gt;&amp; client, int fd, int64_t offset, int64_t length) &#123; //这是一个宏，我们看看它的宏定义 GET_PLAYER_TYPE_IMPL(client, fd, offset, length);&#125;//注册不同播放器的mapMediaPlayerFactory::tFactoryMap MediaPlayerFactory::sFactoryMap;//这就是它的宏定义#define GET_PLAYER_TYPE_IMPL(a...) \ Mutex::Autolock lock_(&amp;sLock); \ \ player_type ret = STAGEFRIGHT_PLAYER; \ float bestScore = 0.0; \ \ for (size_t i = 0; i &lt; sFactoryMap.size(); ++i) &#123; \ \ IFactory* v = sFactoryMap.valueAt(i); \ float thisScore; \ CHECK(v != NULL); \ thisScore = v-&gt;scoreFactory(a, bestScore); \ if (thisScore &gt; bestScore) &#123; \ ret = sFactoryMap.keyAt(i); \ bestScore = thisScore; \ &#125; \ &#125; \ \ if (0.0 == bestScore) &#123; \ ret = getDefaultPlayerType(); \ &#125; \ \ return ret; &#160; &#160; &#160; &#160;这里定义一个宏来选择播放器类型。里面的for循环也很简单，就是查询注册进来的播放器map中的score得分，谁的得分最高，就返回这个播放器。&#160; &#160; &#160; &#160;这里默认返回的是StageFright，舞台恐惧者，Google总喜欢用一些稀奇古怪的名字命名它的模块组件。 &#160; &#160; &#160; &#160;我看可以查看MediaPlayerFactory.cpp这个文件，这个文件里面有许多种类型的播放器工厂，可以创建出不同的播放器，如StagefrightPlayer、NuPlayerDriver、MidiFile等等。如果厂商要定制自己的播放器，就可以在这里做文章。比如Mstar或者海思自己的播放器，MstarPlayer、HisiPlayer。创建自己的工厂类，实现createPlayer方法，然后修改scoreFactory返回得分值为一个较大的数值，再注册进sFactoryMap变量。或者直接修改GET_PLAYER_TYPE_IMPL宏和getDefaultPlayerType方法的规则。 &#160; &#160; &#160; &#160;这里我们就选取默认值STAGEFRIGHT_PLAYER，位于framework/av/media/libmediaplayerservice/MediaPlayerFactory.h：12345678910enum player_type &#123; PV_PLAYER = 1, SONIVOX_PLAYER = 2, STAGEFRIGHT_PLAYER = 3,//这个是默认值 NU_PLAYER = 4, // Test players are available only in the 'test' and 'eng' builds. // The shared library with the test player is passed passed as an // argument to the 'test:' url in the setDataSource call. TEST_PLAYER = 5,&#125;; &#160; &#160; &#160; &#160;2）根据上述选取的STAGEFRIGHT_PLAYER 创建播放器。回到上面继续查看setDataSource_pre方法：1234567891011121314151617181920212223242526272829303132333435363738394041sp&lt;MediaPlayerBase&gt; MediaPlayerService::Client::setDataSource_pre( player_type playerType)&#123; ALOGV("player type = %d", playerType); // create the right type of player //调用createPlayer方法，这一步继续跟进 sp&lt;MediaPlayerBase&gt; p = createPlayer(playerType); if (p == NULL) &#123; return p; &#125; //设置音频输出 if (!p-&gt;hardwareOutput()) &#123; mAudioOutput = new AudioOutput(mAudioSessionId, IPCThreadState::self()-&gt;getCallingUid(), mPid, mAudioAttributes); static_cast&lt;MediaPlayerInterface*&gt;(p.get())-&gt;setAudioSink(mAudioOutput); &#125; return p;&#125;sp&lt;MediaPlayerBase&gt; MediaPlayerService::Client::createPlayer(player_type playerType)&#123; // determine if we have the right player type //确定正确的播放器类型，如果不对，则删除旧的 sp&lt;MediaPlayerBase&gt; p = mPlayer; if ((p != NULL) &amp;&amp; (p-&gt;playerType() != playerType)) &#123; ALOGV("delete player"); p.clear(); &#125; if (p == NULL) &#123; //根据工厂类创建播放器实例 p = MediaPlayerFactory::createPlayer(playerType, this, notify); &#125; if (p != NULL) &#123; p-&gt;setUID(mUID); &#125; return p;&#125; &#160; &#160; &#160; &#160;最后会调用MediaPlayerFactory::createPlayer静态方法，我们进去看看怎么实现：123456789101112131415161718192021222324252627282930313233343536sp&lt;MediaPlayerBase&gt; MediaPlayerFactory::createPlayer( player_type playerType, void* cookie, notify_callback_f notifyFunc) &#123; sp&lt;MediaPlayerBase&gt; p; IFactory* factory; status_t init_result; Mutex::Autolock lock_(&amp;sLock); //查询检测 if (sFactoryMap.indexOfKey(playerType) &lt; 0) &#123; ALOGE("Failed to create player object of type %d, no registered" " factory", playerType); return p; &#125; //找出播放器工厂类 factory = sFactoryMap.valueFor(playerType); CHECK(NULL != factory); //这句才是创建播放器 p = factory-&gt;createPlayer(); if (p == NULL) &#123; ALOGE("Failed to create player object of type %d, create failed", playerType); return p; &#125; //通知回调 init_result = p-&gt;initCheck(); if (init_result == NO_ERROR) &#123; p-&gt;setNotifyCallback(cookie, notifyFunc); &#125; else &#123; ALOGE("Failed to create player object of type %d, initCheck failed" " (res = %d)", playerType, init_result); p.clear(); &#125; return p; &#160; &#160; &#160; &#160;因为我们的播放器类型为STAGEFRIGHT_PLAYER ，所以对应的工厂类为StagefrightPlayerFactory，继续进入MediaPlayerFactory.cpp中查看：123456789101112131415161718192021222324252627282930class StagefrightPlayerFactory : public MediaPlayerFactory::IFactory &#123; public: virtual float scoreFactory(const sp&lt;IMediaPlayer&gt;&amp; /*client*/, int fd, int64_t offset, int64_t /*length*/, float /*curScore*/) &#123; if (getDefaultPlayerType() == STAGEFRIGHT_PLAYER) &#123; char buf[20]; lseek(fd, offset, SEEK_SET); read(fd, buf, sizeof(buf)); lseek(fd, offset, SEEK_SET); uint32_t ident = *((uint32_t*)buf); // Ogg vorbis? if (ident == 0x5367674f) // 'OggS' return 1.0; &#125; return 0.0; &#125; virtual sp&lt;MediaPlayerBase&gt; createPlayer() &#123; ALOGV(" create StagefrightPlayer"); return new StagefrightPlayer(); &#125;&#125;; &#160; &#160; &#160; &#160;这里他就创建了StagefrightPlayer这个播放器对象了。在这里已经看不到openCore的影子了，android高版本已经舍弃了openCore了。 &#160; &#160; &#160; &#160;3）最后一部就是用上面创建的StagefrightPlayer对象设置数据源了。这一步下一节讲Stagefright框架时会讲的。 StageFright 与openCore&#160; &#160; &#160; &#160;Android froyo版本多媒体引擎做了变动，新添加了stagefright框架，并且默认情况android选择stagefright，并没有完全抛弃opencore，主要是做了一个OMX层，仅仅是对 opencore的omx-component部分做了引用。stagefright是在MediaPlayerService这一层加入的，和opencore是并列的。Stagefright在 Android中是以shared library的形式存在(libstagefright.so)，其中的module – AwesomePlayer可用来播放video/audio。 AwesomePlayer提供许多API，可以让上层的应用程序(Java/JNI)来调用。 小结&#160; &#160; &#160; &#160;以上就是本节分析的内容，承接上一节jni层setDataSource，一直到本届C++层setDataSource。步骤如下： jni层调用C++层MediaPlayer的setDataSource； 通过IPC获取MediaPlayerService服务，为客户端进程分配一个回话； 服务端setDataSource过程中创建指定类型的播放器； 播放器设置setDataSource。 &#160; &#160; &#160; &#160;本届内容就这么多，依然止于setDataSource，不过是StageFright的setDataSource，我们在这里做个标记。&#160; &#160; &#160; &#160;下一节我们将详细分析StageFright框架。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android多媒体开发(一)----MediaPlayer框架开始]]></title>
    <url>%2F2016%2F11%2F28%2FAndroid%E5%A4%9A%E5%AA%92%E4%BD%93%E5%BC%80%E5%8F%91-%E4%B8%80-MediaPlayer%E6%A1%86%E6%9E%B6%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;The Android multimedia framework includes support for playing variety of common mediatypes, so that you can easily integrate audio, video and images into your applications. You can play audio or video from media files stored in your application’s resources (raw resources), from standalone files in the filesystem, or from a data stream arriving over a network connection, all using MediaPlayer APIs. 前言(忽略)&#160; &#160; &#160; &#160;做多媒体开发都离不开MediaPlayer这个类，现在android市场已经趋于饱和，在API调用方面人人基本上已是轻车熟路。&#160; &#160; &#160; &#160;这里不详细赘述MediaPlayer的使用，毕竟网上例子一搜一大堆，逛网也给出了详细介绍和用法。&#160; &#160; &#160; &#160;官网介绍：https://developer.android.com/reference/android/media/MediaPlayer.html&#160; &#160; &#160; &#160;官网用法：https://developer.android.com/guide/topics/media/mediaplayer.html (这里官网已经把ExoPlayer贴在导航页了，关于ExoPlayer可以点这里看Github) &#160; &#160; &#160; &#160;一个很简单的demo如下：12345MediaPlayer mediaPlayer = new MediaPlayer(); mediaPlayer.setDataSource(path); mediaPlayer.setDisplay(surfaceView.getHolder()); mediaPlayer.prepare(); mediaPlayer.start(); &#160; &#160; &#160; &#160;我们就从这个很简单的例子着手，一步一步分析整个播放流程。 mediaserver 启动&#160; &#160; &#160; &#160;我们知道，Android系统是基于Linux内核的，而在Linux系统中，所有的进程都是init进程的子孙进程，也就是说，所有的进程都是直接或者间接地由init进程fork出来的。Zygote进程也不例外，它是在系统启动的过程，由init进程创建的。 &#160; &#160; &#160; &#160;在系统启动脚本system/core/rootdir/init.rc文件中，我们可以看到启动mediaserver进程的脚本命令：12345service media /system/bin/mediaserver class main user media group audio camera inet net_bt net_bt_admin net_bw_acct drmrpc mediadrm ioprio rt 4 &#160; &#160; &#160; &#160;mediaserver启动后会把media相关一些服务添加到servicemanager中，其中就有MediaPlayerService。这样应用启动前，系统就有了MediaPlayerService这个服务程序。位于frameworks/av/media/mediaserver/Main_mediaserver.cpp的main函数中：1234567891011121314int main(int argc, char** argv) &#123; ...... sp&lt;ProcessState&gt; proc(ProcessState::self()); sp&lt;IServiceManager&gt; sm = defaultServiceManager(); ALOGI("ServiceManager: %p", sm.get()); AudioFlinger::instantiate(); //初始化MediaPlayerService MediaPlayerService::instantiate(); CameraService::instantiate(); AudioPolicyService::instantiate(); ProcessState::self()-&gt;startThreadPool(); IPCThreadState::self()-&gt;joinThreadPool(); &#125; &#160; &#160; &#160; &#160;然后我们看看MediaPlayerService的初始化，位于frameworks/av/media/libmediaplayerservice/MediaPlayerService.cpp中：1234void MediaPlayerService::instantiate() &#123; defaultServiceManager()-&gt;addService( String16("media.player"), new MediaPlayerService()); &#125; &#160; &#160; &#160; &#160;MediaPlayerService 初始化函数，向ServiceManager注册了一个实名Binder(media.player)，可以dumpsys -l查看启动了哪些实名service。 MediaPlayer 创建&#160; &#160; &#160; &#160;接着我们进入应用层，看看MediaPlayer创建时做了那些事情。 MediaPlayer构造&#160; &#160; &#160; &#160;应用层MediaPlayer mediaPlayer = new MediaPlayer()。123456789101112131415161718 public MediaPlayer() &#123;//这里会构建一个EventHandler的Handler对象，用于处理一些消息回调 Looper looper; if ((looper = Looper.myLooper()) != null) &#123; mEventHandler = new EventHandler(this, looper); &#125; else if ((looper = Looper.getMainLooper()) != null) &#123; mEventHandler = new EventHandler(this, looper); &#125; else &#123; mEventHandler = null; &#125;...... /* Native setup requires a weak reference to our object. * It's easier to create it here than in C++. */ //这个才是重点，jni层创建MediaPlayer，将java层弱引用传递给jni层。 native_setup(new WeakReference&lt;MediaPlayer&gt;(this)); &#125; &#160; &#160; &#160; &#160;构造之前MediaPlayer类有一段静态代码块，加载了media_jni.so库，用于初始jni相关，早于构造方法，在加载类时就执行。一般是全局性的数据，变量，可以放在这。frameworks\base\media\java\android\media\MediaPlayer.java。12345 static &#123; System.loadLibrary("media_jni"); native_init(); &#125;private static native final void native_init(); &#160; &#160; &#160; &#160;这里会先加载libmedia_jni的so库，需要在这里做个标记，下一节会介绍。 &#160; &#160; &#160; &#160;调用本地方法native_init，我们找到它的jni实现，位于frameworks/base/media/jni/android_media_MediaPlayer.cpp中：12345678910111213141516171819202122232425262728293031// This function gets some field IDs, which in turn causes class initialization.// It is called from a static block in MediaPlayer, which won't run until the// first time an instance of this class is used.static void android_media_MediaPlayer_native_init(JNIEnv *env)&#123; jclass clazz; //通过native层调用java层，获取MediaPlayer类 clazz = env-&gt;FindClass("android/media/MediaPlayer"); if (clazz == NULL) &#123; return; &#125; //获取java层mNativeContext变量，是个long型变量，JNI调用经常这么搞，把jni返回结果通过强转为java层long型变量，供上层保存调用 fields.context = env-&gt;GetFieldID(clazz, "mNativeContext", "J"); if (fields.context == NULL) &#123; return; &#125; //找到java层postEventFromNative方法 fields.post_event = env-&gt;GetStaticMethodID(clazz, "postEventFromNative", "(Ljava/lang/Object;IIILjava/lang/Object;)V"); if (fields.post_event == NULL) &#123; return; &#125; //找到java层mNativeSurfaceTexture变量 fields.surface_texture = env-&gt;GetFieldID(clazz, "mNativeSurfaceTexture", "J"); if (fields.surface_texture == NULL) &#123; return; &#125; ......&#125; &#160; &#160; &#160; &#160;这里我们可以看到jni层设置了java层的postEventFromNative方法，从字面意思可以看出就是被native层调用，通过这样反向调用，仅被使用EventHandler post事件回到主线程中。post开头都是post到主线程，用软引用指向java层的MediaPlayer，以便native代码是线程安全的。postEventFromNative实现如下，不难理解：12345678910111213141516 private static void postEventFromNative(Object mediaplayer_ref, int what, int arg1, int arg2, Object obj) &#123; //这就是传入到native层java层MediaPlayer弱引用 MediaPlayer mp = (MediaPlayer)((WeakReference)mediaplayer_ref).get(); if (mp == null) &#123; return; &#125;...... //用EventHandler发送native的消息 if (mp.mEventHandler != null) &#123; Message m = mp.mEventHandler.obtainMessage(what, arg1, arg2, obj); mp.mEventHandler.sendMessage(m); &#125; &#125; &#160; &#160; &#160; &#160;这里native_init的准备工作就做完了，然后就是jni层的native_setup方法的执行了。对应本地方法依然位于frameworks/base/media/jni/android_media_MediaPlayer.cpp中：12345678910111213141516171819static void android_media_MediaPlayer_native_setup(JNIEnv *env, jobject thiz, jobject weak_this)&#123; ALOGV("native_setup"); //创建一个C++层的MediaPlayer sp&lt;MediaPlayer&gt; mp = new MediaPlayer(); if (mp == NULL) &#123; jniThrowException(env, "java/lang/RuntimeException", "Out of memory"); return; &#125; // create new listener and give it to MediaPlayer //创建一个listener给MediaPlayer，以便java层MediaPlayer设置一些监听能产生回调，如setPrepareListener、setOnCompleteListener等等。 sp&lt;JNIMediaPlayerListener&gt; listener = new JNIMediaPlayerListener(env, thiz, weak_this); mp-&gt;setListener(listener); // Stow our new C++ MediaPlayer in an opaque field in the Java object. //C++层的MediaPlayer对于java层的是不透明的，大家互不关心 setMediaPlayer(env, thiz, mp);&#125; &#160; &#160; &#160; &#160;这里创建了一个C++层的MediaPlayer，还有一些Listener回调。这个模式和Android的Looper差不多，也是java层一个Looper，C++层也有一个Looper。关于Android消息机制原理，可以看看这里。 setDataSource过程&#160; &#160; &#160; &#160;构造完MediaPlayer之后，就要设置数据源了。setDataSource有许多重载方法，我们这里就挑一个文件类型处理的方法，也方便分析。文件类型数据源上层java代码会调用本地方法，还是位于frameworks/base/media/jni/android_media_MediaPlayer.cpp中。不过这次根据名字找不到对应方法了，因为它用如下一个结构体数组做了明值映射：（这个也是jni层常用的技巧，如果看过Log系统源码的应该都了解，毕竟学习jni都是从Log系统开始的）12345678910111213static JNINativeMethod gMethods[] = &#123; &#123; "nativeSetDataSource", "(Landroid/os/IBinder;Ljava/lang/String;[Ljava/lang/String;" "[Ljava/lang/String;)V", (void *)android_media_MediaPlayer_setDataSourceAndHeaders &#125;, &#123;"_setDataSource", "(Ljava/io/FileDescriptor;JJ)V", (void *)android_media_MediaPlayer_setDataSourceFD&#125;, //省略许多方法映射 ......&#125;; &#160; &#160; &#160; &#160;这个结构体数组几乎映射了所有MediaPlayer方法处理，我们这里只关心setDataSource，所以照上面找到文件中的android_media_MediaPlayer_setDataSourceFD方法：12345678910111213141516171819static void android_media_MediaPlayer_setDataSourceFD(JNIEnv *env, jobject thiz, jobject fileDescriptor, jlong offset, jlong length)&#123; //获取C++层的MediaPlayer sp&lt;MediaPlayer&gt; mp = getMediaPlayer(env, thiz); if (mp == NULL ) &#123; jniThrowException(env, "java/lang/IllegalStateException", NULL); return; &#125; if (fileDescriptor == NULL) &#123; jniThrowException(env, "java/lang/IllegalArgumentException", NULL); return; &#125; //获取数据源的文件描述符 int fd = jniGetFDFromFileDescriptor(env, fileDescriptor); ALOGV("setDataSourceFD: fd %d", fd); //这一步其实重点在mp-&gt;setDataSource(fd, offset, length) process_media_player_call( env, thiz, mp-&gt;setDataSource(fd, offset, length), "java/io/IOException", "setDataSourceFD failed." );&#125; &#160; &#160; &#160; &#160;这一步先获取C++层的MediaPlayer，然后获取数据源文件描述符，最后调用process_media_player_call检查返回状态，在参数里已经让C++层的MediaPlayer调用了setDataSource方法。我们先看看process_media_player_call的实现：123456789101112131415161718192021222324252627282930// If exception is NULL and opStatus is not OK, this method sends an error// event to the client application; otherwise, if exception is not NULL and// opStatus is not OK, this method throws the given exception to the client// application.static void process_media_player_call(JNIEnv *env, jobject thiz, status_t opStatus, const char* exception, const char *message)&#123; if (exception == NULL) &#123; // Don't throw exception. Instead, send an event. //如果setDataSource过程返回状态不OK，则notify MEDIA_ERROR状态 if (opStatus != (status_t) OK) &#123; sp&lt;MediaPlayer&gt; mp = getMediaPlayer(env, thiz); if (mp != 0) mp-&gt;notify(MEDIA_ERROR, opStatus, 0); &#125; &#125; else &#123; // Throw exception! if ( opStatus == (status_t) INVALID_OPERATION ) &#123;//不合法操作 jniThrowException(env, "java/lang/IllegalStateException", NULL); &#125; else if ( opStatus == (status_t) PERMISSION_DENIED ) &#123;//权限拒绝 jniThrowException(env, "java/lang/SecurityException", NULL); &#125; else if ( opStatus != (status_t) OK ) &#123; if (strlen(message) &gt; 230) &#123; // if the message is too long, don't bother displaying the status code jniThrowException( env, exception, message); &#125; else &#123; char msg[256]; // append the status code to the message sprintf(msg, "%s: status=0x%X", message, opStatus); jniThrowException( env, exception, msg); &#125; &#125; &#125;&#125; &#160; &#160; &#160; &#160;看代码和注释能看出process_media_player_call主要是做错误和异常检测工作，然后notify出去相应错误状态。 &#160; &#160; &#160; &#160;接着就是调用C++层MediaPlayer的setData方法，这个我们下一节详细分析，本节我们知道setDataSource最后调用了C++层MediaPlayer的setData方法。 setDisplay过程&#160; &#160; &#160; &#160;下一步就是java层的setDisplay，依然查看java层MediaPlayer：123456789101112 public void setDisplay(SurfaceHolder sh) &#123; mSurfaceHolder = sh;//保存SurfaceHolder Surface surface; if (sh != null) &#123; surface = sh.getSurface(); &#125; else &#123; surface = null; &#125; _setVideoSurface(surface);//给视频设置surface updateSurfaceScreenOn();//更新surface到屏幕上 &#125;private native void _setVideoSurface(Surface surface); &#160; &#160; &#160; &#160;最后会调用本地方法_setVideoSurface，我们继续找到它的jni实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061static void android_media_MediaPlayer_setVideoSurface(JNIEnv *env, jobject thiz, jobject jsurface)&#123; setVideoSurface(env, thiz, jsurface, true /* mediaPlayerMustBeAlive */);&#125;static void setVideoSurface(JNIEnv *env, jobject thiz, jobject jsurface, jboolean mediaPlayerMustBeAlive)&#123; sp&lt;MediaPlayer&gt; mp = getMediaPlayer(env, thiz);//获取C++的MediaPlayer if (mp == NULL) &#123; if (mediaPlayerMustBeAlive) &#123; jniThrowException(env, "java/lang/IllegalStateException", NULL); &#125; return; &#125; //将旧的IGraphicBufferProducer的强引用减一 decVideoSurfaceRef(env, thiz); //IGraphicBufferProducer图层缓冲区合成器 sp&lt;IGraphicBufferProducer&gt; new_st; if (jsurface) &#123; //得到java层的surface sp&lt;Surface&gt; surface(android_view_Surface_getSurface(env, jsurface)); if (surface != NULL) &#123; //获取IGraphicBufferProducer new_st = surface-&gt;getIGraphicBufferProducer(); if (new_st == NULL) &#123; jniThrowException(env, "java/lang/IllegalArgumentException", "The surface does not have a binding SurfaceTexture!"); return; &#125; //增加IGraphicBufferProducer的强引用+1 new_st-&gt;incStrong((void*)decVideoSurfaceRef); &#125; else &#123; jniThrowException(env, "java/lang/IllegalArgumentException", "The surface has been released"); return; &#125; &#125; //上面我们在native_init方法中将java层mNativeSurfaceTexture查找给了jni层，正好，在这里将IGraphicBufferProducer赋给它 env-&gt;SetLongField(thiz, fields.surface_texture, (jlong)new_st.get()); // This will fail if the media player has not been initialized yet. This // can be the case if setDisplay() on MediaPlayer.java has been called // before setDataSource(). The redundant call to setVideoSurfaceTexture() // in prepare/prepareAsync covers for this case. //如果MediaPlayer没有初始化，这一步会失败。原因可能是setDisplay在setDataSource之前。如果在prepare/prepareAsync 时想规避这个错误而去调用setVideoSurfaceTexture是多余的。 //最终会调用C++层的setVideoSurfaceTexture方法，下一节在分析 mp-&gt;setVideoSurfaceTexture(new_st);&#125;//将旧的IGraphicBufferProducer的强引用减一static void decVideoSurfaceRef(JNIEnv *env, jobject thiz)&#123; sp&lt;MediaPlayer&gt; mp = getMediaPlayer(env, thiz); if (mp == NULL) &#123; return; &#125; sp&lt;IGraphicBufferProducer&gt; old_st = getVideoSurfaceTexture(env, thiz); if (old_st != NULL) &#123; old_st-&gt;decStrong((void*)decVideoSurfaceRef); &#125;&#125; &#160; &#160; &#160; &#160;这一步主要是对图像显示的surface的保存，然后将旧的IGraphicBufferProducer强引用减一，再获得新的IGraphicBufferProducer，最后会调用C++的MediaPlayer的setVideoSurfaceTexture将它折纸进去。 &#160; &#160; &#160; &#160;IGraphicBufferProducer是SurfaceFlinger的内容，一个UI完全显示到diplay的过程，SurfaceFlinger扮演着重要的角色但是它的职责是“Flinger”，即把系统中所有应用程序的最终的“绘图结果”进行“混合”，然后统一显示到物理屏幕上，而其他方面比如各个程序的绘画过程，就由其他东西来担任了。这个光荣的任务自然而然地落在了BufferQueue的肩膀上，它是每个应用程序“一对一”的辅导老师，指导着UI程序的“画板申请”、“作画流程”等一系列细节。下面的图描述了这三者的关系： &#160; &#160; &#160; &#160;虽说是三者的关系，但是他们所属的层却只有两个，app属于Java层，BufferQueue/SurfaceFlinger属于native层。也就是说BufferQueue也是隶属SurfaceFlinger，所有工作围绕SurfaceFlinger展开。&#160; &#160; &#160; &#160;这里IGraphicBufferProducer就是app和BufferQueue重要桥梁，GraphicBufferProducer承担着单个应用进程中的UI显示需求，与BufferQueue打交道的就是它。 &#160; &#160; &#160; &#160;后面的prepare和start直接和C++层MediaPlayer相关，因此我们再下一节分析，这里就简单到此。 小结&#160; &#160; &#160; &#160;本节只是简单分析上层MediaPlayer调用的一些工作，下一节开始将详细分析后续流程，包括对不同数据源创建不同DataSource，prepare过程中对于系统播放器的选择，底层解码和厂商对OpenMax接口的实现，当然还有整个多媒体框架的C/S架构分析。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA线程池简单分析]]></title>
    <url>%2F2016%2F10%2F31%2FJAVA%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;java 1.5引入了java.util.concurrent包，作者是Doug Lea这位史诗级别的老爷子。今天我们只挑选并发包下线程池的部分内容分析。 核心类&#160; &#160; &#160; &#160;对于线程池的使用开发者都已经得心应手了。如果查看源码，都和ThreadPoolExecutor 这个类相关。 配置规则&#160; &#160; &#160; &#160;ThreadPoolExecutor 提供了四种构造方法实现(这里只介绍一种)：123456789101112131415161718192021public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; &#160; &#160; &#160; &#160;参数大概意思如下： corePoolSize：核心运行的poolSize，也就是当超过这个范围的时候，就需要将新的Runnable放入到等待队列workQueue中了，我们把这些Runnable就叫做要去执行的任务吧。 maximumPoolSize：一般你用不到，当大于了这个值就会将任务由一个丢弃处理机制来处理，但是当你发生：newFixedThreadPool的时候，corePoolSize和maximumPoolSize是一样的，而corePoolSize是先执行的，所以他会先被放入等待队列，而不会执行到下面的丢弃处理中，看了后面的代码你就知道了。 workQueue：等待队列，当达到corePoolSize的时候，就向该等待队列放入线程信息（默认为一个LinkedBlockingQueue），运行中的线程属性为：workers，为一个HashSet；我们的Runnable内部被包装了一层，后面会看到这部分代码；这个队列默认是一个无界队列（你也可以设定一个有界队列），所以在生产者疯狂生产的时候，考虑如何控制的问题。 keepAliveTime：默认都是0，当线程没有任务处理后，保持多长时间，当你使用：newCachedThreadPool()，它将是60s的时间。这个参数在运行中的线程从workQueue获取任务时，当(poolSize &gt;corePoolSize || allowCoreThreadTimeOut)会用到，当然allowCoreThreadTimeOut要设置为true，也会先判定keepAliveTime是大于0的。 threadFactory：是构造Thread的方法，你可以自己去包装和传递，主要实现newThread方法即可。 handler：也就是参数maximumPoolSize达到后丢弃处理的方法，java提供了5种丢弃处理的方法，当然你也可以自己根据实际情况去重写，主要是要实现接口：RejectedExecutionHandler中的方法： public void rejectedExecution(Runnabler, ThreadPoolExecutor e) java默认的是使用：AbortPolicy，他的作用是当出现这中情况的时候会抛出一个异常。 其余的handler还包括：1、CallerRunsPolicy：如果发现线程池还在运行，就直接运行这个线程。2、DiscardOldestPolicy：在线程池的等待队列中，将头取出一个抛弃，然后将当前线程放进去。3、DiscardPolicy：什么也不做。4、AbortPolicy：java默认，抛出一个异常：RejectedExecutionException。你可以自己写一个，例如我们想在这个处理中，既不是完全丢弃，也不是完全启动，也不是抛异常，而是控制生产者的线程，那么你就可以尝试某种方式将生产者的线程blocking住，其实就有点类似提到的Semaphor的功能了。 状态转换&#160; &#160; &#160; &#160;ThreadPoolExecutor 类中将线程状态（ runState）分为了以下五种： RUNNING：可以接受新任务并且处理进入队列中的任务SHUTDOWN：不接受新任务，但是仍然执行队列中的任务STOP：不接受新任务也不执行队列中的任务TIDYING：所有任务中止，队列为空，进入该状态下的任务会执行 terminated()方法TERMINATED： terminated()方法执行完成后进入该状态 &#160; &#160; &#160; &#160;状态之间的转换如下： RUNNING -&gt; SHUTDOWN：调用了 shutdown()方法，可能是在 finalize()方法中被隐式调用 (RUNNING or SHUTDOWN) -&gt; STOP：调用 shutdownNow() SHUTDOWN -&gt; TIDYING：当队列和线程池都为空时 STOP -&gt; TIDYING：线程池为空时 TIDYING -&gt; TERMINATED：terminated()方法执行完成 &#160; &#160; &#160; &#160;这几个变量的定义代码如下：1234567891011//原子变量private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; &#160; &#160; &#160; &#160;上面的原子变量ctl 是整个类的核心，AtomicInteger保证了对这个变量的操作是原子的，通过巧妙的操作，ThreadPoolExecutor用这一个变量保存了两个内容： 所有有效线程的数量（workerCount） 各个线程的状态（runState） &#160; &#160; &#160; &#160;低29位存线程数workerCount，高3位存runState。 &#160; &#160; &#160; &#160;所以将上述六个常量打印出来如下： 00011111111111111111111111111111 —— CAPACITY 11100000000000000000000000000000 —— RUNNING 00000000000000000000000000000000 —— SHUTDOWN 00100000000000000000000000000000 —— STOP 01000000000000000000000000000000 —— TIDYING 01100000000000000000000000000000 —— TERMINATED &#160; &#160; &#160; &#160;第一行代表线程容量，后面5行提取高3位得到： 111 - RUNNING 000 - SHUTDOWN 001 - STOP 010 - TIDYING 011 - TERMINATED &#160; &#160; &#160; &#160;分别对应5种状态，可以看到这样定义之后，只需要通过简单的移位操作就可以进行状态的转换。&#160; &#160; &#160; &#160;围绕ctl变量有一些操作，了解这些方法是看懂后面一些晦涩代码的基础：123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 这个方法用于取出runState的值 因为CAPACITY值为：00011111111111111111111111111111 * ~为按位取反操作，则~CAPACITY值为：11100000000000000000000000000000 * 再同参数做&amp;操作，就将低29位置0了，而高3位还是保持原先的值，也就是runState的值 * * @param c * 该参数为存储runState和workerCount的int值 * @return runState的值 */private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY;&#125;/** * 这个方法用于取出workerCount的值 * 因为CAPACITY值为：00011111111111111111111111111111，所以&amp;操作将参数的高3位置0了 * 保留参数的低29位，也就是workerCount的值 * * @param c * ctl, 存储runState和workerCount的int值 * @return workerCount的值 */private static int workerCountOf(int c) &#123; return c &amp; CAPACITY;&#125;/** * 将runState和workerCount存到同一个int中 * “|”运算的意思是，假设rs的值是101000，wc的值是000111，则他们位或运算的值为101111 * * @param rs * runState移位过后的值，负责填充返回值的高3位 * @param wc * workerCount移位过后的值，负责填充返回值的低29位 * @return 两者或运算过后的值 */private static int ctlOf(int rs, int wc) &#123; return rs | wc;&#125;// 只有RUNNING状态会小于0private static boolean isRunning(int c) &#123; return c &lt; SHUTDOWN;&#125; 重要方法execute&#160; &#160; &#160; &#160;给线程池加入任务的方法为execute方法，该方法策略大概分三步：&#160; &#160; &#160; &#160;1）活动线程小于corePoolSize的时候创建新的线程；&#160; &#160; &#160; &#160;2）活动线程大于corePoolSize时都是先加入到任务队列当中；&#160; &#160; &#160; &#160;3）任务队列满了再去启动新的线程，如果线程数达到最大值就拒绝任务。&#160; &#160; &#160; &#160;代码及注释如下：123456789101112131415161718192021222324252627282930313233public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); // 活动线程数 &lt; corePoolSize if (workerCountOf(c) &lt; corePoolSize) &#123; // 直接启动新的线程。第二个参数true:addWorker中会重新检查workerCount是否小于corePoolSize if (addWorker(command, true)) // 添加成功返回 return; c = ctl.get(); &#125; // 活动线程数 &gt;= corePoolSize // runState为RUNNING &amp;&amp; 队列未满 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // double check // 非RUNNING状态 则从workQueue中移除任务并拒绝 if (!isRunning(recheck) &amp;&amp; remove(command)) reject(command);// 采用线程池指定的策略拒绝任务 // 线程池处于RUNNING状态 || 线程池处于非RUNNING状态但是任务移除失败 else if (workerCountOf(recheck) == 0) // 这行代码是为了SHUTDOWN状态下没有活动线程了，但是队列里还有任务没执行这种特殊情况。 // 添加一个null任务是因为SHUTDOWN状态下，线程池不再接受新任务 addWorker(null, false); // 两种情况： // 1.非RUNNING状态拒绝新的任务 // 2.队列满了启动新的线程失败（workCount &gt; maximumPoolSize） &#125; else if (!addWorker(command, false)) reject(command);&#125; &#160; &#160; &#160; &#160;注释比较清楚了就不再解释了，其中比较难理解的应该是addWorker(null, false);这一行，这要结合addWorker一起来看。 主要目的是防止HUTDOWN状态下没有活动线程了，但是队列里还有任务没执行这种特殊情况。 addWorker&#160; &#160; &#160; &#160;addWorker即创建新线程。检查在当前线程池状态和限制下能否创建一个新线程，如果可以，会相应改变workerCount，每个worker都会运行他们的firstTask。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c);// 当前线程池状态 // Check if queue empty only if necessary. // 这条语句等价：rs &gt;= SHUTDOWN &amp;&amp; (rs != SHUTDOWN || firstTask != null || // workQueue.isEmpty()) // 满足下列调价则直接返回false，线程创建失败: // rs &gt; SHUTDOWN:STOP || TIDYING || TERMINATED 此时不再接受新的任务，且所有任务执行结束 // rs = SHUTDOWN:firtTask != null 此时不再接受任务，但是仍然会执行队列中的任务 // rs = SHUTDOWN:firtTask == null见execute方法的addWorker(null, // false)，任务为null &amp;&amp; 队列为空 // 最后一种情况也就是说SHUTDONW状态下，如果队列不为空还得接着往下执行，为什么？add一个null任务目的到底是什么？ // 看execute方法只有workCount==0的时候firstTask才会为null结合这里的条件就是线程池SHUTDOWN了不再接受新任务 // 但是此时队列不为空，那么还得创建线程把任务给执行完才行。 if (rs &gt;= SHUTDOWN &amp;&amp; !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; !workQueue.isEmpty())) return false; // 走到这的情形： // 1.线程池状态为RUNNING // 2.SHUTDOWN状态，但队列中还有任务需要执行 for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c))// 原子操作递增workCount break retry;// 操作成功跳出的重试的循环 c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs)// 如果线程池的状态发生变化则重试 continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; // wokerCount递增成功 boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; final ReentrantLock mainLock = this.mainLock; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; // 并发的访问线程池workers对象必须加锁 mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int c = ctl.get(); int rs = runStateOf(c); // RUNNING状态 || SHUTDONW状态下清理队列中剩余的任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); // 将新启动的线程添加到线程池中 workers.add(w); // 更新largestPoolSize int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // 启动新添加的线程，这个线程首先执行firstTask，然后不停的从队列中取任务执行 // 当等待keepAlieTime还没有任务执行则该线程结束。见runWoker和getTask方法的代码。 if (workerAdded) &#123; t.start();// 最终执行的是ThreadPoolExecutor的runWoker方法 workerStarted = true; &#125; &#125; &#125; finally &#123; // 线程启动失败，则从wokers中移除w并递减wokerCount if (!workerStarted) // 递减wokerCount会触发tryTerminate方法 addWorkerFailed(w); &#125; return workerStarted; &#125; &#160; &#160; &#160; &#160;内部类Worker是对任务的封装，所有submit的Runnable都被封装成了Worker，它本身也是一个Runnable， 然后利用AQS框架实现了一个简单的非重入的互斥锁， 实现互斥锁主要目的是为了中断的时候判断线程是在空闲还是运行，可以看后面shutdown和shutdownNow方法的分析。Worker中获取和释放锁相关代码如下：123456789101112131415// state只有0和1，互斥protected boolean tryAcquire(int unused) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true;// 成功获得锁 &#125; // 线程进入等待队列 return false;&#125;protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); setState(0); return true;&#125; &#160; &#160; &#160; &#160;之所以不用ReentrantLock是为了避免任务执行的代码中修改线程池的变量，如setCorePoolSize，因为ReentrantLock是可重入的。 runWorker&#160; &#160; &#160; &#160;任务添加成功后实际执行的是runWorker这个方法，这个方法非常重要，简单来说它做的就是：&#160; &#160; &#160; &#160;1）第一次启动会执行初始化传进来的任务firstTask；&#160; &#160; &#160; &#160;2）然后会从workQueue中取任务执行，如果队列为空则等待keepAliveTime这么长时间。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; // Worker的构造函数中抑制了线程中断setState(-1)，所以这里需要unlock从而允许中断 w.unlock(); // 用于标识是否异常终止，finally中processWorkerExit的方法会有不同逻辑 // 为true的情况：1.执行任务抛出异常；2.被中断。 boolean completedAbruptly = true; try &#123; // 如果getTask返回null那么getTask中会将workerCount递减，如果异常了这个递减操作会在processWorkerExit中处理 while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; // 任务执行前可以插入一些处理，子类重载该方法 beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run();// 执行用户任务 &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; // 和beforeExecute一样，留给子类去重载 afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; // 结束线程的一些清理工作 processWorkerExit(w, completedAbruptly); &#125; &#125; getTask&#160; &#160; &#160; &#160;runWorker方法里有一段会调用getTask判断，主要用来取出队列中的任务。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. // 1.rs &gt; SHUTDOWN 所以rs至少等于STOP,这时不再处理队列中的任务 // 2.rs = SHUTDOWN 所以rs&gt;=STOP肯定不成立，这时还需要处理队列中的任务除非队列为空 // 这两种情况都会返回null让runWoker退出while循环也就是当前线程结束了，所以必须要decrement // wokerCount if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; // 递减workerCount值 decrementWorkerCount(); return null; &#125; // 标记从队列中取任务时是否设置超时时间 boolean timed; // Are workers subject to culling? // 1.RUNING状态 // 2.SHUTDOWN状态，但队列中还有任务需要执行 for (;;) &#123; int wc = workerCountOf(c); // 1.core thread允许被超时，那么超过corePoolSize的的线程必定有超时 // 2.allowCoreThreadTimeOut == false &amp;&amp; wc &gt; // corePoolSize时，一般都是这种情况，core thread即使空闲也不会被回收，只要超过的线程才会 timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; // 从addWorker可以看到一般wc不会大于maximumPoolSize，所以更关心后面半句的情形： // 1. timedOut == false 第一次执行循环， 从队列中取出任务不为null方法返回 或者 // poll出异常了重试 // 2.timeOut == true &amp;&amp; timed == // false:看后面的代码workerQueue.poll超时时timeOut才为true， // 并且timed要为false，这两个条件相悖不可能同时成立（既然有超时那么timed肯定为true） // 所以超时不会继续执行而是return null结束线程。（重点：线程是如何超时的？？？） if (wc &lt;= maximumPoolSize &amp;&amp; !(timedOut &amp;&amp; timed)) break; // workerCount递减，结束当前thread if (compareAndDecrementWorkerCount(c)) return null; c = ctl.get(); // Re-read ctl // 需要重新检查线程池状态，因为上述操作过程中线程池可能被SHUTDOWN if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; try &#123; // 1.以指定的超时时间从队列中取任务 // 2.core thread没有超时 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true;// 超时 &#125; catch (InterruptedException retry) &#123; timedOut = false;// 线程被中断重试 &#125; &#125; &#125; processWorkerExit&#160; &#160; &#160; &#160;线程退出会执行这个方法做一些清理工作。1234567891011121314151617181920212223242526272829303132333435363738private void processWorkerExit(Worker w, boolean completedAbruptly) &#123; // 正常的话再runWorker的getTask方法workerCount已经被减一了 if (completedAbruptly) decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 累加线程的completedTasks completedTaskCount += w.completedTasks; // 从线程池中移除超时或者出现异常的线程 workers.remove(w); &#125; finally &#123; mainLock.unlock(); &#125; // 尝试停止线程池 tryTerminate(); int c = ctl.get(); // runState为RUNNING或SHUTDOWN if (runStateLessThan(c, STOP)) &#123; // 线程不是异常结束 if (!completedAbruptly) &#123; // 线程池最小空闲数，允许core thread超时就是0，否则就是corePoolSize int min = allowCoreThreadTimeOut ? 0 : corePoolSize; // 如果min == 0但是队列不为空要保证有1个线程来执行队列中的任务 if (min == 0 &amp;&amp; !workQueue.isEmpty()) min = 1; // 线程池还不为空那就不用担心了 if (workerCountOf(c) &gt;= min) return; // replacement not needed &#125; // 1.线程异常退出 // 2.线程池为空，但是队列中还有任务没执行，看addWoker方法对这种情况的处理 addWorker(null, false); &#125; &#125; tryTerminate&#160; &#160; &#160; &#160;processWorkerExit方法中会尝试调用tryTerminate来终止线程池。这个方法在任何可能导致线程池终止的动作后执行：比如减少wokerCount或SHUTDOWN状态下从队列中移除任务。123456789101112131415161718192021222324252627282930313233343536373839404142434445final void tryTerminate() &#123; for (;;) &#123; int c = ctl.get(); // 以下状态直接返回： // 1.线程池还处于RUNNING状态 // 2.SHUTDOWN状态但是任务队列非空 // 3.runState &gt;= TIDYING 线程池已经停止了或在停止了 if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; !workQueue.isEmpty())) return; // 只能是以下情形会继续下面的逻辑：结束线程池。 // 1.SHUTDOWN状态，这时不再接受新任务而且任务队列也空了 // 2.STOP状态，当调用了shutdownNow方法 // workerCount不为0则还不能停止线程池,而且这时线程都处于空闲等待的状态 // 需要中断让线程“醒”过来，醒过来的线程才能继续处理shutdown的信号。 if (workerCountOf(c) != 0) &#123; // Eligible to terminate // runWoker方法中w.unlock就是为了可以被中断,getTask方法也处理了中断。 // ONLY_ONE:这里只需要中断1个线程去处理shutdown信号就可以了。 interruptIdleWorkers(ONLY_ONE); return; &#125; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 进入TIDYING状态 if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; // 子类重载：一些资源清理工作 terminated(); &#125; finally &#123; // TERMINATED状态 ctl.set(ctlOf(TERMINATED, 0)); // 继续awaitTermination termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125; &#125; shutdown和shutdownNow&#160; &#160; &#160; &#160;shutdown这个方法会将runState置为SHUTDOWN，会终止所有空闲的线程。12345678910111213141516public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); // 线程池状态设为SHUTDOWN，如果已经至少是这个状态那么则直接返回 advanceRunState(SHUTDOWN); // 注意这里是中断所有空闲的线程：runWorker中等待的线程被中断 → 进入processWorkerExit → // tryTerminate方法中会保证队列中剩余的任务得到执行。 interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); &#125; &#160; &#160; &#160; &#160;shutdownNow方法将runState置为STOP。和shutdown方法的区别，这个方法会终止所有的线程。12345678910111213141516171819public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); // STOP状态：不再接受新任务且不再执行队列中的任务。 advanceRunState(STOP); // 中断所有线程 interruptWorkers(); // 返回队列中还没有被执行的任务。 tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks;&#125; &#160; &#160; &#160; &#160;主要区别在于shutdown调用的是interruptIdleWorkers这个方法，而shutdownNow实际调用的是Worker类的interruptIfStarted方法：1234567891011121314151617181920212223242526272829303132333435private void interruptIdleWorkers(boolean onlyOne) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) &#123; Thread t = w.thread; // w.tryLock能获取到锁，说明该线程没有在运行，因为runWorker中执行任务会先lock， // 因此保证了中断的肯定是空闲的线程。 if (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; finally &#123; w.unlock(); &#125; &#125; if (onlyOne) break; &#125; &#125; finally &#123; mainLock.unlock(); &#125;&#125;void interruptIfStarted() &#123; Thread t; // 初始化时state == -1 if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; &#125;&#125; &#160; &#160; &#160; &#160;这就是前面提到的Woker类实现AQS的主要作用。AQS这个类在并发包下的地位还是举足轻重的，大家可以在并发编程网找找相关资料，或者这篇。 结语&#160; &#160; &#160; &#160;因为注释比较多，所以看起来比较无聊。核心方法就那么些，实现细节就是那些double check的部分。还有状态和数量的计算。]]></content>
      <categories>
        <category>JAVA基础</category>
      </categories>
      <tags>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android壁纸开发流程分析]]></title>
    <url>%2F2016%2F10%2F08%2FAndroid%E5%A3%81%E7%BA%B8%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;起因是产品汪的一个奇葩需求，要求播放视频使用背景播放，前台还可以随意操作其他东西，就像PS4那样的。所以不得不研究一下动态壁纸了，中途也是遇到了无数坑，但终于还是做出来了。这里就简单介绍一下Android对壁纸的管理流程。 壁纸设置&#160; &#160; &#160; &#160;Android为我们提供了壁纸服务，让我们的桌面看起来更加绚丽。打开设置我们都会发现，壁纸有动态和静态之分。静态壁纸就是一张图片，可以选择系统预留的也可以设置自己的照片；动态壁纸就比较复杂了，内容在不停地变换，系统也给我们提供了许多种选择。接下来我们先说说这两种壁纸如何设置。 静态壁纸&#160; &#160; &#160; &#160;设置静态壁纸有很多途径，但归根结底都是一下三种方法： 使用WallpaperManager的setResource(int ResourceID)方法 使用WallpaperManager的setBitmap(Bitmap bitmap)方法 使用WallpaperManager的setStream(InputStream data)方法 &#160; &#160; &#160; &#160;举个栗子，就选第一个：123456WallpaperManager wallpaperManager = WallpaperManager.getInstance(this);try &#123; wallpaperManager.setResource(R.drawable.picture);&#125; catch (IOException e) &#123; e.printStackTrace();&#125; &#160; &#160; &#160; &#160;其他两个和第一个一样，只不过将要设为壁纸的图片参数换成了Bitmap和InputStream。然后就是不要忘了加上以下权限：1&lt;uses-permission android:name = "android.permission.SET_WALLPAPER"/&gt; 动态壁纸&#160; &#160; &#160; &#160;首先动态壁纸的动态体现出这个组件是实时变化的，也就是说有一个后台在不停的刷新这个组件。联想到后台组件首先想到的就是service，从代码角度看，果然如此。每一个动态壁纸都继承自WallpaperService，其中必须实现的抽象方法onCreateEngine，返回一个Engine对象，实际上所有的绘图与刷新都是由engine完成，service正是提供engine的部分。&#160; &#160; &#160; &#160;打个比方说，在我们设置一个动态壁纸时有预览，这时启动了一个预览的engine来绘制与刷新，当我们设置了壁纸以后又启动了一个填充整个桌面的engine在实时的绘制与刷新。所以动态壁纸的重点在engine里，有几个重要的方法： public void onCreate(SurfaceHolder surfaceHolder)有了surfaceholder我们可以获得canvas对象，有了canvas我们就可以绘图 public void onOffsetsChanged(float xOffset, float yOffset, float xOffsetStep, float yOffsetStep, int xPixelsOffset, int yPixelsOffset)屏幕滑动的时候触发此方法xOffset可以用来判断屏幕序号 百分比的形式，举例说如果你手机上有5个分屏，第一屏这里是0.000，第二屏是0.2000 第三屏是0.4000以此类推xOffsetStep从字面意义就能理解是步进 同样和你的分屏数有关 如果你的分屏数为5 则每次步进xOffsetStep都是0.20000xPixelsOffset就是实际上像素的移动距离，也就是说移动了多少像素，奇怪的是这里左右移动像素点的offset都为负数 public void onVisibilityChanged(boolean visible)当动态壁纸的可见性发生变化时触发此方法，举例说在桌面上时，动态壁纸的visibility为true，当你运行某个程序的时候动态壁纸的visibility变为false，这里很好理解 public Bundle onCommand(String action, int x, int y, int z, Bundle extras, boolean resultRequested)可以监听点击事件，点击时触发此方法，action为 android.wallpaper.tap，x记录了横坐标，y记录了纵坐标，z的作用未知，可能是为3d桌面预留下的？ 没有试验过，z一般为0 还有一些重要的生命周期方法，和activity类似，就不多说了。 &#160; &#160; &#160; &#160;因为动态壁纸是一个Service，还必须在清单文件中进行一些配置，比如：123456789101112&lt;!-- 配置动态壁纸Service --&gt;&lt;service android:label="@string/app_name" android:name=".LiveWallpaper" android:permission="android.permission.BIND_WALLPAPER"&gt; &lt;!-- 为动态壁纸配置intent-filter --&gt; &lt;intent-filter&gt; &lt;action android:name="android.service.wallpaper.WallpaperService" /&gt; &lt;/intent-filter&gt; &lt;!-- 为动态壁纸配置meta-data --&gt; &lt;meta-data android:name="android.service.wallpaper" android:resource="@xml/livewallpaper" /&gt;&lt;/service&gt; &#160; &#160; &#160; &#160;比较重要的部分首先是权限android:permission=”android.permission.BIND_WALLPAPER”；&#160; &#160; &#160; &#160;其次service需要响应action：android:name=”android.service.wallpaper.WallpaperService；&#160; &#160; &#160; &#160;再就是配置文件：12&lt;meta-data android:name="android.service.wallpaper" android:resource="@xml/livewallpaper" /&gt; 接下来接收配置文件。首先在res文件夹下建立一个xml目录，和写appwidget一样。在目录下我们创建一个xml文件：123456&lt;wallpaper xmlns:android="http://schemas.android.com/apk/res/android" android:settingsActivity="LiveWallPreference" android:thumbnail="@drawable/ic_launcher" android:description="@string/wallpaper_description" /&gt; 其中含义如下： android:settingsActivity=”LiveWallPreference” 指定配置动态壁纸的PreferenceActivity，这个PreferenceActivity同样需要在AndroidManifest.xml中注册，不过和一般的activity一样。当我们点击动态壁纸的设置按钮时，导向这个activity。不可缺少，否则点击设置会报错。 android:thumbnail=”@drawable/ic_launcher” android:description=”@string/wallpaper_description” 第一个图标对应动态壁纸列表中的图标，第二条description则是图标右边你创建的动态壁纸的名字。 &#160; &#160; &#160; &#160;至此动态壁纸的框架就算完成了。接下来只需要在wallpaperservice类中加入刷新机制，加入动态内容，一个动态桌面就完成了。 &#160; &#160; &#160; &#160;这里我贴一个魔方的动态壁纸：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270/* A wallpaper service is responsible for showing a live wallpaper behind * applications that would like to sit on top of it. This service object itself * does very little -- its only purpose is to generate instances of * WallpaperService.Engine as needed. Implementing a wallpaper thus involves * subclassing from this, subclassing an Engine implementation, and implementing * onCreateEngine() to return a new instance of your engine */public class CubeWallpaper1 extends WallpaperService &#123; /*常用的都是这样的，用一个handler来动态的去刷新UI，对吧？猜的，看下面代码到底是不是*/ private final Handler mHandler = new Handler(); /** * 这个方法与Activity里面的一样，当这个类的服务被第一次创建时 * 调用，也就是说，这个方法只调用一次.. */ @Override public void onCreate() &#123; super.onCreate(); &#125; /** * 与上面反的，销毁时调用，这个猜下， * 不懂了查文档 */ @Override public void onDestroy() &#123; super.onDestroy(); &#125; /** * 这个方法在类注释中写明了 * implementing onCreateEngine() to return a new instance of your engine * 必须实现这个方法来返回我们自己定义引擎的一个实例 */ @Override public Engine onCreateEngine() &#123; return new CubeEngine(); &#125; /** * * @Title: CubeWallpaper1.java * @Package cube1 * @Description: 自定义引擎类 */ class CubeEngine extends Engine &#123; private final Paint mPaint = new Paint(); private float mOffset; /*用户触摸位置*/ private float mTouchX = -1; private float mTouchY = -1; private long mStartTime; /*屏幕中心坐标，记下，是中心不是原心（0，0）*/ private float mCenterX; private float mCenterY; private final Runnable mDrawCube = new Runnable() &#123; public void run() &#123; drawFrame(); &#125; &#125;; private boolean mVisible; CubeEngine() &#123; /*下面这几行就为了在屏幕中画立方体的线条而做准备*/ final Paint paint = mPaint; paint.setColor(0xffffffff);//画笔颜色 paint.setAntiAlias(true);//抗锯齿 paint.setStrokeWidth(2);//线条粗细，猜的，不知道对不对 paint.setStrokeCap(Paint.Cap.ROUND); paint.setStyle(Paint.Style.STROKE); //系统启动完之后，开始绘制壁纸的时间，这个时间里面包含有系统睡眠时间 mStartTime = SystemClock.elapsedRealtime(); &#125; /** * 大家发现这个onCreate与Activity的方法有什么不同了吧？ * 老规矩的，还是在初始化壁纸引擎的时候调用这个方法，并设置触 * 屏事件为可用 */ @Override public void onCreate(SurfaceHolder surfaceHolder) &#123; super.onCreate(surfaceHolder); setTouchEventsEnabled(true); &#125; @Override public void onDestroy() &#123; super.onDestroy(); mHandler.removeCallbacks(mDrawCube); &#125; /** * 系统壁纸状态改变时会调用这个方法，如： * 壁纸由隐藏转换为显示状态时会调用这个方法 */ @Override public void onVisibilityChanged(boolean visible) &#123; mVisible = visible; /*下面这个判断好玩，就是说，如果屏幕壁纸状态转为显式时重新绘制壁纸，否则黑屏幕，隐藏就可以*/ if (visible) &#123; drawFrame(); &#125; else &#123; mHandler.removeCallbacks(mDrawCube); &#125; &#125; @Override public void onSurfaceChanged(SurfaceHolder holder, int format, int width, int height) &#123; super.onSurfaceChanged(holder, format, width, height); //下面是来保存屏幕显示立方体的，也就是你能看到的正面图的中心位置 mCenterX = width / 2.0f; mCenterY = height / 2.0f; drawFrame(); &#125; /** * 下面两个方法是为了方便调用SurfaceHolder交互来重写的 */ @Override public void onSurfaceCreated(SurfaceHolder holder) &#123; super.onSurfaceCreated(holder); &#125; @Override public void onSurfaceDestroyed(SurfaceHolder holder) &#123; super.onSurfaceDestroyed(holder); mVisible = false; mHandler.removeCallbacks(mDrawCube); &#125; /** * 当手动壁纸时根据偏移量重绘壁纸 */ @Override public void onOffsetsChanged(float xOffset, float yOffset, float xStep, float yStep, int xPixels, int yPixels) &#123; mOffset = xOffset; drawFrame(); &#125; /* * 在这个地方保存触摸的位置，我们会在绘制壁纸的时候使用触摸值 */ @Override public void onTouchEvent(MotionEvent event) &#123; if (event.getAction() == MotionEvent.ACTION_MOVE) &#123; mTouchX = event.getX(); mTouchY = event.getY(); &#125; else &#123; mTouchX = -1; mTouchY = -1; &#125; super.onTouchEvent(event); &#125; /* * 绘制立方体方法实现 */ void drawFrame() &#123; final SurfaceHolder holder = getSurfaceHolder(); Canvas c = null; try &#123; c = holder.lockCanvas(); if (c != null) &#123; drawCube(c); drawTouchPoint(c); &#125; &#125; finally &#123; if (c != null) holder.unlockCanvasAndPost(c); &#125; // 在指定时间里重绘制，这个地方大家可以看效果图，如果你拖动过快的话，立方体 //每个顶点之间会有一个短暂的未连接延迟，就是在这个地方使用了延迟来绘制的 mHandler.removeCallbacks(mDrawCube); if (mVisible) &#123; mHandler.postDelayed(mDrawCube, 1000 / 25); &#125; &#125; /* * 这个地方是以立方体某个顶点为起始端，绘制三条线 * 一堆数字，看着好晕 * 在这小马顺便贴在这个DEMO里面用到的基本的绘制，如下： * graphics.Canvas有四种画矩形的方法。 canvas.drawRect(new RectF(10, 10, 300, 100), paint); canvas.drawRect(10, 150, 300, 200, paint); canvas.drawRect(new Rect(10, 250, 300, 300), paint); 第四种：画圆角的矩形 canvas.drawRoundRect(new RectF(10, 350, 300, 450), 10, 10, paint); 第二个和第三个参数为圆角的宽高。 有兴趣的朋友可以改下下面这些东西 */ void drawCube(Canvas c) &#123; c.save(); c.translate(mCenterX, mCenterY); c.drawColor(0xff000000); drawLine(c, -400, -400, -400, 400, -400, -400); drawLine(c, 400, -400, -400, 400, 400, -400); drawLine(c, 400, 400, -400, -400, 400, -400); drawLine(c, -400, 400, -400, -400, -400, -400); drawLine(c, -400, -400, 400, 400, -400, 400); drawLine(c, 400, -400, 400, 400, 400, 400); drawLine(c, 400, 400, 400, -400, 400, 400); drawLine(c, -400, 400, 400, -400, -400, 400); drawLine(c, -400, -400, 400, -400, -400, -400); drawLine(c, 400, -400, 400, 400, -400, -400); drawLine(c, 400, 400, 400, 400, 400, -400); drawLine(c, -400, 400, 400, -400, 400, -400); c.restore(); &#125; /* * 在屏幕中绘制三维空间的线 */ void drawLine(Canvas c, int x1, int y1, int z1, int x2, int y2, int z2) &#123; /* *因为大家都知道，壁纸是手机启动完成之后就已经开始绘制的，一般取时间什么的 *我们都用Timer System.currentTimeMillis() Calendar来取 *这个地方取系统级启动时间等的，记住这个类，SystemClock，方法自己查 */ long now = SystemClock.elapsedRealtime(); /*取得三维坐标轴的旋转值*/ float xrot = ((float) (now - mStartTime)) / 1000; float yrot = (0.5f - mOffset) * 2.0f; float zrot = 0; // rotation around X-axis ？？？ float newy1 = (float) (Math.sin(xrot) * z1 + Math.cos(xrot) * y1); float newy2 = (float) (Math.sin(xrot) * z2 + Math.cos(xrot) * y2); float newz1 = (float) (Math.cos(xrot) * z1 - Math.sin(xrot) * y1); float newz2 = (float) (Math.cos(xrot) * z2 - Math.sin(xrot) * y2); // rotation around Y-axis ？？？ float newx1 = (float) (Math.sin(yrot) * newz1 + Math.cos(yrot) * x1); float newx2 = (float) (Math.sin(yrot) * newz2 + Math.cos(yrot) * x2); newz1 = (float) (Math.cos(yrot) * newz1 - Math.sin(yrot) * x1); newz2 = (float) (Math.cos(yrot) * newz2 - Math.sin(yrot) * x2); // 3D-to-2D projection ？？？ float startX = newx1 / (4 - newz1 / 400); float startY = newy1 / (4 - newz1 / 400); float stopX = newx2 / (4 - newz2 / 400); float stopY = newy2 / (4 - newz2 / 400); c.drawLine(startX, startY, stopX, stopY, mPaint); &#125; /* * 按位屏幕手动时绘制一个白色的圈 */ void drawTouchPoint(Canvas c) &#123; if (mTouchX &gt;= 0 &amp;&amp; mTouchY &gt;= 0) &#123; c.drawCircle(mTouchX, mTouchY, 80, mPaint); &#125; &#125; &#125;&#125; 流程分析&#160; &#160; &#160; &#160;上面都是一些设置壁纸的方法，但这实现不了产品汪的需求啊。产品汪想要Launcher一直用背景播放视频，设计喵出了许多应用的效果图，要求这些应用都可以和背景进行交互……唉，坑爹的需求，继续看源码吧。 静态壁纸流程&#160; &#160; &#160; &#160;三种设置静态壁纸的方法，其实原理都大同小异。我们只看看WallpaperManager的setBitmap(Bitmap bitmap)方法，其他两种有兴趣的同学也可以自己看看位于framework/base/core/java/android/app/WallpaperManager.java：1234567891011121314151617181920212223242526272829303132333435 public void setBitmap(Bitmap bitmap) throws IOException &#123; if (sGlobals.mService == null) &#123; Log.w(TAG, "WallpaperService not running"); return; &#125; try &#123; //sGlobals.mService声明在WallpaperManager的内部类Globals里，对应系统服务WallpaperManagerService; ParcelFileDescriptor fd = sGlobals.mService.setWallpaper(null); if (fd == null) &#123; return; &#125; FileOutputStream fos = null; try &#123; //获得WallpaperManagerService返回的一个壁纸相关的文件描述符，然后将新壁纸内容写入 fos = new ParcelFileDescriptor.AutoCloseOutputStream(fd); bitmap.compress(Bitmap.CompressFormat.PNG, 90, fos); &#125; finally &#123; if (fos != null) &#123; fos.close(); &#125; &#125; &#125; catch (RemoteException e) &#123; // Ignore &#125; &#125;static class Globals extends IWallpaperManagerCallback.Stub &#123; private IWallpaperManager mService; ...... Globals(Looper looper) &#123; //对应系统服务WallpaperManagerService IBinder b = ServiceManager.getService(Context.WALLPAPER_SERVICE); mService = IWallpaperManager.Stub.asInterface(b); &#125;&#125; &#160; &#160; &#160; &#160;setBitmap方法内部有一段ParcelFileDescriptor fd = sGlobals.mService.setWallpaper(null)，这里sGlobals.mService声明在WallpaperManager的内部类Globals里，这个mService对应系统服务的WallpaperManagerService。它的setWallpaper方法返回的是一个文件描述符，我们猜测这个就是壁纸内容存放的fd，往下的代码就是将我们指定的bitmap写入这个fd。所以我们看看WallpaperManagerService的setWallpaper方法,位于frameworks/base/services/core/java/com/android/server/wallpaper/WallpaperManagerService.java中：123456789101112131415161718192021222324public ParcelFileDescriptor setWallpaper(String name) &#123; checkPermission(android.Manifest.permission.SET_WALLPAPER); synchronized (mLock) &#123; if (DEBUG) Slog.v(TAG, "setWallpaper"); //先获取用户的userId int userId = UserHandle.getCallingUserId(); //根据userId这个键去取WallpaperData这个壁纸包装类的值 WallpaperData wallpaper = mWallpaperMap.get(userId); if (wallpaper == null) &#123; throw new IllegalStateException("Wallpaper not yet initialized for user " + userId); &#125; final long ident = Binder.clearCallingIdentity(); try &#123; //获得壁纸文件的fd ParcelFileDescriptor pfd = updateWallpaperBitmapLocked(name, wallpaper); if (pfd != null) &#123; wallpaper.imageWallpaperPending = true; &#125; return pfd; &#125; finally &#123; Binder.restoreCallingIdentity(ident); &#125; &#125;&#125; &#160; &#160; &#160; &#160;这里分三步：&#160; &#160; &#160; &#160;1）先获取用户的userId，看看UserHandle.的getCallingUserId()方法，位于framework/base/core/java/android/os/UserHandle.java中：123456789101112131415161718192021222324 /** * @hide Range of uids allocated for a user. */ public static final int PER_USER_RANGE = 100000; /** * @hide Enable multi-user related side effects. Set this to false if * there are problems with single user use-cases. */ public static final boolean MU_ENABLED = true;/** @hide */ public static final int getCallingUserId() &#123; return getUserId(Binder.getCallingUid()); &#125; /** * Returns the user id for a given uid. * @hide */ public static final int getUserId(int uid) &#123; if (MU_ENABLED) &#123; return uid / PER_USER_RANGE; &#125; else &#123; return 0; &#125; &#125; &#160; &#160; &#160; &#160;因为Android从4.2之后支持了多用户，所以MU_ENABLED 的值为true。&#160; &#160; &#160; &#160;这里Binder.getCallingUid()获得应用的uid，我们知道系统的uid是1000，之后应用都是从1000往后的。一些重要的system app，比如com.android.phone电话是1001，com.android.bluetooth蓝牙是1002等等；第三方应用则是从10000开始的，多装一个应用分配的uid就会+1，比如我装了个com.bilibili.tv 这个哔哩哔哩动画的uid是10019，之后又装了个tv.danmaku.bilixl哔哩哔哩动画经典版的uid就变为10020了；但是如果应用在清单文件中配置了android:sharedUserId=”android.uid.system”属性，那么这个应用的uid也是1000，不过一般需要打系统签名。&#160; &#160; &#160; &#160;如果要查看应用的uid，可以查看android机器的/data/system/packages.list文件，上面都罗列了每个应用的包名和对应uid。 &#160; &#160; &#160; &#160;回到上面代码，返回值是用应用的uid除以100000，所以这里返回的是0。 &#160; &#160; &#160; &#160;2）根据userId这个键去取WallpaperData这个壁纸包装类的值。&#160; &#160; &#160; &#160;这里userId是0，因为系统崩溃，所以下面那个异常不会抛出，这里mWallpaperMap.get(userId)获得的WallpaperData 对象wallpaper 不为空。&#160; &#160; &#160; &#160;所以我们就得找这个WallpaperData 被存入mWallpaperMap的时机。在浏览WallpaperManagerService的构造方法是发现了存入时机，调用WallpaperManagerService构造方法是在SystemServer启动时调用的。因为android系统开机会启动framework，framework会启动它的SystemServer，代码实现如下，位于framework/base/services/java/com/android/server/SystemServer.java：12345678910111213141516171819202122232425262728293031323334public final class SystemServer &#123; /** * The main entry point from zygote. */ public static void main(String[] args) &#123; new SystemServer().run(); &#125; private void run() &#123; ...... startOtherServices(); ...... &#125; /** * Starts a miscellaneous grab bag of stuff that has yet to be refactored * and organized. */ private void startOtherServices() &#123; ...... WallpaperManagerService wallpaper = null; ...... if (!disableNonCoreServices &amp;&amp; context.getResources().getBoolean( R.bool.config_enableWallpaperService)) &#123; try &#123; Slog.i(TAG, "Wallpaper Service"); //这里启动了WallpaperManagerService系统服务，加入ServiceManager中管理 wallpaper = new WallpaperManagerService(context); ServiceManager.addService(Context.WALLPAPER_SERVICE, wallpaper); &#125; catch (Throwable e) &#123; reportWtf("starting Wallpaper Service", e); &#125; &#125; ...... &#125;&#125; &#160; &#160; &#160; &#160;这里启动了WallpaperManagerService后，然后我们继续分析他的构造方法：123456789101112131415public WallpaperManagerService(Context context) &#123; ...... //生成壁纸相关目录/data/system/users/0 getWallpaperDir(UserHandle.USER_OWNER).mkdirs(); //载入系统保存的配置，UserHandle.USER_OWNER为0 loadSettingsLocked(UserHandle.USER_OWNER);&#125;/** * 返回 /data/system/users/&#123;userId&#125; * @param userId * @return /data/system/users/&#123;userId&#125; */private static File getWallpaperDir(int userId) &#123; return Environment.getUserSystemDirectory(userId);&#125; &#160; &#160; &#160; &#160;这里先生成壁纸相关目录，/data/system/users/0这个目录。然后就是载入系统保存的配置，我们接着看loadSettingsLocked(UserHandle.USER_OWNER)方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657 private void loadSettingsLocked(int userId) &#123;//0 ...... //封装/data/system/users/0/wallpaper_info.xml文件的方法 JournaledFile journal = makeJournaledFile(userId); FileInputStream stream = null; File file = journal.chooseForRead(); ...... //此时mWallpaperMap.get(0)为null WallpaperData wallpaper = mWallpaperMap.get(userId); if (wallpaper == null) &#123;//会进入这里 wallpaper = new WallpaperData(userId); mWallpaperMap.put(userId, wallpaper); &#125; //--------------解析/data/system/users/0/wallpaper_info.xml START---------------- stream = new FileInputStream(file); XmlPullParser parser = Xml.newPullParser(); parser.setInput(stream, null); int type; do &#123; type = parser.next(); if (type == XmlPullParser.START_TAG) &#123; String tag = parser.getName(); if ("wp".equals(tag)) &#123; wallpaper.width = Integer.parseInt(parser.getAttributeValue(null, "width")); wallpaper.height = Integer.parseInt(parser .getAttributeValue(null, "height")); wallpaper.padding.left = getAttributeInt(parser, "paddingLeft", 0); wallpaper.padding.top = getAttributeInt(parser, "paddingTop", 0); wallpaper.padding.right = getAttributeInt(parser, "paddingRight", 0); wallpaper.padding.bottom = getAttributeInt(parser, "paddingBottom", 0); wallpaper.name = parser.getAttributeValue(null, "name"); String comp = parser.getAttributeValue(null, "component"); wallpaper.nextWallpaperComponent = comp != null ? ComponentName.unflattenFromString(comp) : null; if (wallpaper.nextWallpaperComponent == null || "android".equals(wallpaper.nextWallpaperComponent .getPackageName())) &#123; wallpaper.nextWallpaperComponent = mImageWallpaper; &#125; &#125; &#125; &#125; while (type != XmlPullParser.END_DOCUMENT); //--------------解析/data/system/users/0/wallpaper_info.xml END---------------- ...... &#125;static final String WALLPAPER_INFO = "wallpaper_info.xml";//封装/data/system/users/0/wallpaper_info.xml文件的方法 private static JournaledFile makeJournaledFile(int userId) &#123; final String base = new File(getWallpaperDir(userId), WALLPAPER_INFO).getAbsolutePath(); return new JournaledFile(new File(base), new File(base + ".tmp")); &#125; &#160; &#160; &#160; &#160;载入系统保存的配置，这里也是分三步：&#160; &#160; &#160; &#160;1. 封装一下/data/system/users/0/wallpaper_info.xml文件，用JournaledFile这个工具类，这个工具类会包装两个文件，一个是wallpaper_info.xml正式文件，另一个是wallpaper_info.xml.tmp临时文件。如果正式文件存在就选出正式文件，并删除临时文件；如果正式文件不存在就将临时文件重名为正式文件。&#160; &#160; &#160; &#160;2. 创建一个WallpaperData并存入WallpaperData ，我们可以看看WallpaperData 的构造，这是一个内部类：123456static final String WALLPAPER = "wallpaper"; WallpaperData(int userId) &#123;//0 this.userId = userId; //位于/data/system/users/0/wallpaper，是存放壁纸的文件 wallpaperFile = new File(getWallpaperDir(userId), WALLPAPER); &#125; WallpaperData 的构造方法主要是创建了一个/data/system/users/0/wallpaper的File对象。&#160; &#160; &#160; &#160;3. 最后就是解析/data/system/users/0/wallpaper_info.xml文件，用pull解析器简单解析，这个xml也很简单，这里我举个测试栗子：12&lt;?xml version='1.0' encoding='utf-8' standalone='yes' ?&gt;&lt;wp width="1920" height="1920" name="res:com.example.aaaandroid:drawable/ic_launcher" /&gt; &#160; &#160; &#160; &#160;这里获取WallpaperData的流程就分析完了，然后继续往下走。 &#160; &#160; &#160; &#160;3）获得壁纸文件的fd，查看updateWallpaperBitmapLocked(name, wallpaper)方法：1234567891011121314151617181920212223242526ParcelFileDescriptor updateWallpaperBitmapLocked(String name, WallpaperData wallpaper) &#123; if (name == null) name = ""; try &#123; File dir = getWallpaperDir(wallpaper.userId); if (!dir.exists()) &#123; dir.mkdir(); FileUtils.setPermissions( dir.getPath(), FileUtils.S_IRWXU|FileUtils.S_IRWXG|FileUtils.S_IXOTH, -1, -1); &#125; // /data/system/users/0/wallpaper文件 File file = new File(dir, WALLPAPER); //返回这个文件的fd ParcelFileDescriptor fd = ParcelFileDescriptor.open(file, MODE_CREATE|MODE_READ_WRITE|MODE_TRUNCATE); if (!SELinux.restorecon(file)) &#123; return null; &#125; wallpaper.name = name; return fd; &#125; catch (FileNotFoundException e) &#123; Slog.w(TAG, "Error setting wallpaper", e); &#125; return null;&#125; &#160; &#160; &#160; &#160;可以看到，这个方法最终返回了/data/system/users/0/wallpaper文件的fd。 &#160; &#160; &#160; &#160;到这里三步走就完了，千万不能晕了。赶紧回到刚才WallpaperManager的setBitmap方法中。接着就是将我们指定的Bitmap写入到这个fd当中：12345678910111213141516171819202122public void setBitmap(Bitmap bitmap) throws IOException &#123; ...... try &#123; //sGlobals.mService声明在WallpaperManager的内部类Globals里，对应系统服务WallpaperManagerService; ParcelFileDescriptor fd = sGlobals.mService.setWallpaper(null); if (fd == null) &#123; return; &#125; FileOutputStream fos = null; try &#123; //获得WallpaperManagerService返回的一个壁纸相关的文件描述符，然后将新壁纸内容写入 fos = new ParcelFileDescriptor.AutoCloseOutputStream(fd); bitmap.compress(Bitmap.CompressFormat.PNG, 90, fos); &#125; finally &#123; if (fos != null) &#123; fos.close(); &#125; &#125; &#125; catch (RemoteException e) &#123; // Ignore &#125; &#125; &#160; &#160; &#160; &#160;然后我们的设置静态壁纸就完了。&#160; &#160; &#160; &#160;我们的设置静态壁纸就完了。&#160; &#160; &#160; &#160;设置静态壁纸就完了。&#160; &#160; &#160; &#160;静态壁纸就完了。&#160; &#160; &#160; &#160;就完了。&#160; &#160; &#160; &#160;。。。。。。。。。&#160; &#160; &#160; &#160;总感觉不对啊~~我们只是把一个图片的内容写入到了一个目录的文件中而已，怎么就会改变壁纸呢，这不合常理啊！&#160; &#160; &#160; &#160;当时我也是一脸懵逼啊！。。。。。。。&#160; &#160; &#160; &#160;然后我就想：嗯。。。这个fd指向的文件一定另有玄机，然后翻阅WallpaperManagerService时发现了WallpaperObserver这个类，这个类继承FileObserver，这是一个监听文件改变的类，看来八九不离十。&#160; &#160; &#160; &#160;接着查找调用它构造方法的地方，最后顺藤摸瓜还是到了SystemServer里：12345678910111213141516171819202122232425public final class SystemServer &#123; //调用startOtherServices private void startOtherServices() &#123; //wallpaper就是刚才启动的WallpaperManagerService final WallpaperManagerService wallpaperF = wallpaper; // We now tell the activity manager it is okay to run third party // code. It will call back into us once it has gotten to the state // where third party code can really run (but before it has actually // started launching the initial applications), for us to complete our // initialization. //当可以启动Launcher之前会回调 mActivityManagerService.systemReady(new Runnable() &#123; ...... try &#123; //调用WallpaperManagerService 的systemRunning方法 if (wallpaperF != null) wallpaperF.systemRunning(); &#125; catch (Throwable e) &#123; reportWtf("Notifying WallpaperService running", e); &#125; ...... &#125; &#125;&#125; &#160; &#160; &#160; &#160;这是startOtherServices的部分代码，设置了一下监听，ActivityManagerService准备工作完成后可以启动Launcher之前触发回调。我们接着看WallpaperManagerService 的systemRunning方法：12345678910111213141516171819public void systemRunning() &#123; ...... WallpaperData wallpaper = mWallpaperMap.get(UserHandle.USER_OWNER); ...... wallpaper.wallpaperObserver = new WallpaperObserver(wallpaper); wallpaper.wallpaperObserver.startWatching(); ...... &#125; //WallpaperObserver的构造方法 public WallpaperObserver(WallpaperData wallpaper) &#123; //已经对/data/system/users/0目录进行监听 super(getWallpaperDir(wallpaper.userId).getAbsolutePath(), CLOSE_WRITE | MOVED_TO | DELETE | DELETE_SELF); mWallpaperDir = getWallpaperDir(wallpaper.userId); mWallpaper = wallpaper; //这个就是/data/system/users/0/wallpaper这个壁纸文件 mWallpaperFile = new File(mWallpaperDir, WALLPAPER); &#125; &#160; &#160; &#160; &#160;这里已经开始了壁纸文件的改动监听，我们可以看看它的回调方法，即WallpaperObserver的onEvent方法：12345678910111213141516171819202122 @Override public void onEvent(int event, String path) &#123; if (path == null) &#123; return; &#125; synchronized (mLock) &#123; ......//改动的文件 File changedFile = new File(mWallpaperDir, path); //如果改动的文件是/data/system/users/0/wallpaper if (mWallpaperFile.equals(changedFile)) &#123; ...... //绑定壁纸组件，请注意mImageWallpaper这个参数 bindWallpaperComponentLocked(mImageWallpaper, true, false, mWallpaper, null); //保存壁纸信息，和上面的loadSettingsLocked正好相反，一个保存，一个取出 saveSettingsLocked(mWallpaper); &#125; &#125; &#125; &#125; &#160; &#160; &#160; &#160;如果改动的文件是/data/system/users/0/wallpaper，则就要绑定壁纸组件，调用bindWallpaperComponentLocked方法。我们这里需要注意第一个参数mImageWallpaper，我们可以看看它是在哪里初始化的：1234567891011 /** * Name of the component used to display bitmap wallpapers from either the gallery or * built-in wallpapers. */ final ComponentName mImageWallpaper; public WallpaperManagerService(Context context) &#123;...... mImageWallpaper = ComponentName.unflattenFromString( context.getResources().getString(R.string.image_wallpaper_component));...... &#125; &#160; &#160; &#160; &#160;它也是在WallpaperManagerService的构造方法中初始化的，R.string.image_wallpaper_component这个资源是个string类型，位于framework/base/core/res/res/values/config.xml中：12&lt;!-- Component name of the built in wallpaper used to display bitmap wallpapers. This must not be null. --&gt; &lt;string name="image_wallpaper_component" translatable="false"&gt;com.android.systemui/com.android.systemui.ImageWallpaper&lt;/string&gt; 这个mImageWallpaper是个ComponentName，指向SystemUI中的ImageWallpaper。这个类是SystemUI app中的一个类，位于framework/base/packages/SystemUI/src/com/android/systemui/ImageWallpaper.java中。 &#160; &#160; &#160; &#160;我们继续，分析bindWallpaperComponentLocked方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127 boolean bindWallpaperComponentLocked(ComponentName componentName, boolean force, boolean fromUser, WallpaperData wallpaper, IRemoteCallback reply) &#123;//mImageWallpaper, true, false, mWallpaper, null ...... // Has the component changed? if (!force) &#123;//force为true，所以不进入这段逻辑 if (wallpaper.connection != null) &#123; if (wallpaper.wallpaperComponent == null) &#123; if (componentName == null) &#123; // Still using default wallpaper. return true; &#125; &#125; else if (wallpaper.wallpaperComponent.equals(componentName)) &#123; // Changing to same wallpaper. return true; &#125; &#125; &#125; try &#123; if (componentName == null) &#123;//componentName 不为null，所以不进入下面逻辑 componentName = WallpaperManager.getDefaultWallpaperComponent(mContext); if (componentName == null) &#123; // Fall back to static image wallpaper componentName = mImageWallpaper; //clearWallpaperComponentLocked(); //return; &#125; &#125;...... WallpaperInfo wi = null; Intent intent = new Intent(WallpaperService.SERVICE_INTERFACE); if (componentName != null &amp;&amp; !componentName.equals(mImageWallpaper)) &#123; // true &amp;&amp; false ,因此不进入下面逻辑 // Make sure the selected service is actually a wallpaper service. List&lt;ResolveInfo&gt; ris = mIPackageManager.queryIntentServices(intent, intent.resolveTypeIfNeeded(mContext.getContentResolver()), PackageManager.GET_META_DATA, serviceUserId); for (int i=0; i&lt;ris.size(); i++) &#123; ServiceInfo rsi = ris.get(i).serviceInfo; if (rsi.name.equals(si.name) &amp;&amp; rsi.packageName.equals(si.packageName)) &#123; try &#123; wi = new WallpaperInfo(mContext, ris.get(i)); &#125; catch (XmlPullParserException e) &#123; if (fromUser) &#123; throw new IllegalArgumentException(e); &#125; Slog.w(TAG, e); return false; &#125; catch (IOException e) &#123; if (fromUser) &#123; throw new IllegalArgumentException(e); &#125; Slog.w(TAG, e); return false; &#125; break; &#125; &#125; if (wi == null) &#123; String msg = "Selected service is not a wallpaper: " + componentName; if (fromUser) &#123; throw new SecurityException(msg); &#125; Slog.w(TAG, msg); return false; &#125; &#125; // Bind the service! //WallpaperConnection实现了ServiceConnection接口，绑定服务时和解绑时回调两个接口 WallpaperConnection newConn = new WallpaperConnection(wi, wallpaper); //com.android.systemui/.ImageWallpaper intent.setComponent(componentName); intent.putExtra(Intent.EXTRA_CLIENT_LABEL, com.android.internal.R.string.wallpaper_binding_label); intent.putExtra(Intent.EXTRA_CLIENT_INTENT, PendingIntent.getActivityAsUser( mContext, 0, Intent.createChooser(new Intent(Intent.ACTION_SET_WALLPAPER), mContext.getText(com.android.internal.R.string.chooser_wallpaper)), 0, null, new UserHandle(serviceUserId))); //绑定com.android.systemui/.ImageWallpaper这个服务 if (!mContext.bindServiceAsUser(intent, newConn, Context.BIND_AUTO_CREATE | Context.BIND_SHOWING_UI, new UserHandle(serviceUserId))) &#123; String msg = "Unable to bind service: " + componentName; if (fromUser) &#123; throw new IllegalArgumentException(msg); &#125; Slog.w(TAG, msg); return false; &#125; //移除原先壁纸和窗口的管理 if (wallpaper.userId == mCurrentUserId &amp;&amp; mLastWallpaper != null) &#123; detachWallpaperLocked(mLastWallpaper); &#125; wallpaper.wallpaperComponent = componentName; wallpaper.connection = newConn; newConn.mReply = reply; try &#123; if (wallpaper.userId == mCurrentUserId) &#123; //给壁纸窗口加入TYPE_WALLPAPER类型 mIWindowManager.addWindowToken(newConn.mToken, WindowManager.LayoutParams.TYPE_WALLPAPER); mLastWallpaper = wallpaper; &#125; &#125; catch (RemoteException e) &#123; &#125; &#125; catch (RemoteException e) &#123; String msg = "Remote exception for " + componentName + "\n" + e; if (fromUser) &#123; throw new IllegalArgumentException(msg); &#125; Slog.w(TAG, msg); return false; &#125; return true; &#125; &#160; &#160; &#160; &#160;这个方法虽然复杂，然是很多逻辑不会进入，少了干扰项。因为我们是设置静态壁纸，所以核心步骤也不多，大概如下： 绑定com.android.systemui/.ImageWallpaper这个服务 移除原先壁纸和窗口的管理，给壁纸窗口加入TYPE_WALLPAPER类型 绑定壁纸服务&#160; &#160; &#160; &#160;核心就在绑定com.android.systemui/.ImageWallpaper这个服务。&#160; &#160; &#160; &#160;ImageWallpaper继承于WallpaperService，并且我们看一下它的写法，和上述的动态壁纸的模式一模一样啊：12345678910111213141516public class ImageWallpaper extends WallpaperService &#123; @Override public void onCreate() &#123; super.onCreate(); mWallpaperManager = (WallpaperManager) getSystemService(WALLPAPER_SERVICE); ...... &#125; @Override public Engine onCreateEngine() &#123; mEngine = new DrawableEngine(); return mEngine; &#125; class DrawableEngine extends Engine &#123; ......一些Engine的方法,和上面的魔方那个例子几乎一样..... &#125;&#125; &#160; &#160; &#160; &#160;但是我们分析还是按着步骤一步一来，就回到绑定服务那一步。&#160; &#160; &#160; &#160;绑定ImageWallpaper这个服务，会回调ServiceConnection接口的onServiceConnected方法。所以我们看看这个WallpaperConnection类的onServiceConnected方法：1234567891011121314151617181920//ImageWallpaper的onBind方法返回值就是这个mService IWallpaperService mService;@Override public void onServiceConnected(ComponentName name, IBinder service) &#123; synchronized (mLock) &#123; if (mWallpaper.connection == this) &#123; //ImageWallpaper的onBind方法返回值就是这个mService mService = IWallpaperService.Stub.asInterface(service); //接下来会调用的 attachServiceLocked(this, mWallpaper); // XXX should probably do saveSettingsLocked() later // when we have an engine, but I'm not sure about // locking there and anyway we always need to be able to // recover if there is something wrong. //保存壁纸信息，这个方法上面说过的，和loadSettingsLocked方法正好相对 saveSettingsLocked(mWallpaper); &#125; &#125; &#125; &#160; &#160; &#160; &#160;ImageWallpaper的onBind方法返回值就是这个mService ，这个得从ImageWallpaper的父类WallpaperService中查找：1234567891011121314151617181920212223242526/** * Implement to return the implementation of the internal accessibility * service interface. Subclasses should not override. */ @Override public final IBinder onBind(Intent intent) &#123; return new IWallpaperServiceWrapper(this); &#125; /** * Implements the internal &#123;@link IWallpaperService&#125; interface to convert * incoming calls to it back to calls on an &#123;@link WallpaperService&#125;. */ class IWallpaperServiceWrapper extends IWallpaperService.Stub &#123; private final WallpaperService mTarget; public IWallpaperServiceWrapper(WallpaperService context) &#123; mTarget = context; &#125; @Override public void attach(IWallpaperConnection conn, IBinder windowToken, int windowType, boolean isPreview, int reqWidth, int reqHeight, Rect padding) &#123; new IWallpaperEngineWrapper(mTarget, conn, windowToken, windowType, isPreview, reqWidth, reqHeight, padding); &#125; &#125; &#160; &#160; &#160; &#160;这里给这个类型为IWallpaperService 的mService变量赋值的是IWallpaperServiceWrapper这个类的一个实例。 &#160; &#160; &#160; &#160;然后我们回到上面，接着调用attachServiceLocked(this, mWallpaper)，我们再跟进去看看：12345678910 void attachServiceLocked(WallpaperConnection conn, WallpaperData wallpaper) &#123; try &#123; //这里会调用IWallpaperServiceWrapper 的attach方法 conn.mService.attach(conn, conn.mToken, WindowManager.LayoutParams.TYPE_WALLPAPER, false, wallpaper.width, wallpaper.height, wallpaper.padding); &#125; catch (RemoteException e) &#123;...... &#125; &#125; &#160; &#160; &#160; &#160;内部有调用了IWallpaperServiceWrapper 的attach方法，上面代码已经贴出。可以看到，attach方法内部又创建了一个IWallpaperEngineWrapper对象，所以我们还得看看IWallpaperEngineWrapper的构造方法：1234567891011121314151617IWallpaperEngineWrapper(WallpaperService context, IWallpaperConnection conn, IBinder windowToken, int windowType, boolean isPreview, int reqWidth, int reqHeight, Rect padding) &#123; //这个HandlerCaller就是对Handler的一个封装类，大家以后可以参考它 mCaller = new HandlerCaller(context, context.getMainLooper(), this, true); mConnection = conn;//WallpaperManagerService中的WallpaperConnection类的对象 mWindowToken = windowToken;//final Binder mToken = new Binder(); mWindowType = windowType;//WindowManager.LayoutParams.TYPE_WALLPAPER mIsPreview = isPreview;//false mReqWidth = reqWidth;//wallpaper_info.xml中读取的width mReqHeight = reqHeight;//wallpaper_info.xml中读取的height mDisplayPadding.set(padding);//new Rect（0，0，0，0）； //发送DO_ATTACH消息 Message msg = mCaller.obtainMessage(DO_ATTACH); mCaller.sendMessage(msg);&#125; &#160; &#160; &#160; &#160;这里的HandlerCaller就是对Handler的一个封装类，大家以后可以参考它 ，位于framework/base/core/java/com/android/internal/os/HandlerCaller.java，这里不做详细分析。 &#160; &#160; &#160; &#160;IWallpaperEngineWrapper的构造方法最后发送了DO_ATTACH消息，处理消息在IWallpaperEngineWrapper类的executeMessage方法中：12345678910111213141516171819202122public void executeMessage(Message message) &#123; switch (message.what) &#123; case DO_ATTACH: &#123; try &#123; //调用WallpaperConnection的attachEngine方法 mConnection.attachEngine(this); &#125; catch (RemoteException e) &#123; Log.w(TAG, "Wallpaper host disappeared", e); return; &#125; //调用重写的onCreateEngine返回一个Engine类型的对象 Engine engine = onCreateEngine(); mEngine = engine; mActiveEngines.add(engine); //在调用Engine的attach方法 engine.attach(this); return; &#125; ....... &#125;&#125; &#160; &#160; &#160; &#160;处理DO_ATTACH消息会先调用WallpaperConnection的attachEngine方法：1234567891011121314151617181920212223@Overridepublic void attachEngine(IWallpaperEngine engine) &#123; synchronized (mLock) &#123; //用IWallpaperEngineWrapper类对象赋值 mEngine = engine; if (mDimensionsChanged) &#123;//false try &#123; mEngine.setDesiredSize(mWallpaper.width, mWallpaper.height); &#125; catch (RemoteException e) &#123; Slog.w(TAG, "Failed to set wallpaper dimensions", e); &#125; mDimensionsChanged = false; &#125; if (mPaddingChanged) &#123;//false try &#123; mEngine.setDisplayPadding(mWallpaper.padding); &#125; catch (RemoteException e) &#123; Slog.w(TAG, "Failed to set wallpaper padding", e); &#125; mPaddingChanged = false; &#125; &#125;&#125; &#160; &#160; &#160; &#160;这个方法就是用IWallpaperEngineWrapper类对象赋值，然后如果尺寸或者缩进改变了，就调整一下大小。 &#160; &#160; &#160; &#160;处理DO_ATTACH消息接着调用重写的onCreateEngine返回一个Engine类型的对象，并将它存入这个mActiveEngines的ArrayList中； &#160; &#160; &#160; &#160;处理DO_ATTACH消息最后会调用Engine的attach方法，我们进入WallpaperService的内部类Engine的attach方法中看看：1234567891011121314151617181920212223242526272829void attach(IWallpaperEngineWrapper wrapper) &#123; if (DEBUG) Log.v(TAG, "attach: " + this + " wrapper=" + wrapper); if (mDestroyed) &#123; return; &#125; mIWallpaperEngine = wrapper; mCaller = wrapper.mCaller; mConnection = wrapper.mConnection; //一个binder对象 mWindowToken = wrapper.mWindowToken; //mSurfaceHolder是BaseSurfaceHolder类对象，继承与SurfaceHolder，用于控制绘制图层 mSurfaceHolder.setSizeFromLayout(); mInitializing = true; //申请一个window需要一个WindowSession mSession = WindowManagerGlobal.getWindowSession(); //一个BaseIWindow类型的对象，继承于IWindow.Stub mWindow.setSession(mSession); ...... //Engine的初始化工作 onCreate(mSurfaceHolder); mInitializing = false; mReportedVisible = false; //开始往window上的surface里绘制壁纸 updateSurface(false, false, false); &#125; &#160; &#160; &#160; &#160;这个attach方法也做了两件事：&#160; &#160; &#160; &#160;1. 初始化壁纸窗口和绘制图层相关工作&#160; &#160; &#160; &#160;2. 开始往window上的surface里绘制壁纸 &#160; &#160; &#160; &#160;关于初始化窗口相关内容比较复杂，下面篇幅我会给出参考资料，这一部分原理也很复杂，有兴趣的同学可以自行研究（我也没研究过，以后再补上= 。=） &#160; &#160; &#160; &#160;绘制壁纸就是在updateSurface方法里，不过这个方法也是很复杂的，可以看出，这个方法会被很多种情况不停触发。我们简单分析一下触发条件：123456789void updateSurface(boolean forceRelayout, boolean forceReport, boolean redrawNeeded) &#123; ...省略一些逻辑,条件1... onVisibilityChanged(true); ...省略一些逻辑,条件2... onSurfaceRedrawNeeded(mSurfaceHolder); ...省略一些逻辑,条件3... onSurfaceChanged(mSurfaceHolder, mFormat, mCurWidth, mCurHeight); ...省略一些逻辑...&#125; &#160; &#160; &#160; &#160;以上三个条件均会触发绘制，即调用了ImageWallpaper的drawFrame方法。drawFrame方法内部对壁纸的大小和位置都做了很多调整，这里我们不作详细分析，我们只care它的绘制：1234567891011121314void drawFrame() &#123; ...省略大部分调整壁纸大小和窗口位置的逻辑... if (mIsHwAccelerated) &#123;//如果机器支持硬件加速，就用OpenGL绘制 if (!drawWallpaperWithOpenGL(sh, availw, availh, xPixels, yPixels)) &#123; drawWallpaperWithCanvas(sh, availw, availh, xPixels, yPixels); &#125; &#125; else &#123;//否则就用Skia绘制 drawWallpaperWithCanvas(sh, availw, availh, xPixels, yPixels); ...... &#125; &#125;&#125; &#160; &#160; &#160; &#160;关于OpenGL的绘制其实我看不懂，但是我们知道这是往窗口那一层Layer绘制壁纸图像。所以看看使用Canvas开绘制：123456789101112131415161718192021222324252627private void drawWallpaperWithCanvas(SurfaceHolder sh, int w, int h, int left, int top) &#123; Canvas c = sh.lockCanvas(); if (c != null) &#123; try &#123; if (DEBUG) &#123; Log.d(TAG, "Redrawing: left=" + left + ", top=" + top); &#125; final float right = left + mBackground.getWidth() * mScale; final float bottom = top + mBackground.getHeight() * mScale; if (w &lt; 0 || h &lt; 0) &#123; c.save(Canvas.CLIP_SAVE_FLAG); c.clipRect(left, top, right, bottom, Op.DIFFERENCE); c.drawColor(0xff000000); c.restore(); &#125; if (mBackground != null) &#123; RectF dest = new RectF(left, top, right, bottom); // add a filter bitmap? c.drawBitmap(mBackground, null, dest, null); &#125; &#125; finally &#123; sh.unlockCanvasAndPost(c); &#125; &#125;&#125; &#160; &#160; &#160; &#160;这个没啥难度吧，就是往Canvas画图。 &#160; &#160; &#160; &#160;到这里绑定壁纸服务的流程就分析完了，唉，也是绕来绕去，绕的头晕。。。。。 窗口管理&#160; &#160; &#160; &#160;窗口管理这一块其实流程也很复杂，要涉及WindowManagerService等一系列逻辑。上面绑定壁纸服务，在绘制前会申请一个窗口专门用来绘制壁纸。 &#160; &#160; &#160; &#160; 在Android系统中，壁纸窗口和输入法窗口一样，都是一种特殊类型的窗口，而且它们都是喜欢和一个普通的Activity窗口缠绵在一起。大家可以充分地想象这样的一个3W场景：输入法窗口在上面，壁纸窗口在下面，Activity窗口夹在它们的中间。 &#160; &#160; &#160; &#160; 一个Activity窗口如果需要显示壁纸，那么它必须满足以下两个条件： &#160; &#160; &#160; &#160;1. 背景是半透明的，例如，它在AndroidManifest.xml文件中的android:theme属性设置为Theme.Translucent：1234&lt;activity android:name=".WallpaperActivity" android:theme="@android:style/Theme.Translucent"&gt; ......&lt;/activity&gt; &#160; &#160; &#160; &#160;2. 窗口属性中的WindowManager.LayoutParams.FLAG_SHOW_WALLPAPER位设置为1：123456789101112131415public class WallpaperActivity extends Activity &#123; ...... @Override public void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.main); getWindow().addFlags(WindowManager.LayoutParams.FLAG_SHOW_WALLPAPER); &#125; ......&#125; &#160; &#160; &#160; &#160;如果想详细了解Android窗口管理服务WindowManagerService对壁纸窗口（Wallpaper Window）的管理分析 ，可以看看老罗的这篇文章。 动态壁纸流程分析&#160; &#160; &#160; &#160;为了应付产品汪的奇葩需求，就得研究一下动态壁纸是如何设置的。 &#160; &#160; &#160; &#160;所以呢：打开模拟器——打开设置——显示——壁纸——动态壁纸——随便选一个——设置壁纸按钮。用IED工具发现这个界面是这样的：com.android.wallpaper.livepicker/.LiveWallpaperPreview。这就随意多了，果断翻源码。位于packages/wallpapers/LivePicker/src/com/android/wallpaper/livepicker/LiveWallpaperPreview.java中，并找到了设置动态壁纸按钮的源码：123456789101112131415161718192021222324252627282930313233public class LiveWallpaperPreview extends Activity &#123; private WallpaperManager mWallpaperManager; private WallpaperConnection mWallpaperConnection; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); ...... mWallpaperManager = WallpaperManager.getInstance(this); mWallpaperConnection = new WallpaperConnection(mWallpaperIntent); &#125; //设置壁纸 public void setLiveWallpaper(View v) &#123; try &#123; //这句话是设置动态壁纸的核心 mWallpaperManager.getIWallpaperManager().setWallpaperComponent( mWallpaperIntent.getComponent()); mWallpaperManager.setWallpaperOffsetSteps(0.5f, 0.0f); mWallpaperManager.setWallpaperOffsets(v.getRootView().getWindowToken(), 0.5f, 0.0f); setResult(RESULT_OK); &#125; catch (RemoteException e) &#123; // do nothing &#125; catch (RuntimeException e) &#123; Log.w(LOG_TAG, "Failure setting wallpaper", e); &#125; finish(); &#125;&#125; &#160; &#160; &#160; &#160;设置动态壁纸就一句核心代码：&#160; &#160; &#160; &#160;mWallpaperManager.getIWallpaperManager().setWallpaperComponent(mWallpaperIntent.getComponent()); &#160; &#160; &#160; &#160;因为动态壁纸都是Service，所以上述代码中的参数mWallpaperIntent.getComponent()就是每一个Service的ComponentName。 &#160; &#160; &#160; &#160;为了分析流程，我们继续跟进去。最终是调用WallpaperManagerService的setWallpaperComponent方法：12345678910111213141516171819public void setWallpaperComponent(ComponentName name) &#123; checkPermission(android.Manifest.permission.SET_WALLPAPER_COMPONENT); synchronized (mLock) &#123; if (DEBUG) Slog.v(TAG, "setWallpaperComponent name=" + name); int userId = UserHandle.getCallingUserId(); WallpaperData wallpaper = mWallpaperMap.get(userId); if (wallpaper == null) &#123; throw new IllegalStateException("Wallpaper not yet initialized for user " + userId); &#125; final long ident = Binder.clearCallingIdentity(); try &#123; wallpaper.imageWallpaperPending = false; //最终还是调用了bindWallpaperComponentLocked方法，这个我们上面分析静态壁纸分析过了 bindWallpaperComponentLocked(name, false, true, wallpaper, null); &#125; finally &#123; Binder.restoreCallingIdentity(ident); &#125; &#125;&#125; &#160; &#160; &#160; &#160;可以发现，最终还是调用了bindWallpaperComponentLocked方法，只不过第一个参数ComponentName 换了。静态壁纸都是ImageWallpaper这个类，动态壁纸都是自己的类。这个方法我们上面已经分析过了，流程就那么些。 &#160; &#160; &#160; &#160;如果我们想自己绑定自己的壁纸，只要调用mWallpaperManager.getIWallpaperManager().setWallpaperComponent(mWallpaperIntent.getComponent());这句话就行了，只不过需要系统权限，必须在清单文件中配置android:sharedUserId=”android.uid.system”属性，还要打系统签名。 小结&#160; &#160; &#160; &#160;到此Android的壁纸服务就分析完了。&#160; &#160; &#160; &#160;动态壁纸和静态壁纸设置流程本质都是一样的，只不过静态壁纸是ImageWallpaper这个服务，动态壁纸是自己的。&#160; &#160; &#160; &#160;设置静态壁纸可以直接使用WallpaperManager提供的三种方法，setResource(int resId)，setBitmap(Bitmap bm)和setStream(InputStream is)。&#160; &#160; &#160; &#160;设置动态壁纸可以调用：WallpaperManager.getInstance(context).getIWallpaperManager().setWallpaperComponent(componentName)。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>壁纸</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何实现1080P延迟低于500ms的实时超清直播传输技术]]></title>
    <url>%2F2016%2F09%2F18%2F%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B01080P%E5%BB%B6%E8%BF%9F%E4%BD%8E%E4%BA%8E500ms%E7%9A%84%E5%AE%9E%E6%97%B6%E8%B6%85%E6%B8%85%E7%9B%B4%E6%92%AD%E4%BC%A0%E8%BE%93%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;再来当一次技术搬运工，内容来自高可用框架，学霸君工程师袁荣喜的如何实现1080P延迟低于500ms的实时超清直播传输技术。 &#160; &#160; &#160; &#160;导语：视频直播是很多技术团队及架构师关注的问题，在实时性方面，大部分直播是准实时的，存在 1-3 秒延迟。本文由袁荣喜向「高可用架构」投稿，介绍其将直播延迟控制在 500ms 的背后的实现。 &#160; &#160; &#160; &#160;袁荣喜，学霸君工程师，2015 年加入学霸君，负责学霸君的网络实时传输和分布式系统的架构设计和实现，专注于基础技术领域，在网络传输、数据库内核、分布式系统和并发编程方面有一定了解。 &#160; &#160; &#160; &#160;最近由于公司业务关系，需要一个在公网上能实时互动超清视频的架构和技术方案。众所周知，视频直播用 CDN + RTMP 就可以满足绝大部分视频直播业务，我们也接触了和测试了几家 CDN 提供的方案，单人直播没有问题，一旦涉及到多人互动延迟非常大，无法进行正常的互动交谈。对于我们做在线教育的企业来说没有互动的直播是毫无意义的，所以我们决定自己来构建一个超清晰（1080P）实时视频的传输方案。 &#160; &#160; &#160; &#160;先来解释下什么是实时视频，实时视频就是视频图像从产生到消费完成整个过程人感觉不到延迟，只要符合这个要求的视频业务都可以称为实时视频。关于视频的实时性归纳为三个等级： 伪实时：视频消费延迟超过 3 秒，单向观看实时，通用架构是 CDN + RTMP + HLS，现在基本上所有的直播都是这类技术。 准实时： 视频消费延迟 1 ~ 3 秒，能进行双方互动但互动有障碍。有些直播网站通过 TCP/UDP + FLV 已经实现了这类技术，YY 直播属于这类技术。 真实时：视频消费延迟 &lt; 1秒，平均 500 毫秒。这类技术是真正的实时技术，人和人交谈没有明显延迟感。QQ、微信、Skype 和 WebRTC 等都已经实现了这类技术。 &#160; &#160; &#160; &#160;市面上大部分真实时视频都是 480P 或者 480P 以下的实时传输方案，用于在线教育和线上教学有一定困难，而且有时候流畅度是个很大的问题。在实现超清晰实时视频我们做了大量尝试性的研究和探索，在这里会把大部分细节分享出来。 &#160; &#160; &#160; &#160;要实时就要缩短延迟，要缩短延迟就要知道延迟是怎么产生的，视频从产生、编码、传输到最后播放消费，各个环节都会产生延迟，总体归纳为下图：&#160; &#160; &#160; &#160;成像延迟，一般的技术是毫无为力的，涉及到 CCD 相关的硬件，现在市面上最好的 CCD，一秒钟 50 帧，成像延迟也在 20 毫秒左右，一般的 CCD 只有 20 ~ 25 帧左右，成像延迟 40 ~ 50 毫秒。 &#160; &#160; &#160; &#160;编码延迟，和编码器有关系，在接下来的小结介绍，一般优化的空间比较小。 &#160; &#160; &#160; &#160;我们着重针对网络延迟和播放缓冲延迟来进行设计，在介绍整个技术细节之前先来了解下视频编码和网络传输相关的知识和特点。 一、视频编码那些事&#160; &#160; &#160; &#160;我们知道从 CCD 采集到的图像格式一般的 RGB 格式的（BMP），这种格式的存储空间非常大，它是用三个字节描述一个像素的颜色值，如果是 1080P 分辨率的图像空间：1920 x 1080 x 3 = 6MB，就算转换成 JPG 也有近 200KB，如果是每秒 12 帧用 JPG 也需要近 2.4MB/S 的带宽，这带宽在公网上传输是无法接受的。 &#160; &#160; &#160; &#160;视频编码器就是为了解决这个问题的，它会根据前后图像的变化做运动检测，通过各种压缩把变化的发送到对方，1080P 进行过 H.264 编码后带宽也就在 200KB/S ~ 300KB/S 左右。在我们的技术方案里面我们采用 H.264 作为默认编码器（也在研究 H.265）。 1.1 H.264 编码&#160; &#160; &#160; &#160;前面提到视频编码器会根据图像的前后变化进行选择性压缩，因为刚开始接收端是没有收到任何图像，那么编码器在开始压缩的视频时需要做个全量压缩，这个全量压缩在 H.264 中 I 帧，后面的视频图像根据这个I帧来做增量压缩，这些增量压缩帧叫做 P 帧，H.264 为了防止丢包和减小带宽还引入一种双向预测编码的 B 帧，B 帧以前面的 I 或 P 帧和后面的 P 帧为参考帧。H.264 为了防止中间 P 帧丢失视频图像会一直错误它引入分组序列（GOP）编码，也就是隔一段时间发一个全量 I 帧，上一个 I 帧与下一个 I 帧之间为一个分组 GOP。它们之间的关系如下图：&#160; &#160; &#160; &#160;PS：在实时视频当中最好不要加入 B 帧，因为 B 帧是双向预测，需要根据后面的视频帧来编码，这会增大编解码延迟。 1.2 马赛克、卡顿和秒开&#160; &#160; &#160; &#160;前面提到如果 GOP 分组中的P帧丢失会造成解码端的图像发生错误,其实这个错误表现出来的就是马赛克。因为中间连续的运动信息丢失了，H.264 在解码的时候会根据前面的参考帧来补齐，但是补齐的并不是真正的运动变化后的数据，这样就会出现颜色色差的问题，这就是所谓的马赛克现象，如图：&#160; &#160; &#160; &#160;这种现象不是我们想看到的。为了避免这类问题的发生，一般如果发现 P 帧或者 I 帧丢失，就不显示本 GOP 内的所有帧，直到下一个 I 帧来后重新刷新图像。但是 I 帧是按照帧周期来的，需要一个比较长的时间周期，如果在下一个 I 帧来之前不显示后来的图像，那么视频就静止不动了，这就是出现了所谓的卡顿现象。如果连续丢失的视频帧太多造成解码器无帧可解，也会造成严重的卡顿现象。视频解码端的卡顿现象和马赛克现象都是因为丢帧引起的，最好的办法就是让帧尽量不丢。 &#160; &#160; &#160; &#160;知道 H.264 的原理和分组编码技术后所谓的秒开技术就比较简单了，只要发送方从最近一个 GOP 的 I 帧开发发送给接收方，接收方就可以正常解码完成的图像并立即显示。但这会在视频连接开始的时候多发一些帧数据造成播放延迟，只要在接收端播放的时候尽量让过期的帧数据只解码不显示，直到当前视频帧在播放时间范围之内即可。 1.3 编码延迟与码率&#160; &#160; &#160; &#160;前面四个延迟里面我们提到了编码延迟，编码延迟就是从 CCD 出来的 RGB 数据经过 H.264 编码器编码后出来的帧数据过程的时间。我们在一个 8 核 CPU 的普通客户机测试了最新版本 X.264 的各个分辨率的延迟，数据如下：&#160; &#160; &#160; &#160;从上面可以看出，超清视频的编码延迟会达到 50ms，解决编码延迟的问题只能去优化编码器内核让编码的运算更快，我们也正在进行方面的工作。 &#160; &#160; &#160; &#160;在 1080P 分辨率下，视频编码码率会达到 300KB/S，单个 I 帧数据大小达到 80KB，单个 P 帧可以达到 30KB，这对网络实时传输造成严峻的挑战。 二、网络传输质量因素&#160; &#160; &#160; &#160;实时互动视频一个关键的环节就是网络传输技术,不管是早期 VoIP，还是现阶段流行的视频直播，其主要手段是通过 TCP/IP 协议来进行通信。但是 IP 网络本来就是不可靠的传输网络，在这样的网络传输视频很容易造成卡顿现象和延迟。先来看看 IP 网络传输的几个影响网络传输质量关键因素。 2.1 TCP 和 UDP&#160; &#160; &#160; &#160;对直播有过了解的人都会认为做视频传输首选的就是 TCP + RTMP，其实这是比较片面的。在大规模实时多媒体传输网络中，TCP 和 RTMP 都不占优势。TCP 是个拥塞公平传输的协议，它的拥塞控制都是为了保证网络的公平性而不是快速到达，我们知道，TCP 层只有顺序到对应的报文才会提示应用层读数据，如果中间有报文乱序或者丢包都会在 TCP 做等待，所以 TCP 的发送窗口缓冲和重发机制在网络不稳定的情况下会造成延迟不可控，而且传输链路层级越多延迟会越大。 关于 TCP 的原理：http://coolshell.cn/articles/11564.html关于 TCP 重发延迟：http://weibo.com/p/1001603821691477346388 &#160; &#160; &#160; &#160;在实时传输中使用 UDP 更加合理，UDP 避免了 TCP 繁重的三次握手、四次挥手和各种繁杂的传输特性，只需要在 UDP 上做一层简单的链路 QoS 监测和报文重发机制，实时性会比 TCP 好，这一点从 RTP 和 DDCP 协议可以证明这一点，我们正式参考了这两个协议来设计自己的通信协议。 2.2 延迟&#160; &#160; &#160; &#160;要评估一个网络通信质量的好坏和延迟一个重要的因素就是 Round-Trip Time（网络往返延迟）,也就是 RTT。评估两端之间的 RTT 方法很简单，大致如下： 发送端方一个带本地时间戳 T1 的 ping 报文到接收端； 接收端收到 ping 报文，以 ping 中的时间戳 T1 构建一个携带 T1 的 pong 报文发往发送端； 发送端接收到接收端发了的 pong 时，获取本地的时间戳 T2，用 T2 – T1 就是本次评测的 RTT。 示意图如下：&#160; &#160; &#160; &#160;上面步骤的探测周期可以设为 1 秒一次。为了防止网络突发延迟增大，我们采用了借鉴了 TCP 的 RTT 遗忘衰减的算法来计算，假设原来的 RTT 值为 rtt，本次探测的 RTT 值为 keep_rtt。那么新的 RTT 为： &#160; &#160; &#160; &#160;new_rtt = (7 * rtt + keep_rtt) / 8 &#160; &#160; &#160; &#160;可能每次探测出来的 keep_rtt 会不一样，我们需要会计算一个 RTT 的修正值 rtt_var，算法如下： &#160; &#160; &#160; &#160;new_rtt_var = (rtt_var * 3 + abs(rtt – keep_rtt)) / 4 &#160; &#160; &#160; &#160;rtt_var 其实就是网络抖动的时间差值。 &#160; &#160; &#160; &#160;如果 RTT 太大，表示网络延迟很大。我们在端到端之间的网络路径同时保持多条并且实时探测其网络状态，如果 RTT 超出延迟范围会进行传输路径切换（本地网络拥塞除外）。 2.3 抖动和乱序&#160; &#160; &#160; &#160;UDP 除了延迟外，还会出现网络抖动。什么是抖动呢？举个例子，假如我们每秒发送 10 帧视频帧，发送方与接收方的延迟为 50MS，每帧数据用一个 UDP 报文来承载，那么发送方发送数据的频率是 100ms 一个数据报文，表示第一个报文发送时刻 0ms， T2 表示第二个报文发送时刻 100ms . . .，如果是理想状态下接收方接收到的报文的时刻依次是（50ms, 150ms, 250ms, 350ms….），但由于传输的原因接收方收到的报文的相对时刻可能是（50ms, 120ms, 240ms, 360ms ….），接收方实际接收报文的时刻和理想状态时刻的差值就是抖动。如下示意图：&#160; &#160; &#160; &#160;我们知道视频必须按照严格是时间戳来播放，否则的就会出现视频动作加快或者放慢的现象，如果我们按照接收到视频数据就立即播放，那么这种加快和放慢的现象会非常频繁和明显。也就是说网络抖动会严重影响视频播放的质量，一般为了解决这个问题会设计一个视频播放缓冲区，通过缓冲接收到的视频帧，再按视频帧内部的时间戳来播放既可以了。 &#160; &#160; &#160; &#160;UDP 除了小范围的抖动以外，还是出现大范围的乱序现象，就是后发的报文先于先发的报文到达接收方。乱序会造成视频帧顺序错乱，一般解决的这个问题会在视频播放缓冲区里做一个先后排序功能让先发送的报文先进行播放。 &#160; &#160; &#160; &#160;播放缓冲区的设计非常讲究，如果缓冲过多帧数据会造成不必要的延迟，如果缓冲帧数据过少，会因为抖动和乱序问题造成播放无数据可以播的情况发生，会引起一定程度的卡顿。关于播放缓冲区内部的设计细节我们在后面的小节中详细介绍。 2.4 丢包&#160; &#160; &#160; &#160;UDP 在传输过程还会出现丢包，丢失的原因有多种，例如：网络出口不足、中间网络路由拥堵、socket 收发缓冲区太小、硬件问题、传输损耗问题等等。在基于 UDP 视频传输过程中，丢包是非常频繁发生的事情，丢包会造成视频解码器丢帧，从而引起视频播放卡顿。这也是大部分视频直播用 TCP 和 RTMP 的原因，因为 TCP 底层有自己的重传机制，可以保证在网络正常的情况下视频在传输过程不丢。基于 UDP 丢包补偿方式一般有以下几种： 报文冗余&#160; &#160; &#160; &#160;报文冗余很好理解，就是一个报文在发送的时候发送 2 次或者多次。这个做的好处是简单而且延迟小，坏处就是需要额外 N 倍（N 取决于发送的次数）的带宽。 FEC&#160; &#160; &#160; &#160; Forward Error Correction，即向前纠错算法，常用的算法有纠删码技术（EC），在分布式存储系统中比较常见。最简单的就是 A B 两个报文进行 XOR（与或操作）得到 C，同时把这三个报文发往接收端，如果接收端只收到 AC,通过 A 和 C 的 XOR 操作就可以得到 B 操作。这种方法相对增加的额外带宽比较小，也能防止一定的丢包，延迟也比较小，通常用于实时语音传输上。对于 1080P 300KB/S 码率的超清晰视频，哪怕是增加 20% 的额外带宽都是不可接受的，所以视频传输不太建议采用 FEC 机制。 丢包重传&#160; &#160; &#160; &#160;丢包重传有两种方式，一种是 push 方式，一种是 pull 方式。Push 方式是发送方没有收到接收方的收包确认进行周期性重传，TCP 用的是 push 方式。pull 方式是接收方发现报文丢失后发送一个重传请求给发送方，让发送方重传丢失的报文。丢包重传是按需重传，比较适合视频传输的应用场景，不会增加太对额外的带宽，但一旦丢包会引来至少一个 RTT 的延迟。 2.5 MTU 和最大 UDP&#160; &#160; &#160; &#160;IP 网定义单个 IP 报文最大的大小，常用 MTU 情况如下： &#160; &#160; &#160; &#160;超通道 65535&#160; &#160; &#160; &#160;16Mb/s 令牌环 179144&#160; &#160; &#160; &#160;Mb/s 令牌环 4464&#160; &#160; &#160; &#160;FDDI 4352&#160; &#160; &#160; &#160;以太网 1500&#160; &#160; &#160; &#160;IEEE 802.3/802.2 1492&#160; &#160; &#160; &#160;X.25 576&#160; &#160; &#160; &#160;点对点（低时延）296 &#160; &#160; &#160; &#160;红色的是 Internet 使用的上网方式，其中 X.25 是个比较老的上网方式，主要是利用 ISDN 或者电话线上网的设备，也不排除有些家用路由器沿用 X.25 标准来设计。所以我们必须清晰知道每个用户端的 MTU 多大，简单的办法就是在初始化阶段用各种大小的 UDP 报文来探测 MTU 的大小。MTU 的大小会影响到我们视频帧分片的大小，视频帧分片的大小其实就是单个 UDP 报文最大承载的数据大小。 &#160; &#160; &#160; &#160;分片大小 = MTU – IP 头大小 – UDP 头大小 – 协议头大小;&#160; &#160; &#160; &#160;IP 头大小 = 20 字节， UDP 头大小 = 8 字节。 &#160; &#160; &#160; &#160;为了适应网络路由器小包优先的特性，我们如果得到的分片大小超过 800 时，会直接默认成 800 大小的分片。 三、传输模型&#160; &#160; &#160; &#160;我们根据视频编码和网络传输得到特性对 1080P 超清视频的实时传输设计了一个自己的传输模型，这个模型包括一个根据网络状态自动码率的编解码器对象、一个网络发送模块、一个网络接收模块和一个 UDP 可靠到达的协议模型。各个模块的关系示意图如下： 3.1 通信协议&#160; &#160; &#160; &#160;先来看通信协议，我们定义的通信协议分为三个阶段：接入协商阶段、传输阶段、断开阶段。 接入协商阶段&#160; &#160; &#160; &#160;主要是发送端发起一个视频传输接入请求，携带本地的视频的当前状态、起始帧序号、时间戳和 MTU 大小等，接收方在收到这个请求后，根据请求中视频信息初始化本地的接收通道，并对本地 MTU 和发送端 MTU 进行比较取两者中较小的回送给发送方， 让发送方按协商后的 MTU 来分片。示意图如下： 传输阶段&#160; &#160; &#160; &#160;传输阶段有几个协议，一个测试量 RTT 的 PING/PONG 协议、携带视频帧分片的数据协议、数据反馈协议和发送端同步纠正协议。其中数据反馈协议是由接收反馈给发送方的，携带接收方已经接收到连续帧的报文 ID、帧 ID 和请求重传的报文 ID 序列。同步纠正协议是由发送端主动丢弃发送窗口缓冲区中的报文后要求接收方同步到当前发送窗口位置，防止在发送主动丢弃帧数据后接收方一直要求发送方重发丢弃的数据。示意图如下: 断开阶段&#160; &#160; &#160; &#160;就一个断开请求和一个断开确认，发送方和接收方都可以发起断开请求。 3.2 发送&#160; &#160; &#160; &#160;发送主要包括视频帧分片算法、发送窗口缓冲区、拥塞判断算法、过期帧丢弃算法和重传。先一个个来介绍。 帧分片&#160; &#160; &#160; &#160;前面我们提到 MTU 和视频帧大小，在 1080P 下大部分视频帧的大小都大于 UDP 的 MTU 大小，那么就需要对帧进行分片，分片的方法很简单，按照先连接过程协商后的 MTU 大小来确定分片大小（确定分片大小的算法在 MTU 小节已经介绍过），然后将 帧数据按照分片大小切分成若干份，每一份分片以 segment 报文形式发往接收方。 重传&#160; &#160; &#160; &#160;重传比较简单，我们采用 pull 方式来实现重传，当接收方发生丢包，如果丢包的时刻 T1 + rtt_var&lt; 接收方当前的时刻 T2，就认为是丢包了，这个时候就会把所有满足这个条件丢失的报文 ID 构建一个 segment ack 反馈给发送方，发送方收到这个反馈根据 ID 到重发窗口缓冲区中查找对应的报文重发即可。 &#160; &#160; &#160; &#160;为什么要间隔一个 rtt_var 才认为是丢包了？因为报文是有可能乱序到达，所有要等待一个抖动周期后认为丢失的报文还没有来才确认是报文丢失了，如果检测到丢包立即发送反馈要求重传，有可能会让发送端多发数据，造成带宽让费和网络拥塞。 发送窗口缓冲区&#160; &#160; &#160; &#160;发送窗口缓冲区保存这所有正在发送且没有得到发送方连续 ID 确认的报文。当接收方反馈最新的连续报文 ID，发送窗口缓冲就会删除所有小于最新反馈连续的报文 ID，发送窗口缓冲区缓冲的报文都是为了重发而存在。这里解释下接收方反馈的连续的报文 ID，举个例子，假如发送方发送了 1. 2. 3. 4. 5，接收方收到 1.2. 4. 5。这个时候最小连续 ID = 2，如果后面又来了 3，那么接收方最小连续 ID = 5。 拥塞判断&#160; &#160; &#160; &#160;我们把当前时间戳记为 curr_T，把发送窗口缓冲区中最老的报文的时间戳记为 oldest_T，它们之间的间隔记为 delay，那么 &#160; &#160; &#160; &#160;delay = curr_T - oldest_T &#160; &#160; &#160; &#160;在编码器请求发送模块发送新的视频帧时，如果 delay &gt; 拥塞阈值 Tn，我们就认为网络拥塞了，这个时候会根据最近 20 秒接收端确认收到的数据大小计算一个带宽值，并把这个带宽值反馈给编码器，编码器收到反馈后，会根据带宽调整编码码率。如果多次发生要求降低码率的反馈，我们会缩小图像的分辨率来保证视频的流畅性和实时性。Tn 的值可以通过 rtt 和 rtt_var 来确定。 &#160; &#160; &#160; &#160;但是网络可能阶段性拥塞，过后却恢复正常，我们设计了一个定时器来定时检查发送方的重发报文数量和 delay，如果发现恢复正常，会逐步增大编码器编码码率，让视频恢复到指定的分辨率和清晰度。 过期帧丢弃&#160; &#160; &#160; &#160;在网络拥塞时可能发送窗口缓冲区中有很多报文正在发送，为了缓解拥塞和减少延迟我们会对整个缓冲区做检查，如果有超过一定阈值时间的 H.264 GOP 分组存在，我们会将这个 GOP 所有帧的报文从窗口缓冲区移除。并将它下一个 GOP 分组的 I 的帧 ID 和报文 ID 通过 wnd sync 协议同步到接收端上，接收端接收到这个协议，会将最新连续 ID 设置成同步过来的 ID。这里必须要说明的是如果频繁出现过期帧丢弃的动作会造成卡顿，说明当前网络不适合传输高分辨率视频，可以直接将视频设成更小的分辨率。 3.3 接收&#160; &#160; &#160; &#160;接收主要包括丢包管理、播放缓冲区、缓冲时间评估和播放控制，都是围绕播放缓冲区来实现的，一个个来介绍。 丢包管理&#160; &#160; &#160; &#160;丢包管理包括丢包检测和丢失报文 ID 管理两部分。丢包检测过程大致是这样的，假设播放缓冲区的最大报文 ID 为 max_id，网络上新收到的报文 ID 为 new_id，如果 max_id + 1 &lt; new_id，那么可能发生丢包，就会将 [max_id + 1, new_id -1] 区间中所有的 ID 和当前时刻作为 K/V 对加入到丢包管理器当中。如果 new_id &lt; max_id，那么就将丢包管理中的 new_id 对应的 K/V 对删除，表示丢失的报文已经收到。当收包反馈条件满足时，会扫描整个丢包管理，将达到请求重传的丢包 ID 加入到 segment ack 反馈消息中并发往发送方请求重传，如果 ID 被请求了重传，会将当前时刻设置为 K/V 对中，增加对应报文的重传计数器 count，这个扫描过程会统计对包管理器中单个重发最多报文的重发次数 resend_count。 缓冲时间评估&#160; &#160; &#160; &#160;在前面的抖动与乱序小节中我们提到播放端有个缓冲区，这个缓冲区过大时延迟就大，缓冲区过小时又会出现卡顿现象，我们针对这个问题设计了一个缓冲时间评估的算法。缓冲区评估先会算出一个 cache timer，cache timer 是通过扫描对包管理得到的 resend count 和 rtt 得到的，我们知道从请求重传报文到接收方收到重传的报文的时间间隔是一个 RTT 周期，所以 cache timer 的计算方式如下。 &#160; &#160; &#160; &#160;cache timer = (2 resend_count+ 1) (rtt + rtt_var) / 2 &#160; &#160; &#160; &#160;有可能 cache timer 计算出来很小（小于视频帧之间间隔时间 frame timer），那么 cache timer = frame timer，也就是说网络再好，缓冲区缓冲区至少 1 帧视频的数据，否则缓冲区是毫无意义的。 &#160; &#160; &#160; &#160;如果单位时间内没有丢包重传发生，那么 cache timer 会做适当的缩小，这样做的好处是当网络间歇性波动造成 cache timer 很大，恢复正常后 cache timer 也能恢复到相对小位置，缩减不必要的缓冲区延迟。 播放缓冲区&#160; &#160; &#160; &#160;我们设计的播放缓冲区是按帧 ID 为索引的有序循环数组，数组内部的单元是视频帧的具体信息：帧 ID、分片数、帧类型等。缓冲区有两个状态：waiting 和 playing，waiting 状态表示缓冲区处于缓冲状态，不能进行视频播放直到缓冲区中的帧数据达到一定的阈值。Playing 状态表示缓冲区进入播放状态，播放模块可以从中取出帧进行解码播放。我们来介绍下这两个状态的切换关系： 当缓冲区创建时会被初始化成 waiting 状态。 当缓冲区中缓冲的最新帧与最老帧的时间戳间隔 &gt; cache timer 时，进入 playing 状态并更当前时刻设成播放绝对时间戳 play ts。 当缓冲区处于 playing 状态且缓冲区是没有任何帧数据，进入 waiting 状态直到触发第 2 步。 &#160; &#160; &#160; &#160;播放缓冲区的目的就是防止抖动和应对丢包重传，让视频流能按照采集时的频率进行播放，播放缓冲区的设计极其复杂，需要考虑的因素很多，实现的时候需要慎重。 播放控制&#160; &#160; &#160; &#160;接收端最后一个环节就是播放控制，播放控制就是从缓冲区中拿出有效的视频帧进行解码播放。但是怎么拿？什么时候拿？我们知道视频是按照视频帧从发送端携带过来的相对时间戳来做播放，我们每一帧视频都有一个相对时间戳 TS，根据帧与帧之间的 TS 的差值就可以知道上一帧和下一帧播放的时间间隔，假如上一帧播放的绝对时间戳为 prev_play_ts，相对时间戳为 prev_ts，当前系统时间戳为 curr_play_ts，当前缓冲区中最小序号帧的相对时间戳为 frame_ts，只要满足： &#160; &#160; &#160; &#160;Prev_play_ts + (frame_ts – prev_ts) &lt; curr_play_ts 且这一帧数据是所有的报文都收齐了 &#160; &#160; &#160; &#160;这两个条件就可以进行解码播放，取出帧数据后将 Prev_play_ts = cur_play_ts，但更新 prev_ts 有些讲究，为了防止缓冲延迟问题我们做了特殊处理。 &#160; &#160; &#160; &#160;如果 frame_ts + cache timer &lt; 缓冲区中最大帧的 ts，表明缓冲的时延太长，则 prev_ts = 缓冲区中最大帧的 ts - cache timer。 否则 prev_ts = frame_ts。 四、测量&#160; &#160; &#160; &#160;再好的模型也需要有合理的测量方式来验证，在多媒体这种具有时效性的传输领域尤其如此。一般在实验室环境我们采用 netem 来进行模拟公网的各种情况进行测试，如果在模拟环境已经达到一个比较理想的状态后会组织相关人员在公网上进行测试。下面来介绍怎么来测试我们整个传输模型的。 4.1 netem 模拟测试&#160; &#160; &#160; &#160;Netem 是 Linux 内核提供的一个网络模拟工具，可以设置延迟、丢包、抖动、乱序和包损坏等，基本能模拟公网大部分网络情况。 关于 netem 可以访问它的官网：https://wiki.linuxfoundation.org/networking/netem &#160; &#160; &#160; &#160;我们在实验环境搭建了一个基于服务器和客户端模式的测试环境，下面是测试环境的拓扑关系图： &#160; &#160; &#160; &#160;我们利用 Linux 来做一个路由器，服务器和收发端都连接到这个路由器上，服务器负责客户端的登记、数据转发、数据缓冲等，相当于一个简易的流媒体服务器。Sender 负责媒体编码和发送，receiver 负责接收和媒体播放。为了测试延迟，我们把 sender 和 receiver 运行在同一个 PC 机器上，在 sender 从 CCD 获取到 RGB 图像时打一个时间戳，并把这个时间戳记录在这一帧数据的报文发往 server 和 receiver，receiver 收到并解码显示这帧数据时，通过记录的时间戳可以得到整个过程的延迟。我们的测试用例是用 1080P 码率为 300KB/S 视频流，在 router 用 netem 上模拟了以下几种网络状态： 环路延迟 10m，无丢包，无抖动，无乱序 环路延迟 30ms，丢包 0.5%，抖动 5ms, 2% 乱序 环路延迟 60ms，丢包 1%，抖动 20ms, 3% 乱序，0.1% 包损坏 环路延迟 100ms，丢包 4%，抖动 50ms, 4% 乱序，0.1% 包损坏 环路延迟 200ms，丢包 10%，抖动 70ms, 5% 乱序，0.1% 包损坏 环路延迟 300ms，丢包 15%，抖动 100ms, 5% 乱序，0.1% 包损坏 &#160; &#160; &#160; &#160;因为传输机制采用的是可靠到达，那么检验传输机制有效的参数就是视频延迟，我们统计 2 分钟周期内最大延迟，以下是各种情况的延迟曲线图：&#160; &#160; &#160; &#160;从上图可以看出，如果网络控制在环路延迟在 200ms 丢包在 10% 以下，可以让视频延迟在 500ms 毫秒以下，这并不是一个对网络质量要求很苛刻的条件。所以我们在后台的媒体服务部署时，尽量让客户端到媒体服务器之间的网络满足这个条件，如果网路环路延迟在 300ms 丢包 15% 时，依然可以做到小于 1 秒的延迟，基本能满足双向互动交流。 4.2 公网测试&#160; &#160; &#160; &#160;公网测试相对比较简单，我们将 Server 部署到 UCloud 云上，发送端用的是上海电信 100M 公司宽带，接收端用的是河北联通 20M 小区宽带，环路延迟在 60ms 左右。总体测试下来 1080P 在接收端观看视频流畅自然，无抖动，无卡顿，延迟统计平均在 180ms 左右。 五、坑&#160; &#160; &#160; &#160;在整个 1080P 超清视频的传输技术实现过程中，我们遇到过比较多的坑。大致如下： Socket 缓冲区问题&#160; &#160; &#160; &#160;我们前期开发阶段都是使用 socket 默认的缓冲区大小，由于 1080P 图像帧的数据非常巨大（关键帧超过 80KB），我们发现在在内网测试没有设置丢包的网络环境发现接收端有严重的丢包，经查证是 socket 收发缓冲区太小造成丢包的，后来我们把 socket 缓冲区设置到 128KB 大小，问题解决了。 H.264 B 帧延迟问题&#160; &#160; &#160; &#160;前期我们为了节省传输带宽和防丢包开了 B 帧编码，由于 B 帧是前后双向预测编码的，会在编码期滞后几个帧间隔时间，引起了超过 100ms 的编码延时，后来我们为了实时性干脆把 B 帧编码选项去掉。 Push 方式丢包重传&#160; &#160; &#160; &#160;在设计阶段我们曾经使用发送端主动 push 方式来解决丢包重传问题，在测试过程发现在丢包频繁发生的情况下至少增加了 20% 的带宽消耗，而且容易带来延迟和网络拥塞。后来几经论证用现在的 pull 模式来进行丢包重传。 Segment 内存问题&#160; &#160; &#160; &#160;在设计阶段我们对每个视频缓冲区中的帧信息都是动态分配内存对象的，由于 1080P 在传输过程中每秒会发送 400 - 500 个 UDP 报文，在 PC 端长时间运行容易出现内存碎片，在服务器端出现莫名其妙的 clib 假内存泄露和并发问题。我们实现了一个 memory slab 管理频繁申请和释放内存的问题。 音频和视频数据传输问题&#160; &#160; &#160; &#160;在早期的设计之中我们借鉴了 FLV 的方式将音频和视频数据用同一套传输算法传输，好处就是容易实现，但在网络波动的情况下容易引起声音卡顿，也无法根据音频的特性优化传输。后来我们把音频独立出来，针对音频的特性设计了一套低延迟高质量的音频传输体系，定点对音频进行传输优化。 &#160; &#160; &#160; &#160;后续的工作是重点放在媒体器多点分布、多点并发传输、P2P 分发算法的探索上，尽量减少延迟和服务带宽成本,让传输变的更高效和更低廉。 Q&amp;A提问：在优化到 500ms 方案中，哪一块是最关键的？袁荣喜：主要是丢包重传 拥塞和播放缓冲这三者之间的协调工作最为关键，要兼顾延迟控制和视频流畅性。 提问：多方视频和单方有哪些区别，用到了 CDN 推流吗？袁荣喜：我们公司是做在线教育的，很多场景需要老师和学生交谈，用 CDN 推流方式延迟很大，我们这个视频主要是解决多方通信之间交谈延迟的问题。我们现在观看放也有用 CDN 推流，但只是单纯的观看。我们也在研发基于 UDP 的观看端分发协议，目前这部分工作还没有完成。 参考阅读 移动直播技术秒开优化经验（含PPT） 揭秘百万人围观的Facebook视频直播]]></content>
      <categories>
        <category>科普分享</category>
      </categories>
      <tags>
        <tag>移动直播</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[插件开发中的资源问题分析及填坑处理]]></title>
    <url>%2F2016%2F09%2F05%2F%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%E5%8F%8A%E5%A1%AB%E5%9D%91%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;做插件开发有两个问题需要解决，一个是资源文件加载，另一个是关于四大组件生命周期的管理。这里我们就简单分析会遇到那些坑，和一些简单的处理方法或者思路。&#160; &#160; &#160; &#160;插件开发目前已经不是什么最新技术了，目前市面上已有很多成熟的方案和开源工程，比如任玉刚的dynamic-load-apk、阿里的AndFix和dexposed、360的DroidPlugin、QQ空间的nuwa。各家实现方案也是各有不同，这些开源库大多已经广泛应用于很多市面上的软件。&#160; &#160; &#160; &#160;说到未来，不得不提一下ReactNative，移动应用web化一定是一个必然的趋势，就好像曾经的桌面应用由C/S到B/S的转变。而怎么web化才是关键之处。但目前RN在IOS开发中优势很明显，在Android中却是挖坑不断。 普通插件开发开发前提&#160; &#160; &#160; &#160;Android为我们从ClassLoader派生出了两个类：DexClassLoader和PathClassLoader。在加载类的时候，是执行父类ClassLoader的loadClass方法：1234567891011121314151617protected Class&lt;?&gt; loadClass(String className, boolean resolve) throws ClassNotFoundException &#123; Class&lt;?&gt; clazz = findLoadedClass(className); if (clazz == null) &#123; try &#123; clazz = parent.loadClass(className, false); &#125; catch (ClassNotFoundException e) &#123; // Don't want to see this. &#125; if (clazz == null) &#123; clazz = findClass(className); &#125; &#125; return clazz;&#125; &#160; &#160; &#160; &#160;因此DexClassLoader和PathClassLoader都属于符合双亲委派模型的类加载器（因为它们没有重载loadClass方法）。也就是说，它们在加载一个类之前，回去检查自己以及自己以上的类加载器是否已经加载了这个类。如果已经加载过了，就会直接将之返回，而不会重复加载。&#160; &#160; &#160; &#160;这两者的区别在于DexClassLoader需要提供一个可写的outpath路径，用来释放.apk包或者.jar包中的dex文件。换个说法来说，就是PathClassLoader不能主动从zip包中释放出dex，因此只支持直接操作dex格式文件，或者已经安装的apk（因为已经安装的apk在cache中存在缓存的dex文件）。而DexClassLoader可以支持.apk、.jar和.dex文件，并且会在指定的outpath路径释放出dex文件。&#160; &#160; &#160; &#160;因此，我们要实现插件开发，需要用DexClassLoader。 基本流程&#160; &#160; &#160; &#160;如果只需要加载插件apk中一个普通的类，只要构造一个DexClassLoader，它的构造方法对每个参数已经说明的很清楚了，我们可以试验一下。&#160; &#160; &#160; &#160;新建一个插件工程TestPlugin，里面放一个类Plugin.java，再放一个简单的方法，即TestPlugin/src/com/example/plugin/Plugin.java：12345public class Plugin&#123; public String getCommonStr()&#123; return "COMMON"; &#125;&#125; &#160; &#160; &#160; &#160;然后新建一个宿主工程TestHost，在MainActivity里面写一个加载插件的方法，即TestHost/src/com/example/host/MainActivity.java:12345678910111213141516171819202122232425262728293031323334353637public class MainActivity extends Activity &#123; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); loadPluginClass(); &#125;private void loadPluginClass()&#123; ...... //定义DexClassLoader //第一个参数：是dex压缩文件的路径 //第二个参数：是dex解压缩后存放的目录 //第三个参数：是C/C++依赖的本地库文件目录,可以为null //第四个参数：是上一级的类加载器 DexClassLoader dexClassLoader = new DexClassLoader(this.getCacheDir().getAbsolutePath() + File.separator + "TestPlugin.apk", this.getCacheDir().getAbsolutePath(), null, getApplicationContext().getClassLoader()); Class&lt;?&gt; pluginClass = dexClassLoader .loadClass("com.example.plugin.Plugin"); if(pluginClass == null)&#123; Log.e(TAG, "plugin class cann't be found"); return; &#125; Object pluginObject = pluginClass.newInstance(); Method pluginMethod = pluginClass.getMethod("getCommonStr"); if(pluginMethod == null)&#123; Log.e(TAG, "plugin method cann't be found"); return; &#125; String methodStr = (String) pluginMethod .invoke(pluginObject); Log.e(TAG, "Print Method str = " + methodStr); ......&#125;&#125; &#160; &#160; &#160; &#160;先安装宿主程序TestHost.apk，然后将插件TestPlugin.apk放到/data/data/com.example.host/cache/下面，再次运行宿主程序，会打印如下log：&#160; &#160; &#160; &#160;Print Method str = COMMON&#160; &#160; &#160; &#160;这个应该比较随意了，会使用DexClassLoader这个类的开发者都是轻车熟路。 加载资源普通资源&#160; &#160; &#160; &#160;我们知道插件apk中的资源文件是无法直接加载的，因为插件apk并没有安装，所以没有给每个资源生成特定的资源id，所以我们没法使用R.XXX去引用。&#160; &#160; &#160; &#160;不过我们通过android系统安装apk时对资源文件的处理流程中发现可以通过AssetManager这个类完成对插件中资源的引用。Java的源码中发现，它有一个私有方法addAssetPath，只需要将apk的路径作为参数传入，我们就可以获得对应的AssetsManager对象，然后我们就可以使用AssetsManager对象，创建一个Resources对象，然后就可以从Resource对象中访问apk中的资源了。总结如下： 新建一个AssetManager对象 通过反射调用addAssetPath方法 以AssetsManager对象为参数，创建Resources对象即可 &#160; &#160; &#160; &#160;我们测试demo可以写一个工具类，省略了一部分，如下：1234567891011121314151617181920public class PluginBaseImpl extends PluginBase &#123; ...... @Override public Resources loadResource(Context parentContext, String apkPath) &#123; Resources ret = null; try &#123; AssetManager assetManager = AssetManager.class.newInstance(); Method method = assetManager.getClass().getDeclaredMethod("addAssetPath", String.class); method.setAccessible(true); method.invoke(assetManager, apkPath); ret = new Resources(assetManager, parentContext.getResources().getDisplayMetrics(), parentContext.getResources().getConfiguration()); Log.e(TAG, "loadResources succeed"); &#125; catch (Exception e) &#123; Log.e(TAG, "loadResources faided"); e.printStackTrace(); &#125; return ret; &#125; ......&#125; &#160; &#160; &#160; &#160;然后我们再插件工程里面再添加一个方法，再放入一个简单的资源：123456public class Plugin&#123; ...... public String getContextStr(Resources resources)&#123; return resources.getString(R.string.plugin_str);//&lt;string name="plugin_str"&gt;PLUGIN&lt;/string&gt; &#125;&#125; &#160; &#160; &#160; &#160;测试如下：123456789101112131415161718192021public class MainActivity extends Activity &#123; ...省略一些初始化代码... private void loadPluginClass()&#123; ...... //构造一个DexClassLoader DexClassLoader dexClassLoader = mPluginBase.makeDexClassLoader(APK_PATH, DEX_PATH, null, getApplicationContext().getClassLoader()); Class&lt;?&gt; pluginClass = mDexClassLoader.loadClass("com.example.plugin.Plugin"); ...... Object pluginObject = pluginClass.newInstance(); //加载插件apk资源 Resources pluginResources = mPluginBase.loadResource(this, APK_PATH); Method m2 = pluginClass.getMethod("getContextStr", Resources.class); String methodStr2 = (String) m2.invoke(pluginObject, pluginResources); Log.e(TAG, "Print Resource str = " + methodStr2); ...... &#125;&#125; &#160; &#160; &#160; &#160;运行之后，打印log如下：&#160; &#160; &#160; &#160;Print Resource str = PLUGIN Layout资源&#160; &#160; &#160; &#160;如果要使用插件apk里面的layout资源，比如引用某个布局文件TestPlugin/res/layout/plugin.xml，就需要做一做处理。&#160; &#160; &#160; &#160;一般从layout转换成view需要用到LayoutInflate，比如：1View view = LayoutInflater.from(context).inflate(R.layout.plugin, null); &#160; &#160; &#160; &#160;但是这个context不能直接传宿主程序的context，否则回报一个资源id没有找到异常。我们跟着LayoutInflate的源码进去看看，问题出在哪儿：12345678910111213141516public abstract class LayoutInflater &#123; //Inflate时会调用到 public View inflate(int resource, ViewGroup root, boolean attachToRoot) &#123; //这句返回的resource是宿主程序ContextImpl里的resource，即宿主程序的resource final Resources res = getContext().getResources(); ...... //所以这里在宿主resource里当然找不到插件资源id了，这个里面抛出了异常 final XmlResourceParser parser = res.getLayout(resource); try &#123; return inflate(parser, root, attachToRoot); &#125; finally &#123; parser.close(); &#125; &#125;&#125; &#160; &#160; &#160; &#160;我们看到inflate时还是在宿主程序的资源里查找了插件资源，因此回报异常。不过我们可以投机取巧一下，重写一个LayoutInflate的Inflate第二个重载方法。在插件工程里可以做如下测试：12345678910111213141516171819202122232425262728public class Plugin&#123; ...... public LinearLayout getLinearLayout(Context context, final Resources resources)&#123; LayoutInflater inflater = new LayoutInflater(context) &#123; @Override public LayoutInflater cloneInContext(Context newContext) &#123; // TODO Auto-generated method stub return null; &#125; @Override public View inflate(int resource, ViewGroup root, boolean attachToRoot) &#123;// final Resources res = getContext().getResources(); //注释掉这行 final Resources res = resources; //替换为插件apk资源 final XmlResourceParser parser = res.getLayout(resource); try &#123; return inflate(parser, root, attachToRoot); &#125; finally &#123; parser.close(); &#125; &#125; &#125;; return (LinearLayout) inflater.inflate(R.layout.plugin_layout, null); &#125;&#125; &#160; &#160; &#160; &#160;然后在宿主程序里写上测试demo：123456789101112131415161718192021public class MainActivity extends Activity &#123; ...省略一些初始化代码... private void loadPluginClass()&#123; ...... //构造一个DexClassLoader DexClassLoader dexClassLoader = mPluginBase.makeDexClassLoader(APK_PATH, DEX_PATH, null, getApplicationContext().getClassLoader()); Class&lt;?&gt; pluginClass = mDexClassLoader.loadClass("com.example.plugin.Plugin"); ...... Object pluginObject = pluginClass.newInstance(); //加载插件apk资源 Resources pluginResources = mPluginBase.loadResource(this, APK_PATH); //测试插件layout文件 Method m3 = pluginClass.getMethod("getLinearLayout", Context.class, Resources.class); LinearLayout pluginView = (LinearLayout) m3.invoke(pluginObject, this, pluginResources ); this.addContentView(pluginView, new RelativeLayout.LayoutParams(LayoutParams.MATCH_PARENT, LayoutParams.MATCH_PARENT)); ...... &#125;&#125; &#160; &#160; &#160; &#160;经过测试，插件的layout布局被加入到了宿主界面上，图片就不贴了。 另外三种方式&#160; &#160; &#160; &#160;上面的方法其实还是有些繁琐，如果要封装的完善一些可以尝试下面三种方案： 创建一个自己的ContextImpl，Override其方法 通过反射，直接替换当前context的mResources私有成员变量 反射替换ActivityThread里的Instrumentation，将插件资源和宿主资源整合 (1) 创建自己的Context：&#160; &#160; &#160; &#160;要构建自己的Context，就得继承ContextWrapper类，（Context类和它的一些子类大家应该都清楚）然后重写里面的一些重要方法。实例代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class PluginContext extends ContextWrapper &#123; private static final String TAG = "PluginContext"; private DexClassLoader mClassLoader ; private Resources mResources; private LayoutInflater mInflater; PluginContext(Context context, String pluginPath, String optimizedDirectory, String libraryPath) &#123; super(context.getApplicationContext()); Resources resc = context.getResources(); //隐藏API是这样的 //AssetManager assets = new AssetManager(); AssetManager assets = AssetManager.class.newInstance(); assets.addAssetPath(pluginPath); mClassLoader = new DexClassLoader(pluginPath, optimizedDirectory, libraryPath, context.getClassLoader()); mResources = new Resources(assets, resc.getDisplayMetrics(), resc.getConfiguration(), resc.getCompatibilityInfo(), null); //隐藏API是这样的 //mInflater = PolicyManager.makeNewLayoutInflater(this); mInflater = new LayoutInflater(context) &#123; @Override public LayoutInflater cloneInContext(Context newContext) &#123; // TODO Auto-generated method stub return null; &#125; @Override public View inflate(int resource, ViewGroup root, boolean attachToRoot) &#123;// final Resources res = getContext().getResources(); final Resources res = mResources; final XmlResourceParser parser = res.getLayout(resource); try &#123; return inflate(parser, root, attachToRoot); &#125; finally &#123; parser.close(); &#125; &#125; &#125;; &#125; @Override public ClassLoader getClassLoader() &#123; return mClassLoader ; &#125; @Override public AssetManager getAssets() &#123; return mResources.getAssets(); &#125; @Override public Resources getResources() &#123; return mResources; &#125; @Override public Object getSystemService(String name) &#123; if (name == Context.LAYOUT_INFLATER_SERVICE) return mInflater; return super.getSystemService(name); &#125; private Theme mTheme; @Override public Resources.Theme getTheme() &#123; if (mTheme == null) &#123; int resid = Resources.selectDefaultTheme(0, getBaseContext().getApplicationInfo().targetSdkVersion); mTheme = mResources.newTheme(); mTheme.applyStyle(resid, true); &#125; return mTheme; &#125;&#125; &#160; &#160; &#160; &#160;这样我们插件的Context就构造完成了，以后就可以使用这个Context加载插件中的资源文件了。 (2) 替换当前context的mResources私有成员变量：&#160; &#160; &#160; &#160;这个需要在Activity的attachBaseContext方法中替换它的Context，如下：123456789101112131415161718192021222324public class MainActivity extends Activity &#123; ...... @Override protected void attachBaseContext(Context newBase) &#123; replaceContextResources(newBase); super.attachBaseContext(newBase); &#125; /** * 使用反射的方式，使用mPluginResources对象，替换Context的mResources对象 * @param context */ public void replaceContextResources(Context context)&#123; try &#123; Field field = context.getClass().getDeclaredField("mResources"); field.setAccessible(true); field.set(context, mPluginResources); Log.e(TAG, "replace resources succeed"); &#125; catch (Exception e) &#123; Log.e(TAG, "replace resources failed"); e.printStackTrace(); &#125; &#125; ......&#125; (3) 反射替换ActivityThread里的Instrumentation，将插件资源和宿主资源整合:&#160; &#160; &#160; &#160;AssetManager的addAssetPath()法调用native层AssetManager对象的addAssetPath()法，通过查看c++代码可以知道，该方法可以被调用多次，每次调用都会把对应资源添加起来，而后来添加的在使用资源是会被首先搜索到。可以怎么理解，C++层的AssetManager有一个存放资源的栈，每次调用addAssetPath()法都会把资源对象压如栈，而在读取搜索资源时是从栈顶开始搜索，找不到就往下查。所以我们可以这样来处理AssetManager并得到Resources。&#160; &#160; &#160; &#160;使用到资源的地方归纳起来有两处，一处是在Java代码中通过Context.getResources获取，一处是在xml文件（如布局文件）里指定资源，其实xml文件里最终也是通过Context来获取资源的只不过是他一般获取的是Resources里的AssetManager。所以，我们可以在Context对象被创建后且还未使用时把它里面的Resources（mResources）替换掉。整个应用的Context数目等于Application+Activity+Service的数目，Context会在这几个类创建对象的时候创建并添加进去。而这些行为都是在ActivityTHread和Instrumentation里做的。&#160; &#160; &#160; &#160;以Activity为例，步骤如下：&#160; &#160; &#160; &#160;1. Activity对象的创建是在ActivityThread里调用Instrumentation的newActivity方法：123456789101112131415//ActivityThread类private Activity performLaunchActivity(ActivityClientRecord r, Intent customIntent) &#123; ...... java.lang.ClassLoader cl = r.packageInfo.getClassLoader(); activity = mInstrumentation.newActivity( cl, component.getClassName(), r.intent); ...... &#125; //Instrumentation类 public Activity newActivity(ClassLoader cl, String className, Intent intent) throws InstantiationException, IllegalAccessException, ClassNotFoundException &#123; return (Activity)cl.loadClass(className).newInstance(); &#125; &#160; &#160; &#160; &#160;2.Context对象的创建是在ActivityThread里调用createBaseContextForActivity方法:123456//ActivityThread类private Activity performLaunchActivity(ActivityClientRecord r, Intent customIntent) &#123; ...... Context appContext = createBaseContextForActivity(r, activity); ...... &#125; &#160; &#160; &#160; &#160;3.Activity绑定Context是在ActivityThread里调用Activity对象的attach方法，其中appContext就是上面创建的Context对象:123456789//ActivityThread类private Activity performLaunchActivity(ActivityClientRecord r, Intent customIntent) &#123; ...... activity.attach(appContext, this, getInstrumentation(), r.token, r.ident, app, r.intent, r.activityInfo, title, r.parent, r.embeddedID, r.lastNonConfigurationInstances, config, r.voiceInteractor); ...... &#125; &#160; &#160; &#160; &#160;替换掉Activity里Context里的Resources最好要早，基于上面的观察，我们可以在调用Instrumentation的callActivityOnCreate（）方法时把Resources替换掉。那么问题又来了，我们如何控制callActivityOnCreate（）方法的执行，这里又得使用hook的思想了，即把ActivityThread里面的Instrumentation对象（mInstrumentation）给替换掉，同样得使用反射。步骤如下:&#160; &#160; &#160; &#160;1. 获取ActivityThread对象:&#160; &#160; &#160; &#160;ActivityThread里面有一个静态方法，该方法返回的是ActivityThread对象本身，所以我们可以调用该方法来获取ActivityTHread对象:1234//ActivityThread类 public static ActivityThread currentActivityThread() &#123; return sCurrentActivityThread; &#125; &#160; &#160; &#160; &#160;然而ActivityThread是被hide的，所以得通过反射来处理，处理如下：1234567//获取ActivityThread类Class&lt;?&gt; activityThreadClass = Class.forName("android.app.ActivityThread");//获取currentActivityThread方法Method currentActivityThreadMethod = activityThreadClass.getDeclaredMethod("currentActivityThread");currentActivityThreadMethod.setAccessible(true);//获取ActivityThread对象Object CurrentActivityThread = currentActivityThreadMethod.invoke(null); &#160; &#160; &#160; &#160;2. 获取ActivityThread里的Instrumentation对象:123Field mInstrumentationField = activityThreadClass.getDeclaredField("mInstrumentation");mInstrumentationField.setAccessible(true);Instrumentation mInstrumentation = (Instrumentation) mInstrumentationField.get(CurrentActivityThread); &#160; &#160; &#160; &#160;3. 构建我们自己的Instrumentation对象，并从写callActivityOnCreate方法在callActivityOnCreate方法里要先获取当前Activity对象里的Context（mBase），再获取Context对象里的Resources（mResources）变量，在把mResources变量指向我们构造的Resources对象，做到移花接木。构建我们的MyInstrumentation类：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class MyInstrumentation extends Instrumentation &#123; private Instrumentation mInstrumentationParent; private Context mContextParent; public MyInstrumentation(Instrumentation instrumentation, Context context) &#123; super(); mInstrumentationParent = instrumentation; mContextParent = context; &#125; @Override public void callActivityOnCreate(Activity activity, Bundle icicle) &#123; try &#123; Field mBaseField = Activity.class.getSuperclass().getSuperclass().getDeclaredField("mBase"); mBaseField.setAccessible(true); Context mBase = (Context) mBaseField.get(activity); Class&lt;?&gt; contextImplClazz = Class.forName("android.app.ContextImpl"); Field mResourcesField = contextImplClazz.getDeclaredField("mResources"); mResourcesField.setAccessible(true); String dexPath = activity.getCacheDir() + File.separator + "TestPlugin.apk"; String dexPath2 = mContextParent.getApplicationContext().getPackageCodePath(); AssetManager assetManager = AssetManager.class.newInstance(); Method addAssetPath = assetManager.getClass().getDeclaredMethod("addAssetPath", String.class); addAssetPath.setAccessible(true); addAssetPath.invoke(assetManager, dexPath); addAssetPath.invoke(assetManager, dexPath2); Method ensureStringBlocksMethod = AssetManager.class.getDeclaredMethod("ensureStringBlocks"); ensureStringBlocksMethod.setAccessible(true); ensureStringBlocksMethod.invoke(assetManager); Resources superRes = mContextParent.getResources(); Resources resources = new Resources(assetManager, superRes.getDisplayMetrics(), superRes.getConfiguration()); mResourcesField.set(mBase, resources); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; super.callActivityOnCreate(activity, icicle); &#125;&#125; &#160; &#160; &#160; &#160;4. 最后，使ActivityThread里面的mInstrumentation变量指向我们构建的MyInstrumentation对象。12345678910111213141516171819202122public static void hookResources(Context context)&#123; //获取ActivityThread类 Class&lt;?&gt; activityThreadClass; try &#123; activityThreadClass = Class.forName("android.app.ActivityThread"); //获取currentActivityThread方法 Method currentActivityThreadMethod = activityThreadClass.getDeclaredMethod("currentActivityThread"); currentActivityThreadMethod.setAccessible(true); //获取ActivityThread对象 Object CurrentActivityThread = currentActivityThreadMethod.invoke(null); //获取Instrumentation变量 Field mInstrumentationField = activityThreadClass.getDeclaredField("mInstrumentation"); mInstrumentationField.setAccessible(true); Instrumentation mInstrumentation = (Instrumentation) mInstrumentationField.get(CurrentActivityThread); //构建自己的Instrumentation对象 Instrumentation proxy = new MyInstrumentation(mInstrumentation, context); //移花接木 mInstrumentationField.set(CurrentActivityThread, proxy); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 加载SO库流程分析和填坑&#160; &#160; &#160; &#160;插件加载带有动态库的apk时，会报UnsatisfiedLinkError找不到动态库的错误原因是我们没有动态指定so库的路径。&#160; &#160; &#160; &#160;解决方法是在DexClassLoader中第三个参数书指定so库的目录路径，因此我们需要把动态库给解压出来放到data/data/xx（package）目录下。&#160; &#160; &#160; &#160;这个，我把so文件放到了/data/data/com.example.host/cache/下面，然后给我们的DexClassLoader第三个参数指定了这个目录，然后在插件工程里调用System.loadLibrary方法就不会报错了。 &#160; &#160; &#160; &#160;关于解压so文件和获取手机CPU的ABI类型这里就不在赘述，网上也是大把的代码。我们主要分析一下Android找寻so和加载的流程： SO库加载过程&#160; &#160; &#160; &#160;在Android中如果想使用so的话，首先得先加载，加载现在主要有两种方法，一种是直接System.loadLibrary方法加载工程中的libs目录下的默认so文件，这里的加载文件名是xxx，而整个so的文件名为：libxxx.so。还有一种是加载指定目录下的so文件，使用System.load方法，这里需要加载的文件名是全路径，比如：xxx/xxx/libxxx.so。&#160; &#160; &#160; &#160;我们可以看看System类的这两个方法：1234567public static void load(String pathName) &#123; Runtime.getRuntime().load(pathName, VMStack.getCallingClassLoader());&#125;public static void loadLibrary(String libName) &#123; Runtime.getRuntime().loadLibrary(libName, VMStack.getCallingClassLoader());&#125; &#160; &#160; &#160; &#160;这两个方法都会进入到Runtime类的不同方法中，我们继续跟进去：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//load方法比较简单 void load(String absolutePath, ClassLoader loader) &#123; if (absolutePath == null) &#123; throw new NullPointerException("absolutePath == null"); &#125; //都会调用doLoad方法 String error = doLoad(absolutePath, loader); if (error != null) &#123; throw new UnsatisfiedLinkError(error); &#125; &#125; //loadLibrary比较复杂 void loadLibrary(String libraryName, ClassLoader loader) &#123; if (loader != null) &#123;//这个loader就是加载目标类的ClassLoader，宿主工程为系统指定的PathClassLoader，插件工程为我们构造的DexClassLoader //首先会从一些指定目录中查找指定名字的so文件 String filename = loader.findLibrary(libraryName); //如果没有找到就会抛异常 if (filename == null) &#123;//这个异常就是我们没有指定DexClassLoader第三个参数时报的异常 // It's not necessarily true that the ClassLoader used // System.mapLibraryName, but the default setup does, and it's // misleading to say we didn't find "libMyLibrary.so" when we // actually searched for "liblibMyLibrary.so.so". throw new UnsatisfiedLinkError(loader + " couldn't find \"" + System.mapLibraryName(libraryName) + "\""); &#125; //都会调用doLoad方法 String error = doLoad(filename, loader); if (error != null) &#123; throw new UnsatisfiedLinkError(error); &#125; return; &#125; //下面逻辑是当指定ClassLoader为null时，就在一些系统so库目录中查找 String filename = System.mapLibraryName(libraryName); List&lt;String&gt; candidates = new ArrayList&lt;String&gt;(); String lastError = null; for (String directory : mLibPaths) &#123; String candidate = directory + filename; candidates.add(candidate); if (IoUtils.canOpenReadOnly(candidate)) &#123; String error = doLoad(candidate, loader); if (error == null) &#123; return; // We successfully loaded the library. Job done. &#125; lastError = error; &#125; &#125; if (lastError != null) &#123; throw new UnsatisfiedLinkError(lastError); &#125; throw new UnsatisfiedLinkError("Library " + libraryName + " not found; tried " + candidates); &#125; &#160; &#160; &#160; &#160;我们这里详细分析一下loadLibrary方法。首先会判断指定的ClassLoader是否为空，这里传入的值为VMStack.getCallingClassLoader()，就是加载目标类的ClassLoader，宿主工程为系统指定的PathClassLoader，插件工程为我们构造的DexClassLoader。 &#160; &#160; &#160; &#160;然后执行：String filename = loader.findLibrary(libraryName);这一步其实是调用PathClassLoader和DexClassLoader共同父类BaseDexClassLoader的findLibrary方法：1234567891011121314private final DexPathList pathList; public BaseDexClassLoader(String dexPath, File optimizedDirectory, String libraryPath, ClassLoader parent) &#123; super(parent); //pathList在构造方法中赋值 this.pathList = new DexPathList(this, dexPath, libraryPath, optimizedDirectory); &#125; //BaseDexClassLoader的findLibrary方法 @Override public String findLibrary(String name) &#123; return pathList.findLibrary(name); &#125; &#160; &#160; &#160; &#160;BaseDexClassLoader的findLibrary方法内部又调用了DexPathList的findLibrary方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263//DexPathList的findLibrary方法 public String findLibrary(String libraryName) &#123; //转换指定libraryName为so库文件名，例如turn "MyLibrary" into "libMyLibrary.so". String fileName = System.mapLibraryName(libraryName); //在nativeLibraryDirectories中遍历目标so库是否存在 for (File directory : nativeLibraryDirectories) &#123; String path = new File(directory, fileName).getPath(); if (IoUtils.canOpenReadOnly(path)) &#123; return path; &#125; &#125; return null; &#125; private final File[] nativeLibraryDirectories;public DexPathList(ClassLoader definingContext, String dexPath, String libraryPath, File optimizedDirectory) &#123; ...... //也是在构造方法中给nativeLibraryDirectories 赋值； //libraryPath就是我们在DexClassLoader中指定的第三个参数,系统的PathClassLoader指定为/data/app-lib/xxx(包名) this.nativeLibraryDirectories = splitLibraryPath(libraryPath); &#125; //GO ON 继续跟踪 private static File[] splitLibraryPath(String path) &#123; // Native libraries may exist in both the system and // application library paths, and we use this search order: // // 1. this class loader's library path for application libraries // 2. the VM's library path from the system property for system libraries // // This order was reversed prior to Gingerbread; see http://b/2933456. //System.getProperty("java.library.path")返回的是/vendor/lib:/system/lib ArrayList&lt;File&gt; result = splitPaths(path, System.getProperty("java.library.path"), true); return result.toArray(new File[result.size()]); &#125; //NEXT path1为我们在DexClassLoader中指定的第三个参数,系统的PathClassLoader指定为/data/app-lib/xxx(包名)；path2为/vendor/lib:/system/lib；wantDirectories为true private static ArrayList&lt;File&gt; splitPaths(String path1, String path2, boolean wantDirectories) &#123;// ArrayList&lt;File&gt; result = new ArrayList&lt;File&gt;(); splitAndAdd(path1, wantDirectories, result); splitAndAdd(path2, wantDirectories, result); return result; &#125;//FINALLY 用“:”分割路径字符串，并且将这些路径都放入到一个ArrayList中 private static void splitAndAdd(String searchPath, boolean directoriesOnly, ArrayList&lt;File&gt; resultList) &#123; if (searchPath == null) &#123; return; &#125; for (String path : searchPath.split(":")) &#123; try &#123; StructStat sb = Libcore.os.stat(path); if (!directoriesOnly || S_ISDIR(sb.st_mode)) &#123; resultList.add(new File(path)); &#125; &#125; catch (ErrnoException ignored) &#123; &#125; &#125; &#125; &#160; &#160; &#160; &#160;上述代码就是查找so库文件的逻辑了，会分别在/vendor/lib、/system/lib、/data/app-lib/xxx(包名)、和指定目录下查找，如果找不到，就会报UnsatisfiedLinkError异常。 &#160; &#160; &#160; &#160;查找逻辑就先到这里，继续回到Runtime类中。接着就会调用doLoad方法：12345678910111213141516 private String doLoad(String name, ClassLoader loader) &#123; ...... String ldLibraryPath = null; if (loader != null &amp;&amp; loader instanceof BaseDexClassLoader) &#123; //ldLibraryPath就是上面提到的vendor/lib、/system/lib、/data/app-lib/xxx(包名)、和指定目录用“:”连接的字符串 ldLibraryPath = ((BaseDexClassLoader) loader).getLdLibraryPath(); &#125; synchronized (this) &#123; //最后会调用nativeLoad方法 return nativeLoad(name, loader, ldLibraryPath); &#125; &#125; private static native String nativeLoad(String filename, ClassLoader loader, String ldLibraryPath); &#160; &#160; &#160; &#160;这里调用了本地方法，不过悲催的是，我的ART版本代码没有找到，所以只能看 Dalvik版本的。 Runtime类的成员函数nativeLoad在C++层对应的函数为Dalvik_java_lang_Runtime_nativeLoad，这个函数定义在文件dalvik/vm/native/java_lang_Runtime.c中，如下所示：12345678910111213141516171819202122232425static void Dalvik_java_lang_Runtime_nativeLoad(const u4* args, JValue* pResult) &#123; StringObject* fileNameObj = (StringObject*) args[0]; //so库名 Object* classLoader = (Object*) args[1]; //类加载器 char* fileName = NULL; StringObject* result = NULL; char* reason = NULL; bool success; assert(fileNameObj != NULL); //将 fileNameObj 转化为C++层字符串 fileName = dvmCreateCstrFromString(fileNameObj); //调用dvmLoadNativeCode方法 success = dvmLoadNativeCode(fileName, classLoader, &amp;reason); if (!success) &#123; const char* msg = (reason != NULL) ? reason : "unknown failure"; result = dvmCreateStringFromCstr(msg); dvmReleaseTrackedAlloc((Object*) result, NULL); &#125; free(reason); free(fileName); RETURN_PTR(result); &#125; &#160; &#160; &#160; &#160; 参数args[0]保存的是一个Java层的String对象，这个String对象描述的就是要加载的so文件，函数Dalvik_java_lang_Runtime_nativeLoad首先是调用函数dvmCreateCstrFromString来将它转换成一个C++层的字符串fileName，然后再调用函数dvmLoadNativeCode来执行加载so文件的操作。 &#160; &#160; &#160; &#160;接下来，我们就继续分析函数dvmLoadNativeCode的实现，这个函数定义在文件dalvik/vm/Native.c中：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182bool dvmLoadNativeCode(const char* pathName, Object* classLoader, char** detail) &#123; SharedLib* pEntry; void* handle; ...... pEntry = findSharedLibEntry(pathName); if (pEntry != NULL) &#123; if (pEntry-&gt;classLoader != classLoader) &#123; ...... return false; &#125; ...... if (!checkOnLoadResult(pEntry)) return false; return true; &#125; ...... handle = dlopen(pathName, RTLD_LAZY); ...... /* create a new entry */ SharedLib* pNewEntry; pNewEntry = (SharedLib*) calloc(1, sizeof(SharedLib)); pNewEntry-&gt;pathName = strdup(pathName); pNewEntry-&gt;handle = handle; pNewEntry-&gt;classLoader = classLoader; ...... /* try to add it to the list */ SharedLib* pActualEntry = addSharedLibEntry(pNewEntry); if (pNewEntry != pActualEntry) &#123; ...... freeSharedLibEntry(pNewEntry); return checkOnLoadResult(pActualEntry); &#125; else &#123; ...... bool result = true; void* vonLoad; int version; vonLoad = dlsym(handle, "JNI_OnLoad"); if (vonLoad == NULL) &#123; LOGD("No JNI_OnLoad found in %s %p, skipping init\n", pathName, classLoader); &#125; else &#123; ...... OnLoadFunc func = vonLoad; ...... version = (*func)(gDvm.vmList, NULL); ...... if (version != JNI_VERSION_1_2 &amp;&amp; version != JNI_VERSION_1_4 &amp;&amp; version != JNI_VERSION_1_6) &#123; ....... result = false; &#125; else &#123; LOGV("+++ finished JNI_OnLoad %s\n", pathName); &#125; &#125; ...... if (result) pNewEntry-&gt;onLoadResult = kOnLoadOkay; else pNewEntry-&gt;onLoadResult = kOnLoadFailed; ...... return result; &#125; &#125; &#160; &#160; &#160; &#160;函数dvmLoadNativeCode首先是检查参数pathName所指定的so文件是否已经加载过了，这是通过调用函数findSharedLibEntry来实现的。如果已经加载过，那么就可以获得一个SharedLib对象pEntry。这个SharedLib对象pEntry描述了有关参数pathName所指定的so文件的加载信息，例如，上次用来加载它的类加载器和上次的加载结果。如果上次用来加载它的类加载器不等于当前所使用的类加载器，或者上次没有加载成功，那么函数dvmLoadNativeCode就回直接返回false给调用者，表示不能在当前进程中加载参数pathName所描述的so文件。 这里有一个检测异常的代码，而这个错误，是我们在使用插件开发加载so的时候可能会遇到的错误，比如现在我们使用DexClassLoader类去加载插件，但是因为我们为了插件能够实时更新，所以每次都会赋值新的DexClassLoader对象，但是第一次加载so文件到内存中了，这时候退出程序，但是没有真正意义上的退出，只是关闭了Activity了，这时候再次启动又会赋值新的加载器对象，那么原先so已经加载到内存中了，但是这时候是新的类加载器那么就报错了，解决办法其实很简单，主要有两种方式：第一种方式：在退出程序的时候采用真正意义上的退出，比如调用System.exit(0)方法，这时候进程被杀了，加载到内存的so也就被释放了，那么下次赋值新的类加载就在此加载so到内存了，第二种方式：就是全局定义一个static类型的类加载DexClassLoader也是可以的，因为static类型是保存在当前进程中，如果进程没有被杀就一直存在这个对象，下次进入程序的时候判断当前类加载器是否为null，如果不为null就不要赋值了，但是这个方法有一个弊端就是类加载器没有从新赋值，如果插件这时候更新了，但是还是使用之前的加载器，那么新插件将不会进行加载。 &#160; &#160; &#160; &#160;我们假设参数pathName所指定的so文件还没有被加载过，这时候函数dvmLoadNativeCode就会先调用dlopen来在当前进程中加载它，并且将获得的句柄保存在变量handle中，接着再创建一个SharedLib对象pNewEntry来描述它的加载信息。这个SharedLib对象pNewEntry还会通过函数addSharedLibEntry被缓存起来，以便可以知道当前进程都加载了哪些so文件。 &#160; &#160; &#160; &#160; 注意，在调用函数addSharedLibEntry来缓存新创建的SharedLib对象pNewEntry的时候，如果得到的返回值pActualEntry指向的不是SharedLib对象pNewEntry，那么就表示另外一个线程也正在加载参数pathName所指定的so文件，并且比当前线程提前加载完成。在这种情况下，函数addSharedLibEntry就什么也不用做而直接返回了。否则的话，函数addSharedLibEntry就要继续负责调用前面所加载的so文件中的一个指定的函数来注册它里面的JNI方法。 &#160; &#160; &#160; &#160; 这个指定的函数的名称为“JNI_OnLoad”，也就是说，每一个用来实现JNI方法的so文件都应该定义有一个名称为“JNI_OnLoad”的函数，并且这个函数的原型为：1jint JNI_OnLoad(JavaVM* vm, void* reserved); &#160; &#160; &#160; &#160; 函数dvmLoadNativeCode通过调用函数dlsym就可以获得在前面加载的so中名称为“JNI_OnLoad”的函数的地址，最终保存在函数指针func中。有了这个函数指针之后，我们就可以直接调用它来执行注册JNI方法的操作了。注意，在调用该JNI_OnLoad函数时，第一个要传递进行的参数是一个JavaVM对象，这个JavaVM对象描述的是在当前进程中运行的Dalvik虚拟机，第二个要传递的参数可以设置为NULL，这是保留给以后使用的。 &#160; &#160; &#160; &#160;到这里我们就总结一下Android中加载so的流程： 调用System.loadLibrary和System.load方法进行加载so文件 通过Runtime.java类的nativeLoad方法进行最终调用，这里需要通过类加载器获取到nativeLib路径 到底层之后，就开始使用dlopen方法加载so文件，然后使用dlsym方法调用JNI_OnLoad方法，最终开始了so的执行 释放SO库文件&#160; &#160; &#160; &#160;我们在使用System.loadLibrary加载so的时候，传递的是so文件的libxxx.so中的xxx部分，那么系统是如何找到这个so文件然后进行加载的呢？这个就要先从apk文件安装时机说起。 &#160; &#160; &#160; &#160;Android系统在启动的过程中，会启动一个应用程序管理服务PackageManagerService，这个服务负责扫描系统中特定的目录，找到里面的应用程序文件，即以Apk为后缀的文件，然后对这些文件进解析，得到应用程序的相关信息，完成应用程序的安装过程。&#160; &#160; &#160; &#160;应用程序管理服务PackageManagerService安装应用程序的过程，其实就是解析析应用程序配置文件AndroidManifest.xml的过程，并从里面得到得到应用程序的相关信息，例如得到应用程序的组件Activity、Service、Broadcast Receiver和Content Provider等信息，有了这些信息后，通过ActivityManagerService这个服务，我们就可以在系统中正常地使用这些应用程序了。 &#160; &#160; &#160; &#160;下面我们一步一步分析： &#160; &#160; &#160; &#160;我们知道Android系统系统启动时会启动Zygote进程，Zygote进程又会启动SystemServer组件，启动的时候就会调用它的main函数，然后会初始化一系列服务。123456789101112131415161718192021public final class SystemServer &#123; private PackageManagerService mPackageManagerService; ...... public static void main(String[] args) &#123; new SystemServer().run(); &#125; private void run() &#123; ...... startBootstrapServices(); ...... &#125; private void startBootstrapServices() &#123; mPackageManagerService = PackageManagerService.main(mSystemContext, mInstaller, mFactoryTestMode != FactoryTest.FACTORY_TEST_OFF, mOnlyCore); &#125; ......&#125; &#160; &#160; &#160; &#160;中间会启动PackageManagerService，这个函数定义在frameworks/base/services/java/com/android/server/PackageManagerService.java文件中：123456789101112public class PackageManagerService extends IPackageManager.Stub &#123; ...... public static final PackageManagerService main(Context context, Installer installer, boolean factoryTest, boolean onlyCore) &#123; PackageManagerService m = new PackageManagerService(context, installer, factoryTest, onlyCore); ServiceManager.addService("package", m); return m; &#125; ......&#125; &#160; &#160; &#160; &#160;这个函数创建了一个PackageManagerService服务实例，然后把这个服务添加到ServiceManager中去， 在创建这个PackageManagerService服务实例时，会在PackageManagerService类的构造函数中开始执行安装应用程序的过程：1234567 public PackageManagerService(Context context, Installer installer, boolean factoryTest, boolean onlyCore) &#123;......scanPackageLI(scanFile, reparseFlags, scanFlags, 0, null);......&#125; &#160; &#160; &#160; &#160;PackageManagerService的构造方法中就完成了对apk文件的解包，还有对xm文件的解析等等，感兴趣的可以自己分析。这里我们限于篇幅，就只分析so文件的解包过程。&#160; &#160; &#160; &#160;这里会调用scanPackageLI方法：1234567891011121314151617private PackageParser.Package scanPackageLI(File scanFile, int parseFlags, int scanFlags, long currentTime, UserHandle user) throws PackageManagerException &#123; ...... // Note that we invoke the following method only if we are about to unpack an application PackageParser.Package scannedPkg = scanPackageLI(pkg, parseFlags, scanFlags | SCAN_UPDATE_SIGNATURE, currentTime, user); ...... &#125;private PackageParser.Package scanPackageLI(PackageParser.Package pkg, int parseFlags, int scanFlags, long currentTime, UserHandle user) throws PackageManagerException &#123; ...... final PackageParser.Package res = scanPackageDirtyLI(pkg, parseFlags, scanFlags, currentTime, user); ...... &#125; &#160; &#160; &#160; &#160;经过一系列重载方法调用，最终会调用scanPackageDirtyLI方法：123456789101112131415161718192021222324252627282930313233343536373839404142 private PackageParser.Package scanPackageDirtyLI(PackageParser.Package pkg, int parseFlags, int scanFlags, long currentTime, UserHandle user) throws PackageManagerException &#123; //初始化so库放置的目录，并赋值给pkgsetNativeLibraryPaths(pkg); final boolean isAsec = isForwardLocked(pkg) || isExternal(pkg); //nativeLibraryRootStr 指定为/data/app-lib/xxx(包名) final String nativeLibraryRootStr = pkg.applicationInfo.nativeLibraryRootDir; //false final boolean useIsaSpecificSubdirs = pkg.applicationInfo.nativeLibraryRootRequiresIsa; NativeLibraryHelper.Handle handle = null; ...... //标记打开apk handle = NativeLibraryHelper.Handle.create(scanFile); final File nativeLibraryRoot = new File(nativeLibraryRootStr); // Null out the abis so that they can be recalculated. pkg.applicationInfo.primaryCpuAbi = null; pkg.applicationInfo.secondaryCpuAbi = null; ...... String[] abiList = (cpuAbiOverride != null) ? new String[] &#123; cpuAbiOverride &#125; : Build.SUPPORTED_ABIS; ...... final int copyRet; if (isAsec) &#123; copyRet = NativeLibraryHelper.findSupportedAbi(handle, abiList); &#125; else &#123; //解压对应ABI的so文件到指定目录 copyRet = NativeLibraryHelper.copyNativeBinariesForSupportedAbi(handle, nativeLibraryRoot, abiList, useIsaSpecificSubdirs); &#125; ...... &#125; &#160; &#160; &#160; &#160;scanPackageDirtyLI首先调用setNativeLibraryPaths方法，这个方法主要是指定一下so库释放路径：12345678910111213141516171819202122232425262728293031 private void setNativeLibraryPaths(PackageParser.Package pkg) &#123;final ApplicationInfo info = pkg.applicationInfo; final String codePath = pkg.codePath; final File codeFile = new File(codePath); final boolean bundledApp = isSystemApp(info) &amp;&amp; !isUpdatedSystemApp(info); final boolean asecApp = isForwardLocked(info) || isExternal(info); info.nativeLibraryRootDir = null; info.nativeLibraryRootRequiresIsa = false; info.nativeLibraryDir = null; info.secondaryNativeLibraryDir = null; if (isApkFile(codeFile)) &#123; // Monolithic install if (bundledApp) &#123; ...... &#125; else if (asecApp) &#123; ...... &#125; else &#123; final String apkName = deriveCodePathName(codePath); //mAppLib32InstallDir为/data/app-lib/ info.nativeLibraryRootDir = new File(mAppLib32InstallDir, apkName) .getAbsolutePath(); &#125; info.nativeLibraryRootRequiresIsa = false; info.nativeLibraryDir = info.nativeLibraryRootDir; &#125; else &#123; ...... &#125; &#125; &#160; &#160; &#160; &#160;然后调用NativeLibraryHelper.Handle.create(scanFile)标记打开apk文件：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 public static class Handle implements Closeable &#123; ...... final long[] apkHandles; final boolean multiArch; public static Handle create(File packageFile) throws IOException &#123; try &#123; final PackageLite lite = PackageParser.parsePackageLite(packageFile, 0); return create(lite); &#125; catch (PackageParserException e) &#123; throw new IOException("Failed to parse package: " + packageFile, e); &#125; &#125; public static Handle create(Package pkg) throws IOException &#123; return create(pkg.getAllCodePaths(), (pkg.applicationInfo.flags &amp; ApplicationInfo.FLAG_MULTIARCH) != 0); &#125; public static Handle create(PackageLite lite) throws IOException &#123; return create(lite.getAllCodePaths(), lite.multiArch); &#125; //最后调用到这里 private static Handle create(List&lt;String&gt; codePaths, boolean multiArch) throws IOException &#123; final int size = codePaths.size(); final long[] apkHandles = new long[size]; for (int i = 0; i &lt; size; i++) &#123; final String path = codePaths.get(i); //调用这个native方法，打开apk，并将JNI层返回的句柄保留到java层 apkHandles[i] = nativeOpenApk(path); ...... &#125; return new Handle(apkHandles, multiArch); &#125; Handle(long[] apkHandles, boolean multiArch) &#123; this.apkHandles = apkHandles; this.multiArch = multiArch; mGuard.open("close"); &#125; &#125;//NativeLibraryHelper的nativeOpenApk方法private static native long nativeOpenApk(String path); &#160; &#160; &#160; &#160;经过一系列重载方法调用，最后会调用NativeLibraryHelper的nativeOpenApk方法，打开apk，并将JNI层返回的句柄保留到java层。这个方法的实现位于frameworks/base/core/jni/com_android_internal_content_NativeLibraryHelper.cpp中：123456789static jlongcom_android_internal_content_NativeLibraryHelper_openApk(JNIEnv *env, jclass, jstring apkPath)&#123; ScopedUtfChars filePath(env, apkPath); ZipFileRO* zipFile = ZipFileRO::open(filePath.c_str()); return reinterpret_cast&lt;jlong&gt;(zipFile);&#125; &#160; &#160; &#160; &#160;上述代码调用了ZipFileRO的open方法，并返回一个ZipFileRO类型的指针，然后强转为java层的long型对象返回给java层。open方法实现位于frameworks/base/libs/androidfw/ZipFileRO.cpp中：12345678910111213141516/* * Open the specified file read-only. We memory-map the entire thing and * close the file before returning. *//* static */ ZipFileRO* ZipFileRO::open(const char* zipFileName)&#123; ZipArchiveHandle handle; //调用ZipArchive库打开zip文件 const int32_t error = OpenArchive(zipFileName, &amp;handle); if (error) &#123; ALOGW("Error opening archive %s: %s", zipFileName, ErrorCodeString(error)); return NULL; &#125; return new ZipFileRO(handle, strdup(zipFileName));&#125; &#160; &#160; &#160; &#160;这些就是JNI层打开apk文件的操作了。我么继续回到scanPackageDirtyLI方法中，接着调用NativeLibraryHelper.copyNativeBinariesForSupportedAbi方法：1234567891011121314151617181920212223242526272829303132333435 public static int copyNativeBinariesForSupportedAbi(Handle handle, File libraryRoot, String[] abiList, boolean useIsaSubdir) throws IOException &#123; //如果目录不存或者是个文件，就重新创建目录 createNativeLibrarySubdir(libraryRoot); /* * If this is an internal application or our nativeLibraryPath points to * the app-lib directory, unpack the libraries if necessary. */ //查找对应的ABI类型 int abi = findSupportedAbi(handle, abiList); if (abi &gt;= 0) &#123; /* * If we have a matching instruction set, construct a subdir under the native * library root that corresponds to this instruction set. */ //获取so释放之后的目录 final String instructionSet = VMRuntime.getInstructionSet(abiList[abi]); final File subDir; if (useIsaSubdir) &#123; final File isaSubdir = new File(libraryRoot, instructionSet); createNativeLibrarySubdir(isaSubdir); subDir = isaSubdir; &#125; else &#123; subDir = libraryRoot; &#125;//拷贝so int copyRet = copyNativeBinaries(handle, subDir, abiList[abi]); if (copyRet != PackageManager.INSTALL_SUCCEEDED) &#123; return copyRet; &#125; &#125; return abi; &#125; &#160; &#160; &#160; &#160;我们挑一些重要的分析一下。这里先获取abiList的值，这个通过Build.SUPPORTED_ABIS来获取到的：1public static final String[] SUPPORTED_ABIS = getStringList("ro.product.cpu.abilist", ","); &#160; &#160; &#160; &#160;最终是通过获取系统属性ro.product.cpu.abilist的值来得到的，我们可以使用getprop命令来查看这个属性值，或者直接cat一下/system/build.prop文件:&#160; &#160; &#160; &#160;这里获取到的值是x86。然后去分析findSupportedAbi方法：1234567891011121314151617181920212223242526 public static int findSupportedAbi(Handle handle, String[] supportedAbis) &#123; int finalRes = NO_NATIVE_LIBRARIES; for (long apkHandle : handle.apkHandles) &#123; //这里调用了native方法 final int res = nativeFindSupportedAbi(apkHandle, supportedAbis); if (res == NO_NATIVE_LIBRARIES) &#123; // No native code, keep looking through all APKs. &#125; else if (res == INSTALL_FAILED_NO_MATCHING_ABIS) &#123; // Found some native code, but no ABI match; update our final // result if we haven't found other valid code. if (finalRes &lt; 0) &#123; finalRes = INSTALL_FAILED_NO_MATCHING_ABIS; &#125; &#125; else if (res &gt;= 0) &#123; // Found valid native code, track the best ABI match if (finalRes &lt; 0 || res &lt; finalRes) &#123; finalRes = res; &#125; &#125; else &#123; // Unexpected error; bail return res; &#125; &#125; return finalRes; &#125;private native static int nativeFindSupportedAbi(long handle, String[] supportedAbis); &#160; &#160; &#160; &#160;NativeLibraryHelper类的findSupportedAbi方法，其实这个方法就是查找系统当前支持的架构型号索引值。调用的本地方法实现为：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162static jintcom_android_internal_content_NativeLibraryHelper_findSupportedAbi(JNIEnv *env, jclass clazz, jlong apkHandle, jobjectArray javaCpuAbisToSearch)&#123; return (jint) findSupportedAbi(env, apkHandle, javaCpuAbisToSearch);&#125;//会调用这个方法static int findSupportedAbi(JNIEnv *env, jlong apkHandle, jobjectArray supportedAbisArray) &#123; const int numAbis = env-&gt;GetArrayLength(supportedAbisArray); Vector&lt;ScopedUtfChars*&gt; supportedAbis; for (int i = 0; i &lt; numAbis; ++i) &#123; supportedAbis.add(new ScopedUtfChars(env, (jstring) env-&gt;GetObjectArrayElement(supportedAbisArray, i))); &#125; //读取apk文件 ZipFileRO* zipFile = reinterpret_cast&lt;ZipFileRO*&gt;(apkHandle); if (zipFile == NULL) &#123; return INSTALL_FAILED_INVALID_APK; &#125; UniquePtr&lt;NativeLibrariesIterator&gt; it(NativeLibrariesIterator::create(zipFile)); if (it.get() == NULL) &#123; return INSTALL_FAILED_INVALID_APK; &#125; ZipEntryRO entry = NULL; char fileName[PATH_MAX]; int status = NO_NATIVE_LIBRARIES; //这里开始遍历apk中每一个文件 while ((entry = it-&gt;next()) != NULL) &#123; // We're currently in the lib/ directory of the APK, so it does have some native // code. We should return INSTALL_FAILED_NO_MATCHING_ABIS if none of the // libraries match. if (status == NO_NATIVE_LIBRARIES) &#123; status = INSTALL_FAILED_NO_MATCHING_ABIS; &#125; const char* fileName = it-&gt;currentEntry(); const char* lastSlash = it-&gt;lastSlash(); // Check to see if this CPU ABI matches what we are looking for. const char* abiOffset = fileName + APK_LIB_LEN; const size_t abiSize = lastSlash - abiOffset; //遍历apk中的子文件，获取so文件的全路径，如果这个路径包含了cpu架构值，就记录返回索引 for (int i = 0; i &lt; numAbis; i++) &#123; const ScopedUtfChars* abi = supportedAbis[i]; if (abi-&gt;size() == abiSize &amp;&amp; !strncmp(abiOffset, abi-&gt;c_str(), abiSize)) &#123; // The entry that comes in first (i.e. with a lower index) has the higher priority. if (((i &lt; status) &amp;&amp; (status &gt;= 0)) || (status &lt; 0) ) &#123; status = i; &#125; &#125; &#125; &#125; for (int i = 0; i &lt; numAbis; ++i) &#123; delete supportedAbis[i]; &#125; return status;&#125; &#160; &#160; &#160; &#160;这里看到了，会先读取apk文件，然后遍历apk文件中的so文件，得到全路径然后在和传递进来的abiList进行比较，得到合适的索引值。我们刚才拿到的abiList为：x86，然后就开始比较apk中有没有这些架构平台的so文件，如果有，就直接返回abiList中的索引值即可。比如apk中libs结构如下： &#160; &#160; &#160; &#160;那么这个时候就只有这么一种架构，libs文件下也有相关的ABI类型，就只能返回0了； &#160; &#160; &#160; &#160;假设我们的abiList为：arm64-v8a,armeabi-v7a,armeabi。那么这时候返回来的索引值就是0，代表的是arm64-v8a架构的。如果apk文件中没有arm64-v8a目录的话，那么就返回1，代表的是armeabi-v7a架构的。依次类推。得到应用支持的架构索引之后就可以获取so释放到设备中的目录了。 &#160; &#160; &#160; &#160;下一步就是获取so释放之后的目录，调用VMRuntime.java中的getInstructionSet方法：12345678910111213141516171819public static String getInstructionSet(String abi) &#123; final String instructionSet = ABI_TO_INSTRUCTION_SET_MAP.get(abi); if (instructionSet == null) &#123; throw new IllegalArgumentException("Unsupported ABI: " + abi); &#125; return instructionSet;&#125;private static final Map&lt;String, String&gt; ABI_TO_INSTRUCTION_SET_MAP = new HashMap&lt;String, String&gt;();static &#123; ABI_TO_INSTRUCTION_SET_MAP.put("armeabi", "arm"); ABI_TO_INSTRUCTION_SET_MAP.put("armeabi-v7a", "arm"); ABI_TO_INSTRUCTION_SET_MAP.put("mips", "mips"); ABI_TO_INSTRUCTION_SET_MAP.put("mips64", "mips64"); ABI_TO_INSTRUCTION_SET_MAP.put("x86", "x86"); ABI_TO_INSTRUCTION_SET_MAP.put("x86_64", "x86_64"); ABI_TO_INSTRUCTION_SET_MAP.put("arm64-v8a", "arm64");&#125; &#160; &#160; &#160; &#160;这一步主要是对获得的ABI架构字符串做了一下转换，比如从x86—&gt;x86，armeabi—&gt;arm等等。 &#160; &#160; &#160; &#160;最后就是释放so了，调用copyNativeBinaries方法：1234567891011public static int copyNativeBinaries(Handle handle, File sharedLibraryDir, String abi) &#123; for (long apkHandle : handle.apkHandles) &#123; int res = nativeCopyNativeBinaries(apkHandle, sharedLibraryDir.getPath(), abi); if (res != INSTALL_SUCCEEDED) &#123; return res; &#125; &#125; return INSTALL_SUCCEEDED;&#125;private native static int nativeCopyNativeBinaries(long handle, String sharedLibraryPath, String abiToCopy); &#160; &#160; &#160; &#160;JNI层实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849static jintcom_android_internal_content_NativeLibraryHelper_copyNativeBinaries(JNIEnv *env, jclass clazz, jlong apkHandle, jstring javaNativeLibPath, jstring javaCpuAbi)&#123; //调用iterateOverNativeFiles方法，copyFileIfChanged是个函数指针，完成释放 return (jint) iterateOverNativeFiles(env, apkHandle, javaCpuAbi, copyFileIfChanged, &amp;javaNativeLibPath);&#125;static install_status_titerateOverNativeFiles(JNIEnv *env, jlong apkHandle, jstring javaCpuAbi, iterFunc callFunc, void* callArg) &#123; ZipFileRO* zipFile = reinterpret_cast&lt;ZipFileRO*&gt;(apkHandle); if (zipFile == NULL) &#123; return INSTALL_FAILED_INVALID_APK; &#125; UniquePtr&lt;NativeLibrariesIterator&gt; it(NativeLibrariesIterator::create(zipFile)); if (it.get() == NULL) &#123; return INSTALL_FAILED_INVALID_APK; &#125; const ScopedUtfChars cpuAbi(env, javaCpuAbi); if (cpuAbi.c_str() == NULL) &#123; // This would've thrown, so this return code isn't observable by // Java. return INSTALL_FAILED_INVALID_APK; &#125; ZipEntryRO entry = NULL; while ((entry = it-&gt;next()) != NULL) &#123; const char* fileName = it-&gt;currentEntry(); const char* lastSlash = it-&gt;lastSlash(); // Check to make sure the CPU ABI of this file is one we support. const char* cpuAbiOffset = fileName + APK_LIB_LEN; const size_t cpuAbiRegionSize = lastSlash - cpuAbiOffset; if (cpuAbi.size() == cpuAbiRegionSize &amp;&amp; !strncmp(cpuAbiOffset, cpuAbi.c_str(), cpuAbiRegionSize)) &#123; //释放so，这一句才是关键，copyFileIfChanged完成释放 install_status_t ret = callFunc(env, callArg, zipFile, entry, lastSlash + 1); if (ret != INSTALL_SUCCEEDED) &#123; ALOGV("Failure for entry %s", lastSlash + 1); return ret; &#125; &#125; &#125; return INSTALL_SUCCEEDED; &#160; &#160; &#160; &#160;最后的释放工作都交给了copyFileIfChanged函数，我们看看这个函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/* * Copy the native library if needed. * * This function assumes the library and path names passed in are considered safe. */static install_status_tcopyFileIfChanged(JNIEnv *env, void* arg, ZipFileRO* zipFile, ZipEntryRO zipEntry, const char* fileName)&#123; jstring* javaNativeLibPath = (jstring*) arg; ScopedUtfChars nativeLibPath(env, *javaNativeLibPath); size_t uncompLen; long when; long crc; time_t modTime; if (!zipFile-&gt;getEntryInfo(zipEntry, NULL, &amp;uncompLen, NULL, NULL, &amp;when, &amp;crc)) &#123; ALOGD("Couldn't read zip entry info\n"); return INSTALL_FAILED_INVALID_APK; &#125; else &#123; struct tm t; ZipUtils::zipTimeToTimespec(when, &amp;t); modTime = mktime(&amp;t); &#125; // Build local file path const size_t fileNameLen = strlen(fileName); char localFileName[nativeLibPath.size() + fileNameLen + 2]; if (strlcpy(localFileName, nativeLibPath.c_str(), sizeof(localFileName)) != nativeLibPath.size()) &#123; ALOGD("Couldn't allocate local file name for library"); return INSTALL_FAILED_INTERNAL_ERROR; &#125; *(localFileName + nativeLibPath.size()) = '/'; if (strlcpy(localFileName + nativeLibPath.size() + 1, fileName, sizeof(localFileName) - nativeLibPath.size() - 1) != fileNameLen) &#123; ALOGD("Couldn't allocate local file name for library"); return INSTALL_FAILED_INTERNAL_ERROR; &#125; // Only copy out the native file if it's different. //只有so本地文件改变了才拷贝 struct stat64 st; if (!isFileDifferent(localFileName, uncompLen, modTime, crc, &amp;st)) &#123; return INSTALL_SUCCEEDED; &#125; char localTmpFileName[nativeLibPath.size() + TMP_FILE_PATTERN_LEN + 2]; if (strlcpy(localTmpFileName, nativeLibPath.c_str(), sizeof(localTmpFileName)) != nativeLibPath.size()) &#123; ALOGD("Couldn't allocate local file name for library"); return INSTALL_FAILED_INTERNAL_ERROR; &#125; *(localFileName + nativeLibPath.size()) = '/'; if (strlcpy(localTmpFileName + nativeLibPath.size(), TMP_FILE_PATTERN, TMP_FILE_PATTERN_LEN - nativeLibPath.size()) != TMP_FILE_PATTERN_LEN) &#123; ALOGI("Couldn't allocate temporary file name for library"); return INSTALL_FAILED_INTERNAL_ERROR; &#125; //生成一个临时文件，用于拷贝 int fd = mkstemp(localTmpFileName); if (fd &lt; 0) &#123; ALOGI("Couldn't open temporary file name: %s: %s\n", localTmpFileName, strerror(errno)); return INSTALL_FAILED_CONTAINER_ERROR; &#125; //解压so文件 if (!zipFile-&gt;uncompressEntry(zipEntry, fd)) &#123; ALOGI("Failed uncompressing %s to %s\n", fileName, localTmpFileName); close(fd); unlink(localTmpFileName); return INSTALL_FAILED_CONTAINER_ERROR; &#125; close(fd); // Set the modification time for this file to the ZIP's mod time. struct timeval times[2]; times[0].tv_sec = st.st_atime; times[1].tv_sec = modTime; times[0].tv_usec = times[1].tv_usec = 0; if (utimes(localTmpFileName, times) &lt; 0) &#123; ALOGI("Couldn't change modification time on %s: %s\n", localTmpFileName, strerror(errno)); unlink(localTmpFileName); return INSTALL_FAILED_CONTAINER_ERROR; &#125; // Set the mode to 755 static const mode_t mode = S_IRUSR | S_IWUSR | S_IXUSR | S_IRGRP | S_IXGRP | S_IROTH | S_IXOTH; if (chmod(localTmpFileName, mode) &lt; 0) &#123; ALOGI("Couldn't change permissions on %s: %s\n", localTmpFileName, strerror(errno)); unlink(localTmpFileName); return INSTALL_FAILED_CONTAINER_ERROR; &#125; // Finally, rename it to the final name. if (rename(localTmpFileName, localFileName) &lt; 0) &#123; ALOGI("Couldn't rename %s to %s: %s\n", localTmpFileName, localFileName, strerror(errno)); unlink(localTmpFileName); return INSTALL_FAILED_CONTAINER_ERROR; &#125; ALOGV("Successfully moved %s to %s\n", localTmpFileName, localFileName); return INSTALL_SUCCEEDED;&#125; &#160; &#160; &#160; &#160;上述就是解压so文件的实现。先判断so名字合不合法，然后判断是不是文件改变了，再者创建一个临时文件，最后解压，用临时文件拷贝so到指定目录，结尾处关闭一些链接。 &#160; &#160; &#160; &#160;小结一下上述SO释放流程： 通过遍历apk文件中的so文件的全路径，然后和系统的abiList中的类型值进行比较，如果匹配到了就返回arch类型的索引值 得到了应用所支持的arch类型之后，就开始获取创建本地释放so的目录 然后开始释放so文件 失败的尝试&#160; &#160; &#160; &#160;上面我们分析了插件apk中加载so库，必须指定DexClassLoader中第三个参数，这就要我们解压apk中的so了。所以我试着调用系统的NativeLibraryHelper相关方法，做了如下实验：123456789101112131415161718192021222324252627282930313233343536373839@SuppressLint("NewApi")@Overridepublic boolean loadSO(File apkFile, File nativeLibraryRoot) &#123; NativeLibraryHelper.Handle handle = null; try &#123; handle = NativeLibraryHelper.Handle.create(apkFile); //private static Handle create(List&lt;String&gt; codePaths, boolean multiArch) throws IOException /*Method create2 = NativeLibraryHelper.Handle.class.getDeclaredMethod("create", List.class, boolean.class); create2.setAccessible(true); List&lt;String&gt; apkList = new ArrayList&lt;String&gt;(); apkList.add(apkFile.getAbsolutePath()); handle = (Handle) create2.invoke(null, apkList, false);*/ /*Method nativeOpenApk = NativeLibraryHelper.class.getDeclaredMethod("nativeOpenApk", String.class); nativeOpenApk.setAccessible(true); long apkHandle = (long) nativeOpenApk.invoke(null, apkFile.getAbsolutePath()); Method nativeClose = NativeLibraryHelper.class.getDeclaredMethod("nativeClose", long.class); nativeOpenApk.setAccessible(true); nativeClose.invoke(null, apkHandle); Constructor&lt;Handle&gt; constructMethod = NativeLibraryHelper.Handle.class.getConstructor(long[].class, boolean.class); constructMethod.setAccessible(true); handle = constructMethod.newInstance(new long[]&#123;apkHandle&#125;, false);*/ NativeLibraryHelper.copyNativeBinariesForSupportedAbi(handle, nativeLibraryRoot, Build.SUPPORTED_ABIS, false); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; if (handle != null) &#123; try &#123; handle.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; return false;&#125; &#160; &#160; &#160; &#160;然而并无卵用。。。。。。还有那些注释的尝试，也毫无作用= 。 =&#160; &#160; &#160; &#160;如果大家知道原因的话，或者对这一块儿还有更好的实现方案，麻烦多多指教，在此提前献上妹子图。 剩下的坑&#160; &#160; &#160; &#160;关于四大组件生命周期的管理也是一个难点，这里限于篇幅只能止步于此。如果以后有时间的话，我会努力补上。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>插件开发</tag>
        <tag>SO库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能指针简单分析]]></title>
    <url>%2F2016%2F08%2F23%2F%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;前一阵子好忙，忙的根本停不下来。现在还好点，能闲一些。所以抽空把之前没研究完的Android智能指针继续研究。 基础简介&#160; &#160; &#160; &#160;写C++程序时对指针的使用一定要十分谨慎，自己new出来的对象，如果忘记了delete，多次之后就会有比较严重的内存泄露；如果对指针忘记初始化，有时会出现野指针问题；已经释放了申请的内存，但是还在使用指向它的指针，又是一个悬垂指针……其实还有很多使用不当的情况，但是C++又不是java，jvm拥有强大的垃圾回收机制。但是为了避免种种指针使用不当引发的问题，STL标准模板库提供了智能指针这个工具。&#160; &#160; &#160; &#160;Google在Android源码中也引入了智能指针这个概念，参考了java垃圾收集器和STL的智能指针，从而自己封装了一套工具类。 设计理念&#160; &#160; &#160; &#160;如果要合理管理内存回收，一般的做法就是使用引用计数的方法。每当有一个指针指向了一个new出来的对象时，就对这个对象的引用计数增加1，每当有一个指针不再使用这个对象时，就对这个对象的引用计数减少1，每次减1之后，如果发现引用计数值为0时，那么，就要delete这个对象了，这样就避免了忘记delete对象或者这个对象被delete之后其它地方还在使用的问题了。但是该如何实现这个引用计数呢？我们知道，C++中对象的创建和销毁会分别调用构造函数和析构函数，所以引用计数的工作就落实在他们身上，同时还应该重载一些运算符，重载一些拷贝构造函数，以完善智能指针的功能。 &#160; &#160; &#160; &#160;在计算机科学领域中，提供垃圾收集（Garbage Collection）功能的系统框架，即提供对象托管功能的系统框架，例如Java应用程序框架，也是采用上述的引用计数技术方案来实现的，然而，简单的引用计数技术不能处理系统中对象间循环引用的情况。考虑这样的一个场景，系统中有两个对象A和B，在对象A的内部引用了对象B，而在对象B的内部也引用了对象A。当两个对象A和B都不再使用时，垃圾收集系统会发现无法回收这两个对象的所占据的内存的，因为系统一次只能收集一个对象，而无论系统决定要收回对象A还是要收回对象B时，都会发现这个对象被其它的对象所引用，因而就都回收不了，这样就造成了内存泄漏。这样，就要采取另外的一种引用计数技术了，即对象的引用计数同时存在强引用和弱引用两种计数，例如，Apple公司提出的Cocoa框架，当父对象要引用子对象时，就对子对象使用强引用计数技术，而当子对象要引用父对象时，就对父对象使用弱引用计数技术，而当垃圾收集系统执行对象回收工作时，只要发现对象的强引用计数为0，而不管它的弱引用计数是否为0，都可以回收这个对象，但是，如果我们只对一个对象持有弱引用计数，当我们要使用这个对象时，就不直接使用了，必须要把这个弱引用升级成为强引用时，才能使用这个对象，在转换的过程中，如果对象已经不存在，那么转换就失败了，这时候就说明这个对象已经被销毁了，不能再使用了。 智能指针&#160; &#160; &#160; &#160;Android系统提供了强大的智能指针技术供我们使用，这些智能指针实现方案既包括简单的引用计数技术，也包括了复杂的引用计数技术，即对象既有强引用计数，也有弱引用计数。我们最常见的智能指针就是强指针sp（Strong Pointer）和弱指针wp（Weak Pointer）。要实现内存的自动释放，sp、wp必须结合RefBase这个类来使用，在Android中，大多数类的最上层基类都是RefBase类。我们就只看这两种比较重要的吧。 强指针&#160; &#160; &#160; &#160;强指针使用的引用计数类为RefBase，我们看看它的定义，位于system/core/include/utils/RefBash.h：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class RefBase&#123;public: void incStrong(const void* id) const;//增加强引用计数 void decStrong(const void* id) const;//减少强引用计数 void forceIncStrong(const void* id) const;//强制增加强引用计数 ...... class weakref_type //实现引用计数功能的接口类 &#123; public: RefBase* refBase() const; void incWeak(const void* id);//增加弱引用计数 void decWeak(const void* id);//减少弱引用计数 // acquires a strong reference if there is already one. bool attemptIncStrong(const void* id); // acquires a weak reference if there is already one. // This is not always safe. see ProcessState.cpp and BpBinder.cpp // for proper use. bool attemptIncWeak(const void* id); ...... &#125;; weakref_type* createWeak(const void* id) const; weakref_type* getWeakRefs() const; ...... typedef RefBase basetype;protected: RefBase(); virtual ~RefBase(); //! Flags for extendObjectLifetime() enum &#123; OBJECT_LIFETIME_STRONG = 0x0000, OBJECT_LIFETIME_WEAK = 0x0001, OBJECT_LIFETIME_MASK = 0x0001 &#125;; void extendObjectLifetime(int32_t mode); //! Flags for onIncStrongAttempted() enum &#123; FIRST_INC_STRONG = 0x0001 &#125;; virtual void onFirstRef(); virtual void onLastStrongRef(const void* id); virtual bool onIncStrongAttempted(uint32_t flags, const void* id); virtual void onLastWeakRef(const void* id); ...... private: friend class weakref_type; //将辅助类设为友元，方便操作完成计数功能 class weakref_impl; RefBase(const RefBase&amp; o); //重载拷贝构造函数，避免默认拷贝构造函数的浅拷贝现象，因此实现类里要实现深拷贝 RefBase&amp; operator=(const RefBase&amp; o);//赋值运算符重载 weakref_impl* const mRefs;//通过这个类实现引用计数&#125;; &#160; &#160; &#160; &#160;RefBase类提供了incStrong和decStrong成员函数来操作它的引用计数器，复杂的引用计数技术同时支持强引用计数和弱引用计数，在RefBase类中，这两种计数功能是通过其成员变量mRefs来提供的。&#160; &#160; &#160; &#160;RefBase类的成员变量mRefs的类型为weakref_impl指针，从weakref_impl的类名来看，它应该是一个实现类，那么，就必然有一个对应的接口类，这个对应的接口类的就是RefBase类内部定义的weakref_type类了，这是一种把类的实现与接口定义分离的设计方法，也是一种多态的展现。weakref_impl的实现位于system/core/libutils/RefBash.cpp中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148class RefBase::weakref_impl : public RefBase::weakref_type&#123;public: volatile int32_t mStrong; volatile int32_t mWeak; RefBase* const mBase; volatile int32_t mFlags;#if !DEBUG_REFS //如果是Release版本， weakref_impl(RefBase* base) : mStrong(INITIAL_STRONG_VALUE) , mWeak(0) , mBase(base) , mFlags(0) &#123; &#125; void addStrongRef(const void* /*id*/) &#123; &#125; void removeStrongRef(const void* /*id*/) &#123; &#125; void addWeakRef(const void* /*id*/) &#123; &#125; void removeWeakRef(const void* /*id*/) &#123; &#125;#else //那就只有这么多，下面都是Debug版本的 weakref_impl(RefBase* base) : mStrong(INITIAL_STRONG_VALUE) , mWeak(0) , mBase(base) , mFlags(0) , mStrongRefs(NULL) , mWeakRefs(NULL) , mTrackEnabled(!!DEBUG_REFS_ENABLED_BY_DEFAULT) , mRetain(false) &#123; &#125; ~weakref_impl() &#123; bool dumpStack = false; if (!mRetain &amp;&amp; mStrongRefs != NULL) &#123; dumpStack = true; ref_entry* refs = mStrongRefs; while (refs) &#123; refs = refs-&gt;next; &#125; &#125; if (!mRetain &amp;&amp; mWeakRefs != NULL) &#123; dumpStack = true; ref_entry* refs = mWeakRefs; while (refs) &#123; refs = refs-&gt;next; &#125; &#125; &#125; void addStrongRef(const void* id) &#123; addRef(&amp;mStrongRefs, id, mStrong); &#125; void removeStrongRef(const void* id) &#123; if (!mRetain) &#123; removeRef(&amp;mStrongRefs, id); &#125; else &#123; addRef(&amp;mStrongRefs, id, -mStrong); &#125; &#125; void addWeakRef(const void* id) &#123; addRef(&amp;mWeakRefs, id, mWeak); &#125; void removeWeakRef(const void* id) &#123; if (!mRetain) &#123; removeRef(&amp;mWeakRefs, id); &#125; else &#123; addRef(&amp;mWeakRefs, id, -mWeak); &#125; &#125;private: struct ref_entry &#123; ref_entry* next; const void* id; int32_t ref; &#125;; void addRef(ref_entry** refs, const void* id, int32_t mRef) &#123; if (mTrackEnabled) &#123; AutoMutex _l(mMutex); ref_entry* ref = new ref_entry; // Reference count at the time of the snapshot, but before the // update. Positive value means we increment, negative--we // decrement the reference count. ref-&gt;ref = mRef; ref-&gt;id = id; ref-&gt;next = *refs; *refs = ref; &#125; &#125; void removeRef(ref_entry** refs, const void* id) &#123; if (mTrackEnabled) &#123; AutoMutex _l(mMutex); ref_entry* const head = *refs; ref_entry* ref = head; while (ref != NULL) &#123; if (ref-&gt;id == id) &#123; *refs = ref-&gt;next; delete ref; return; &#125; refs = &amp;ref-&gt;next; ref = *refs; &#125; ref = head; while (ref) &#123; ref = ref-&gt;next; &#125; &#125; &#125; mutable Mutex mMutex; ref_entry* mStrongRefs; ref_entry* mWeakRefs; bool mTrackEnabled; // Collect stack traces on addref and removeref, instead of deleting the stack references // on removeref that match the address ones. bool mRetain;#endif//Debug版本结束&#125;; &#160; &#160; &#160; &#160;这个类看起来很复杂，其实分为两部分看，还是很简单的。我们可以看到将它一分为二的条件宏：12345#if !DEBUG_REFS ...Release版本...#else...Debug版本...#endif &#160; &#160; &#160; &#160;if宏里面的都是Release版本的，它的函数都是空实现；else宏下面的是Debug版本代码，它的成员函数都是有实现的，实现这些函数的目的都是为了方便开发人员调试引用计数用的，除此之外，还在内部实现了一个结构体ref_entry：123456struct ref_entry&#123; ref_entry* next; const void* id; int32_t ref;&#125;; &#160; &#160; &#160; &#160;这个结构体也是为了方便调试的，这是一个链表的节点，那么我猜测Debug版本的引用计数调试时应该用的链表实现的。本来Debug版本我们可以不用关注，但这里本着学习C++的态度还是稍微看一看。比如我们先挑一个addRef(ref_entry* refs, const void id, int32_t mRef)函数看看：1234567891011121314151617void addRef(ref_entry** refs, const void* id, int32_t mRef)//要改变一个指针，必须用到二级指针 &#123; if (mTrackEnabled) &#123;//ture AutoMutex _l(mMutex); ref_entry* ref = new ref_entry; //新建一个节点 // Reference count at the time of the snapshot, but before the // update. Positive value means we increment, negative--we // decrement the reference count. //填充节点数据 ref-&gt;ref = mRef; ref-&gt;id = id; //头插法插入 ref-&gt;next = *refs;//将新节点的next指针指向头指针的地址 *refs = ref; //将头指针重新指向新节点 &#125; &#125; &#160; &#160; &#160; &#160;这个应该很随意吧，就是头插法插入新节点。先把需要插入的节点的next域直线目标链表的头节点内容，然后把头结点重新指向新节点即可。接着我们再看看removeRef(ref_entry* refs, const void id)函数：12345678910111213141516171819202122232425void removeRef(ref_entry** refs, const void* id)///要改变一个指针，必须用到二级指针 &#123; if (mTrackEnabled) &#123;//true AutoMutex _l(mMutex); ref_entry* const head = *refs;//拿到指向目标指针的地址的指针 ref_entry* ref = head;//拿到这个指针的头结点 while (ref != NULL) &#123;//从头往后遍历 if (ref-&gt;id == id) &#123;//如果发现指定节点 *refs = ref-&gt;next;//则跳过这个节点的指向，直接指向下一个节点 delete ref;//然后删除指定节点 return; &#125; //如果没有找到指定项，则继续向后遍历 refs = &amp;ref-&gt;next; ref = *refs; &#125; ref = head; while (ref) &#123; ref = ref-&gt;next; &#125; &#125; &#125; &#160; &#160; &#160; &#160;这一段应该也不难，就是从头到尾依次遍历每个链表节点，发现了指定项就把指向它的节点跳过指向它，直接指向它的下一个节点，然后删除指定项ok。 &#160; &#160; &#160; &#160;总的来说，weakref_impl类只要提供了以下四个成员变量来维护对象的引用计数：1234volatile int32_t mStrong;volatile int32_t mWeak;RefBase* const mBase;volatile int32_t mFlags; &#160; &#160; &#160; &#160;其中mStrong和mWeak分别表示对象的强引用计数和弱引用计数；RefBase类包含了一个weakref_impl类指针mRefs，而这里的 weakref_impl类也有一个成员变量mBase来指向它的宿主类RefBase；mFlags是一个标志位，它指示了维护对象引用计数所使用的策略，它的取值为0，或者以下的枚举值：123456//! Flags for extendObjectLifetime()enum &#123; OBJECT_LIFETIME_STRONG = 0x0000, OBJECT_LIFETIME_WEAK = 0x0001, OBJECT_LIFETIME_MASK = 0x0001&#125;; &#160; &#160; &#160; &#160;引用计数器的类RefBase我们就暂时介绍到这里,现在先来看看强指针类和弱指针类的定义。强指针就是sp类，位于system/core/include/utils/StrongPointer.h下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950template&lt;typename T&gt;class sp &#123;public: inline sp() : m_ptr(0) &#123; &#125;//构造函数 //拷贝构造函数 sp(T* other); sp(const sp&lt;T&gt;&amp; other); template&lt;typename U&gt; sp(U* other); template&lt;typename U&gt; sp(const sp&lt;U&gt;&amp; other); ~sp(); // Assignment //赋值运算符重载 sp&amp; operator = (T* other); sp&amp; operator = (const sp&lt;T&gt;&amp; other); template&lt;typename U&gt; sp&amp; operator = (const sp&lt;U&gt;&amp; other); template&lt;typename U&gt; sp&amp; operator = (U* other); //! Special optimization for use by ProcessState (and nobody else). void force_set(T* other); // Reset void clear(); // Accessors //地址相关运算符重载 inline T&amp; operator* () const &#123; return *m_ptr; &#125; inline T* operator-&gt; () const &#123; return m_ptr; &#125; inline T* get() const &#123; return m_ptr; &#125; // Operators COMPARE(==) COMPARE(!=) COMPARE(&gt;) COMPARE(&lt;) COMPARE(&lt;=) COMPARE(&gt;=)private: //友元类 template&lt;typename Y&gt; friend class sp; template&lt;typename Y&gt; friend class wp; void set_pointer(T* ptr); T* m_ptr;&#125;; &#160; &#160; &#160; &#160;上面是对sp类的一些定义，对于实现我们一个个来看。先看看拷贝构造函数：123456template&lt;typename T&gt; sp&lt;T&gt;::sp(T* other) : m_ptr(other) &#123; if (other) other-&gt;incStrong(this); &#125; &#160; &#160; &#160; &#160;这里传进来的参数other一定是继承于RefBase类的，因此，在函数的内部，它调用的是RefBase类的incStrong函数，它定义在system/core/libutils/RefBash.cpp文件中：12345678910111213141516void RefBase::incStrong(const void* id) const&#123; weakref_impl* const refs = mRefs; refs-&gt;incWeak(id);//增加弱引用计数 refs-&gt;addStrongRef(id);//这玩意儿在Release版本是空实现，上面分析过 const int32_t c = android_atomic_inc(&amp;refs-&gt;mStrong);//增加强引用计数 if (c != INITIAL_STRONG_VALUE) &#123;//如果不是第一调用则返回 return; &#125; //如果是第一次调用，则要减去mStrong的初始化值 android_atomic_add(-INITIAL_STRONG_VALUE, &amp;refs-&gt;mStrong); refs-&gt;mBase-&gt;onFirstRef();//第一次调用的回调方法，可自己重写&#125; &#160; &#160; &#160; &#160;这个IncStrong函数主要做了三件事：1.增加若引用计数；2.增加强引用计数；3.如果发现是首次调用这个对象的incStrong函数，就会调用一个这个对象的onFirstRef函数，让对象有机会在对象被首次引用时做一些处理逻辑。&#160; &#160; &#160; &#160;android_atomic_inc函数相当于i++表达式，会对变量加一，但是返回的是加一前的值。这里的c返回的是refs-&gt;mStrong加1前的值，如果发现等于INITIAL_STRONG_VALUE，就说明这个对象的强引用计数是第一次被增加，因此，refs-&gt;mStrong就是初始化为INITIAL_STRONG_VALUE的，它的值为：1#define INITIAL_STRONG_VALUE (1&lt;&lt;28) &#160; &#160; &#160; &#160;因为首次调用初始值为INITIAL_STRONG_VALUE，所以为了以后能够正确计数，因此首次需要减去它的初始值，所以才有了—INITIAL_STRONG_VALUE操作，然后android_atomic_inc函数会自动加一，所以首次调用后引用计数为1，逻辑正确。 &#160; &#160; &#160; &#160;然后我们继续回到incStrong函数中，看看弱引用计数是如何增加的。首先是调用weakref_impl类的addWeakRef函数，我们知道，在Release版本中，这个函数也不做，而在Debug版本中，这个函数增加了一个ref_entry对象到了weakref_impl对象的mWeakRefs列表中，表示此weakref_impl对象的弱引用计数被增加了一次。接着又调用了weakref_impl类的incWeak函数，真正增加弱引用计数值就是在这个函数实现的了，weakref_impl类的incWeak函数继承于其父类weakref_type的incWeak函数：123456void RefBase::weakref_type::incWeak(const void* id)&#123; weakref_impl* const impl = static_cast&lt;weakref_impl*&gt;(this);//强制类型转换 impl-&gt;addWeakRef(id);//及时是Debug版本，为啥这儿又多调了一遍这个调试方法？excuse me？ const int32_t c __unused = android_atomic_inc(&amp;impl-&gt;mWeak);//增加弱引用计数&#125; &#160; &#160; &#160; &#160;可以看到最后一行增加了弱引用计数。不过第二行那个再次增加一次Debug版本的调试链表节点很诡异唉，不知为何。有人问过Android源码Binder通信机制模块的作者Dianne Hackborn ，他是这样回答的： Ah I see. Well the debug code may be broken, though I wouldn’t leap to that conclusion without actually testing it; I know it has been used in the past. Anyway, these things get compiled out in non-debug builds, so there is no reason to change them unless you are actually trying to use this debug code and it isn’t working and need to do this to fix it. &#160; &#160; &#160; &#160;然而大神也不知道，那我们也不用管了= 。= &#160; &#160; &#160; &#160;所以sp在其构造函数里面所做的事情就是分别为目标对象的强引用计数和弱引和计数增加了1。 &#160; &#160; &#160; &#160;然后我们再看看析构函数：12345template&lt;typename T&gt; sp&lt;T&gt;::~sp() &#123; if (m_ptr) m_ptr-&gt;decStrong(this); &#125; &#160; &#160; &#160; &#160;同样，这里的m_ptr指向的目标对象一定是继承了RefBase类的，因此，这里调用的是RefBase类的decStrong函数，这也是定义在system/core/libutils/RefBash.cpp文件中：1234567891011121314void RefBase::decStrong(const void* id) const&#123; weakref_impl* const refs = mRefs; refs-&gt;removeStrongRef(id);//Debug版本的，没什么卵用 const int32_t c = android_atomic_dec(&amp;refs-&gt;mStrong);//减少强引用计数 if (c == 1) &#123;//如果强引用计数减为0了 refs-&gt;mBase-&gt;onLastStrongRef(id);//最后以一次引用了的回调 if ((refs-&gt;mFlags&amp;OBJECT_LIFETIME_MASK) == OBJECT_LIFETIME_STRONG) &#123;//如果是强引用主导生命周期 delete this;//则直接回收这个对象 &#125; &#125; refs-&gt;decWeak(id); //减少弱引用计数&#125; &#160; &#160; &#160; &#160;decStrong函数每次先减少强引用计数；然后判断是不是强引用计数是否为0，如果为0则看看标志位是不是设了OBJECT_LIFETIME_WEAK或者其他的，如果没有设置，则直接delete对象；如果受OBJECT_LIFETIME_WEAK等标志位影响，比如受弱应用计数主导生命周期，则等到弱引用计数也为0才能delete对象；最后在减少弱引用计数。&#160; &#160; &#160; &#160;真正实现强引用计数减1的操作下面的refs-&gt;decWeak函数，weakref_impl类没有实现自己的decWeak函数，它继承了weakref_type类的decWeak函数：12345678910111213141516171819202122232425262728293031void RefBase::weakref_type::decWeak(const void* id)&#123; weakref_impl* const impl = static_cast&lt;weakref_impl*&gt;(this);//强制类型转换 impl-&gt;removeWeakRef(id);//这个和上面那个为毛一个道理，Google大神都不知道，不用鸟 const int32_t c = android_atomic_dec(&amp;impl-&gt;mWeak);//减少弱引用计数 if (c != 1) return;//如果没有减少到0，则直接返回 //如果弱引用计数减到0了 if ((impl-&gt;mFlags&amp;OBJECT_LIFETIME_WEAK) == OBJECT_LIFETIME_STRONG) &#123;//如果生命周期是强引用主导，则直接删除对象 // This is the regular lifetime case. The object is destroyed // when the last strong reference goes away. Since weakref_impl // outlive the object, it is not destroyed in the dtor, and // we'll have to do it here. if (impl-&gt;mStrong == INITIAL_STRONG_VALUE) &#123; // Special case: we never had a strong reference, so we need to // destroy the object now. delete impl-&gt;mBase; &#125; else &#123; // ALOGV("Freeing refs %p of old RefBase %p\n", this, impl-&gt;mBase); delete impl; &#125; &#125; else &#123;//如果生命周期受弱引用或者FOREVER类型主导 // less common case: lifetime is OBJECT_LIFETIME_&#123;WEAK|FOREVER&#125; impl-&gt;mBase-&gt;onLastWeakRef(id); if ((impl-&gt;mFlags&amp;OBJECT_LIFETIME_MASK) == OBJECT_LIFETIME_WEAK) &#123;//如果生命周期受弱引用主导 // this is the OBJECT_LIFETIME_WEAK case. The last weak-reference // is gone, we can destroy the object. delete impl-&gt;mBase; &#125; //如果是FOREVER，则需要手动删除 &#125;&#125; &#160; &#160; &#160; &#160;这里减少了弱引用计数， 减1前如果发现不等于1，那么就什么也不用做就返回了，如果发现等于1，就说明当前对象的弱引用计数值为0了，这时候，就要看看是否要delete这个对象了：&#160; &#160; &#160; &#160;1. 如果目标对象的如果生命周期是强引用主导，就执行下面语句：123456789101112// This is the regular lifetime case. The object is destroyed// when the last strong reference goes away. Since weakref_impl// outlive the object, it is not destroyed in the dtor, and// we'll have to do it here.if (impl-&gt;mStrong == INITIAL_STRONG_VALUE) &#123; // Special case: we never had a strong reference, so we need to // destroy the object now. delete impl-&gt;mBase;&#125; else &#123; // ALOGV("Freeing refs %p of old RefBase %p\n", this, impl-&gt;mBase); delete impl;&#125; &#160; &#160; &#160; &#160;这一段的注释很重要，通过看注释，这段意思是：这里是减少对象的弱引用计数的地方，如果调用到这里，那么就说明前面一定有增加过此对象的弱引用计数，而增加对象的弱引用计数有两种场景的，一种场景是增加对象的强引用计数的时候，会同时增加对象的弱引用计数，另一种场景是当我们使用一个弱指针来指向对象时，在弱指针对象的构造函数里面，也会增加对象的弱引用计数，不过这时候，就只是增加对象的弱引用计数了，并没有同时增加对象的强引用计数。因此，这里在减少对象的弱引用计数时，就要分两种情况来考虑。 &#160; &#160; &#160; &#160;如果是前一种场景，这里的impl-&gt;mStrong就必然等于0，而不会等于INITIAL_STRONG_VALUE值，因此，这里就不需要delete目标对象了（impl-&gt;mBase），因为前面的RefBase::decStrong函数会负责delete这个对象。这里唯一需要做的就是把weakref_impl对象delete掉，但是，为什么要在这里delete这个weakref_impl对象呢？这里的weakref_impl对象是在RefBase的构造函数里面new出来的，理论上说应该在在RefBase的析构函数里delete掉这个weakref_impl对象的。在RefBase的析构函数里面，的确是会做这件事情：1234567891011121314151617181920RefBase::~RefBase()&#123; if (mRefs-&gt;mStrong == INITIAL_STRONG_VALUE) &#123; // we never acquired a strong (and/or weak) reference on this object. delete mRefs; &#125; else &#123; // life-time of this object is extended to WEAK or FOREVER, in // which case weakref_impl doesn't out-live the object and we // can free it now. if ((mRefs-&gt;mFlags &amp; OBJECT_LIFETIME_MASK) != OBJECT_LIFETIME_STRONG) &#123; // It's possible that the weak count is not 0 if the object // re-acquired a weak reference in its destructor if (mRefs-&gt;mWeak == 0) &#123; delete mRefs; &#125; &#125; &#125; // for debugging purposes, clear this. const_cast&lt;weakref_impl*&amp;&gt;(mRefs) = NULL;&#125; &#160; &#160; &#160; &#160;但是不要忘记，在这个场景下，目标对象是前面的RefBase::decStrong函数delete掉的，这时候目标对象就会被析构，但是它的弱引用计数值尚未执行减1操作，因此，这里的mRefs-&gt;mWeak == 0条件就不成立，于是就不会delete这个weakref_impl对象，因此，就延迟到执行这里decWeak函数时再执行。 &#160; &#160; &#160; &#160;如果是后一种情景，这里的impl-&gt;mStrong值就等于INITIAL_STRONG_VALUE了，这时候由于没有地方会负责delete目标对象，因此，就需要把目标对象（imp-&gt;mBase）delete掉了，否则就会造成内存泄漏。在delete这个目标对象的时候，就会执行RefBase类的析构函数，这时候目标对象的弱引用计数等于0，于是，就会把weakref_impl对象也一起delete掉了。 &#160; &#160; &#160; &#160;2. 然后回到else逻辑中，如果目标对象的生命周期是受弱引用计数或者OBJECT_LIFETIME_FOREVER控制的：1234567impl-&gt;mBase-&gt;onLastWeakRef(id); if ((impl-&gt;mFlags&amp;OBJECT_LIFETIME_MASK) == OBJECT_LIFETIME_WEAK) &#123;//如果生命周期受弱引用主导 // this is the OBJECT_LIFETIME_WEAK case. The last weak-reference // is gone, we can destroy the object. delete impl-&gt;mBase; &#125; //如果是FOREVER，则需要手动删除 &#160; &#160; &#160; &#160;理论上说，如果目标对象的生命周期是受弱引用计数控制的，那么当强引用计数和弱引用计数都为0的时候，这时候就应该delete目标对象了，但是这里还有另外一层控制，我们可以设置目标对象的标志值为OBJECT_LIFETIME_FOREVER，即目标对象的生命周期完全不受强引用计数和弱引用计数控制，在这种情况下，即使目标对象的强引用计数和弱引用计数都同时为0，这里也不能delete这个目标对象，那么，由谁来delete掉呢？当然是谁new出来的，就谁来delete掉了，这时候智能指针就完全退化为普通指针了，这里的智能指针设计的非常强大。 &#160; &#160; &#160; &#160;强指针这里就分析完了，小结一下： 如果对象的标志位被设置为0，那么只要发现对象的强引用计数值为0，那就会自动delete掉这个对象； 如果对象的标志位被设置为OBJECT_LIFETIME_WEAK，那么只有当对象的强引用计数和弱引用计数都为0的时候，才会自动delete掉这个对象； 如果对象的标志位被设置为OBJECT_LIFETIME_FOREVER，那么对象就永远不会自动被delete掉，谁new出来的对象谁来delete掉。 弱指针&#160; &#160; &#160; &#160;弱指针所使用的引用计数类与强指针一样，都是RefBase类。我们直接看弱指针的实现，即wp类，位于system/core/include/utils/RefBase.h中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990template &lt;typename T&gt;class wp&#123;public: typedef typename RefBase::weakref_type weakref_type; inline wp() : m_ptr(0) &#123; &#125; wp(T* other); wp(const wp&lt;T&gt;&amp; other); wp(const sp&lt;T&gt;&amp; other); template&lt;typename U&gt; wp(U* other); template&lt;typename U&gt; wp(const sp&lt;U&gt;&amp; other); template&lt;typename U&gt; wp(const wp&lt;U&gt;&amp; other); ~wp(); // Assignment wp&amp; operator = (T* other); wp&amp; operator = (const wp&lt;T&gt;&amp; other); wp&amp; operator = (const sp&lt;T&gt;&amp; other); template&lt;typename U&gt; wp&amp; operator = (U* other); template&lt;typename U&gt; wp&amp; operator = (const wp&lt;U&gt;&amp; other); template&lt;typename U&gt; wp&amp; operator = (const sp&lt;U&gt;&amp; other); void set_object_and_refs(T* other, weakref_type* refs); // promotion to sp sp&lt;T&gt; promote() const; // Reset void clear(); // Accessors inline weakref_type* get_refs() const &#123; return m_refs; &#125; inline T* unsafe_get() const &#123; return m_ptr; &#125; // Operators COMPARE_WEAK(==) COMPARE_WEAK(!=) COMPARE_WEAK(&gt;) COMPARE_WEAK(&lt;) COMPARE_WEAK(&lt;=) COMPARE_WEAK(&gt;=) inline bool operator == (const wp&lt;T&gt;&amp; o) const &#123; return (m_ptr == o.m_ptr) &amp;&amp; (m_refs == o.m_refs); &#125; template&lt;typename U&gt; inline bool operator == (const wp&lt;U&gt;&amp; o) const &#123; return m_ptr == o.m_ptr; &#125; inline bool operator &gt; (const wp&lt;T&gt;&amp; o) const &#123; return (m_ptr == o.m_ptr) ? (m_refs &gt; o.m_refs) : (m_ptr &gt; o.m_ptr); &#125; template&lt;typename U&gt; inline bool operator &gt; (const wp&lt;U&gt;&amp; o) const &#123; return (m_ptr == o.m_ptr) ? (m_refs &gt; o.m_refs) : (m_ptr &gt; o.m_ptr); &#125; inline bool operator &lt; (const wp&lt;T&gt;&amp; o) const &#123; return (m_ptr == o.m_ptr) ? (m_refs &lt; o.m_refs) : (m_ptr &lt; o.m_ptr); &#125; template&lt;typename U&gt; inline bool operator &lt; (const wp&lt;U&gt;&amp; o) const &#123; return (m_ptr == o.m_ptr) ? (m_refs &lt; o.m_refs) : (m_ptr &lt; o.m_ptr); &#125; inline bool operator != (const wp&lt;T&gt;&amp; o) const &#123; return m_refs != o.m_refs; &#125; template&lt;typename U&gt; inline bool operator != (const wp&lt;U&gt;&amp; o) const &#123; return !operator == (o); &#125; inline bool operator &lt;= (const wp&lt;T&gt;&amp; o) const &#123; return !operator &gt; (o); &#125; template&lt;typename U&gt; inline bool operator &lt;= (const wp&lt;U&gt;&amp; o) const &#123; return !operator &gt; (o); &#125; inline bool operator &gt;= (const wp&lt;T&gt;&amp; o) const &#123; return !operator &lt; (o); &#125; template&lt;typename U&gt; inline bool operator &gt;= (const wp&lt;U&gt;&amp; o) const &#123; return !operator &lt; (o); &#125;private: template&lt;typename Y&gt; friend class sp; template&lt;typename Y&gt; friend class wp; T* m_ptr; weakref_type* m_refs;&#125;; &#160; &#160; &#160; &#160;与强指针类相比，它们都有一个成员变量m_ptr指向目标对象，但是弱指针还有一个额外的成员变量m_refs，它的类型是weakref_type指针，下面我们分析弱指针的构造函数时再看看它是如果初始化的。这里我们需要关注的仍然是弱指针的构造函数和析构函数。 &#160; &#160; &#160; &#160;先看拷贝构造函数：123456template&lt;typename T&gt; wp&lt;T&gt;::wp(T* other) : m_ptr(other) &#123; if (other) m_refs = other-&gt;createWeak(this); &#125; &#160; &#160; &#160; &#160;这里的参数other一定是继承了RefBase类，因此，这里调用了RefBase类的createWeak函数，它定义在system/core/libutils/RefBase.cpp文件中： 12345RefBase::weakref_type* RefBase::createWeak(const void* id) const&#123; mRefs-&gt;incWeak(id); return mRefs;&#125; &#160; &#160; &#160; &#160;这里的成员变量mRefs的类型为weakref_impl指针，weakref_impl类的incWeak函数我们在前面已经看过了，它的作用就是增加对象的弱引用计数。函数最后返回mRefs，于是，弱指针对象的成员变量m_refs就指向目标对象的weakref_impl对象了。 &#160; &#160; &#160; &#160;再看看析构函数：12345template&lt;typename T&gt;wp&lt;T&gt;::~wp()&#123; if (m_ptr) m_refs-&gt;decWeak(this);&#125; &#160; &#160; &#160; &#160;这里，弱指针在析构的时候，与强指针析构不一样，它直接就调用目标对象的weakref_impl对象的decWeak函数来减少弱引用计数了，当弱引用计数为0的时候，就会根据在目标对象的标志位（0、OBJECT_LIFETIME_WEAK或者OBJECT_LIFETIME_FOREVER）来决定是否要delete目标对象，前面我们已经介绍过了，这里就不再介绍了。 &#160; &#160; &#160; &#160;分析到这里，弱指针还没介绍完，它最重要的特性我们还没有分析到。前面我们说过，弱指针的最大特点是它不能直接操作目标对象，这是怎么样做到的呢？秘密就在于弱指针类没有重载*和-&gt;操作符号，而强指针重载了这两个操作符号。但是，如果我们要操作目标对象，应该怎么办呢，这就要把弱指针升级为强指针了：123456789template&lt;typename T&gt;sp&lt;T&gt; wp&lt;T&gt;::promote() const&#123; sp&lt;T&gt; result; if (m_ptr &amp;&amp; m_refs-&gt;attemptIncStrong(&amp;result)) &#123; result.set_pointer(m_ptr); &#125; return result;&#125; &#160; &#160; &#160; &#160;升级的方式就使用成员变量m_ptr和m_refs来构造一个强指针sp，这里的m_ptr为指目标对象的一个指针，而m_refs则是指向目标对象里面的weakref_impl对象。主要就是初始化指向目标对象的成员变量m_ptr了，如果目标对象还存在，这个m_ptr就指向目标对象，如果目标对象已经不存在，m_ptr就为NULL，升级成功与否就要看refs-&gt;attemptIncStrong函数的返回结果了：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495bool RefBase::weakref_type::attemptIncStrong(const void* id)&#123; incWeak(id);//先增加弱引用计数 weakref_impl* const impl = static_cast&lt;weakref_impl*&gt;(this); int32_t curCount = impl-&gt;mStrong; while (curCount &gt; 0 &amp;&amp; curCount != INITIAL_STRONG_VALUE) &#123;//如果强引用计数大于0，且不是第一次引用，直接强引用计数加一 // we're in the easy/common case of promoting a weak-reference // from an existing strong reference. if (android_atomic_cmpxchg(curCount, curCount+1, &amp;impl-&gt;mStrong) == 0) &#123; break; &#125; // the strong count has changed on us, we need to re-assert our // situation. curCount = impl-&gt;mStrong; &#125; if (curCount &lt;= 0 || curCount == INITIAL_STRONG_VALUE) &#123;//强引用计数小于等于0，或者从未被强指针引用过 // we're now in the harder case of either: // - there never was a strong reference on us // - or, all strong references have been released if ((impl-&gt;mFlags&amp;OBJECT_LIFETIME_WEAK) == OBJECT_LIFETIME_STRONG) &#123;//如果生命周期是强引用计数导向 // this object has a "normal" life-time, i.e.: it gets destroyed // when the last strong reference goes away if (curCount &lt;= 0) &#123;//强引用计数为0，说明对象已被销毁，则升级失败 // the last strong-reference got released, the object cannot // be revived. decWeak(id); return false; &#125; // here, curCount == INITIAL_STRONG_VALUE, which means // there never was a strong-reference, so we can try to // promote this object; we need to do that atomically. while (curCount &gt; 0) &#123;//从未被强指针引用过，则强引用计数+1 if (android_atomic_cmpxchg(curCount, curCount + 1, &amp;impl-&gt;mStrong) == 0) &#123; break; &#125; // the strong count has changed on us, we need to re-assert our // situation (e.g.: another thread has inc/decStrong'ed us) curCount = impl-&gt;mStrong; &#125; if (curCount &lt;= 0) &#123;如果多线程操作，入另一个线程销毁了这个对象，则升级失败 // promote() failed, some other thread destroyed us in the // meantime (i.e.: strong count reached zero). decWeak(id); return false; &#125; &#125; else &#123;//如果生命周期收弱引用计数导向，或者是BJECT_LIFETIME_FOREVER // this object has an "extended" life-time, i.e.: it can be // revived from a weak-reference only. // Ask the object's implementation if it agrees to be revived if (!impl-&gt;mBase-&gt;onIncStrongAttempted(FIRST_INC_STRONG, id)) &#123;//尝试升级，默认为true // it didn't so give-up. decWeak(id); return false; &#125; // grab a strong-reference, which is always safe due to the // extended life-time. curCount = android_atomic_inc(&amp;impl-&gt;mStrong);//升级后强引用计数+1 &#125; // If the strong reference count has already been incremented by // someone else, the implementor of onIncStrongAttempted() is holding // an unneeded reference. So call onLastStrongRef() here to remove it. // (No, this is not pretty.) Note that we MUST NOT do this if we // are in fact acquiring the first reference. if (curCount &gt; 0 &amp;&amp; curCount &lt; INITIAL_STRONG_VALUE) &#123; impl-&gt;mBase-&gt;onLastStrongRef(id); &#125; &#125; impl-&gt;addStrongRef(id);//没什么卵用 // now we need to fix-up the count if it was INITIAL_STRONG_VALUE // this must be done safely, i.e.: handle the case where several threads // were here in attemptIncStrong(). curCount = impl-&gt;mStrong; while (curCount &gt;= INITIAL_STRONG_VALUE) &#123;//如果此弱指针是允计提升为强指针的，并且此目标对象是第一次被强指针引用，还需要调整一下目标对象的强引用计数值 if (android_atomic_cmpxchg(curCount, curCount-INITIAL_STRONG_VALUE, &amp;impl-&gt;mStrong) == 0) &#123; break; &#125; // the strong-count changed on us, we need to re-assert the situation, // for e.g.: it's possible the fix-up happened in another thread. curCount = impl-&gt;mStrong; &#125; return true;&#125; &#160; &#160; &#160; &#160;这个函数的作用是试图增加目标对象的强引用计数，但是有可能会失败，失败的原因可能是因为目标对象已经被delete掉了，或者是其它的原因，下面会分析到。前面我们在讨论强指针的时候说到，增加目标对象的强引用计数的同时，也会增加目标对象的弱引用计数，因此，函数在开始的地方首先就是调用incWeak函数来先增加目标对象的引用计数，如果后面试图增加目标对象的强引用计数失败时，会调用decWeak函数来回滚前面的incWeak操作。 &#160; &#160; &#160; &#160;这里试图增加目标对象的强引用计数时，分两种情况讨论，一种情况是此时目标对象正在被其它强指针引用，即它的强引用计数大于0，并且不等于INITIAL_STRONG_VALUE，另一种情况是此时目标对象没有被任何强指针引用，即它的强引用计数小于等于0，或者等于INITIAL_STRONG_VALUE。 &#160; &#160; &#160; &#160; 第一种情况比较简单，因为这时候说明目标对象一定存在，因此，是可以将这个弱指针提升为强指针的，在这种情况下，只要简单地增加目标对象的强引用计数值就行了：12345678910while (curCount &gt; 0 &amp;&amp; curCount != INITIAL_STRONG_VALUE) &#123;//如果强引用计数大于0，且不是第一次引用，直接强引用计数加一 // we're in the easy/common case of promoting a weak-reference // from an existing strong reference. if (android_atomic_cmpxchg(curCount, curCount+1, &amp;impl-&gt;mStrong) == 0) &#123; break; &#125; // the strong count has changed on us, we need to re-assert our // situation. curCount = impl-&gt;mStrong; &#125; &#160; &#160; &#160; &#160;当我们在这里对目标对象的强引用计数执行加1操作时，要保证原子性，因为其它地方也有可能正在对这个目标对象的强引用计数执行加1的操作，前面我们一般是调用android_atomic_inc函数来完成，但是这里是通过调用android_atomic_cmpxchg函数来完成，android_atomic_cmpxchg函数是体系结构相关的函数，在提供了一些特殊的指令的体系结构上，调用android_atomic_cmpxchg函数来执行加1操作的效率会比调用android_atomic_inc函数更高一些。函数android_atomic_cmpxchg是在system/core/include/cutils/atomic.h文件中定义的一个宏：1234int android_atomic_release_cas(int32_t oldvalue, int32_t newvalue, volatile int32_t* addr); #define android_atomic_cmpxchg android_atomic_release_cas &#160; &#160; &#160; &#160;它实际执行的函数是android_atomic_release_cas，这个函数的工作原理大概是这样的：如果它发现addr == oldvalue，就会执行addr = newvalue的操作，然后返回0，否则什么也不做，返回1。在我们讨论的这个场景中，oldvalue等于curCount，而newvalue等于curCount + 1，于是，在addr == oldvalue的条件下，就相当于是对目标对象的强引用计数值增加了1。什么情况下addr != oldvalue呢？在调用android_atomic_release_cas函数之前，oldvalue和值就是从地址addr读出来的，如果在执行android_atomic_release_cas函数的时候，有其它地方也对地址addr进行操作，那么就会有可能出现*addr != oldvalue的情况，这时候就说明其它地方也在操作目标对象的强引用计数了，因此，这里就不能执行增加目标对象的强引用计数的操作了，它必须要等到其它地方操作完目标对象的强引用计数之后再重新执行，这就是为什么要通过一个while循环来执行了。 &#160; &#160; &#160; &#160;第二种情况比较复杂一点，因为这时候目标对象可能还存在，也可能不存了，这要根据实际情况来判断。如果此时目标对象的强引用计数值等于INITIAL_STRONG_VALUE，说明此目标对象还从未被强指针引用过，这时候弱指针能够被提升为强指针的条件就为:12345678910111213141516171819202122232425262728293031323334353637383940414243444546if (curCount &lt;= 0 || curCount == INITIAL_STRONG_VALUE) &#123;//强引用计数小于等于0，或者从未被强指针引用过 // we're now in the harder case of either: // - there never was a strong reference on us // - or, all strong references have been released if ((impl-&gt;mFlags&amp;OBJECT_LIFETIME_WEAK) == OBJECT_LIFETIME_STRONG) &#123;//如果生命周期是强引用计数导向 // this object has a "normal" life-time, i.e.: it gets destroyed // when the last strong reference goes away if (curCount &lt;= 0) &#123;//强引用计数为0，说明对象已被销毁，则升级失败 // the last strong-reference got released, the object cannot // be revived. decWeak(id); return false; &#125; // here, curCount == INITIAL_STRONG_VALUE, which means // there never was a strong-reference, so we can try to // promote this object; we need to do that atomically. while (curCount &gt; 0) &#123;//从未被强指针引用过，则强引用计数+1 if (android_atomic_cmpxchg(curCount, curCount + 1, &amp;impl-&gt;mStrong) == 0) &#123; break; &#125; // the strong count has changed on us, we need to re-assert our // situation (e.g.: another thread has inc/decStrong'ed us) curCount = impl-&gt;mStrong; &#125; if (curCount &lt;= 0) &#123;如果多线程操作，入另一个线程销毁了这个对象，则升级失败 // promote() failed, some other thread destroyed us in the // meantime (i.e.: strong count reached zero). decWeak(id); return false; &#125; &#125; else &#123;//如果生命周期收弱引用计数导向，或者是BJECT_LIFETIME_FOREVER // this object has an "extended" life-time, i.e.: it can be // revived from a weak-reference only. // Ask the object's implementation if it agrees to be revived if (!impl-&gt;mBase-&gt;onIncStrongAttempted(FIRST_INC_STRONG, id)) &#123;//尝试升级，默认为true // it didn't so give-up. decWeak(id); return false; &#125; // grab a strong-reference, which is always safe due to the // extended life-time. curCount = android_atomic_inc(&amp;impl-&gt;mStrong);//升级后强引用计数+1 &#125; &#160; &#160; &#160; &#160;1) 即如果目标对象的生命周期只受到强引用计数控制或者在目标对象的具体实现中总是允许这种情况发生。怎么理解呢？如果目标对象的生命周期只受强引用计数控制（它的标志位mFlags为0），而这时目标对象又还未被强指针引用过，它自然就不会被delete掉，因此，这时候可以判断出目标对象是存在的；如果目标对象的生命周期受弱引用计数控制（OBJECT_LIFETIME_WEAK），这时候由于目标对象正在被弱指针引用，因此，弱引用计数一定不为0，目标对象一定存在；如果目标对象的生命周期不受引用计数控制（BJECT_LIFETIME_FOREVER），这时候目标对象也是下在被弱指针引用，因此，目标对象的所有者必须保证这个目标对象还没有被delete掉，否则就会出问题了。 &#160; &#160; &#160; &#160;在后面两种场景下，因为目标对象的生命周期都是不受强引用计数控制的，而现在又要把弱指针提升为强指针，就需要进一步调用目标对象的onIncStrongAttempted来看看是否允许这种情况发生，这又该怎么理解呢？可以这样理解，目标对象的设计者可能本身就不希望这个对象被强指针引用，只能通过弱指针来引用它，因此，这里它就可以重载其父类的onIncStrongAttempted函数，然后返回false，这样就可以阻止弱指针都被提升为强指针。在RefBase类中，其成员函数onIncStrongAttempted默认是返回true的：1234bool RefBase::onIncStrongAttempted(uint32_t flags, const void* id) &#123; return (flags&amp;FIRST_INC_STRONG) ? true : false; &#125; &#160; &#160; &#160; &#160;2) 如果此时目标对象的强引用计数值小于等于0，那就说明该对象之前一定被强指针引用过，这时候就必须保证目标对象是被弱引用计数控制的（BJECT_LIFETIME_WEAK），否则的话，目标对象就已经被delete了。同样，这里也要调用一下目标对象的onIncStrongAttempted成员函数，来询问一下目标对象在强引用计数值小于等于0的时候，是否允计将弱指针提升为强指针。 &#160; &#160; &#160; &#160;然后将强引用计数加一，还有一些收尾处理，函数的最后，如果此弱指针是允计提升为强指针的，并且此目标对象是第一次被强指针引用，还需要调整一下目标对象的强引用计数值。 &#160; &#160; &#160; &#160;到这里弱指针就分析完了，它和强指针逻辑大同小异。 使用前提&#160; &#160; &#160; &#160;说了这么多原理，下面该看看智能指针该如何使用了。假设现在有一个类MyClass，如果要使用智能指针来引用这个类的对象，那么这个类需满足下列两个前提条件：1：这个类是基类RefBase的子类或间接子类；2：这个类必须定义虚构造函数，即它的构造函数需要这样定义：&#160; &#160; &#160; &#160;virtual ~MyClass();&#160; &#160; &#160; &#160;满足了上述条件的类就可以定义为Android智能指针了，定义方法和普通指针类似。比如&#160; &#160; &#160; &#160;普通指针是这样定义：&#160; &#160; &#160; &#160;MyClass p_obj;&#160; &#160; &#160; &#160;智能指针是这样定义：&#160; &#160; &#160; &#160;sp p_obj;&#160; &#160; &#160; &#160;注意不要定义成sp p_obj。这是初学者很容易犯的错误，这样其实相当于定义了一个指针的指针。尽管在语法上没有问题，但是最好不要这样定义。&#160; &#160; &#160; &#160;定义了一个智能指针的变量，就可以象普通指针那样使用它，包括赋值、访问对象成员、作为函数的返回值、作为函数的参数等。比如：12345p_obj = new MyClass(); // 注意不要写成 p_obj = new sp&lt;MyClass&gt; sp&lt;MyClass&gt; p_obj2 = p_obj; p_obj-&gt;func(); p_obj = create_obj(); some_func(p_obj); &#160; &#160; &#160; &#160;注意不要试图delete一个智能指针，即 delete p_obj。不要担心对象的销毁问题，智能指针的最大作用就是自动销毁不再使用的对象。不需要再使用一个对象后，直接将指针赋值为NULL即可：&#160; &#160; &#160; &#160;p_obj = NULL;&#160; &#160; &#160; &#160;上面说的都是强指针，弱指针的定义方法和强指针类似，但是不能通过弱指针来访问对象的成员。下面是弱指针的示例：123wp&lt;MyClass&gt; wp_obj = new MyClass(); p_obj = wp_obj.promote(); // 升级为强指针。不过这里要用.而不是-&gt;，真是有负其指针之名啊 wp_obj = NULL; 结语&#160; &#160; &#160; &#160;至此，智能指针部分就分析完了，我们不得不赞叹Google设计的强大。]]></content>
      <categories>
        <category>C++拾遗</category>
      </categories>
      <tags>
        <tag>智能指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android消息处理零散分析]]></title>
    <url>%2F2016%2F07%2F31%2FAndroid%E6%B6%88%E6%81%AF%E6%9C%BA%E5%88%B6%E9%9B%B6%E6%95%A3%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;Android消息机制是开发人员用滥的内容，不过内部原理比较复杂，因此今天抽空从头到位彻底分析一下。 前言&#160; &#160; &#160; &#160;我们知道，Android应用程序是通过消息来驱动的。Google参考了Windows的消息处理机制，在Android系统中实现了一套类似的消息处理机制。&#160; &#160; &#160; &#160;了解过Win32开发的同学应该都知道windows是事件驱动的，事件驱动围绕着消息的产生与处理展开，事件驱动是靠消息循环机制来实现的。消息机制的三大要点：消息队列、消息循环(分发)、消息处理。其结构如下： 消息队列就是存放消息的一种队列，具有先进先出的特点。每产生一个消息都会添加进消息队列中，在Windows中消息队列是在操作系统中定义的。消息队列就如同一群排队打饭的同学，这群人中光景较好的排在前面，光景较差的排在后面，可以理解成是一种优先级队列！ 消息循环就是通过循环(如while)不断地从消息队列中取得队首的消息，并将消息分发出去。类似于食堂打饭的阿姨。 消息处理就是在接收到消息之后根据不同的消息类型做出不同的处理。食堂阿姨根据学生点的不同类型的菜名给他们打不同的饭菜就是消息处理，学生手点的菜名就是消息所携带的信息 事件是根据接收到的消息的具体信息做出的特定的处理，放在代码中是事件响应函数。上面的例子中学生拿到饭菜后吃饭就是具体的事件。 &#160; &#160; &#160; &#160;Android的消息处理机制也是基于上述模型的，实现消息驱动对应模型为： Message：消息，理解为线程间通讯的数据单元。 MessageQueue：消息队列，用来存放通过Handler发布的消息，按照先进先出执行。 Looper：消息循环，扮演MessageQueue和Handler之间桥梁的角色，循环取出MessageQueue里面的Message，并交付给相应的Handler进行处理。 Handler：Handler是Message的主要处理者，负责将Message添加到消息队列以及对消息队列中的Message进行处理。 流程分析初始化消息队列&#160; &#160; &#160; &#160;Android应用程序进程在启动时，会在进程中加载ActivityThread类，并且执行这个类的main方法，应用程序的消息循环过程就是在这个main方法里面实现的。ActivityThread位于frameworks/base/core/java/android/app/ActivityThread.java：12345678910111213public final class ActivityThread &#123; public static void main(String[] args) &#123; ...... Looper.prepareMainLooper(); ActivityThread thread = new ActivityThread(); thread.attach(false); ...... Looper.loop(); ...... &#125;&#125; &#160; &#160; &#160; &#160;main方法做了两件事，一是在主线程创建了一个ActivityThread实例，二是通过Looper类使主线程进入消息循环。我们只关注后者。先看Looper.prepareMainLooper方法：1234567891011121314151617181920212223242526272829303132 static final ThreadLocal&lt;Looper&gt; sThreadLocal = new ThreadLocal&lt;Looper&gt;(); private static Looper sMainLooper; final MessageQueue mQueue;//消息队列 final Thread mThread;//当前线程，这里是主线程 public static void prepareMainLooper() &#123; prepare(false);//调用prepare(false)方法 synchronized (Looper.class) &#123; if (sMainLooper != null) &#123; throw new IllegalStateException("The main Looper has already been prepared."); &#125; sMainLooper = myLooper(); &#125; &#125; //带参数prepare方法，参数quitAllowed表示是否可以退出消息循环 private static void prepare(boolean quitAllowed) &#123; if (sThreadLocal.get() != null) &#123; throw new RuntimeException("Only one Looper may be created per thread"); &#125; sThreadLocal.set(new Looper(quitAllowed));//存入sThreadLocal线程局部变量 &#125; private Looper(boolean quitAllowed) &#123; mQueue = new MessageQueue(quitAllowed);//初始化一个消息队列 mThread = Thread.currentThread(); &#125; public static Looper myLooper() &#123; return sThreadLocal.get(); &#125; &#160; &#160; &#160; &#160;prepareMainLooper方法其实就是在线程中创建了一个Looper对象，这个Looper对象存放在类型为ThreadLocal的sThreadLocal线程局部变量中，保证每一个调用prepareMainLooper方法的线程只有一个独立的Looper对象。&#160; &#160; &#160; &#160;Looper对象的构造方法里又创建了一个消息队列MessageQueue，后续消息Message就是存放在这个队列中。我们看看它的创建过程：12345678910// True if the message queue can be quit.private final boolean mQuitAllowed; private long mPtr; // used by native codeMessageQueue(boolean quitAllowed) &#123;//quitAllowed为false，说明main线程消息循环不允许退出 mQuitAllowed = quitAllowed; mPtr = nativeInit();&#125;private native static long nativeInit(); &#160; &#160; &#160; &#160;MessageQueue的初始化都交给了nativeInit这个本地方法，我们看看它的具体实现，位于frameworks/base/core/jni/android_os_MessageQueue.cpp文件中：12345678910static jlong android_os_MessageQueue_nativeInit(JNIEnv* env, jclass clazz) &#123; NativeMessageQueue* nativeMessageQueue = new NativeMessageQueue();//建立一个NativeMessageQueue消息队列对象 if (!nativeMessageQueue) &#123; jniThrowRuntimeException(env, "Unable to allocate native queue"); return 0; &#125; nativeMessageQueue-&gt;incStrong(env);//强引用计数+1 return reinterpret_cast&lt;jlong&gt;(nativeMessageQueue);//强转为java层long类型变量，返回给java层mPtr变量保存&#125; &#160; &#160; &#160; &#160;在JNI层也相应建立一个消息队列NativeMessageQueue，建立成功后会将它的强引用计数加一，Google的智能指针需要这么干；然后通过C++操作符reinterpret_cast将NativeMessageQueue类型变量强转为jlong类型，并返回到java层，方便java层操作。 NativeMessageQueue同样定义在frameworks/base/core/jni/android_os_MessageQueue.cpp中，我们看看它的构造函数：1234567NativeMessageQueue::NativeMessageQueue() : mInCallback(false), mExceptionObj(NULL) &#123; mLooper = Looper::getForThread(); if (mLooper == NULL) &#123; mLooper = new Looper(false);//创建一个JNI层的Looper对象 Looper::setForThread(mLooper); &#125;&#125; &#160; &#160; &#160; &#160;它主要就是在内部创建了一个Looper对象，注意，这个Looper对象是实现在JNI层的，它与上面Java层中的Looper是不一样的，不过它们是对应的，下面我们进一步分析消息循环的过程的时候，就会清楚地了解到它们之间的关系。我们接着看JNI层Looper的构造函数实现，位于system/core/libutils/Looper.cpp中：123456789101112131415161718192021222324252627282930313233343536Looper::Looper(bool allowNonCallbacks) : mAllowNonCallbacks(allowNonCallbacks), mSendingMessage(false), mResponseIndex(0), mNextMessageUptime(LLONG_MAX) &#123; int wakeFds[2];//准备两个文件描述符 int result = pipe(wakeFds);//创建一个管道 LOG_ALWAYS_FATAL_IF(result != 0, "Could not create wake pipe. errno=%d", errno); mWakeReadPipeFd = wakeFds[0];//管道读端 mWakeWritePipeFd = wakeFds[1];//管道写端 result = fcntl(mWakeReadPipeFd, F_SETFL, O_NONBLOCK);//将管道读端设为非阻塞模式 LOG_ALWAYS_FATAL_IF(result != 0, "Could not make wake read pipe non-blocking. errno=%d", errno); result = fcntl(mWakeWritePipeFd, F_SETFL, O_NONBLOCK);//管道写端同样设为非阻塞 LOG_ALWAYS_FATAL_IF(result != 0, "Could not make wake write pipe non-blocking. errno=%d", errno); mIdling = false; // Allocate the epoll instance and register the wake pipe. mEpollFd = epoll_create(EPOLL_SIZE_HINT);//创建一个epoll专用的文件描述符 LOG_ALWAYS_FATAL_IF(mEpollFd &lt; 0, "Could not create epoll instance. errno=%d", errno); //epoll其中一个专用结构体 struct epoll_event eventItem; //把结构体清零 memset(&amp; eventItem, 0, sizeof(epoll_event)); // zero out unused members of data field union //重新赋值 eventItem.events = EPOLLIN;//EPOLLIN ：表示对应的文件描述符可以读； eventItem.data.fd = mWakeReadPipeFd;//fd：关联的文件描述符； //epoll_ctl函数用于控制某个epoll文件描述符上的事件，可以注册事件，修改事件，删除事件。这里是添加事件 result = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, mWakeReadPipeFd, &amp; eventItem); LOG_ALWAYS_FATAL_IF(result != 0, "Could not add wake read pipe to epoll instance. errno=%d", errno);&#125; &#160; &#160; &#160; &#160;这个构造函数做的事情非常重要，它跟我们后面要介绍的应用程序主线程在消息队列中没有消息时要进入等待状态以及当消息队列有消息时要把应用程序主线程唤醒的这两个知识点息息相关。&#160; &#160; &#160; &#160;它主要做了两件事：&#160; &#160; &#160; &#160;（1）通过pipe系统调用来创建了一个管道了：123456int wakeFds[2]; int result = pipe(wakeFds); ...... mWakeReadPipeFd = wakeFds[0]; mWakeWritePipeFd = wakeFds[1]; &#160; &#160; &#160; &#160;管道是Linux系统中的一种进程间通信机制。简单来说，管道就是一个文件，在管道的两端，分别是两个打开文件文件描述符，这两个打开文件描述符都是对应同一个文件，其中一个是用来读的，另一个是用来写的。一般的使用方式就是，一个线程通过读文件描述符中来读管道的内容，当管道没有内容时，这个线程就会进入等待状态；而另外一个线程通过写文件描述符来向管道中写入内容，写入内容的时候，如果另一端正有线程正在等待管道中的内容，那么这个线程就会被唤醒。这个等待和唤醒的操作是如何进行的呢，这就要借助Linux系统中的epoll机制了。&#160; &#160; &#160; &#160;Linux系统中的epoll机制为处理大批量句柄而作了改进的poll，是Linux下多路复用I/O接口select/poll的增强版本，它能显著减少程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。但是这里我们其实只需要监控的I/O接口只有mWakeReadPipeFd一个，即前面我们所创建的管道的读端，为什么还需要用到epoll呢？有点用牛刀来杀鸡的味道。其实不然，这个Looper类是非常强大的，它除了监控内部所创建的管道接口之外，还提供了addFd接口供外界面调用，外界可以通过这个接口把自己想要监控的I/O事件一并加入到这个Looper对象中去，当所有这些被监控的I/O接口上面有事件发生时，就会唤醒相应的线程来处理，不过这里我们只关心刚才所创建的管道的I/O事件的发生。 &#160; &#160; &#160; &#160;（2）epoll机制相关初始化：&#160; &#160; &#160; &#160;要使用Linux系统的epoll机制，首先要通过epoll_create来创建一个epoll专用的文件描述符：1mEpollFd = epoll_create(EPOLL_SIZE_HINT); &#160; &#160; &#160; &#160;epoll_create(int size)函数生成一个epoll专用的文件描述符。它其实是在内核申请一空间，用来存放你想关注的fd上是否发生以及发生了什么事件。size就是你在这个epoll fd上能关注的最大fd数。我们这里传入的二是EPOLL_SIZE_HINT。 &#160; &#160; &#160; &#160;接着还要通过epoll_ctl函数来告诉epoll要监控相应的文件描述符的什么事件：123456789//epoll其中一个专用结构体 struct epoll_event eventItem; //把结构体清零 memset(&amp; eventItem, 0, sizeof(epoll_event)); // zero out unused members of data field union //重新赋值 eventItem.events = EPOLLIN;//EPOLLIN ：表示对应的文件描述符可以读； eventItem.data.fd = mWakeReadPipeFd;//fd：关联的文件描述符； //epoll_ctl函数用于控制某个epoll文件描述符上的事件，可以注册事件，修改事件，删除事件。这里是添加事件 result = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, mWakeReadPipeFd, &amp; eventItem); &#160; &#160; &#160; &#160;epoll_ctl函数如下：函数声明：int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)该函数用于控制某个epoll文件描述符上的事件，可以注册事件，修改事件，删除事件。参数：epfd：由 epoll_create 生成的epoll专用的文件描述符；op：要进行的操作例如注册事件，可能的取值EPOLL_CTL_ADD 注册、EPOLL_CTL_MOD 修 改、EPOLL_CTL_DEL 删除fd：关联的文件描述符；event：指向epoll_event结构体的指针；返回值：如果调用成功返回0,不成功返回-1 &#160; &#160; &#160; &#160;用到的结构体如下：1234567891011struct epoll_event &#123; __uint32_t events; /* Epoll events */ //事件类型epoll_data_t data; /* User data variable */ &#125;; typedef union epoll_data &#123; void *ptr; int fd; //关联的文件描述符__uint32_t u32; __uint64_t u64; &#125; epoll_data_t; &#160; &#160; &#160; &#160;epoll_event 结构体常用的事件类型:EPOLLIN ：表示对应的文件描述符可以读；EPOLLOUT：表示对应的文件描述符可以写；EPOLLPRI：表示对应的文件描述符有紧急的数据可读EPOLLERR：表示对应的文件描述符发生错误；EPOLLHUP：表示对应的文件描述符被挂断；EPOLLET：表示对应的文件描述符有事件发生； &#160; &#160; &#160; &#160;这里就是告诉mEpollFd，它要监控mWakeReadPipeFd文件描述符的EPOLLIN事件，即当管道中有内容可读时，就唤醒当前正在等待管道中的内容的线程。&#160; &#160; &#160; &#160;C++层的这个Looper对象创建好了之后，就返回到JNI层的NativeMessageQueue的构造函数，最后就返回到Java层的消息队列MessageQueue的创建过程，这样，Java层的Looper对象就准备好了。 &#160; &#160; &#160; &#160;对上面内容小结一下就是：&#160; &#160; &#160; &#160;①在Java层，创建了一个Looper对象，这个Looper对象是用来进入消息循环的，它的内部有一个消息队列MessageQueue对象mQueue；&#160; &#160; &#160; &#160;②在JNI层，创建了一个NativeMessageQueue对象，这个NativeMessageQueue对象保存在Java层的消息队列对象mQueue的成员变量mPtr中；&#160; &#160; &#160; &#160;③在C++层，创建了一个Looper对象，保存在JNI层的NativeMessageQueue对象的成员变量mLooper中，这个对象的作用是，当Java层的消息队列中没有消息时，就使Android应用程序主线程进入等待状态，而当Java层的消息队列中来了新的消息后，就唤醒Android应用程序的主线程来处理这个消息。 消息循环&#160; &#160; &#160; &#160;继续回到上面ActivitThread的main方法里，在上面这些工作都准备好之后就调用Looper.loop方法进入到消息循环中了。&#160; &#160; &#160; &#160;消息循环就会取出消息进行处理，在看消息处理之前，先看一下消息是怎么被添加到消息队列的。 发送消息&#160; &#160; &#160; &#160;在Java层，Message类表示一个消息对象，要发送消息首先就要先获得一个消息对象，Message类的构造函数是public的，但是不建议直接new Message，Message内部保存了一个缓存的消息池，我们可以用obtain从缓存池获得一个消息，Message使用完后系统会调用recycle回收，如果自己new很多Message，每次使用完后系统放入缓存池，会占用很多内存的。我们看看Message类相关方法和变量：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private static Message sPool;// sometimes we store linked lists of these things/*package*/ Message next;private static final int MAX_POOL_SIZE = 50; public static Message obtain() &#123; synchronized (sPoolSync) &#123; if (sPool != null) &#123; Message m = sPool;//取出表头 sPool = m.next; m.next = null; m.flags = 0; // clear in-use flag sPoolSize--; return m; &#125; &#125; return new Message(); &#125; public void recycle() &#123; if (isInUse()) &#123; if (gCheckRecycle) &#123; throw new IllegalStateException("This message cannot be recycled because it " + "is still in use."); &#125; return; &#125; recycleUnchecked(); &#125; void recycleUnchecked() &#123; // Mark the message as in use while it remains in the recycled object pool. // Clear out all other details. flags = FLAG_IN_USE; what = 0; arg1 = 0; arg2 = 0; obj = null; replyTo = null; sendingUid = -1; when = 0; target = null; callback = null; data = null; synchronized (sPoolSync) &#123; if (sPoolSize &lt; MAX_POOL_SIZE) &#123; next = sPool; sPool = this; sPoolSize++; &#125; &#125; &#125; &#160; &#160; &#160; &#160;Message内部通过next成员实现了一个链表，这样sPool就了为了一个Messages的缓存链表。 &#160; &#160; &#160; &#160;消息对象获取到了怎么发送呢，大家都知道是通过Handler的post、sendMessage等方法，其实这些方法最终都是调用的同一个方法sendMessageAtTime:12345678910public boolean sendMessageAtTime(Message msg, long uptimeMillis) &#123; MessageQueue queue = mQueue; if (queue == null) &#123; RuntimeException e = new RuntimeException( this + " sendMessageAtTime() called with no mQueue"); Log.w("Looper", e.getMessage(), e); return false; &#125; return enqueueMessage(queue, msg, uptimeMillis);&#125; &#160; &#160; &#160; &#160;sendMessageAtTime获取到消息队列然后调用enqueueMessage方法，消息队列mQueue是从与Handler关联的Looper获得的。1234567private boolean enqueueMessage(MessageQueue queue, Message msg, long uptimeMillis) &#123; msg.target = this; if (mAsynchronous) &#123; msg.setAsynchronous(true); &#125; return queue.enqueueMessage(msg, uptimeMillis);&#125; &#160; &#160; &#160; &#160;enqueueMessage将message的target设置为当前的handler，然后调用MessageQueue的enqueueMessage，在调用queue.enqueueMessage之前判断了mAsynchronous，从名字看是异步消息的意思，要明白Asynchronous的作用，需要先了解一个概念Barrier。 Barrier与Asynchronous Message&#160; &#160; &#160; &#160;Barrier是什么意思呢，从名字看是一个拦截器，在这个拦截器后面的消息都暂时无法执行，直到这个拦截器被移除了，MessageQueue有一个函数叫enqueueSyncBarier可以添加一个Barrier。1234567891011121314151617181920212223242526272829303132//消息队列的队头，消息队列按时间从小到大排序，这是最小的Message mMessages; int enqueueSyncBarrier(long when) &#123; // Enqueue a new sync barrier token. // We don't need to wake the queue because the purpose of a barrier is to stall it. synchronized (this) &#123; final int token = mNextBarrierToken++;//一个用来返回的自增int值作为token final Message msg = Message.obtain();//创建一个路障 msg.markInUse();//标记为正在使用 msg.when = when;//路障的时间 msg.arg1 = token;//打个token Message prev = null; Message p = mMessages;//访问消息队列队头 if (when != 0) &#123; while (p != null &amp;&amp; p.when &lt;= when) &#123;//遍历一遍消息队列，按时间找到路障位置 prev = p; p = p.next; &#125; &#125; if (prev != null) &#123; // invariant: p == prev.next //在时间所在队列位置插入路障 msg.next = p; prev.next = msg; &#125; else &#123;//路障时间小于队头时间，或者消息队列为空，将路障设为队头 msg.next = p; mMessages = msg; &#125; return token; &#125; &#125; &#160; &#160; &#160; &#160;在enqueueSyncBarrier中，obtain了一个Message，并设置msg.arg1=token，token仅是一个每次调用enqueueSyncBarrier时自增的int值，目的是每次调用enqueueSyncBarrier时返回唯一的一个token，这个Message同样需要设置执行时间，然后插入到消息队列，特殊的是这个Message没有设置target，即msg.target为null。&#160; &#160; &#160; &#160;如果队列头部的消息的target为null就表示它是个Barrier，因为只有两种方法往mMessages中添加消息，一种是enqueueMessage，另一种是enqueueBarrier，而enqueueMessage中如果mst.target为null是直接抛异常的，后面会看到。 &#160; &#160; &#160; &#160;所谓的异步消息其实就是这样的，我们可以通过enqueueBarrier往消息队列中插入一个Barrier，那么队列中执行时间在这个Barrier以后的同步消息都会被这个Barrier拦截住无法执行，直到我们调用removeBarrier移除了这个Barrier，而异步消息则没有影响，消息默认就是同步消息，除非我们调用了Message的setAsynchronous，这个方法是隐藏的。只有在初始化Handler时通过参数指定往这个Handler发送的消息都是异步的，这样在Handler的enqueueMessage中就会调用Message的setAsynchronous设置消息是异步的，从上面Handler.enqueueMessage的代码中可以看到。&#160; &#160; &#160; &#160;所谓异步消息，其实只有一个作用，就是在设置Barrier时仍可以不受Barrier的影响被正常处理，如果没有设置Barrier，异步消息就与同步消息没有区别，可以通过removeSyncBarrier移除Barrier：12345678910111213141516171819202122232425262728293031void removeSyncBarrier(int token) &#123; // Remove a sync barrier token from the queue. // If the queue is no longer stalled by a barrier then wake it. synchronized (this) &#123; Message prev = null; Message p = mMessages;//取出队头 while (p != null &amp;&amp; (p.target != null || p.arg1 != token)) &#123;//循环遍历消息队列，找出路障 prev = p; p = p.next; &#125; if (p == null) &#123;//参数token就是enqueueSyncBarrier的返回值，如果没有调用指定的token是会抛异常的 throw new IllegalStateException("The specified message queue synchronization " + " barrier token has not been posted or has already been removed."); &#125; final boolean needWake; if (prev != null) &#123;//找到路障，移除路障 prev.next = p.next; needWake = false; &#125; else &#123; mMessages = p.next; needWake = mMessages == null || mMessages.target != null; &#125; p.recycleUnchecked(); // If the loop is quitting then it is already awake. // We can assume mPtr != 0 when mQuitting is false. if (needWake &amp;&amp; !mQuitting) &#123; nativeWake(mPtr);//唤醒操作下面再讲 &#125; &#125;&#125; enqueueMessage&#160; &#160; &#160; &#160;接着我们顺着上面内容，看一下是MessageQueue的enqueueMessage的方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657boolean enqueueMessage(Message msg, long when) &#123; if (msg.target == null) &#123;//注意这里，当msg.target为null时是直接抛异常的，上面提到过 throw new IllegalArgumentException("Message must have a target."); &#125; if (msg.isInUse()) &#123;//正在使用时不能重复加入队列 throw new IllegalStateException(msg + " This message is already in use."); &#125; synchronized (this) &#123; if (mQuitting) &#123;//如果已经退出消息循环，则会异常返回 IllegalStateException e = new IllegalStateException( msg.target + " sending message to a Handler on a dead thread"); Log.w("MessageQueue", e.getMessage(), e); msg.recycle(); return false; &#125; msg.markInUse();//给消息打上正在使用标签 msg.when = when; Message p = mMessages;//取出队头 boolean needWake; //如果当前的消息队列为空，或者新添加的消息的执行时间when是0， //或者新添加的消息的执行时间比消息队列头的消息的执行时间还早， //就把消息添加到消息队列头（消息队列按时间排序） if (p == null || when == 0 || when &lt; p.when) &#123; // New head, wake up the event queue if blocked. msg.next = p; mMessages = msg; needWake = mBlocked;//当前消息队列为空时，这时候应用程序的主线程一般就是处于空闲等待状态了，这时候就要唤醒它 &#125; else &#123;//否则就要找到合适的位置将当前消息添加到消息队列 // Inserted within the middle of the queue. Usually we don't have to wake // up the event queue unless there is a barrier at the head of the queue // and the message is the earliest asynchronous message in the queue. //通常我们不用唤醒队列，除非有路障在队头并且插入的消息是更早的异步消息 needWake = mBlocked &amp;&amp; p.target == null &amp;&amp; msg.isAsynchronous(); Message prev; for (;;) &#123;//循环遍历队列，找到应该插入的位置 prev = p; p = p.next; if (p == null || when &lt; p.when) &#123; break; &#125; if (needWake &amp;&amp; p.isAsynchronous()) &#123;// 消息队列中有异步消息并且执行时间在新消息之前，所以不需要唤醒 needWake = false; &#125; &#125; msg.next = p; // invariant: p == prev.next 将目标消息插入队列位置 prev.next = msg; &#125; // We can assume mPtr != 0 because mQuitting is false. if (needWake) &#123;//唤醒下面再讲 nativeWake(mPtr); &#125; &#125; return true;&#125; &#160; &#160; &#160; &#160;注意方法内第一行，当msg.target为null时是直接抛异常的。 &#160; &#160; &#160; &#160;在enqueueMessage中首先判断，如果当前的消息队列为空，这时候应用程序的主线程一般就是处于空闲等待状态了，这时候就要唤醒它（唤醒我们下面再讲）。或者新添加的消息的执行时间when是0，或者新添加的消息的执行时间比消息队列头的消息的执行时间还早，就把消息添加到消息队列头（消息队列按时间排序），否则就要找到合适的位置将当前消息添加到消息队列。 消息循环&#160; &#160; &#160; &#160;消息队列初始化好了，也知道怎么发消息了，下面就是怎么处理消息了，看Looper.loop函数： 123456789101112131415161718192021222324 public static void loop() &#123; final Looper me = myLooper(); if (me == null) &#123; throw new RuntimeException("No Looper; Looper.prepare() wasn't called on this thread."); &#125; final MessageQueue queue = me.mQueue;...... for (;;) &#123; Message msg = queue.next(); // might block if (msg == null) &#123; // No message indicates that the message queue is quitting. return; &#125; ...... msg.target.dispatchMessage(msg); ...... msg.recycleUnchecked(); &#125; &#125; &#160; &#160; &#160; &#160;这里就是进入到消息循环中去了，它不断地从消息队列mQueue中去获取下一个要处理的消息msg，如果消息的target成员变量为null，就表示要退出消息循环了，否则的话就要调用这个target对象的dispatchMessage成员函数来处理这个消息，这个target对象的类型为Handler，下面我们分析消息的发送时会看到这个消息对象msg是如设置的。&#160; &#160; &#160; &#160;这个函数最关键的地方便是从消息队列中获取下一个要处理的消息了，即MessageQueue.next函数，我们看看它的实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104 Message next() &#123; // Return here if the message loop has already quit and been disposed. // This can happen if the application tries to restart a looper after quit // which is not supported. final long ptr = mPtr; if (ptr == 0) &#123; return null; &#125; int pendingIdleHandlerCount = -1; // -1 only during first iteration int nextPollTimeoutMillis = 0;//当前消息队列中没有消息，它要等待的时间 for (;;) &#123;......//本地方法，一次消息轮训 nativePollOnce(ptr, nextPollTimeoutMillis); synchronized (this) &#123; // Try to retrieve the next message. Return if found. final long now = SystemClock.uptimeMillis(); Message prevMsg = null; Message msg = mMessages;//取出队头消息 if (msg != null &amp;&amp; msg.target == null) &#123;//如果队头消息是路障，就往后找到第一个异步消息 // Stalled by a barrier. Find the next asynchronous message in the queue. do &#123; prevMsg = msg; msg = msg.next; &#125; while (msg != null &amp;&amp; !msg.isAsynchronous());//循环查找第一个异步消息 &#125; if (msg != null) &#123;//取出消息不为空 if (now &lt; msg.when) &#123;//如果时间还未到，就要等待 // Next message is not ready. Set a timeout to wake up when it is ready. nextPollTimeoutMillis = (int) Math.min(msg.when - now, Integer.MAX_VALUE); &#125; else &#123;//取出这个消息，并从消息队列移除 // Got a message. mBlocked = false; if (prevMsg != null) &#123; prevMsg.next = msg.next; &#125; else &#123; mMessages = msg.next; &#125; msg.next = null; if (false) Log.v("MessageQueue", "Returning message: " + msg); //返回取出的消息 return msg; &#125; &#125; else &#123;//如果消息队列中没有消息，那就要进入无穷等待状态直到有新消息了 // No more messages. nextPollTimeoutMillis = -1; &#125; // Process the quit message now that all pending messages have been handled. if (mQuitting) &#123; dispose(); return null; &#125; // If first time idle, then get the number of idlers to run. // Idle handles only run if the queue is empty or if the first message // in the queue (possibly a barrier) is due to be handled in the future. if (pendingIdleHandlerCount &lt; 0 &amp;&amp; (mMessages == null || now &lt; mMessages.when)) &#123; pendingIdleHandlerCount = mIdleHandlers.size(); &#125; if (pendingIdleHandlerCount &lt;= 0) &#123; // No idle handlers to run. Loop and wait some more. mBlocked = true; continue; &#125; if (mPendingIdleHandlers == null) &#123; mPendingIdleHandlers = new IdleHandler[Math.max(pendingIdleHandlerCount, 4)]; &#125; mPendingIdleHandlers = mIdleHandlers.toArray(mPendingIdleHandlers); &#125; // Run the idle handlers. // We only ever reach this code block during the first iteration. for (int i = 0; i &lt; pendingIdleHandlerCount; i++) &#123; final IdleHandler idler = mPendingIdleHandlers[i]; mPendingIdleHandlers[i] = null; // release the reference to the handler boolean keep = false; try &#123; keep = idler.queueIdle(); &#125; catch (Throwable t) &#123; Log.wtf("MessageQueue", "IdleHandler threw exception", t); &#125; if (!keep) &#123; synchronized (this) &#123; mIdleHandlers.remove(idler); &#125; &#125; &#125; // Reset the idle handler count to 0 so we do not run them again. pendingIdleHandlerCount = 0; // While calling an idle handler, a new message could have been delivered // so go back and look again for a pending message without waiting. nextPollTimeoutMillis = 0; &#125; &#125; &#160; &#160; &#160; &#160;调用这个函数的时候，有可能会让线程进入等待状态。什么情况下，线程会进入等待状态呢？两种情况，一是当消息队列中没有消息时，它会使线程进入等待状态；二是消息队列中有消息，但是消息指定了执行的时间，而现在还没有到这个时间，线程也会进入等待状态。消息队列中的消息是按时间先后来排序的，上面我们分析过了。 &#160; &#160; &#160; &#160;执行下面语句是看看当前消息队列中有没有消息：1nativePollOnce(mPtr, nextPollTimeoutMillis); &#160; &#160; &#160; &#160;这是一个JNI方法，我们等一下再分析，这里传入的参数mPtr就是指向前面我们在JNI层创建的NativeMessageQueue对象了，而参数nextPollTimeoutMillis则表示如果当前消息队列中没有消息，它要等待的时候，for循环开始时，传入的值为0，表示不等待。&#160; &#160; &#160; &#160;当前nativePollOnce返回后，就去看看消息队列中有没有消息：12345678910111213141516171819202122232425262728293031Message prevMsg = null;Message msg = mMessages;//取出队头消息if (msg != null &amp;&amp; msg.target == null) &#123;//如果队头消息是路障，就往后找到第一个异步消息 // Stalled by a barrier. Find the next asynchronous message in the queue. do &#123; prevMsg = msg; msg = msg.next; &#125; while (msg != null &amp;&amp; !msg.isAsynchronous());//循环查找第一个异步消息&#125;if (msg != null) &#123;//取出消息不为空 if (now &lt; msg.when) &#123;//如果时间还未到，就要等待 // Next message is not ready. Set a timeout to wake up when it is ready. nextPollTimeoutMillis = (int) Math.min(msg.when - now, Integer.MAX_VALUE); &#125; else &#123;//取出这个消息，并从消息队列移除 // Got a message. mBlocked = false; if (prevMsg != null) &#123; prevMsg.next = msg.next; &#125; else &#123; mMessages = msg.next; &#125; msg.next = null; if (false) Log.v("MessageQueue", "Returning message: " + msg); //返回取出的消息 return msg; &#125;&#125; else &#123;//如果消息队列中没有消息，那就要进入无穷等待状态直到有新消息了 // No more messages. nextPollTimeoutMillis = -1;&#125; &#160; &#160; &#160; &#160;取消息队列头部的消息，如果头部消息是Barrier（target==null）就往后遍历找到第一个异步消息； &#160; &#160; &#160; &#160;提前说明：nextPollTimeoutMillis 的值的意义是，-1表示下次调用nativePollOnce时，如果消息中没有消息，就进入无限等待状态中去。&#160; &#160; &#160; &#160;这里计算出来的等待时间都是在下次调用nativePollOnce时使用的。 &#160; &#160; &#160; &#160;接下来检测获取到的消息（消息队列头部的消息或者第一个异步消息），如果为null表示没有消息要执行，设置nextPollTimeoutMillis = -1，就要无限等待下去；&#160; &#160; &#160; &#160;否则检测这个消息要执行的时间，如果到执行时间了就将这个消息markInUse并从消息队列移除，然后从next返回到loop；&#160; &#160; &#160; &#160;否则设置nextPollTimeoutMillis = (int) Math.min(msg.when - now, Integer.MAX_VALUE)，即距离最近要执行的消息还需要多久。 &#160; &#160; &#160; &#160;这里说的等待，是空闲等待，而不是忙等待，因此，在进入空闲等待状态前，如果应用程序注册了IdleHandler接口来处理一些事情，那么就会先执行这里IdleHandler，然后再进入等待状态。IdlerHandler是定义在MessageQueue的一个内部类：1234567891011121314/** * Callback interface for discovering when a thread is going to block * waiting for more messages. */public static interface IdleHandler &#123; /** * Called when the message queue has run out of messages and will now * wait for more. Return true to keep your idle handler active, false * to have it removed. This may be called if there are still messages * pending in the queue, but they are all scheduled to be dispatched * after the current time. */ boolean queueIdle();&#125; &#160; &#160; &#160; &#160;它只有一个成员函数queueIdle，执行这个函数时，如果返回值为false，那么就会从应用程序中移除这个IdleHandler，否则的话就会在应用程序中继续维护着这个IdleHandler，下次空闲时仍会再执会这个IdleHandler。MessageQueue提供了addIdleHandler和removeIdleHandler两注册和删除IdleHandler。 &#160; &#160; &#160; &#160;回到MessageQueue的next方法中，它接下来就是在进入等待状态前，看看有没有IdleHandler是需要执行的：1234567891011121314151617// If first time idle, then get the number of idlers to run. // Idle handles only run if the queue is empty or if the first message // in the queue (possibly a barrier) is due to be handled in the future. if (pendingIdleHandlerCount &lt; 0 &amp;&amp; (mMessages == null || now &lt; mMessages.when)) &#123; pendingIdleHandlerCount = mIdleHandlers.size(); &#125; if (pendingIdleHandlerCount &lt;= 0) &#123; // No idle handlers to run. Loop and wait some more. mBlocked = true; continue; &#125; if (mPendingIdleHandlers == null) &#123; mPendingIdleHandlers = new IdleHandler[Math.max(pendingIdleHandlerCount, 4)]; &#125; mPendingIdleHandlers = mIdleHandlers.toArray(mPendingIdleHandlers); &#160; &#160; &#160; &#160;如果没有，即pendingIdleHandlerCount等于0，那下面的逻辑就不执行了，通过continue语句直接进入下一次循环，否则就要把注册在mIdleHandlers中的IdleHandler取出来，放在mPendingIdleHandlers数组中去。 &#160; &#160; &#160; &#160;接下来就是执行这些注册了的IdleHanlder了：12345678910111213141516171819// Run the idle handlers. // We only ever reach this code block during the first iteration. for (int i = 0; i &lt; pendingIdleHandlerCount; i++) &#123; final IdleHandler idler = mPendingIdleHandlers[i]; mPendingIdleHandlers[i] = null; // release the reference to the handler boolean keep = false; try &#123; keep = idler.queueIdle(); &#125; catch (Throwable t) &#123; Log.wtf("MessageQueue", "IdleHandler threw exception", t); &#125; if (!keep) &#123; synchronized (this) &#123; mIdleHandlers.remove(idler); &#125; &#125; &#125; &#160; &#160; &#160; &#160;执行完这些IdleHandler之后，线程下次调用nativePollOnce函数时，就不设置超时时间了，因为，很有可能在执行IdleHandler的时候，已经有新的消息加入到消息队列中去了，因此，要重置nextPollTimeoutMillis的值：123// While calling an idle handler, a new message could have been delivered // so go back and look again for a pending message without waiting. nextPollTimeoutMillis = 0; &#160; &#160; &#160; &#160;到这里MessageQueue的next方法的java部分就分析完了，我们重点看一下native方法nativePollOnce，看看它是如何进入等待状态的。这个函数定义在frameworks/base/core/jni/android_os_MessageQueue.cpp文件中：12345static void android_os_MessageQueue_nativePollOnce(JNIEnv* env, jclass clazz, jlong ptr, jint timeoutMillis) &#123; NativeMessageQueue* nativeMessageQueue = reinterpret_cast&lt;NativeMessageQueue*&gt;(ptr); nativeMessageQueue-&gt;pollOnce(env, timeoutMillis);&#125; &#160; &#160; &#160; &#160;这个函数首先是通过传进入的参数ptr取回前面在Java层创建MessageQueue对象时在JNI层创建的NatvieMessageQueue对象，然后调用它的pollOnce函数：12345void NativeMessageQueue::pollOnce(JNIEnv* env, int timeoutMillis) &#123; ...... mLooper-&gt;pollOnce(timeoutMillis); ......&#125; &#160; &#160; &#160; &#160;这里将操作转发给mLooper对象的pollOnce函数处理，这里的mLooper对象是在C++层的对象，它也是在前面在JNI层创建的NatvieMessageQueue对象时创建的，它的pollOnce函数定义在system/core/libutils/Looper.cpp中：1234567891011121314int Looper::pollOnce(int timeoutMillis, int* outFd, int* outEvents, void** outData) &#123; int result = 0; for (;;) &#123; ...... if (result != 0) &#123; ...... return result; &#125; result = pollInner(timeoutMillis); &#125; &#125; &#160; &#160; &#160; &#160;省略一些障眼法，它主要就是调用pollInner函数来进一步操作，如果pollInner返回值不等于0，这个函数就可以返回了。函数pollInner的定义如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859int Looper::pollInner(int timeoutMillis) &#123; ...... int result = ALOOPER_POLL_WAKE; ...... #ifdef LOOPER_USES_EPOLL struct epoll_event eventItems[EPOLL_MAX_EVENTS]; int eventCount = epoll_wait(mEpollFd, eventItems, EPOLL_MAX_EVENTS, timeoutMillis); bool acquiredLock = false; #else ...... #endif if (eventCount &lt; 0) &#123; if (errno == EINTR) &#123; goto Done; &#125; LOGW("Poll failed with an unexpected error, errno=%d", errno); result = ALOOPER_POLL_ERROR; goto Done; &#125; if (eventCount == 0) &#123; ...... result = ALOOPER_POLL_TIMEOUT; goto Done; &#125; ...... #ifdef LOOPER_USES_EPOLL for (int i = 0; i &lt; eventCount; i++) &#123; int fd = eventItems[i].data.fd; uint32_t epollEvents = eventItems[i].events; if (fd == mWakeReadPipeFd) &#123; if (epollEvents &amp; EPOLLIN) &#123; awoken(); &#125; else &#123; LOGW("Ignoring unexpected epoll events 0x%x on wake read pipe.", epollEvents); &#125; &#125; else &#123; ...... &#125; &#125; if (acquiredLock) &#123; mLock.unlock(); &#125; Done: ; #else ...... #endif ...... return result; &#125; &#160; &#160; &#160; &#160;省略一些障眼法，这里，首先是调用epoll_wait函数来看看epoll专用文件描述符mEpollFd所监控的文件描述符是否有IO事件发生，它设置监控的超时时间为timeoutMillis：1int eventCount = epoll_wait(mEpollFd, eventItems, EPOLL_MAX_EVENTS, timeoutMillis); &#160; &#160; &#160; &#160;epoll_wait函数 如下：函数声明:int epoll_wait(int epfd,struct epoll_event * events,int maxevents,int timeout)该函数用于轮询I/O事件的发生；参数：epfd:由epoll_create 生成的epoll专用的文件描述符；epoll_event:用于回传代处理事件的数组；maxevents:每次能处理的事件数；timeout:等待I/O事件发生的超时值；-1相当于阻塞，0相当于非阻塞。一般用-1即可返回值：返回发生事件数。 &#160; &#160; &#160; &#160;epoll_wait运行的原理是 ：等侍注册在epfd上的fd的事件的发生，如果发生则将发生的fd和事件类型放入到events数组中。并 且将注册在epfd上的fd的事件类型给清空，所以如果下一个循环你还要关注这个fd的话，则需要用epoll_ctl(epfd,EPOLL_CTL_MOD,listenfd,&amp;ev)来重新设置fd的事件类型。这时不用EPOLL_CTL_ADD,因为fd并未清空，只是事件类型清空。 &#160; &#160; &#160; &#160;回忆一下前面的Looper的构造函数，我们在里面设置了要监控mWakeReadPipeFd文件描述符的EPOLLIN事件。&#160; &#160; &#160; &#160;当mEpollFd所监控的文件描述符发生了要监控的I/O事件后或者监控时间超时后，线程就从epoll_wait返回了，否则线程就会在epoll_wait函数中进入睡眠状态了。返回后如果eventCount等于0，就说明是超时了：12345if (eventCount == 0) &#123; ...... result = ALOOPER_POLL_TIMEOUT; goto Done; &#125; &#160; &#160; &#160; &#160; 如果eventCount不等于0，就说明发生要监控的事件：12345678910111213for (int i = 0; i &lt; eventCount; i++) &#123; int fd = eventItems[i].data.fd; uint32_t epollEvents = eventItems[i].events; if (fd == mWakeReadPipeFd) &#123; if (epollEvents &amp; EPOLLIN) &#123; awoken(); &#125; else &#123; LOGW("Ignoring unexpected epoll events 0x%x on wake read pipe.", epollEvents); &#125; &#125; else &#123; ...... &#125; &#125; &#160; &#160; &#160; &#160;这里我们只关注mWakeReadPipeFd文件描述符上的事件，如果在mWakeReadPipeFd文件描述符上发生了EPOLLIN就说明应用程序中的消息队列里面有新的消息需要处理了，接下来它就会先调用awoken函数清空管道中的内容，以便下次再调用pollInner函数时，知道自从上次处理完消息队列中的消息后，有没有新的消息加进来。&#160; &#160; &#160; &#160;函数awoken的实现很简单，它只是把管道中的内容都读取出来：123456789void Looper::awoken() &#123; ...... char buffer[16]; ssize_t nRead; do &#123; nRead = read(mWakeReadPipeFd, buffer, sizeof(buffer)); &#125; while ((nRead == -1 &amp;&amp; errno == EINTR) || nRead == sizeof(buffer)); &#125; &#160; &#160; &#160; &#160;因为当其它的线程向应用程序的消息队列加入新的消息时，会向这个管道写入新的内容来通知应用程序主线程有新的消息需要处理了，从而唤醒它，下面我们分析一下。 唤醒时机&#160; &#160; &#160; &#160;上面讲发送消息时讲过，把消息加入到消息队列时，分两种情况，一种当前消息队列为空时，这时候应用程序的主线程一般就是处于空闲等待状态了，这时候就要唤醒它，另一种情况是应用程序的消息队列不为空，这时候就不需要唤醒应用程序的主线程了，因为这时候它一定是在忙着处于消息队列中的消息，因此不会处于空闲等待的状态。&#160; &#160; &#160; &#160;把消息加入到消息队列去后，如果应用程序的主线程正处于空闲等待状态，就需要调用natvieWake函数来唤醒它了，这是一个JNI方法，定义在frameworks/base/core/jni/android_os_MessageQueue.cpp文件中：1234static void android_os_MessageQueue_nativeWake(JNIEnv* env, jclass clazz, jlong ptr) &#123; NativeMessageQueue* nativeMessageQueue = reinterpret_cast&lt;NativeMessageQueue*&gt;(ptr); return nativeMessageQueue-&gt;wake();&#125; &#160; &#160; &#160; &#160;这个JNI层的NativeMessageQueue对象我们在前面分析消息循环的时候创建好的，保存在Java层的MessageQueue对象的mPtr成员变量中，这里把它取回来之后，就调用它的wake函数来唤醒应用程序的主线程，这个函数也是定义在frameworks/base/core/jni/android_os_MessageQueue.cpp文件中：123void NativeMessageQueue::wake() &#123; mLooper-&gt;wake();&#125; &#160; &#160; &#160; &#160;这里它又通过成员变量mLooper的wake函数来执行操作，这里的mLooper成员变量是一个C++层实现的Looper对象，它定义在system/core/libutils/Looper.cpp文件中：12345678910void Looper::wake() &#123; ...... ssize_t nWrite; do &#123; nWrite = write(mWakeWritePipeFd, "W", 1); &#125; while (nWrite == -1 &amp;&amp; errno == EINTR); ....... &#125; &#160; &#160; &#160; &#160;这个wake函数很简单，只是通过打开文件描述符mWakeWritePipeFd往管道的写入一个”W”字符串。其实，往管道写入什么内容并不重要，往管道写入内容的目的是为了唤醒应用程序的主线程。前面我们在分析应用程序的消息循环时说到，当应用程序的消息队列中没有消息处理时，应用程序的主线程就会进入空闲等待状态，而这个空闲等待状态就是通过调用这个Looper类的pollInner函数来进入的，具体就是在pollInner函数中调用epoll_wait函数来等待管道中有内容可读的。&#160; &#160; &#160; &#160;这时候既然管道中有内容可读了，应用程序的主线程就会从这里的Looper类的pollInner函数返回到JNI层的nativePollOnce函数，最后返回到Java层中的MessageQueue.next函数中去，这里它就会发现消息队列中有新的消息需要处理了，于就会处理这个消息。 &#160; &#160; &#160; &#160;剩下就是对消息的处理了，这个就更随意了。回到Looper.loop方法中：1234567891011121314151617181920212223242526272829303132public class Looper &#123; ...... public static final void loop() &#123; Looper me = myLooper(); MessageQueue queue = me.mQueue; ...... while (true) &#123; Message msg = queue.next(); // might block ...... if (msg != null) &#123; if (msg.target == null) &#123; // No target is a magic identifier for the quit message. return; &#125; ...... msg.target.dispatchMessage(msg); //处理消息 ...... msg.recycle(); &#125; &#125; &#125; ...... &#125; &#160; &#160; &#160; &#160;它从消息队列中获得消息对象msg后，就会调用它的target成员变量的dispatchMessage函数来处理这个消息。在前面分析消息的发送时说过，这个消息对象msg的成员变量target是在发送消息的时候设置好的，一般就通过哪个Handler来发送消息，就通过哪个Handler来处理消息。&#160; &#160; &#160; &#160;我们这里的Handler是ActivityThread的成员变量mH，是一个类型为H的Handler。 H类没有实现自己的dispatchMessage函数，但是它继承了父类Handler的dispatchMessage函数，我们看看Handler的dispatchMessage方法：123456789101112public void dispatchMessage(Message msg) &#123; if (msg.callback != null) &#123; handleCallback(msg); &#125; else &#123; if (mCallback != null) &#123; if (mCallback.handleMessage(msg)) &#123; return; &#125; &#125; handleMessage(msg); &#125;&#125; &#160; &#160; &#160; &#160;这里的消息对象msg的callback成员变量和Handler类的mCallBack成员变量一般都为null，于是，就会调用Handler类的handleMessage函数来处理这个消息，由于H类在继承Handler类时，重写了handleMessage函数，因此，这里调用的实际上是H类的handleMessage函数。 总结&#160; &#160; &#160; &#160;至此，我们就分析完Android应用程序的消息处理机制了，简单做一个总结： Android应用程序的主线程在进入消息循环过程前，会在内部创建一个Linux管道（Pipe），这个管道的作用是使得Android应用程序主线程在消息队列为空时可以进入空闲等待状态，并且使得当应用程序的消息队列有消息需要处理时唤醒应用程序的主线程。 Android应用程序的主线程进入空闲等待状态的方式实际上就是在管道的读端等待管道中有新的内容可读，具体来说就是是通过Linux系统的Epoll机制中的epoll_wait函数进行的。 当往Android应用程序的消息队列中加入新的消息时，会同时往管道中的写端写入内容，通过这种方式就可以唤醒正在等待消息到来的应用程序主线程。 当应用程序主线程在进入空闲等待前，会认为当前线程处理空闲状态，于是就会调用那些已经注册了的IdleHandler接口，使得应用程序有机会在空闲的时候处理一些事情。 &#160; &#160; &#160; &#160;上述分析的是系统启动过程中的消息机制，如果我们想看一下一个线程想实现消息循环应该怎么做，可以看看HandlerThread的实现，和上面内容大同小异，有兴趣的同学可以自己研究研究。 &#160; &#160; &#160; &#160;结尾处贴个妹纸吧，这是今年ChinaJoy会场一个show girl，前同事拍的（我设备太渣了，所以在馆内只参加了活动，玩了些游戏= 。=）。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>消息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析Bitmap占据内存大小]]></title>
    <url>%2F2016%2F07%2F12%2F%E6%B5%85%E6%9E%90Bitmap%E5%8D%A0%E6%8D%AE%E5%86%85%E5%AD%98%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160; Bitmap的使用是开发时绕不过的坑，使用时要处处留意内存问题，稍有不慎就会报OOM（out of memory）。所以这次我们就研究研究程序中Bitmap到底占据多少内存。 前奏&#160; &#160; &#160; &#160; 比如我们使用一张图片，将其放入到工程目录中，想当然的会以为为这张图片建立的bitmap使用内存大小为：宽×高×像素大小。为了验证这个猜想，我在度娘上随便找了幅图： &#160; &#160; &#160; &#160; 规格是768×1152，大小为153.3KB，格式为支持ARGB四阶的32位色的JPG图片。&#160; &#160; &#160; &#160; 我们猜想，如果按照内存大小计算公式，所占内存应为：768×1152×4=3538944，字节。因为JPG格式是有损压缩格式，所以存储大小比内存大小小多了。&#160; &#160; &#160; &#160; 然后将这张图片放到res/drawable-xhdpi下，通过如下代码计算内存大小：1234567891011float density = this.getResources().getDisplayMetrics().density;int dpi = this.getResources().getDisplayMetrics().densityDpi;Log.e(TAG, "density = " + density + "------" + "dpi = " + dpi);Bitmap b = BitmapFactory.decodeResource(getResources(), R.drawable.picture);int w = b.getWidth();int h = b.getHeight();int size = b.getByteCount();int config = b.getConfig().ordinal();Log.e(TAG, "w = " + w + ";" + "h = " + h + ";" + "size = " + size + ";" + "config = " + config); &#160; &#160; &#160; &#160;测试机器规格为：Google Nexus 5 - 5.1.0 - API 22 - 1080×1920(480dpi)。&#160; &#160; &#160; &#160;打印log如下： density = 3.0——dpi = 480w = 1152;h = 1728;size = 7962624;config = 3 &#160; &#160; &#160; &#160;Why？How did you do it？这个不按套路出牌啊，宽高明显被拉伸了啊。。。。。。然后我又试了下将这张图片放到了res/drawable-xxhdpi下，打印log如下： density = 3.0——dpi = 480w = 768;h = 1152;size = 3538944;config = 3 &#160; &#160; &#160; &#160;这次倒是和理论计算的大小一样了，我们大概猜到了什么。。。。。接着我又把这张图片放到了assets目录下，然后修改了一下获取图片的代码，打印log如下： density = 3.0——dpi = 480w = 768;h = 1152;size = 3538944;config = 3 &#160; &#160; &#160; &#160;这次也是和理论值一样的，因为放到assets目录下的图片是不会被压缩的。 &#160; &#160; &#160; &#160;如果多试几次，把图片放入不同目录下再运行几遍，我们也能够总结出规律的。但这些都是现象，我们组的老大也曾经说过：开发人员不要轻易根据现象得出结论…….所以我们也要分析一下本质原因。 求证&#160; &#160; &#160; &#160;做适配的同学要经常和density、densityDpi搞好关系，简单来说，可以理解为 density 的数值是 1dp=density px；densityDpi 是屏幕每英寸对应多少个点（不是像素点），在 DisplayMetrics 当中，这两个的关系是线性的： density 0.75 1 1.5 2 3 3.5 4 densityDpi 120 160 240 320 480 560 640 DpiFolder ldpi mdpi hdpi xhdpi xxhdpi xxxhdpi xxxxhdpi &#160; &#160; &#160; &#160;这些内容每个人应该都知道，先放到这里，方便后面查表。 非压缩计算&#160; &#160; &#160; &#160;如果图片不被压缩，按照常规计算内存大小方法为：12345678910//Bitmap的getByteCount方法 public final int getByteCount() &#123; // int result permits bitmaps up to 46,340 x 46,340 return getRowBytes() * getHeight(); &#125; //Bitmap的getRowBytes方法 public final int getRowBytes() &#123; return nativeRowBytes(mNativeBitmap); &#125;private static native int nativeRowBytes(long nativeBitmap); &#160; &#160; &#160; &#160;getHeight 就是图片的高度（单位：px），getRowBytes 从字面意思看应该是行字节大小。我们往下看，找找JNI实现，查看 frameworks/base/core/jni/android/graphics/Bitmap.cpp文件：1234static jint Bitmap_rowBytes(JNIEnv* env, jobject, jlong bitmapHandle) &#123; SkBitmap* bitmap = reinterpret_cast&lt;SkBitmap*&gt;(bitmapHandle); return static_cast&lt;jint&gt;(bitmap-&gt;rowBytes());&#125; &#160; &#160; &#160; &#160;（reinterpret_cast和static_cast是C++经常用到的用来处理无关类型之间转换的强制类型转换符，建议有时间可以研究研究，或者把C++回顾一下，毕竟挺重要的。这里先给个科普文章）&#160; &#160; &#160; &#160;上一篇关于的弹幕文章提到过，java层的Bitmap对应native层是由skia图形引擎创建的SkBitmap，关于skia这玩意儿东西比较多，不是专业的一时半会儿也玩不转。所以我们还是简单看看，继续往下找SkBitmap：(/external/skia/include/core/SkBitmap.h)12/** Return the number of bytes between subsequent rows of the bitmap. */size_t rowBytes() const &#123; return fRowBytes; &#125; &#160; &#160; &#160; &#160;得到上述fRowBytes的大小会在SkBitmap.cpp文件里计算：（/external/skia/src/core/SkBitmap.cpp）1234567891011121314151617181920212223242526272829303132333435//计算fRowBytes大小size_t SkBitmap::ComputeRowBytes(Config c, int width) &#123; return SkColorTypeMinRowBytes(SkBitmapConfigToColorType(c), width);//SkColorTypeMinRowBytes是/SkImageInfo.h的方法；SkBitmapConfigToColorType是SkImagePriv.cpp的方法&#125;//SkImageInfo.h的SkColorTypeMinRowBytes方法static inline size_t SkColorTypeMinRowBytes(SkColorType ct, int width) &#123; return width * SkColorTypeBytesPerPixel(ct);&#125;//SkImageInfo.h的SkColorTypeBytesPerPixel方法static int SkColorTypeBytesPerPixel(SkColorType ct) &#123; static const uint8_t gSize[] = &#123; 0, // Unknown 1, // Alpha_8 2, // RGB_565 2, // ARGB_4444 4, // RGBA_8888 4, // BGRA_8888 1, // kIndex_8 &#125;; ...省略障眼法的宏... return gSize[ct];&#125;//SkBitmapConfigToColorType是SkImagePriv.cpp的方法SkColorType SkBitmapConfigToColorType(SkBitmap::Config config) &#123; static const SkColorType gCT[] = &#123; kUnknown_SkColorType, // kNo_Config kAlpha_8_SkColorType, // kA8_Config kIndex_8_SkColorType, // kIndex8_Config kRGB_565_SkColorType, // kRGB_565_Config kARGB_4444_SkColorType, // kARGB_4444_Config kN32_SkColorType, // kARGB_8888_Config &#125;; SkASSERT((unsigned)config &lt; SK_ARRAY_COUNT(gCT)); return gCT[config];&#125; &#160; &#160; &#160; &#160;跟踪到这里，还记得我们上面大log的地方么。int config = b.getConfig().ordinal()返回的是3，那么在Bitmap.Config里面索引第4个枚举变量：1234567891011121314151617181920public enum Config &#123; ALPHA_8 (1), RGB_565 (3), ARGB_4444 (4), ARGB_8888 (5);//索引第四个是这个 final int nativeInt; //从这个列表可以看出它与skia支持的图片格式一一对应，但是Android只支持上面4种 private static Config sConfigs[] = &#123; null, ALPHA_8, null, RGB_565, ARGB_4444, ARGB_8888 &#125;; Config(int ni) &#123; this.nativeInt = ni; &#125; static Config nativeToConfig(int ni) &#123; return sConfigs[ni]; &#125;&#125; &#160; &#160; &#160; &#160;依照上面C++文件，我们发现 ARGB_8888（也就是我们最常用的 Bitmap 的格式）的一个像素占用 4byte，那么 rowBytes 实际上就是 4*width bytes。则理论上 ARGB_8888 的 Bitmap 占用内存的计算公式为： bitmapInRam = bitmapWidth × bitmapHeight × 4 bytes 压缩计算&#160; &#160; &#160; &#160;如果我们不将图片放到assets目录下，内存大小计算方式就和上面完全不同了。我们读取的是 drawable 目录下面的图片，用的是 decodeResource 方法,该方法本质上就两步： 读取原始资源，这个调用了 Resource.openRawResource 方法，这个方法调用完成之后会对 TypedValue 进行赋值，其中包含了原始资源的 density 等信息； 调用 decodeResourceStream 对原始资源进行解码和适配。这个过程实际上就是原始资源的 density 到屏幕 density 的一个映射。&#160; &#160; &#160; &#160;原始资源的 density 其实取决于资源存放的目录（比如 xxhdpi 对应的是480），而屏幕 density 的赋值，请看下面这段代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 public static Bitmap decodeResource(Resources res, int id) &#123; return decodeResource(res, id, null); &#125; public static Bitmap decodeResource(Resources res, int id, Options opts) &#123; Bitmap bm = null; InputStream is = null; try &#123; final TypedValue value = new TypedValue(); is = res.openRawResource(id, value);//对 TypedValue 进行赋值，其中包含了原始资源的 density 等信息 bm = decodeResourceStream(res, value, is, null, opts); &#125; catch (Exception e) &#123; ...... &#125; finally&#123; ...... &#125;...... return bm; &#125; public static Bitmap decodeResourceStream(Resources res, TypedValue value, InputStream is, Rect pad, Options opts) &#123; if (opts == null) &#123;//opt为null opts = new Options(); &#125; if (opts.inDensity == 0 &amp;&amp; value != null) &#123; final int density = value.density; if (density == TypedValue.DENSITY_DEFAULT) &#123; opts.inDensity = DisplayMetrics.DENSITY_DEFAULT; &#125; else if (density != TypedValue.DENSITY_NONE) &#123; opts.inDensity = density;//这里density的值如果对应资源目录为xhdpi的话，就是320 &#125; &#125; if (opts.inTargetDensity == 0 &amp;&amp; res != null) &#123; //请注意，inTargetDensity就是当前的显示密度，比如Google Nexus 5就是480 opts.inTargetDensity = res.getDisplayMetrics().densityDpi; &#125; return decodeStream(is, pad, opts); &#125; public static Bitmap decodeStream(InputStream is, Rect outPadding, Options opts) &#123;...... bm = decodeStreamInternal(is, outPadding, opts);...... return bm; &#125; private static Bitmap decodeStreamInternal(InputStream is, Rect outPadding, Options opts) &#123;...... return nativeDecodeStream(is, tempStorage, outPadding, opts); &#125; private static native Bitmap nativeDecodeStream(InputStream is, byte[] storage, Rect padding, Options opts); &#160; &#160; &#160; &#160;我们看到 opts 这个值被初始化，而它的构造居然如此简单：12345public Options() &#123; inDither = false; inScaled = true; inPremultiplied = true;&#125; &#160; &#160; &#160; &#160;所以我们就很容易的看到，Option.inScreenDensity 这个值没有被初始化，而实际上后面我们也会看到这个值根本不会用到；我们最应该关心的是什么呢？是 inDensity 和 inTargetDensity，这两个值与下面 cpp 文件里面的 density 和 targetDensity 相对应——重复一下，inDensity 就是原始资源的 density，inTargetDensity 就是屏幕的 density。&#160; &#160; &#160; &#160;紧接着，用到了 nativeDecodeStream 方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465static jobject nativeDecodeStream(JNIEnv* env, jobject clazz, jobject is, jbyteArray storage, jobject padding, jobject options) &#123; jobject bitmap = NULL; ...... bitmap = doDecode(env, bufferedStream, padding, options); return bitmap;&#125;static jobject doDecode(JNIEnv* env, SkStreamRewindable* stream, jobject padding, jobject options) &#123; ...... if (env-&gt;GetBooleanField(options, gOptions_scaledFieldID)) &#123; const int density = env-&gt;GetIntField(options, gOptions_densityFieldID);//对应xhdpi的时候，是320 const int targetDensity = env-&gt;GetIntField(options, gOptions_targetDensityFieldID);//Google Nexus 5为480 const int screenDensity = env-&gt;GetIntField(options, gOptions_screenDensityFieldID); if (density != 0 &amp;&amp; targetDensity != 0 &amp;&amp; density != screenDensity) &#123; scale = (float) targetDensity / density; &#125; &#125;&#125; const bool willScale = scale != 1.0f;......SkBitmap decodingBitmap;if (!decoder-&gt;decode(stream, &amp;decodingBitmap, prefColorType,decodeMode)) &#123; return nullObjectReturn("decoder-&gt;decode returned false");&#125;//这里这个deodingBitmap就是解码出来的bitmap，大小是图片原始的大小int scaledWidth = decodingBitmap.width();int scaledHeight = decodingBitmap.height();if (willScale &amp;&amp; decodeMode != SkImageDecoder::kDecodeBounds_Mode) &#123; scaledWidth = int(scaledWidth * scale + 0.5f); scaledHeight = int(scaledHeight * scale + 0.5f);&#125;if (willScale) &#123; const float sx = scaledWidth / float(decodingBitmap.width()); const float sy = scaledHeight / float(decodingBitmap.height()); // TODO: avoid copying when scaled size equals decodingBitmap size SkColorType colorType = colorTypeForScaledOutput(decodingBitmap.colorType()); // FIXME: If the alphaType is kUnpremul and the image has alpha, the // colors may not be correct, since Skia does not yet support drawing // to/from unpremultiplied bitmaps. outputBitmap-&gt;setInfo(SkImageInfo::Make(scaledWidth, scaledHeight, colorType, decodingBitmap.alphaType())); if (!outputBitmap-&gt;allocPixels(outputAllocator, NULL)) &#123; return nullObjectReturn("allocation failed for scaled bitmap"); &#125; // If outputBitmap's pixels are newly allocated by Java, there is no need // to erase to 0, since the pixels were initialized to 0. if (outputAllocator != &amp;javaAllocator) &#123; outputBitmap-&gt;eraseColor(0); &#125; SkPaint paint; paint.setFilterLevel(SkPaint::kLow_FilterLevel); SkCanvas canvas(*outputBitmap); canvas.scale(sx, sy); canvas.drawBitmap(decodingBitmap, 0.0f, 0.0f, &amp;paint);&#125;......&#125; &#160; &#160; &#160; &#160;注意到其中有个 density 和 targetDensity，前者是 decodingBitmap 的 density，这个值跟这张图片的放置的目录有关（比如 xhdpi 是320，xxhdpi 是480），这部分代码我跟了一下，太长了，就不列出来了；targetDensity 实际上是我们加载图片的目标 density，这个值的来源我们已经在前面给出了，就是 DisplayMetrics 的 densityDpi，如果是Google Nexus 5那么这个数值就是480。sx 和sy 实际上是约等于 scale 的，因为 scaledWidth 和 scaledHeight 是由 width 和 height 乘以 scale 得到的。我们看到 Canvas 放大了 scale 倍，然后又把读到内存的这张 bitmap 画上去，相当于把这张 bitmap 放大了 scale 倍。 &#160; &#160; &#160; &#160;然后我们再次验证上面打log的地方，win + r ，输入calc呼出计算器。这里千万不要忘了了精度： float scale = 480/320f = 1.5int scaledWidth = int(768 * 1.5 + 0.5f) = 1152int scaledHeight = int(1152 * 1.5 + 0.5f) = 1728 size = 1152 1728 4 = 7962624 &#160; &#160; &#160; &#160;果然和上面log打印的一模一样！因此我们可以得出结论。Bitmap在内存中大小取决于： 色彩格式，前面我们已经提到，如果是 ARG_B8888 那么就是一个像素4个字节，如果是 RGB_565 那就是2个字节 原始文件存放的资源目录(是 hdpi 还是 xxhdpi 等等) 目标屏幕的密度（所以同等条件下，红米在资源方面消耗的内存肯定是要小于三星S6的） &#160; &#160; &#160; &#160;内存大小计算公式大概为（压缩计算情况下）（已忽略精度）： 内存大小 = （设备屏幕dpi / 资源所在目录dpi）^ 2 × 图片原始宽 × 图片原始高 × 像素大小 瞎猜&#160; &#160; &#160; &#160;上面分析Bitmap.Config时发现Android官方并不完全支持skia图形引擎的所有像素格式，供java层设置的Config只有这么4个：12345678910public enum Config &#123; // these native values must match up with the enum in SkBitmap.h ALPHA_8 (1), RGB_565 (3), ARGB_4444 (4), ARGB_8888 (5); inal int nativeInt;&#125; &#160; &#160; &#160; &#160;其实 Java 层的枚举变量的 nativeInt 对应的就是 Skia 库当中枚举的索引值；而skia却支持这么多：123456789//Skbitmap.h文件 enum Config &#123; kNo_Config, //!&lt; bitmap has not been configured kA8_Config, //!&lt; 8-bits per pixel, with only alpha specified (0 is transparent, 0xFF is opaque) kIndex8_Config, //!&lt; 8-bits per pixel, using SkColorTable to specify the colors kRGB_565_Config, //!&lt; 16-bits per pixel, (see SkColorPriv.h for packing) kARGB_4444_Config, //!&lt; 16-bits per pixel, (see SkColorPriv.h for packing) kARGB_8888_Config, //!&lt; 32-bits per pixel, (see SkColorPriv.h for packing) &#125;; &#160; &#160; &#160; &#160;上述枚举中第三个类型为索引图类型。索引位图，每个像素只占 1 Byte，不仅支持 RGB，还支持 alpha。微软画图工具应该都玩过吧（win + r，输入mspaint），里面的调色板就是索引色盘。&#160; &#160; &#160; &#160;而Android其他的config类型一个像素点占的字节比这个大多了，所以我们有时候能不能也用索引色去悄悄替换原来格式呢？&#160; &#160; &#160; &#160;我的猜想是，反射构造一个Bitmap.Config枚举对象，然后反射设置nativeInt字段的值为2，猜想代码如下：123Options op = new Options();op.inPreferredConfig = ...反射构建Bitmap.Config相关内容...BitmapFactory.decodeResource(getResources(), R.drawable.picture, op); &#160; &#160; &#160; &#160;不过我没有实践过，也是瞎猜的，不知道能不能行的通。。。。。。 &#160; &#160; &#160; &#160;但是我对上一篇文章种调skia生成弹幕bitmap处的代码做了修改，修改了DanmakuFlameMaster工程里的NativeBitmapFactory.java文件：123456 private static Bitmap createNativeBitmap(int width, int height, Config config, boolean hasAlpha) &#123;// int nativeConfig = getNativeConfig(config); int nativeConfig = 2;//直接改为索引色 return android.os.Build.VERSION.SDK_INT == 19 ? createBitmap19(width, height, nativeConfig, hasAlpha) : createBitmap(width, height, nativeConfig, hasAlpha); &#125; &#160; &#160; &#160; &#160;将色彩格式改为索引色，然后重新编译运行。。。。。。然而弹幕压根没出来。。。。。等以后有机会问问ctiao吧，请教一下为何。&#160; &#160; &#160; &#160;这些瞎猜只能暂时放着，等以后有机会再验证吧。。。。。。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>Bitmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[弹幕框架DanmakuFlameMaster简单分析]]></title>
    <url>%2F2016%2F07%2F02%2FDanmakuFlameMaster%E7%AE%80%E5%8D%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;随着B站逐渐崛起，其开源弹幕项目DanmakuFlameMaster应用场景也越来越多。我也是在一次偶然机会下发现了这个项目，被其惊艳的效果震撼。以前我就对弹幕技术很感兴趣，可能是因为B站动漫看多，几乎每一部番都是漫天的弹幕乱飞，如果哪部剧没有弹幕反而觉得不适应；久而久之就愈发倾向钻研其原理。&#160; &#160; &#160; &#160;看到效果后，我猜想绘制原理应该是创建一个定时器作为全部弹幕的时间参考，然后每条弹幕出现的位置都以这个定时器去计算x、y值，然后定时任务定期postInvalidate，弹幕画布重新绘制onDraw；弹幕如此之多，应该有缓存机制，也许建立了一个弹幕池让出现过的弹幕缓存起来，新弹幕可以复用旧弹幕item。&#160; &#160; &#160; &#160;先这么假设吧，然后验证我们的猜想，看看有哪些坑。 基本使用&#160; &#160; &#160; &#160;首先是添加控件，项目里提供了三个控件：DanmakuSurfaceView、DanmakuTextureView和DanmakuView，使用其中三个任意一个都可以。我们选个DanmakuView方便分析。12345678910111213141516&lt;FrameLayout xmlns:android="http://schemas.android.com/apk/res/android" xmlns:tools="http://schemas.android.com/tools" android:layout_width="match_parent" android:layout_height="match_parent" tools:context=".MainActivity"&gt; 省略一些布局... &lt;master.flame.danmaku.ui.widget.DanmakuView android:id="@+id/sv_danmaku" android:layout_width="match_parent" android:layout_height="match_parent" /&gt; 省略一些布局... &lt;/FrameLayout&gt; &#160; &#160; &#160; &#160;然后是代码配置，先看一下初始化相关：12345678910111213141516171819202122232425262728293031323334 @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); findViews(); &#125; private void findViews() &#123;省略一些代码... // DanmakuViewmDanmakuView = (IDanmakuView) findViewById(R.id.sv_danmaku); // 设置最大显示行数 HashMap&lt;Integer, Integer&gt; maxLinesPair = new HashMap&lt;Integer, Integer&gt;(); maxLinesPair.put(BaseDanmaku.TYPE_SCROLL_RL, 5); // 滚动弹幕最大显示5行 // 设置是否禁止重叠 HashMap&lt;Integer, Boolean&gt; overlappingEnablePair = new HashMap&lt;Integer, Boolean&gt;(); overlappingEnablePair.put(BaseDanmaku.TYPE_SCROLL_RL, true); overlappingEnablePair.put(BaseDanmaku.TYPE_FIX_TOP, true); //创建弹幕控件上下文，类似Context，里面可以进行一系列配置 mContext = DanmakuContext.create();mContext.setDanmakuStyle(IDisplayer.DANMAKU_STYLE_STROKEN, 3)//设置描边样式.setDuplicateMergingEnabled(false) //设置不合并相同内容弹幕.setScrollSpeedFactor(1.2f) //设置弹幕滚动速度缩放比例，越大速度越慢.setScaleTextSize(1.2f) //设置字体缩放比例 .setCacheStuffer(new SpannedCacheStuffer(), mCacheStufferAdapter) // 图文混排使用SpannedCacheStuffer //.setCacheStuffer(new BackgroundCacheStuffer()) // 绘制背景使用BackgroundCacheStuffer .setMaximumLines(maxLinesPair) //设置最大行数策略 .preventOverlapping(overlappingEnablePair); //设置禁止重叠策略 省略一些代码... &#125; &#160; &#160; &#160; &#160;DanmakuContext设置setCacheStuffer(CacheStuffer, Proxy)时，如果不设置此方法，则CacheStuffer默认为SimpleTextCacheStuffer，proxy默认为null；第一个参数，项目例子中提供了BackgroundCacheStuffer和SpannedCacheStuffer，其实也可以自己扩展，第二个参数例子中也写了一个mCacheStufferAdapter，同理也可以自己扩展。这个sample中注释也写得比较明确，我们往下分析原理时会解释。&#160; &#160; &#160; &#160;然后设置数据源：1234567891011121314151617181920212223242526272829303132333435 //替换为A站弹幕数据源，因为A站弹幕数据是json，B站是xml，为了方便分析因此替换为A站源 //mParser = createParser(this.getResources().openRawResource(R.raw.comments)); try &#123; mParser = createParser(this.getAssets().open("comment.json")); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; private BaseDanmakuParser createParser(InputStream stream) &#123; if (stream == null) &#123; return new BaseDanmakuParser() &#123; @Override protected Danmakus parse() &#123; return new Danmakus(); &#125; &#125;; &#125;// ILoader loader = DanmakuLoaderFactory.create(DanmakuLoaderFactory.TAG_BILI); ILoader loader = DanmakuLoaderFactory.create(DanmakuLoaderFactory.TAG_ACFUN); try &#123; loader.load(stream); &#125; catch (IllegalDataException e) &#123; e.printStackTrace(); &#125;// BaseDanmakuParser parser = new BiliDanmukuParser(); BaseDanmakuParser parser = new AcFunDanmakuParser(); IDataSource&lt;?&gt; dataSource = loader.getDataSource(); parser.load(dataSource); return parser; &#125; &#160; &#160; &#160; &#160;最后启动弹幕：12345678910111213141516171819202122232425//设置弹幕view相关回调mDanmakuView.setCallback(new DrawHandler.Callback() &#123; @Override public void updateTimer(DanmakuTimer timer) &#123; &#125; @Override public void drawingFinished() &#123; &#125; @Override public void danmakuShown(BaseDanmaku danmaku) &#123; //Log.d("DFM", "danmakuShown(): text=" + danmaku.text); &#125; @Override public void prepared() &#123; Log.d("DFM", "MainActivity inline callback's method prepared"); mDanmakuView.start(); &#125; &#125;); mDanmakuView.prepare(mParser, mContext); mDanmakuView.showFPS(true); mDanmakuView.enableDanmakuDrawingCache(true); &#160; &#160; &#160; &#160;基本使用在项目的例子中都写的很清楚，这些应该难度不大。接下来应该是分析流程了。 流程分析&#160; &#160; &#160; &#160;DanmakuFlameMaster流程确实十分复杂，因为变量实在太多了，所以分析时推荐先整体看个大概，然后一步一步打断点确认细节。 初始配置&#160; &#160; &#160; &#160;上面写基本使用法，第一步是初始配置，我们看看到底初始化了哪些参数。对比上面的调用顺序，首先进入DanmakuContext看看： 123456789101112131415161718192021222324252627 //相关配置如下，主要初始化一下变量 mContext.setDanmakuStyle(IDisplayer.DANMAKU_STYLE_STROKEN, 3)//设置描边样式 .setDuplicateMergingEnabled(false) //设置不合并相同内容弹幕 .setScrollSpeedFactor(1.2f) //设置弹幕滚动速度缩放比例，越大速度越慢 .setScaleTextSize(1.2f) //设置字体缩放比例 .setCacheStuffer(new SpannedCacheStuffer(), mCacheStufferAdapter) // 图文混排使用SpannedCacheStuffer //.setCacheStuffer(new BackgroundCacheStuffer()) // 绘制背景使用BackgroundCacheStuffer .setMaximumLines(maxLinesPair) //设置最大行数策略 .preventOverlapping(overlappingEnablePair); //设置禁止重叠策略 //DanmakuContext 类重要方法 /*------------DanmakuContext STAET-----------*/private final AbsDisplayer mDisplayer = new AndroidDisplayer();//创建DanmakuContext 对象时直接new了个mDisplayer 全局变量 /** * 设置缓存绘制填充器，默认使用SimpleTextCacheStuffer只支持纯文字显示, 如果需要图文混排请设置SpannedCacheStuffer * 如果需要定制其他样式请扩展SimpleTextCacheStuffer或者SpannedCacheStuffer */ public DanmakuContext setCacheStuffer(BaseCacheStuffer cacheStuffer, BaseCacheStuffer.Proxy cacheStufferAdapter) &#123; this.mCacheStuffer = cacheStuffer; if (this.mCacheStuffer != null) &#123; this.mCacheStuffer.setProxy(cacheStufferAdapter); mDisplayer.setCacheStuffer(this.mCacheStuffer); &#125; return this; &#125; /*------------DanmakuContext END-----------*/ &#160; &#160; &#160; &#160;以上配置主要配置一些常规参数，记不住也没关系，我们可以打断点一一查看。 加载资源&#160; &#160; &#160; &#160;然后就是加载弹幕源：1234567891011121314151617181920 private BaseDanmakuParser createParser(InputStream stream) &#123;......//创建A站弹幕加载器 ILoader loader = DanmakuLoaderFactory.create(DanmakuLoaderFactory.TAG_ACFUN); try &#123; //将数据流载入加载器里 loader.load(stream); &#125; catch (IllegalDataException e) &#123; e.printStackTrace(); &#125; //创建弹幕解析器 BaseDanmakuParser parser = new AcFunDanmakuParser(); //取出数据源 IDataSource&lt;?&gt; dataSource = loader.getDataSource(); //解析器放入数据源 parser.load(dataSource); return parser; &#125; &#160; &#160; &#160; &#160;我们一步一步来，先创建A站弹幕加载器：12345678//根据不同标签创建不同加载器，可以根据不同业务自己扩展定制public static ILoader create(String tag) &#123; if (TAG_BILI.equalsIgnoreCase(tag)) &#123; return BiliDanmakuLoader.instance(); &#125; else if(TAG_ACFUN.equalsIgnoreCase(tag))//我们到了这里 return AcFunDanmakuLoader.instance(); return null; &#125; &#160; &#160; &#160; &#160;载入数据流：1234567891011121314151617181920212223242526public void load(InputStream in) throws IllegalDataException &#123; try &#123; dataSource = new JSONSource(in);//这里创建了一个JSONSource &#125; catch (Exception e) &#123; throw new IllegalDataException(e); &#125;&#125;//JSONSource构造方法public JSONSource(InputStream in) throws JSONException&#123; init(in);&#125;private void init(InputStream in) throws JSONException &#123; ...... mInput = in; String json = IOUtils.getString(mInput);//将流转成字符串 init(json);&#125;private void init(String json) throws JSONException &#123; if(!TextUtils.isEmpty(json))&#123; mJSONArray = new JSONArray(json);//将json字符串保存到一个JSONArray全局变量 &#125;&#125;//取出JSONSourcepublic JSONSource getDataSource() &#123; return dataSource;&#125; &#160; &#160; &#160; &#160;载入数据流就是读取弹幕数据文件流，然后转成字符串，最后保存到一个JSONArray变量里存起来。&#160; &#160; &#160; &#160;继续往下分析创建弹幕解析器、取出数据源、解析器放入数据源：12345//AcFunDanmakuParser的load方法，将上一步得到的JSONSource放入到AcFunDanmakuParser中public BaseDanmakuParser load(IDataSource&lt;?&gt; source) &#123; mDataSource = source; return this; &#125; &#160; &#160; &#160; &#160;到这里数据就载入到解析器里了，parser里有弹幕源数据了。 启动弹幕&#160; &#160; &#160; &#160;启动弹幕重要的就是这一句：1mDanmakuView.prepare(mParser, mContext); &#160; &#160; &#160; &#160;此时mParser和mContext都已经初始化完成。123456789101112public void prepare(BaseDanmakuParser parser, DanmakuContext config) &#123; prepare();///创建一个 DrawHandler handler.setConfig(config); handler.setParser(parser); handler.setCallback(mCallback); handler.prepare();//然后调用DrawHandler的prepare方法 &#125;//创建一个 DrawHandlerprivate void prepare() &#123; if (handler == null) handler = new DrawHandler(getLooper(mDrawingThreadType), this, mDanmakuVisible);//mDanmakuVisible为true &#125; &#160; &#160; &#160; &#160;设置一些全局变量后，会调用DrawHandler的prepare方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public void prepare() &#123; sendEmptyMessage(DrawHandler.PREPARE);&#125;public void handleMessage(Message msg) &#123; int what = msg.what; switch (what) &#123; case PREPARE: mTimeBase = SystemClock.uptimeMillis(); if (mParser == null || !mDanmakuView.isViewReady()) &#123;// false || false sendEmptyMessageDelayed(PREPARE, 100); &#125; else &#123; prepare(new Runnable() &#123;//会继续调用prepare重载方法 @Override public void run() &#123; pausedPosition = 0; mReady = true; if (mCallback != null) &#123; mCallback.prepared(); &#125; &#125; &#125;); &#125; break; ...... &#125; &#125;private DanmakuTimer timer = new DanmakuTimer();//已经初始化timerprivate void prepare(final Runnable runnable) &#123; if (drawTask == null) &#123;//会继续调用createDrawTask方法 drawTask = createDrawTask(mDanmakuView.isDanmakuDrawingCacheEnabled(), timer, mDanmakuView.getContext(), mDanmakuView.getWidth(), mDanmakuView.getHeight(), mDanmakuView.isHardwareAccelerated(), new IDrawTask.TaskListener() &#123; @Override public void ready() &#123; initRenderingConfigs(); runnable.run(); &#125; ...... &#125;); &#125; else &#123; runnable.run(); &#125;&#125;//继续调用createDrawTask(true, timer, context, width, height, true, listener)方法private IDrawTask createDrawTask(boolean useDrwaingCache, DanmakuTimer timer, Context context, int width, int height, boolean isHardwareAccelerated, IDrawTask.TaskListener taskListener) &#123; mDisp = mContext.getDisplayer();//AndroidDisplayer赋给它，顾名思义，Displayer就是显示器 mDisp.setSize(width, height);//设置弹幕视图宽高 DisplayMetrics displayMetrics = context.getResources().getDisplayMetrics(); mDisp.setDensities(displayMetrics.density, displayMetrics.densityDpi, displayMetrics.scaledDensity);//设置密度先关 mDisp.resetSlopPixel(mContext.scaleTextSize);//设置字体缩放比例，之前设过了1.2 mDisp.setHardwareAccelerated(isHardwareAccelerated);//硬件加速，true //useDrwaingCache 为true IDrawTask task = useDrwaingCache ? new CacheManagingDrawTask(timer, mContext, taskListener, 1024 * 1024 * AndroidUtils.getMemoryClass(context) / 3) : new DrawTask(timer, mContext, taskListener); task.setParser(mParser);//把存放数据源的mParser放入CacheManagingDrawTask中 task.prepare();//这个才是重点，调用CacheManagingDrawTask的prepare方法 obtainMessage(NOTIFY_DISP_SIZE_CHANGED, false).sendToTarget(); return task;&#125; &#160; &#160; &#160; &#160;上述过程最后一个调用了createDrawTask方法，这里先初始化了一下AndroidDisplayer配置，就把他当做显示器吧，我猜ctiao当初设计时也是这么比喻的吧。&#160; &#160; &#160; &#160;设置好弹幕显示相关的参数，然后就是创建绘制任务IDrawTask 了。这里有两个选择，如果使用缓存就创建CacheManagingDrawTask，不使用就创建DrawTask。不过CacheManagingDrawTask比DrawTask复杂很多。 CacheManagingDrawTask绘制任务&#160; &#160; &#160; &#160;我们的useDrwaingCache为true（其实把它改为false也没关系，并且这样就用不上那些so库了），则创建CacheManagingDrawTask绘制任务，然后调用prepare方法。123456789101112131415161718192021222324252627 public CacheManagingDrawTask(DanmakuTimer timer, DanmakuContext config, TaskListener taskListener, int maxCacheSize) &#123;//传入定时器timer，config，listener，还有三分一应用分配内存大小的maxCacheSize super(timer, config, taskListener);//会调用父类DrawTask的构造方法 NativeBitmapFactory.loadLibs();//加载so库，用于创建bitmap，同时测试时候加载成功 mMaxCacheSize = maxCacheSize; if (NativeBitmapFactory.isInNativeAlloc()) &#123;//true,将最大内存扩大到2倍 mMaxCacheSize = maxCacheSize * 2; &#125; mCacheManager = new CacheManager(maxCacheSize, MAX_CACHE_SCREEN_SIZE); mRenderer.setCacheManager(mCacheManager); &#125; //看看父类的构造方法 public DrawTask(DanmakuTimer timer, DanmakuContext context, TaskListener taskListener) &#123;...... mContext = context; mDisp = context.getDisplayer(); mTaskListener = taskListener; mRenderer = new DanmakuRenderer(context);...... initTimer(timer);//初始化相关定时器...... &#125; protected void initTimer(DanmakuTimer timer) &#123; mTimer = timer; mCacheTimer = new DanmakuTimer(); mCacheTimer.update(timer.currMillisecond); &#125; &#160; &#160; &#160; &#160;CacheManagingDrawTask的构造方法设置了一些变量。其中NativeBitmapFactory.loadLibs()加载了用于创建bitmap的so文件，就是用skia图形处理库直接创建bitmap，Android对2D图形处理采用的就是skia，3D图形处理用的是OpenGL ES。这样通过native层创建bitmap直接跳过Dalvik，毕竟java层内存用多了很容易oom。因为以前我就对native层比较感兴趣，所以我要任性的跟一遍源码 ^O.O^。为了怕跟完后自己晕了，找不到现在分析的地方了，所以在这里打个标签，mark一下。如不感兴趣，可以跳过= 。=12345678910111213141516171819202122232425262728293031//NativeBitmapFactorypublic static void loadLibs() &#123; ...... System.loadLibrary("ndkbitmap");//载入so ...... //测试功能 if (nativeLibLoaded) &#123; boolean libInit = init();//这是一个native方法 if (!libInit) &#123; release(); notLoadAgain = true; nativeLibLoaded = false; &#125; else &#123;//初始化成功后 initField();//反射Bitmap.Config的nativeInt字段 boolean confirm = testLib();//测试例子 &#125; &#125; Log.e("NativeBitmapFactory", "loaded" + nativeLibLoaded); &#125; //反射Bitmap.Config的nativeInt字段 static void initField() &#123; try &#123; nativeIntField = Bitmap.Config.class.getDeclaredField("nativeInt"); nativeIntField.setAccessible(true); &#125; catch (NoSuchFieldException e) &#123; nativeIntField = null; e.printStackTrace(); &#125; &#125;private static native boolean init(); &#160; &#160; &#160; &#160;这里会调用测试方法testLib：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 private static boolean testLib() &#123; if (nativeIntField == null) &#123; return false; &#125; Bitmap bitmap = null; Canvas canvas = null; ...... //用native方法创建一个bitmap bitmap = createNativeBitmap(2, 2, Bitmap.Config.ARGB_8888, true); boolean result = (bitmap != null &amp;&amp; bitmap.getWidth() == 2 &amp;&amp; bitmap.getHeight() == 2);...... canvas = new Canvas(bitmap); Paint paint = new Paint(); paint.setColor(Color.RED); paint.setTextSize(20f); canvas.drawRect(0f, 0f, (float) bitmap.getWidth(), (float) bitmap.getHeight(), paint); canvas.drawText("TestLib", 0, 0, paint); ...... return result; &#125; private static Bitmap createNativeBitmap(int width, int height, Config config, boolean hasAlpha) &#123; int nativeConfig = getNativeConfig(config);//反射设置Bitmap.Config.ARGB_8888 return android.os.Build.VERSION.SDK_INT == 19 ? createBitmap19(width, height, nativeConfig, hasAlpha) : createBitmap(width, height, nativeConfig, hasAlpha); &#125; public static int getNativeConfig(Bitmap.Config config) &#123; try &#123; if (nativeIntField == null) &#123; return 0; &#125; return nativeIntField.getInt(config); &#125; catch (IllegalArgumentException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; return 0; &#125; private static native boolean init(); private static native boolean release(); private static native Bitmap createBitmap(int width, int height, int nativeConfig, boolean hasAlpha); private static native Bitmap createBitmap19(int width, int height, int nativeConfig, boolean hasAlpha); &#160; &#160; &#160; &#160;上述最终用native方法创建bitmap，C++文件地址为 https://github.com/Bilibili/NativeBitmapFactory ，接着继续查看native方法具体实现NativeBitmapFactory.cpp。12345678910111213141516171819202122232425262728293031//先看java层init方法对应的本地方法jboolean Java_tv_cjump_jni_NativeBitmapFactory_init(JNIEnv *env)&#123; ...... //继续看Start方法 int r = Start(); return r == SUCCESS;&#125;static int Start()&#123; //创建一个类型为ndkbitmap_obj 的结构体指针 ndkbitmap_obj = (ndkbitmap_object_t *)malloc(sizeof(*ndkbitmap_obj)); int r = Open(ndkbitmap_obj); ...... return SUCCESS;&#125;static int Open(ndkbitmap_object_t *obj)&#123; //创建一个类型为skbitmap_sys_t 的结构体指针 skbitmap_sys_t *sys = (skbitmap_sys_t *)malloc(sizeof (*sys)); ...... //打开libskia.so动态链接库，初始化一些参数并返回动态链接库的句柄 sys-&gt;libskia = InitLibrary(sys); ...... //打开libandroid_runtime.so动态链接库，初始化一些参数并返回动态链接库的句柄 sys-&gt;libjnigraphics = InitLibrary2(sys); ...... //将初始化过后的结构指针sys赋给结构体obj的sys成员 obj-&gt;sys = sys; return SUCCESS;&#125; &#160; &#160; &#160; &#160;init方法主要是打开和skia相关的动态链接库，并初始化一些配置。(InitLibrary和InitLibrary2方法的细节我没有贴，里面实现需要一些专业知识，有兴趣的可以找资料钻研)然后就是createBitmap：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374jobject Java_tv_cjump_jni_NativeBitmapFactory_createBitmap(JNIEnv *env , jobject obj, jint w, jint h, jint config, jboolean hasAlpha)&#123; return createBitmap(env, obj, w, h, config, hasAlpha, true, 0);&#125;jobject Java_tv_cjump_jni_NativeBitmapFactory_createBitmap19(JNIEnv *env , jobject obj, jint w, jint h, jint config, jboolean hasAlpha)&#123; return createBitmap(env, obj, w, h, config, hasAlpha, 0x3, 19);&#125;jobject createBitmap(JNIEnv *env , jobject obj, jint w, jint h, jint config, jboolean hasAlpha, int isMuttable, int api)&#123; void *bm = createSkBitmap(ndkbitmap_obj, config, w, h);//调用重载方法创建bitmap指针 if (bm == NULL) &#123; return NULL; &#125; jobject result = NULL; skbitmap_sys_t *p_sys = ndkbitmap_obj-&gt;sys; if(p_sys-&gt;libjnigraphics) &#123; if(p_sys-&gt;gjni_createBitmap) &#123;//SDK版本小于19 //通过这个函数指针把JNI层bitmap的转换对象return给java层 result = p_sys-&gt;gjni_createBitmap(env, bm, isMuttable, NULL, -1); &#125; else if(p_sys-&gt;gjni_createBitmap_19later) &#123;//SDK版本19以后返回值 result = p_sys-&gt;gjni_createBitmap_19later(env, bm, NULL, isMuttable, NULL, NULL, -1); &#125; &#125; return result;&#125;//创建bitmap指针，并通过相关指针函数设置bitmap参数inline void *createSkBitmap(ndkbitmap_object_t *obj, int config, int w, int h)&#123; skbitmap_sys_t *p_sys = obj-&gt;sys; if (p_sys == NULL || p_sys-&gt;libskia == NULL) &#123; return NULL; &#125; //申请内存，创建skBitmap 指针 void *skBitmap = malloc(SIZE_OF_SKBITMAP); if (!skBitmap) &#123; return NULL; &#125; *((uint32_t *) ((uint32_t)skBitmap + SIZE_OF_SKBITMAP - 4)) = 0xbaadbaad; //ctor p_sys-&gt;sk_ctor(skBitmap); if (p_sys-&gt;sk_setConfig) &#123; p_sys-&gt;sk_setConfig(skBitmap, config, w, h, 0); &#125; else if (p_sys-&gt;sk_setConfig_19later) &#123; p_sys-&gt;sk_setConfig_19later(skBitmap, config, w, h, 0, (uint8_t)kPremul_SkAlphaType); &#125; else if (p_sys-&gt;sk_setInfo) &#123; int imageInfo[4] = &#123;w, h, SkBitmapConfigToColorType(config), kPremul_SkAlphaType&#125;; p_sys-&gt;sk_setInfo(skBitmap, imageInfo, 0); &#125; p_sys-&gt;sk_allocPixels(skBitmap, NULL, NULL); p_sys-&gt;sk_eraseARGB(skBitmap, 0, 0, 0, 0); if (!(*((uint32_t *) ((uint32_t)skBitmap + SIZE_OF_SKBITMAP - 4)) == 0xbaadbaad) ) &#123; free(skBitmap); return NULL; &#125; return skBitmap;&#125; &#160; &#160; &#160; &#160;通过skia图形库创建bitmap流程大概就是这些，其实skia的东西也是巨多无比，如果是从事这一方面工作应该都轻车熟路，我是完全的小白，能力有限，只能先到这儿。 &#160; &#160; &#160; &#160;好了，继续回到上次打标签的地方。接着该调用CacheManagingDrawTask的prepare方法：12345public void prepare() &#123; assert (mParser != null); loadDanmakus(mParser); mCacheManager.begin();&#125; &#160; &#160; &#160; &#160;先调用loadDanmakus方法：12345678910111213protected IDanmakus danmakuList; protected void loadDanmakus(BaseDanmakuParser parser) &#123; danmakuList = parser.setConfig(mContext) .setDisplayer(mDisp) .setTimer(mTimer) .getDanmakus();//从parser中取出弹幕数据,做出相关处理 ...... if(danmakuList != null) &#123; mLastDanmaku = danmakuList.last(); &#125; &#125; &#160; &#160; &#160; &#160;parser设置完DanmakuContext，AndroidDisplayer，DanmakuTimer之后，再调用getDanmakus取出弹幕信息：123456789public IDanmakus getDanmakus() &#123; if (mDanmakus != null) return mDanmakus; mContext.mDanmakuFactory.resetDurationsData();//重庆内置一些变量为null mDanmakus = parse();//解析弹幕 releaseDataSource();//关闭JSONSource mContext.mDanmakuFactory.updateMaxDanmakuDuration();//修正弹幕最大时长 return mDanmakus;&#125; &#160; &#160; &#160; &#160;进入AcFunDanmakuParser的parse方法：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788 public Danmakus parse() &#123; if (mDataSource != null &amp;&amp; mDataSource instanceof JSONSource) &#123; JSONSource jsonSource = (JSONSource) mDataSource; return doParse(jsonSource.data());//go on &#125; return new Danmakus(); &#125; private Danmakus doParse(JSONArray danmakuListData) &#123; Danmakus danmakus = new Danmakus(); if (danmakuListData == null || danmakuListData.length() == 0) &#123; return danmakus; &#125; for (int i = 0; i &lt; danmakuListData.length(); i++) &#123; try &#123; JSONObject danmakuArray = danmakuListData.getJSONObject(i); if (danmakuArray != null) &#123; danmakus = _parse(danmakuArray, danmakus);//解析每一条弹幕 &#125; &#125; catch (JSONException e) &#123; e.printStackTrace(); &#125; &#125; return danmakus; &#125; /** * &#123;"c":"19.408,16777215,1,25,178252,1376325904","m":"金刚如来！"&#125; // 0:时间(弹幕出现时间) // 1:颜色 // 2:类型(1从右往左滚动弹幕|6从右至左滚动弹幕|5顶端固定弹幕|4底端固定弹幕|7高级弹幕|8脚本弹幕) // 3:字号 // 4:用户id ? // 5:时间戳 ? */ private Danmakus _parse(JSONObject jsonObject, Danmakus danmakus) &#123; if (danmakus == null) &#123; danmakus = new Danmakus(); &#125; if (jsonObject == null || jsonObject.length() == 0) &#123; return danmakus; &#125; for (int i = 0; i &lt; jsonObject.length(); i++) &#123; try &#123; JSONObject obj = jsonObject; String c = obj.getString("c");//弹幕配置信息 String[] values = c.split(","); if (values.length &gt; 0) &#123; int type = Integer.parseInt(values[2]); // 弹幕类型 if (type == 7) // FIXME : hard code // TODO : parse advance danmaku json continue; long time = (long) (Float.parseFloat(values[0]) * 1000); // 出现时间 int color = Integer.parseInt(values[1]) | 0xFF000000; // 颜色 float textSize = Float.parseFloat(values[3]); // 字体大小 //使用弹幕工厂创建一条弹幕 BaseDanmaku item = mContext.mDanmakuFactory.createDanmaku(type, mContext); if (item != null) &#123; item.time = time; item.textSize = textSize * (mDispDensity - 0.6f); item.textColor = color; item.textShadowColor = color &lt;= Color.BLACK ? Color.WHITE : Color.BLACK; //弹幕文字内容，如果多行文本会拆分内容 DanmakuUtils.fillText(item, obj.optString("m", "....")); item.index = i; item.setTimer(mTimer);//将定时器设置给每一条弹幕 danmakus.addItem(item); &#125; &#125; &#125; catch (JSONException e) &#123; &#125; catch (NumberFormatException e) &#123; &#125; &#125; return danmakus; &#125; //DanmakuUtilsdefillText方法，多行文本会拆分 public static void fillText(BaseDanmaku danmaku, CharSequence text) &#123; danmaku.text = text; //如果文本没有换行符则不用拆分 if (TextUtils.isEmpty(text) || !text.toString().contains(BaseDanmaku.DANMAKU_BR_CHAR)) &#123; return; &#125; //如果有换行符则拆分，然后将拆分的数组付给lines 属性 String[] lines = String.valueOf(danmaku.text).split(BaseDanmaku.DANMAKU_BR_CHAR, -1); if (lines.length &gt; 1) &#123; danmaku.lines = lines; &#125; &#125; &#125; &#160; &#160; &#160; &#160;从JSONSource里解析每一条弹幕，接着我们看看弹幕工厂DanmakuFactory创建弹幕的方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889 public BaseDanmaku createDanmaku(int type, DanmakuContext context) &#123; if (context == null) return null; sLastConfig = context; sLastDisp = context.getDisplayer(); return createDanmaku(type, sLastDisp.getWidth(), sLastDisp.getHeight(), CURRENT_DISP_SIZE_FACTOR, context.scrollSpeedFactor);// go on overload method &#125; public BaseDanmaku createDanmaku(int type, int viewportWidth, int viewportHeight, float viewportScale, float scrollSpeedFactor) &#123; return createDanmaku(type, (float) viewportWidth, (float) viewportHeight, viewportScale, scrollSpeedFactor); &#125; public BaseDanmaku createDanmaku(int type, float viewportWidth, float viewportHeight, float viewportSizeFactor, float scrollSpeedFactor) &#123; int oldDispWidth = CURRENT_DISP_WIDTH; // 默认是0 int oldDispHeight = CURRENT_DISP_HEIGHT; // 默认是0 //修正试图宽高，缩放比，弹幕时长 boolean sizeChanged = updateViewportState(viewportWidth, viewportHeight, viewportSizeFactor); //滚动弹幕的Duration赋值 if (MAX_Duration_Scroll_Danmaku == null) &#123; MAX_Duration_Scroll_Danmaku = new Duration(REAL_DANMAKU_DURATION); MAX_Duration_Scroll_Danmaku.setFactor(scrollSpeedFactor); &#125; else if (sizeChanged) &#123; MAX_Duration_Scroll_Danmaku.setValue(REAL_DANMAKU_DURATION); &#125;//固定位置弹幕的Duration赋值 if (MAX_Duration_Fix_Danmaku == null) &#123; MAX_Duration_Fix_Danmaku = new Duration(COMMON_DANMAKU_DURATION); &#125; if (sizeChanged &amp;&amp; viewportWidth &gt; 0) &#123;// true &amp;&amp; true updateMaxDanmakuDuration();// 修正弹幕最长时长 ...... &#125; BaseDanmaku instance = null; switch (type) &#123; case 1: // 从右往左滚动 instance = new R2LDanmaku(MAX_Duration_Scroll_Danmaku); break; case 4: // 底端固定 instance = new FBDanmaku(MAX_Duration_Fix_Danmaku); break; case 5: // 顶端固定 instance = new FTDanmaku(MAX_Duration_Fix_Danmaku); break; case 6: // 从左往右滚动 instance = new L2RDanmaku(MAX_Duration_Scroll_Danmaku); break; case 7: // 特殊弹幕 instance = new SpecialDanmaku(); sSpecialDanmakus.addItem(instance); break; &#125; return instance; &#125; //修正试图宽高，缩放比，弹幕时长 public boolean updateViewportState(float viewportWidth, float viewportHeight, float viewportSizeFactor) &#123; boolean sizeChanged = false; if (CURRENT_DISP_WIDTH != (int) viewportWidth || CURRENT_DISP_HEIGHT != (int) viewportHeight || CURRENT_DISP_SIZE_FACTOR != viewportSizeFactor) &#123; sizeChanged = true; //弹幕时长 t = 3800 * (1.2 * 视图宽 / 682) REAL_DANMAKU_DURATION = (long) (COMMON_DANMAKU_DURATION * (viewportSizeFactor * viewportWidth / BILI_PLAYER_WIDTH)); // t = min(t, 9000) REAL_DANMAKU_DURATION = Math.min(MAX_DANMAKU_DURATION_HIGH_DENSITY, REAL_DANMAKU_DURATION); // t = max(t, 4000) REAL_DANMAKU_DURATION = Math.max(MIN_DANMAKU_DURATION, REAL_DANMAKU_DURATION); CURRENT_DISP_WIDTH = (int) viewportWidth; CURRENT_DISP_HEIGHT = (int) viewportHeight; CURRENT_DISP_SIZE_FACTOR = viewportSizeFactor; &#125; return sizeChanged; &#125; //修正弹幕最长时长 public void updateMaxDanmakuDuration() &#123; long maxScrollDuration = (MAX_Duration_Scroll_Danmaku == null ? 0: MAX_Duration_Scroll_Danmaku.value), maxFixDuration = (MAX_Duration_Fix_Danmaku == null ? 0 : MAX_Duration_Fix_Danmaku.value), maxSpecialDuration = (MAX_Duration_Special_Danmaku == null ? 0: MAX_Duration_Special_Danmaku.value); MAX_DANMAKU_DURATION = Math.max(maxScrollDuration, maxFixDuration); MAX_DANMAKU_DURATION = Math.max(MAX_DANMAKU_DURATION, maxSpecialDuration); MAX_DANMAKU_DURATION = Math.max(COMMON_DANMAKU_DURATION, MAX_DANMAKU_DURATION); MAX_DANMAKU_DURATION = Math.max(REAL_DANMAKU_DURATION, MAX_DANMAKU_DURATION); &#125; &#160; &#160; &#160; &#160;DanmakuFactory创建弹幕主要是计算了弹幕时长，然后根据不同类型创建不同的弹幕。 &#160; &#160; &#160; &#160;到此CacheManagingDrawTask的loadDanmakus方法走完了。loadDanmakus方法主要从 mParser里的JSONSource解析弹幕数据源，根据不同类型的type用DanmakuFactory创建不同的Danmaku，分别计算Duration，最后存放到一个Danmakus对象里。 &#160; &#160; &#160; &#160; 继续回到刚才的prepare方法，往下继续执行:123456789101112131415161718192021222324 @Override public void prepare() &#123; assert (mParser != null); loadDanmakus(mParser);//走完了 mCacheManager.begin();//走这个 &#125; //CacheManager的方法 public void begin() &#123; mEndFlag = false; //创建一个HandlerThread用于在工作线程处理事务 if (mThread == null) &#123; mThread = new HandlerThread("DFM Cache-Building Thread"); mThread.start(); &#125; //创建一个Handler和HandlerThread搭配用 if (mHandler == null) mHandler = new CacheHandler(mThread.getLooper()); mHandler.begin();// 走到这里 &#125;//HandlerThread的begin方法 public void begin() &#123; sendEmptyMessage(PREPARE); ...... &#125; &#160; &#160; &#160; &#160;我们可以看到创建了一个HandlerThread，然后创建了一个CacheHandler，所以CacheHandler发送消息后，处理消息内容都是在子线程。&#160; &#160; &#160; &#160;然后发送了PREPARE消息，然后就是回调handleMessage方法：12345678910111213141516171819202122232425262728293031323334 DrawingCachePoolManager mCachePoolManager = new DrawingCachePoolManager(); //创建一个缓存个数上限为800的FinitePool池 Pool&lt;DrawingCache&gt; mCachePool = Pools.finitePool(mCachePoolManager, 800); //Pools的finitePool方法 public static &lt;T extends Poolable&lt;T&gt;&gt; Pool&lt;T&gt; finitePool(PoolableManager&lt;T&gt; manager, int limit) &#123; return new FinitePool&lt;T&gt;(manager, limit); &#125; //CacheHandler的handleMessage方法 public void handleMessage(Message msg) &#123; int what = msg.what; switch (what) &#123; case PREPARE: evictAllNotInScreen();//清除所有不在屏幕内的缓存，此时还没有缓存 for (int i = 0; i &lt; 300; i++) &#123;//在池里放300个预留缓存，以链式存储方式存放 mCachePool.release(new DrawingCache()); &#125; ...... &#125; &#125;//FinitePool的release方法：回收缓存对象，并且用头插法，以链式存储（类似链表） public void release(T element) &#123; if (!element.isPooled()) &#123; if (mInfinite || mPoolCount &lt; mLimit) &#123; mPoolCount++; element.setNextPoolable(mRoot); element.setPooled(true); mRoot = element; &#125; mManager.onReleased(element); &#125; else &#123; System.out.print("[FinitePool] Element is already in pool: " + element); &#125; &#125; &#160; &#160; &#160; &#160;处理完PREPARE消息后，会继续进入DISPATCH_ACTIONS逻辑处理中：12345678910111213141516171819202122232425262728293031323334 ...... case DISPATCH_ACTIONS: long delayed = dispatchAction();//走到这里 if (delayed &lt;= 0) &#123;// true //会没隔半条弹幕时间发送一次DISPATCH_ACTIONS消息 delayed = mContext.mDanmakuFactory.MAX_DANMAKU_DURATION / 2; &#125; sendEmptyMessageDelayed(DISPATCH_ACTIONS, delayed); break; ...... /*----------dispatchAction方法START----------*/ private long dispatchAction() &#123; ...省略一些第一次不会执行的逻辑... removeMessages(BUILD_CACHES); sendEmptyMessage(BUILD_CACHES);//发送BUILD_CACHES消息 return 0; &#125; /*----------dispatchAction方法END----------*/ ...... case BUILD_CACHES: removeMessages(BUILD_CACHES); boolean repositioned = ((mTaskListener != null &amp;&amp; mReadyState == false) || mSeekedFlag);// 为true prepareCaches(repositioned);//调用prepareCaches方法 if (repositioned) mSeekedFlag = false; if (mTaskListener != null &amp;&amp; mReadyState == false) &#123; mTaskListener.ready();//然后回到mTaskListener监听ready方法 mReadyState = true; &#125; break; ...... &#160; &#160; &#160; &#160;我们发现，处理接着处理DISPATCH_ACTIONS消息时，会每隔半条弹幕时间发送一次DISPATCH_ACTIONS消息。&#160; &#160; &#160; &#160;处理DISPATCH_ACTIONS消息内会执行dispatchAction方法，这个方法内逻辑情况比较多，我们先挖个坑，先把刚开始时会走的逻辑执行了，其他逻辑以后用时会填上。（挖坑 ^O_O^）&#160; &#160; &#160; &#160;首次调用dispatchAction方法内发送了BUILD_CACHES消息消息，会先调用prepareCaches(true)方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private long prepareCaches(boolean repositioned) &#123; long curr = mCacheTimer.currMillisecond;// 0 //3条弹幕时间 long end = curr + mContext.mDanmakuFactory.MAX_DANMAKU_DURATION * mScreenSize; if (end &lt; mTimer.currMillisecond) &#123; return 0; &#125; long startTime = SystemClock.uptimeMillis(); IDanmakus danmakus = null; int tryCount = 0; boolean hasException = false; do &#123; try &#123; //截取三条弹幕时间中所有的弹幕 danmakus = danmakuList.subnew(curr, end); &#125; catch (Exception e) &#123; hasException = true; SystemClock.sleep(10); &#125; &#125; while (++tryCount &lt; 3 &amp;&amp; danmakus == null &amp;&amp; hasException);//截取成功后跳出循环 if (danmakus == null) &#123; mCacheTimer.update(end); return 0; &#125; ...... IDanmakuIterator itr = danmakus.iterator(); BaseDanmaku item = null; int sizeInScreen = danmakus.size(); while (!mPause &amp;&amp; !mCancelFlag) &#123;// boolean hasNext = itr.hasNext(); if (!hasNext) &#123; break; &#125; item = itr.next(); ...... // build cache ，省略了一些障眼法,这才是重点，建立缓存 if (buildCache(item, false) == RESULT_FAILED) &#123; break; &#125; ...... &#125; ...... if (item != null) &#123;//截取的最后一条弹幕,更新缓存定时器时间 mCacheTimer.update(item.time); &#125; else &#123; mCacheTimer.update(end); &#125; return consumingTime; &#125; &#160; &#160; &#160; &#160;为截取的每一条弹幕建立缓存会调用buildCache(item, false)方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 private byte buildCache(BaseDanmaku item, boolean forceInsert) &#123; // measure ,先测量每一条弹幕的宽高 if (!item.isMeasured()) &#123; item.measure(mDisp, true); &#125; DrawingCache cache = null; try &#123; // try to find reuseable cache, 在mCaches缓存的20条内查找和目标弹幕样式完全一样的弹幕(文字、大小、边框、下划线、颜色完全相同) BaseDanmaku danmaku = findReuseableCache(item, true, 20); if (danmaku != null) &#123;//如果查找出了这样的弹幕 cache = (DrawingCache) danmaku.cache; &#125; if (cache != null) &#123;//如果找到的弹幕有缓存 cache.increaseReference();//则将引用计数 +1 item.cache = cache;//将目标弹幕缓存的引用指向查找出来的弹幕缓存，即多个引用指向同一个对象 //将这个目标弹幕的引用放入缓存Danmakus中(mCaches)，同时更新已使用大小mRealSize mCacheManager.push(item, 0, forceInsert); return RESULT_SUCCESS; &#125; // try to find reuseable cache from timeout || no-refrerence caches //如果上述的查找样式完全相同的弹幕没有找到，则在前50条缓存中查找对比当前时间已经过时的 //，没有被重复引用的(只有上面那种情况才会增加引用计数，其他情况都不会) //，而且宽高和目标弹幕差值在规定范围内的弹幕 danmaku = findReuseableCache(item, false, 50); if (danmaku != null) &#123;// 如果找到了这样的弹幕 cache = (DrawingCache) danmaku.cache; &#125; if (cache != null) &#123;//如果找到的弹幕有缓存 danmaku.cache = null;//先清除过时弹幕的缓存 //再根据目标弹幕样式，重新设置缓存(为每条弹幕创建一个bitmap和canvas，然后画出边框、下划线、文字等等) cache = DanmakuUtils.buildDanmakuDrawingCache(item, mDisp, cache); //redraw item.cache = cache;//将缓存应用赋给目标弹幕 mCacheManager.push(item, 0, forceInsert);//将这个目标弹幕的引用放入缓存Danmakus中(mCaches)，同时更新已使用大小mRealSize return RESULT_SUCCESS; &#125;//如果上述两次查找缓存都没找到，则进入下面逻辑 // guess cache size if (!forceInsert) &#123;//如果forceInsert为false，则表示不检测内存超出 //计算此弹幕bitmap的大小，width * height * 4 //(因为用native创建的Bitmap的Config为ARGB_8888，所以一个像素占4个字节) int cacheSize = DanmakuUtils.getCacheSize((int) item.paintWidth, (int) item.paintHeight); //如果当前已经使用大小 + 此弹幕缓存大小 &gt; 设置的最大内存（2/3 应用内存） if (mRealSize + cacheSize &gt; mMaxSize) &#123;//没有超 return RESULT_FAILED; &#125; &#125;//从FinitePool中的300个DrawingCache对象中取出来一个 cache = mCachePool.acquire(); //如果从上面的FinitePool取完了，则会直接new一个DrawingCache，配置DrawingCache cache = DanmakuUtils.buildDanmakuDrawingCache(item, mDisp, cache); item.cache = cache; //将item存入mCaches缓存，同时更新已使用大小mRealSize boolean pushed = mCacheManager.push(item, sizeOf(item), forceInsert); if (!pushed) &#123;//如果item存放失败(使用内存超出规定大小) releaseDanmakuCache(item, cache);//释放DrawingCache &#125; return pushed ? RESULT_SUCCESS : RESULT_FAILED; &#125; catch (OutOfMemoryError e) &#123; releaseDanmakuCache(item, cache); return RESULT_FAILED; &#125; catch (Exception e) &#123; releaseDanmakuCache(item, cache); return RESULT_FAILED; &#125; &#125; &#160; &#160; &#160; &#160;buildCache(item, false)为每一条弹幕建立缓存，其中有几处： 先测量弹幕的宽高 在mCaches缓存的20条内查找和目标弹幕样式完全一样的弹幕(文字、大小、边框、下划线、颜色完全相同) 如果上述的查找样式完全相同的弹幕没有找到，则在前50条缓存中查找对比当前时间已经过时的 ，没有被重复引用的(只有上面那种情况才会增加引用计数，其他情况都不会)，而且宽高和目标弹幕差值在规定范围内的弹幕，再根据目标弹幕样式，重新设置缓存(为每条弹幕创建一个bitmap和canvas，然后画出边框、下划线、文字等等) 如果上述两次查找缓存都没找到，则从FinitePool中取出一个，没有就new一个，然后同上配置DrawingCache &#160; &#160; &#160; &#160;1）我们一个一个来，先测量：123456789101112//弹幕的基类都是BaseDanmaku,只有子类R2LDanmaku重写了measure方法 @Override //R2LDanmaku的measure方法 public void measure(IDisplayer displayer, boolean fromWorkerThread) &#123; super.measure(displayer, fromWorkerThread);//调用了父类的方法 mDistance = (int) (displayer.getWidth() + paintWidth);//滚动弹幕的距离都是 视图宽度+弹幕宽度，很好理解 mStepX = mDistance / (float) duration.value; //每秒步长就是总滚动距离除以弹幕时长 &#125; //父类BaseDanmaku的measure方法 public void measure(IDisplayer displayer, boolean fromWorkerThread) &#123; displayer.measure(this, fromWorkerThread);//AndroidDisplayer的measure方法 this.measureResetFlag = flags.MEASURE_RESET_FLAG;//设置已经测量过了的标签 &#125; &#160; &#160; &#160; &#160;接着会调用AndroidDisplayer的measure方法：123456789101112 @Override public void measure(BaseDanmaku danmaku, boolean fromWorkerThread) &#123; ...设置画笔style,color,alpha,省略... calcPaintWH(danmaku, paint, fromWorkerThread);//计算宽高 ...设置画笔style,color,alpha,省略... &#125;private BaseCacheStuffer sStuffer = new SimpleTextCacheStuffer();//默认是SimpleTextCacheStuffer private void calcPaintWH(BaseDanmaku danmaku, TextPaint paint, boolean fromWorkerThread) &#123; sStuffer.measure(danmaku, paint, fromWorkerThread);//sStuffer就是我们在MainActivity里配置DanmakuContext时设置的，默认是SimpleTextCacheStuffer ...加上描边，padding等额外值，省略... &#125; &#160; &#160; &#160; &#160;还记得在MainActivity里配置DanmakuContext吗？当时是这么写的： .setCacheStuffer(new SpannedCacheStuffer(), mCacheStufferAdapter) // 图文混排使用SpannedCacheStuffer// .setCacheStuffer(new BackgroundCacheStuffer()) // 绘制背景使用BackgroundCacheStuffer &#160; &#160; &#160; &#160;比如SpannedCacheStuffer的measure方法是这样的：123456789101112131415@Override//SpannedCacheStuffer的measure方法 public void measure(BaseDanmaku danmaku, TextPaint paint, boolean fromWorkerThread) &#123; if (danmaku.text instanceof Spanned) &#123; CharSequence text = danmaku.text; if (text != null) &#123; //可看到将弹幕的宽高，文字等信息包在了一个StaticLayout对象中，然后付给danmaku的obj对象 StaticLayout staticLayout = new StaticLayout(text, paint, (int) Math.ceil(StaticLayout.getDesiredWidth(danmaku.text, paint)), Layout.Alignment.ALIGN_NORMAL, 1.0f, 0.0f, true); danmaku.paintWidth = staticLayout.getWidth(); danmaku.paintHeight = staticLayout.getHeight(); danmaku.obj = new SoftReference&lt;&gt;(staticLayout); return; &#125; &#125; super.measure(danmaku, paint, fromWorkerThread);//如果不是图文混排类型，则调用父类SimpleTextCacheStuffer的方法 &#125; &#160; &#160; &#160; &#160;可以看到measure方法创建了一个StaticLayout对象，并将它的软引用赋给了danmaku的obj属性；如果是图文混排类型弹幕，则danmaku.obj不为空；如果是普通弹幕则danmaku.obj为空。&#160; &#160; &#160; &#160;BackgroundCacheStuffer也差不多，都是对弹幕样式的一些改造。 &#160; &#160; &#160; &#160;然后我们看SimpleTextCacheStuffer的measure方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344//SimpleTextCacheStuffer的measure方法 public void measure(BaseDanmaku danmaku, TextPaint paint, boolean fromWorkerThread) &#123; if (mProxy != null) &#123;//这个mProxy 是BaseCacheStuffer.Proxy类型的对象，也是初始化DanmakuContext调用setCacheStuffer(cacheStuffer, proxy)时设置的 mProxy.prepareDrawing(danmaku, fromWorkerThread);//根据你的条件检查是否需要需要更新弹幕 &#125; float w = 0; Float textHeight = 0f; if (danmaku.lines == null) &#123;//不是多行文本 if (danmaku.text == null) &#123; w = 0; &#125; else &#123; w = paint.measureText(danmaku.text.toString());//测量出文字宽度 textHeight = getCacheHeight(danmaku, paint);//计算出文字高度 &#125; danmaku.paintWidth = w; danmaku.paintHeight = textHeight; &#125; else &#123;//如果是多行文本 textHeight = getCacheHeight(danmaku, paint);//计算出单行文字高度 for (String tempStr : danmaku.lines) &#123;//计算出多行文本总宽高 if (tempStr.length() &gt; 0) &#123; float tr = paint.measureText(tempStr); w = Math.max(tr, w); &#125; &#125; danmaku.paintWidth = w; danmaku.paintHeight = danmaku.lines.length * textHeight; &#125; &#125; private final static Map&lt;Float, Float&gt; sTextHeightCache = new HashMap&lt;Float, Float&gt;();//key是字号大小，value是字体高度 protected Float getCacheHeight(BaseDanmaku danmaku, Paint paint) &#123; Float textSize = paint.getTextSize(); Float textHeight = sTextHeightCache.get(textSize); if (textHeight == null) &#123; Paint.FontMetrics fontMetrics = paint.getFontMetrics(); //Android对文字绘制有些特殊，基准点是baseline，也就是例如canvas.drawText(text, baseX, baseY, textPaint)中写入的baseY大小 //Ascent是baseline之上字符最高处的y值； //Descent是baseline之下字符最低处的y值； //Leading其实是上一行字符的descent到下一行的ascent之间的距离。 //所以文本高度就是descent - ascent + leading textHeight = fontMetrics.descent - fontMetrics.ascent + fontMetrics.leading; sTextHeightCache.put(textSize, textHeight); &#125; return textHeight; &#125; &#160; &#160; &#160; &#160;这样就计算完了每一条弹幕的宽高，完成了测量。 &#160; &#160; &#160; &#160;2） 在mCaches缓存的20条内查找和目标弹幕样式完全一样的弹幕(文字、大小、边框、下划线、颜色完全相同)：&#160; &#160; &#160; &#160;先回到buildCache方法中这个位置。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 private byte buildCache(BaseDanmaku item, boolean forceInsert) &#123;//item, false ...测量已经完成... DrawingCache cache = null; try &#123; // try to find reuseable cache, 在mCaches缓存的20条内查找和目标弹幕样式完全一样的弹幕(文字、大小、边框、下划线、颜色完全相同) BaseDanmaku danmaku = findReuseableCache(item, true, 20); if (danmaku != null) &#123;//如果查找出了这样的弹幕 cache = (DrawingCache) danmaku.cache; &#125; if (cache != null) &#123;//如果找到的弹幕有缓存 cache.increaseReference();//则将引用计数 +1 item.cache = cache;//将目标弹幕缓存的引用指向查找出来的弹幕缓存，即多个引用指向同一个对象 //将这个目标弹幕的引用放入缓存Danmakus中(mCaches)，同时更新已使用大小mRealSize mCacheManager.push(item, 0, forceInsert); return RESULT_SUCCESS; &#125; ...... &#125; //在mCaches缓存的20条内查找和目标弹幕样式完全一样的弹幕(文字、大小、边框、下划线、颜色完全相同) private BaseDanmaku findReuseableCache(BaseDanmaku refDanmaku, boolean strictMode, int maximumTimes) &#123;//item， true， 20 IDanmakuIterator it = mCaches.iterator();...... int count = 0; while (it.hasNext() &amp;&amp; count++ &lt; maximumTimes) &#123; // limit maximum times 20 BaseDanmaku danmaku = it.next(); IDrawingCache&lt;?&gt; cache = danmaku.getDrawingCache(); if (cache == null || cache.get() == null) &#123; continue; &#125; //对比mCaches中的弹幕和目标的内幕文字、大小、边框、下划线、颜色是否完全相同 if (danmaku.paintWidth == refDanmaku.paintWidth &amp;&amp; danmaku.paintHeight == refDanmaku.paintHeight &amp;&amp; danmaku.underlineColor == refDanmaku.underlineColor &amp;&amp; danmaku.borderColor == refDanmaku.borderColor &amp;&amp; danmaku.textColor == refDanmaku.textColor &amp;&amp; danmaku.text.equals(refDanmaku.text)) &#123; return danmaku; &#125; if (strictMode) &#123;//true continue; &#125; ...... &#125; return null; &#125; //CacheManagingDrawTask.CacheManager的push方法 //将这个目标弹幕的引用放入缓存Danmakus中(mCaches)，同时更新已使用大小mRealSize private boolean push(BaseDanmaku item, int itemSize, boolean forcePush) &#123;//item，0，false int size = itemSize; //0......//这里注意mCaches是Danmakus类型，addItem方法里面实现其实是类型为TreeSet的集合去添加，如果是同一个对象，则不会添加 this.mCaches.addItem(item); mRealSize += size;//因为已经存在相同的缓存，因此已经使用缓存总大小不再增加 return true; &#125; //Danmakus的addItem方法 public boolean addItem(BaseDanmaku item) &#123; if (items != null) &#123;//items 类型为TreeSet try &#123; if (items.add(item)) &#123;//如果是相同对象，则返回false，mSize个数不会增加 mSize++; return true; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; return false; &#125; &#160; &#160; &#160; &#160;上述情况仅仅在相同样式，大小，颜色等都相同的弹幕第二次和以后的才会进入这段逻辑。对于不同的弹幕不会进入这个逻辑。（而且即使是相同弹幕，mCaches也只会存一个对象的，因为内部TreeSet的特性）&#160; &#160; &#160; &#160;所以我们继续看下一种逻辑。 &#160; &#160; &#160; &#160;3）在前50条缓存中查找对比当前时间已经过时的 ，没有被重复引用的(只有上面那种情况才会增加引用计数，其他情况都不会)，而且宽高和目标弹幕差值在规定范围内的弹幕，再根据目标弹幕样式，重新设置缓存(为每条弹幕创建一个bitmap和canvas，然后画出边框、下划线、文字等等)：&#160; &#160; &#160; &#160;继续回到buildCache方法这个位置：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364private byte buildCache(BaseDanmaku item, boolean forceInsert) &#123;//item, false ...测量过了... ...第一策略已经pass... // try to find reuseable cache from timeout || no-refrerence caches //如果上述的查找样式完全相同的弹幕没有找到，则在前50条缓存中查找对比当前时间已经过时的 //，没有被重复引用的(只有上面那种情况才会增加引用计数，其他情况都不会) //，而且宽高和目标弹幕差值在规定范围内的弹幕 danmaku = findReuseableCache(item, false, 50); if (danmaku != null) &#123;// 如果找到了这样的弹幕 cache = (DrawingCache) danmaku.cache; &#125; if (cache != null) &#123;//如果找到的弹幕有缓存 danmaku.cache = null;//先清除过时弹幕的缓存 //再根据目标弹幕样式，重新设置缓存(为每条弹幕创建一个bitmap和canvas，然后画出边框、下划线、文字等等) cache = DanmakuUtils.buildDanmakuDrawingCache(item, mDisp, cache); //redraw item.cache = cache;//将缓存应用赋给目标弹幕 mCacheManager.push(item, 0, forceInsert);//将这个目标弹幕的引用放入缓存Danmakus中(mCaches)，同时更新已使用大小mRealSize return RESULT_SUCCESS; &#125; ...... &#125; private BaseDanmaku findReuseableCache(BaseDanmaku refDanmaku, boolean strictMode, int maximumTimes) &#123;//item,false,50 IDanmakuIterator it = mCaches.iterator(); int slopPixel = 0; if (!strictMode) &#123;//进入逻辑，非严苛模式 slopPixel = mDisp.getSlopPixel() * 2;//允许目标弹幕与mCaches中找到的弹幕宽高偏差 &#125; int count = 0; while (it.hasNext() &amp;&amp; count++ &lt; maximumTimes) &#123; // limit maximum times 20 BaseDanmaku danmaku = it.next(); IDrawingCache&lt;?&gt; cache = danmaku.getDrawingCache(); if (cache == null || cache.get() == null) &#123; continue; &#125; //在这种第二策略中这段逻辑根本不会执行，因为以已经被上面的第一策略拦截了 if (danmaku.paintWidth == refDanmaku.paintWidth &amp;&amp; danmaku.paintHeight == refDanmaku.paintHeight &amp;&amp; danmaku.underlineColor == refDanmaku.underlineColor &amp;&amp; danmaku.borderColor == refDanmaku.borderColor &amp;&amp; danmaku.textColor == refDanmaku.textColor &amp;&amp; danmaku.text.equals(refDanmaku.text)) &#123; return danmaku; &#125; if (strictMode) &#123;//false continue; &#125; if (!danmaku.isTimeOut()) &#123;//还必须在mCaches中过时的弹幕中查找 break; &#125; if (cache.hasReferences()) &#123;//如果是相同弹幕被重新引用的，第二策略没有这样的 continue; &#125; //所以会走到这里，比较mCaches中过时的弹幕和目标弹幕宽高在不在允许的偏差内，如果在就返回查找出的这个弹幕 float widthGap = cache.width() - refDanmaku.paintWidth; float heightGap = cache.height() - refDanmaku.paintHeight; if (widthGap &gt;= 0 &amp;&amp; widthGap &lt;= slopPixel &amp;&amp; heightGap &gt;= 0 &amp;&amp; heightGap &lt;= slopPixel) &#123; return danmaku; &#125; &#125; return null; &#125; &#160; &#160; &#160; &#160;如果在上述第二策略中，在过时的缓存中找到了和目标弹幕宽高差不多的缓存项，则根据目标弹幕样式，重新设置缓存(为每条弹幕创建一个bitmap和canvas，然后画出边框、下划线、文字等等)，调用DanmakuUtils.buildDanmakuDrawingCache(item, mDisp, cache)方法：123456789101112131415161718 public static DrawingCache buildDanmakuDrawingCache(BaseDanmaku danmaku, IDisplayer disp, DrawingCache cache) &#123; if (cache == null) cache = new DrawingCache();//组建弹幕缓存(bitmap,canvas) cache.build((int) Math.ceil(danmaku.paintWidth), (int) Math.ceil(danmaku.paintHeight), disp.getDensityDpi(), false); DrawingCacheHolder holder = cache.get(); if (holder != null) &#123; //绘制弹幕内容 ((AbsDisplayer) disp).drawDanmaku(danmaku, holder.canvas, 0, 0, true); if(disp.isHardwareAccelerated()) &#123;//如果有硬件加速 //超过一屏的弹幕要切割 holder.splitWith(disp.getWidth(), disp.getHeight(), disp.getMaximumCacheWidth(), disp.getMaximumCacheHeight()); &#125; &#125; return cache; &#125; &#160; &#160; &#160; &#160;重新设置缓存分三步：1.组建弹幕缓存,2.绘制弹幕内容,3.切割超过一屏的弹幕。 &#160; &#160; &#160; &#160;No.1 组建弹幕缓存：123456789101112131415161718192021222324252627282930313233//DrawingCache的build方法 public void build(int w, int h, int density, boolean checkSizeEquals) &#123;//checkSizeEquals为false final DrawingCacheHolder holder = mHolder; //每个DrawingCache都有一个DrawingCacheHolder holder.buildCache(w, h, density, checkSizeEquals);//DrawingCacheHolder的buildCache方法 mSize = mHolder.bitmap.getRowBytes() * mHolder.bitmap.getHeight();//返回创建的bitmap的大小 &#125; //DrawingCacheHolder的buildCache方法 public void buildCache(int w, int h, int density, boolean checkSizeEquals) &#123; boolean reuse = checkSizeEquals ? (w == width &amp;&amp; h == height) : (w &lt;= width &amp;&amp; h &lt;= height);//检测大小 ？ 宽高相等 ： 小于已经缓存的bitmap宽高 if (reuse &amp;&amp; bitmap != null) &#123;//如果能够复用bitmap bitmap.eraseColor(Color.TRANSPARENT);//擦出之前的颜色 canvas.setBitmap(bitmap);//给Canvas重新预设bitmap recycleBitmapArray();//回收超过一屏弹幕切割后的bitmap数组，这个接下来会讲 return; &#125; if (bitmap != null) &#123;//如果不能复用，则回收旧的缓存bitmap recycle(); &#125; width = w; height = h; bitmap = NativeBitmapFactory.createBitmap(w, h, Bitmap.Config.ARGB_8888);//用native方法创建一个bitmap if (density &gt; 0) &#123;//设置density mDensity = density; bitmap.setDensity(density); &#125; //设置canvas if (canvas == null)&#123; canvas = new Canvas(bitmap); canvas.setDensity(density); &#125;else canvas.setBitmap(bitmap); &#125; &#160; &#160; &#160; &#160;组建弹幕缓存就是为个DrawingCache根据目标弹幕大小创建bitmap和canvas。 &#160; &#160; &#160; &#160;No.2 绘制弹幕内容：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394 public synchronized void drawDanmaku(BaseDanmaku danmaku, Canvas canvas, float left, float top, boolean fromWorkerThread) &#123;//danmaku, holder.canvas, 0, 0, true float _left = left; float _top = top; ...一些杂项，忽略... TextPaint paint = getPaint(danmaku, fromWorkerThread);//获取画笔 //绘制背景，sStuffer可以自己设置，默认是SimpleTextCacheStuffer,默认drawBackground为空 //这个可以自己扩展，上面讲过 sStuffer.drawBackground(danmaku, canvas, _left, _top); if (danmaku.lines != null) &#123;//如果是多行文本 String[] lines = danmaku.lines; if (lines.length == 1) &#123;//多行文本行数为1 if (hasStroke(danmaku)) &#123;//如果有描边，则绘制描边 //重设画笔（绘制描边） applyPaintConfig(danmaku, paint, true); float strokeLeft = left; float strokeTop = top - paint.ascent(); ...... //绘制描边 sStuffer.drawStroke(danmaku, lines[0], canvas, strokeLeft, strokeTop, paint); &#125; //再次重设画笔（绘制文字） applyPaintConfig(danmaku, paint, false); //绘制文字 sStuffer.drawText(danmaku, lines[0], canvas, left, top - paint.ascent(), paint, fromWorkerThread); &#125; else &#123;//多行文本行数大于1 //先计算每行文本的高度 float textHeight = (danmaku.paintHeight - 2 * danmaku.padding) / lines.length; //循环绘制每一行文本 for (int t = 0; t &lt; lines.length; t++) &#123; ...... if (hasStroke(danmaku)) &#123;//如果有描边，则绘制描边 //重设画笔（绘制描边） applyPaintConfig(danmaku, paint, true); float strokeLeft = left; float strokeTop = t * textHeight + top - paint.ascent(); ...... //绘制描边 sStuffer.drawStroke(danmaku, lines[t], canvas, strokeLeft, strokeTop, paint); &#125; //再次重设画笔（绘制文字） applyPaintConfig(danmaku, paint, false); //绘制文字 sStuffer.drawText(danmaku, lines[t], canvas, left, t * textHeight + top - paint.ascent(), paint, fromWorkerThread); &#125; &#125; &#125; else &#123;//如果是单行文本 if (hasStroke(danmaku)) &#123;//如果有描边，则绘制描边 //重设画笔（绘制描边） applyPaintConfig(danmaku, paint, true); float strokeLeft = left; float strokeTop = top - paint.ascent(); ...... //绘制描边 sStuffer.drawStroke(danmaku, null, canvas, strokeLeft, strokeTop, paint); &#125; //再次重设画笔（绘制文字） applyPaintConfig(danmaku, paint, false); //绘制文字 sStuffer.drawText(danmaku, null, canvas, left, top - paint.ascent(), paint, fromWorkerThread); &#125; // draw underline if (danmaku.underlineColor != 0) &#123;//绘制下划线（if） Paint linePaint = getUnderlinePaint(danmaku); float bottom = _top + danmaku.paintHeight - UNDERLINE_HEIGHT; canvas.drawLine(_left, bottom, _left + danmaku.paintWidth, bottom, linePaint); &#125; //draw border if (danmaku.borderColor != 0) &#123;//绘制外框 Paint borderPaint = getBorderPaint(danmaku); canvas.drawRect(_left, _top, _left + danmaku.paintWidth, _top + danmaku.paintHeight, borderPaint); &#125; &#125; //设置画笔 private void applyPaintConfig(BaseDanmaku danmaku, Paint paint, boolean stroke) &#123; ...... if (stroke) &#123; paint.setStyle(HAS_PROJECTION ? Style.FILL : Style.STROKE); paint.setColor(danmaku.textShadowColor &amp; 0x00FFFFFF); int alpha = HAS_PROJECTION ? sProjectionAlpha : AlphaValue.MAX; paint.setAlpha(alpha); &#125; else &#123; paint.setStyle(Style.FILL); paint.setColor(danmaku.textColor &amp; 0x00FFFFFF); paint.setAlpha(AlphaValue.MAX); &#125; &#125; &#160; &#160; &#160; &#160;上述就是绘制弹幕内容过程，主要就是sStuffer的drawStroke，drawText方法。如果你在DanmakuContext中没有设置CacheStuffer，则上述drawDanmaku方法中的sStuffer为默认的SimpleTextCacheStuffer。&#160; &#160; &#160; &#160;drawStroke方法及其扩展都一样：12345678//SimpleTextCacheStuffer的drawStroke方法 public void drawStroke(BaseDanmaku danmaku, String lineText, Canvas canvas, float left, float top, Paint paint) &#123; if (lineText != null) &#123; canvas.drawText(lineText, left, top, paint); &#125; else &#123; canvas.drawText(danmaku.text.toString(), left, top, paint); &#125; &#125; &#160; &#160; &#160; &#160;我们设了SpannedCacheStuffer, drawText方法有些区别：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//SimpleTextCacheStuffer的drawText方法 public void drawText(BaseDanmaku danmaku, String lineText, Canvas canvas, float left, float top, TextPaint paint, boolean fromWorkerThread) &#123; if (lineText != null) &#123; canvas.drawText(lineText, left, top, paint); &#125; else &#123; canvas.drawText(danmaku.text.toString(), left, top, paint); &#125; &#125; //SpannedCacheStuffer的drawText方法 public void drawText(BaseDanmaku danmaku, String lineText, Canvas canvas, float left, float top, TextPaint paint, boolean fromWorkerThread) &#123; if (danmaku.obj == null) &#123;//普通弹幕 super.drawText(danmaku, lineText, canvas, left, top, paint, fromWorkerThread); return; &#125; //如果是图文混排弹幕 SoftReference&lt;StaticLayout&gt; reference = (SoftReference&lt;StaticLayout&gt;) danmaku.obj; StaticLayout staticLayout = reference.get(); //按位与，判断标志位是否有效。这里判断是否请求重新测量 boolean requestRemeasure = 0 != (danmaku.requestFlags &amp; BaseDanmaku.FLAG_REQUEST_REMEASURE); //判断是否请求重绘 boolean requestInvalidate = 0 != (danmaku.requestFlags &amp; BaseDanmaku.FLAG_REQUEST_INVALIDATE); if (requestInvalidate || staticLayout == null) &#123;//如果请求重绘或者staticLayout 软引用被回收了 if (requestInvalidate) &#123; //与非操作，清除标志位。清除请求重绘标志位 danmaku.requestFlags &amp;= ~BaseDanmaku.FLAG_REQUEST_INVALIDATE; &#125; else if (mProxy != null) &#123;//这个在设置DanmakuContext时设置，上面讲过，可以自己扩展 mProxy.prepareDrawing(danmaku, fromWorkerThread); &#125; CharSequence text = danmaku.text; if (text != null) &#123; if (requestRemeasure) &#123;//重新测量 staticLayout = new StaticLayout(text, paint, (int) Math.ceil(StaticLayout.getDesiredWidth(danmaku.text, paint)), Layout.Alignment.ALIGN_NORMAL, 1.0f, 0.0f, true); danmaku.paintWidth = staticLayout.getWidth(); danmaku.paintHeight = staticLayout.getHeight(); danmaku.requestFlags &amp;= ~BaseDanmaku.FLAG_REQUEST_REMEASURE;//清除标志位 &#125; else &#123;//不用重新测量 staticLayout = new StaticLayout(text, paint, (int) danmaku.paintWidth, Layout.Alignment.ALIGN_NORMAL, 1.0f, 0.0f, true); &#125; danmaku.obj = new SoftReference&lt;&gt;(staticLayout); &#125; else &#123; return; &#125; &#125; //staticLayout可以继续用 boolean needRestore = false; if (left != 0 &amp;&amp; top != 0) &#123; canvas.save(); canvas.translate(left, top + paint.ascent()); needRestore = true; &#125; //绘制弹幕内容 staticLayout.draw(canvas); if (needRestore) &#123; canvas.restore(); &#125; &#125; &#160; &#160; &#160; &#160;绘制弹幕内容就完了，主要是绘制描边，绘制文字，绘制下划线，边框等等。 &#160; &#160; &#160; &#160;No.3 切割超过一屏的弹幕：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849//DrawingCacheHolder的splitWith方法 public void splitWith(int dispWidth, int dispHeight, int maximumCacheWidth, int maximumCacheHeight) &#123; recycleBitmapArray();//回收已存的bitmapArray数组 if (width &lt;= 0 || height &lt;= 0 || bitmap == null) &#123; return; &#125; //如果弹幕的宽高都没有超过屏幕宽高，则不切割bitmap if (width &lt;= maximumCacheWidth &amp;&amp; height &lt;= maximumCacheHeight) &#123; return; &#125; //切割超过一屏的弹幕 maximumCacheWidth = Math.min(maximumCacheWidth, dispWidth); maximumCacheHeight = Math.min(maximumCacheHeight, dispHeight); //计算弹幕宽高是屏幕宽高的倍数，然后决定切割成多少块 int xCount = width / maximumCacheWidth + (width % maximumCacheWidth == 0 ? 0 : 1); int yCount = height / maximumCacheHeight + (height % maximumCacheHeight == 0 ? 0 : 1); //然后求切割后弹幕每一块宽和高的平均值 int averageWidth = width / xCount; int averageHeight = height / yCount; //建立二位bitmap数组，用于存放切割碎片 final Bitmap[][] bmpArray = new Bitmap[yCount][xCount]; if (canvas == null)&#123; canvas = new Canvas(); if (mDensity &gt; 0) &#123; canvas.setDensity(mDensity); &#125; &#125; Rect rectSrc = new Rect(); Rect rectDst = new Rect(); //切割bitmap到bitmapArray中 for (int yIndex = 0; yIndex &lt; yCount; yIndex++) &#123; for (int xIndex = 0; xIndex &lt; xCount; xIndex++) &#123; //创建每一块小块bitmap Bitmap bmp = bmpArray[yIndex][xIndex] = NativeBitmapFactory.createBitmap( averageWidth, averageHeight, Bitmap.Config.ARGB_8888); if (mDensity &gt; 0) &#123; bmp.setDensity(mDensity); &#125; //将弹幕的大bitmap绘制进每个小块bitmap中 canvas.setBitmap(bmp); int left = xIndex * averageWidth, top = yIndex * averageHeight; rectSrc.set(left, top, left + averageWidth, top + averageHeight); rectDst.set(0, 0, bmp.getWidth(), bmp.getHeight()); canvas.drawBitmap(bitmap, rectSrc, rectDst, null); &#125; &#125; canvas.setBitmap(bitmap); bitmapArray = bmpArray; &#125; &#160; &#160; &#160; &#160;切割超过一屏的弹幕，就像玩切田字格游戏一样，完成后保存了一个bitmapArray数组。 &#160; &#160; &#160; &#160;到这里我们buildCache(item, false)的策略二中的重新设置缓存DanmakuUtils.buildDanmakuDrawingCache(item, mDisp, cache)就走完了。然后将这个目标弹幕的引用放入缓存Danmakus中(mCaches)，同时更新已使用大小mRealSize。同时注意mCaches内部成员items是TreeSet类型，不能添加相同的对象。 &#160; &#160; &#160; &#160;策略二设计的挺复杂的，我们可以看到这个策略应该是弹幕已经播放时不断执行的，对过时弹幕缓存的重复利用。不过我们刚开始，这一策略还未起作用，所以跳过，进入下一阶段： &#160; &#160; &#160; &#160;4）如果上述两次查找缓存都没找到，则从FinitePool中取出一个，没有就new一个，然后同上配置DrawingCache： &#160; &#160; &#160; &#160;继续回到buildCache方法这个位置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 private byte buildCache(BaseDanmaku item, boolean forceInsert) &#123;//item, false ...测量过了... ...第一策略已经pass... ...第二策略已经pass... //如果上述两次查找缓存都没找到，则进入下面逻辑 // guess cache size if (!forceInsert) &#123;//如果forceInsert为false，则表示不检测内存超出 //计算此弹幕bitmap的大小，width * height * 4 //(因为用native创建的Bitmap的Config为ARGB_8888，所以一个像素占4个字节) int cacheSize = DanmakuUtils.getCacheSize((int) item.paintWidth, (int) item.paintHeight); //如果当前已经使用大小 + 此弹幕缓存大小 &gt; 设置的最大内存（2/3 应用内存） if (mRealSize + cacheSize &gt; mMaxSize) &#123;//没有超 return RESULT_FAILED; &#125; &#125; //从FinitePool中的300个DrawingCache对象中取出来一个 cache = mCachePool.acquire(); //如果从上面的FinitePool取完了，则会直接new一个DrawingCache，配置DrawingCache cache = DanmakuUtils.buildDanmakuDrawingCache(item, mDisp, cache); item.cache = cache; //将item存入mCaches缓存，同时更新已使用大小mRealSize boolean pushed = mCacheManager.push(item, sizeOf(item), forceInsert); if (!pushed) &#123;//如果item存放失败(使用内存超出规定大小) releaseDanmakuCache(item, cache);//释放DrawingCache &#125; return pushed ? RESULT_SUCCESS : RESULT_FAILED; ......&#125; //FinitePool的acquire方法，从缓存链表头取出一个对象 public T acquire() &#123; T element; //mRoot 就是缓存链表表头指向的对象 if (mRoot != null) &#123; element = mRoot; mRoot = element.getNextPoolable(); mPoolCount--; &#125; else &#123; element = mManager.newInstance(); &#125; if (element != null) &#123; element.setNextPoolable(null); element.setPooled(false); mManager.onAcquired(element); &#125; return element; &#125; &#160; &#160; &#160; &#160;上述策略三是直接新建一个缓存DrawingCache，然后根据目标弹幕样式等配置它然后将它付给目标弹幕，再将目标弹幕放入缓存mCaches中。&#160; &#160; &#160; &#160;刚开始时会执行策略三，因为刚开始时还没有缓存供我们使用，所以只能新建。 &#160; &#160; &#160; &#160;到此buildCache方法就走完了。我们可以看到buildCache主要截取了从当前时间开始的3倍弹幕时间内所有弹幕，然后为每一条弹幕建立缓存（创建DrawingCache对象，然后测量弹幕大小，再绘制弹幕内容，最后将信息保存到DrawingCache中，然后将它赋给目标弹幕的cache属性），并将这些弹幕保存到缓存mCaches中。 &#160; &#160; &#160; &#160;再次回顾一下上面的逻辑： 子线程从发送PREPARE消息开始，然后接着发送了DISPATCH_ACTIONS消息； DISPATCH_ACTIONS消息处理逻辑内部又会发送DISPATCH_ACTIONS消息，时间间隔为半条弹幕时间就这样不断循环发送； DISPATCH_ACTIONS消息处理会调用dispatchAction方法，dispatchAction方法会发送BUILD_CACHES消息； BUILD_CACHES消息处理会调用prepareCaches方法，prepareCaches方法内部会调用buildCache方法为从当前时间开始的3倍弹幕时间内所有的弹幕做缓存。 &#160; &#160; &#160; &#160;buildCache走完后，赶紧回到它之前调用方法的地方，不要把自己搞晕了= 。=&#160; &#160; &#160; &#160;回到CacheManagingDrawTask的prepareCaches方法中，最后更新一下缓存定时器的时间，到缓存的最后一条弹幕的出现时间：12345678910private long prepareCaches(boolean repositioned) &#123; ...截取三倍弹幕时间内所有弹幕，并为他们一一建立缓存... if (item != null) &#123;//截取的最后一条弹幕,更新缓存定时器时间到它的出现时间 mCacheTimer.update(item.time); &#125; else &#123; mCacheTimer.update(end); &#125;&#125; &#160; &#160; &#160; &#160;prepareCaches方法走完后，回到处理原先处理BUILD_CACHES消息的逻辑中，继续执行剩余部分：1234567891011121314151617 //CacheHandler的handleMessage方法public void handleMessage(Message msg) &#123; ...... case BUILD_CACHES: removeMessages(BUILD_CACHES); boolean repositioned = ((mTaskListener != null &amp;&amp; mReadyState == false) || mSeekedFlag);// 为true prepareCaches(repositioned);//首次建立缓存已经完毕 if (repositioned) mSeekedFlag = false; if (mTaskListener != null &amp;&amp; mReadyState == false) &#123; mTaskListener.ready();//然后回到mTaskListener监听ready方法 mReadyState = true;//将mReadyState标志位置为true，下次BUILD_CACHES不会进入这段逻辑了 &#125; break; ...... &#125; &#160; &#160; &#160; &#160;执行mTaskListener.ready()方法，得回到上层逻辑DrawHandler的prepare(runnable)方法中：12345678910111213141516171819202122232425//DrawHandler的prepare方法 private void prepare(final Runnable runnable) &#123; if (drawTask == null) &#123; drawTask = createDrawTask(mDanmakuView.isDanmakuDrawingCacheEnabled(), timer, mDanmakuView.getContext(), mDanmakuView.getWidth(), mDanmakuView.getHeight(), mDanmakuView.isHardwareAccelerated(), new IDrawTask.TaskListener() &#123; @Override public void ready() &#123; initRenderingConfigs();//初始化一些渲染参数 runnable.run();//执行runnable的run方法，继续追踪 &#125; ...... &#125;); &#125; else &#123; runnable.run(); &#125; &#125; //DrawHandler的initRenderingConfigs方法 private void initRenderingConfigs() &#123; long averageFrameConsumingTime = 16;//平均每帧渲染间隔 mCordonTime = Math.max(33, (long) (averageFrameConsumingTime * 2.5f));//40，警戒值1 mCordonTime2 = (long) (mCordonTime * 2.5f);//100，警戒值2 mFrameUpdateRate = Math.max(16, averageFrameConsumingTime / 15 * 15);//16，每帧渲染间隔 mThresholdTime = mFrameUpdateRate + 3;//19，渲染间隔阀值 &#125; &#160; &#160; &#160; &#160;初始化一些渲染参数，主要就是计算一下警戒时间和渲染频率。然后继续追踪runnable.run()方法，这个得回到DrawHandler的handleMessage方法中处理DrawHandler.PREPARE逻辑处：12345678910111213141516171819//DrawHandler的handleMessage方法 public void handleMessage(Message msg) &#123; int what = msg.what; switch (what) &#123; case PREPARE: ...... prepare(new Runnable() &#123; @Override public void run() &#123;//会回调到这里 pausedPosition = 0; mReady = true;//将mReady 标志位置为true if (mCallback != null) &#123; mCallback.prepared();//回调callback监听 &#125; &#125; &#125;); ...... break; &#125; &#160; &#160; &#160; &#160;继续追踪mCallback.prepared()，会回到MainActivity当中我们设置DanmakuView的地方：1234567891011121314151617//MainActivity中设置mDanmakuView mDanmakuView.setCallback(new master.flame.danmaku.controller.DrawHandler.Callback() &#123; ...... @Override public void prepared() &#123; mDanmakuView.start(); &#125; &#125;); //继续产看DanmaKuView的start方法 public void start() &#123; start(0); &#125; public void start(long postion) &#123; ...... handler.obtainMessage(DrawHandler.START, postion).sendToTarget();//DrawHandler发送START消息 &#125; &#160; &#160; &#160; &#160;然后就是DrawHandler发送START消息：1234567891011121314151617181920212223242526272829303132333435//DrawHandler的handleMessage方法public void handleMessage(Message msg) &#123; ...... case START: Long startTime = (Long) msg.obj;//0 if (startTime != null) &#123; pausedPosition = startTime;//0 &#125; else &#123; pausedPosition = 0; &#125; case SEEK_POS: ...... case RESUME: quitFlag = false; if (mReady) &#123;//true ...... mTimeBase = SystemClock.uptimeMillis() - pausedPosition;//将时间基线设为当前时间 timer.update(pausedPosition);//更新主定时器时间到初始位置，为0 removeMessages(RESUME); sendEmptyMessage(UPDATE);//发送UPDATE消息 drawTask.start();//CacheManagingDrawTask的start方法 ...... &#125; else &#123; ...... &#125; break; case UPDATE: if (mUpdateInNewThread) &#123;//在DrawHandler构造方法里赋值的变量，只有当可用CPU个数大于3时才为true updateInNewThread();//四核，八核的请进 &#125; else &#123; updateInCurrentThread();//单核，双核的请进 &#125; break; ...... &#125; &#160; &#160; &#160; &#160;上述逻辑最后会进入RESUME消息处理中，先调用CacheManagingDrawTask的start方法，然后处理UPDATE消息。我们先看看CacheManagingDrawTask的start方法：12345678910111213141516171819//CacheManagingDrawTask的start方法 public void start() &#123; ...... mCacheManager.resume();//CacheManager的resume方法 &#125; //继续跟CacheManager的resume方法 public void resume() &#123; ...... mHandler.resume();//CacheManagingDrawTask的resume方法 ...... &#125; //继续跟CacheManagingDrawTask的resume方法 public void resume() &#123; mCancelFlag = false; mPause = false; removeMessages(DISPATCH_ACTIONS); sendEmptyMessage(DISPATCH_ACTIONS);//发送DISPATCH_ACTIONS消息，我们上面分析过，就是建立缓存 sendEmptyMessageDelayed(CLEAR_TIMEOUT_CACHES, mContext.mDanmakuFactory.MAX_DANMAKU_DURATION);//延时发送CLEAR_TIMEOUT_CACHES消息 &#125; &#160; &#160; &#160; &#160;我们可以看到CacheManagingDrawTask的start方法最终做了两件事，一件是发送DISPATCH_ACTIONS再次建立缓存，这个流程我们上面分析过；第二件是延时发送CLEAR_TIMEOUT_CACHES消息。 &#160; &#160; &#160; &#160;所以我们看看CLEAR_TIMEOUT_CACHES消息处理逻辑：1234567891011121314151617181920212223242526//CacheHandler的handleMessage方法public void handleMessage(Message msg) &#123; ....... case CLEAR_TIMEOUT_CACHES: clearTimeOutCaches();//继续跟这个 break; ...... &#125;//调用 clearTimeOutCaches方法 private void clearTimeOutCaches() &#123; clearTimeOutCaches(mTimer.currMillisecond);//调用重载方法，参数为主定时器当前时间 &#125; //调用重载方法，参数为主定时器当前时间 private void clearTimeOutCaches(long time) &#123; IDanmakuIterator it = mCaches.iterator();//从之前buildCache中建立的缓存中一一遍历 while (it.hasNext() &amp;&amp; !mEndFlag) &#123;//mEndFlag = false BaseDanmaku val = it.next(); if (val.isTimeOut()) &#123;//如果缓存的弹幕已经超时 ...... entryRemoved(false, val, null);//销毁缓存 it.remove();//从缓存mCaches中移除此引用 &#125; else &#123; break; &#125; &#125; &#125; &#160; &#160; &#160; &#160;顺着逻辑看看entryRemoved(false, val, null)方法：1234567891011121314151617181920212223242526272829303132333435363738protected void entryRemoved(boolean evicted, BaseDanmaku oldValue, BaseDanmaku newValue) &#123;//第1个和第3个参数没用到 IDrawingCache&lt;?&gt; cache = oldValue.getDrawingCache(); if (cache != null) &#123; long releasedSize = clearCache(oldValue);//调用了clearCache方法 if (oldValue.isTimeOut()) &#123; //这个方法最终会调用我们最初设置DanmakuContext.setCacheStuffer(new SpannedCacheStuffer(), mCacheStufferAdapter) //中第二个参数类型为BaseCacheStuffer.Proxy的releaseResource方法, //方法注释是这么写的 TODO 重要:清理含有ImageSpan的text中的一些占用内存的资源 例如drawable mContext.getDisplayer().getCacheStuffer().releaseResource(oldValue); &#125; if (releasedSize &lt;= 0) return; mRealSize -= releasedSize;//真正缓存大小减去需要释放的缓存大小 mCachePool.release((DrawingCache) cache);//将Drawingcache放回到FinitePool中，已供下次取出 &#125;&#125;//往下看，看看clearCache方法private long clearCache(BaseDanmaku oldValue) &#123; IDrawingCache&lt;?&gt; cache = oldValue.cache; if (cache == null) &#123; return 0; &#125; if (cache.hasReferences()) &#123;//如果DrawingCache缓存还被重复引用 cache.decreaseReference();//则将引用计数-1 oldValue.cache = null; return 0;//不销毁缓存(bitmap,canvas等)，只有等到引用计数为0时才会销毁 &#125; long size = sizeOf(oldValue);//计算缓存的bitmap大小 cache.destroy();//同时销毁bitmap等 oldValue.cache = null; return size;&#125;//缓存的bitmap的大小protected int sizeOf(BaseDanmaku value) &#123; if (value.cache != null &amp;&amp; !value.cache.hasReferences()) &#123; return value.cache.size();//返回的是Drawing中bitmap对象的大小，上面讲过的 &#125; return 0;&#125; &#160; &#160; &#160; &#160;CLEAR_TIMEOUT_CACHES消息处理就分析完了，就是移除缓存弹幕mCache中过时的弹幕，并且销毁他们持有的DrawingCache，同时销毁内部的bitmap、canvas等。 缓存机制&#160; &#160; &#160; &#160;现在重点来了！还记得我们之前挖的一个大坑么？就是妹子图那个地方。那是CacheHandler给工作线程发送DISPATCH_ACTIONS消息时调用的dispatchAction方法。因为CacheHandler每个半条弹幕时间就会发DISPATCH_ACTIONS消息，所以我们得仔细分析一下dispatchAction方法的各种情况：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 private long dispatchAction() &#123; //如果上一次buildCache完成后得到的缓存弹幕末尾项的时间（上面分析过，这个值存在mCacheTimer.currMillisecond中） //和主定时器当前时间之间的时间差值已经大于一条弹幕时间, //则会清除所有不在屏幕内的缓存，然后重新buildCache建立缓存 if (mCacheTimer.currMillisecond &lt;= mTimer.currMillisecond - mContext.mDanmakuFactory.MAX_DANMAKU_DURATION) &#123; evictAllNotInScreen();//则会清除所有不在屏幕内的缓存 mCacheTimer.update(mTimer.currMillisecond); sendEmptyMessage(BUILD_CACHES);//重新建立缓存 return 0; &#125; float level = getPoolPercent();//获得缓存实际大小占设置最大内存的百分比 BaseDanmaku firstCache = mCaches.first(); //TODO 如果firstcache大于当前时间超过半屏并且水位在0.5f以下,就要往里蓄水 long gapTime = firstCache != null ? firstCache.time - mTimer.currMillisecond : 0; long doubleScreenDuration = mContext.mDanmakuFactory.MAX_DANMAKU_DURATION * 2; if (level &lt; 0.6f &amp;&amp; gapTime &gt; mContext.mDanmakuFactory.MAX_DANMAKU_DURATION) &#123; mCacheTimer.update(mTimer.currMillisecond); removeMessages(BUILD_CACHES); sendEmptyMessage(BUILD_CACHES);//重新建立缓存 return 0; &#125; else if (level &gt; 0.4f &amp;&amp; gapTime &lt; -doubleScreenDuration) &#123;//如果水位在0.5以上，并且上一次蓄水距离现在已经超过两条弹幕时间了，就要开闸放水 // clear timeout caches removeMessages(CLEAR_TIMEOUT_CACHES); sendEmptyMessage(CLEAR_TIMEOUT_CACHES);//CLEAR_TIMEOUT_CACHES消息刚分析过了，清除过时缓存 return 0; &#125; if (level &gt;= 0.9f) &#123;//水位快满了，等待下次放水 return 0; &#125; // check cache time long deltaTime = mCacheTimer.currMillisecond - mTimer.currMillisecond; //缓存的第一条弹幕已经过时了，并且缓存弹幕末尾时间和现在时间差值已经超过一条弹幕时间了 if (firstCache != null &amp;&amp; firstCache.isTimeOut() &amp;&amp; deltaTime &lt; -mContext.mDanmakuFactory.MAX_DANMAKU_DURATION) &#123; mCacheTimer.update(mTimer.currMillisecond); sendEmptyMessage(CLEAR_OUTSIDE_CACHES);//先清除过时缓存 sendEmptyMessage(BUILD_CACHES);//再重组缓存 return 0; &#125; else if (deltaTime &gt; doubleScreenDuration) &#123;//如果缓存的最后一条弹幕时间距离现在还有双倍弹幕时间多，则啥都不做 return 0; &#125;//剩余情况组建缓存 removeMessages(BUILD_CACHES); sendEmptyMessage(BUILD_CACHES); return 0; &#125; //则会清除所有不在屏幕内的缓存 private void evictAllNotInScreen() &#123; evictAllNotInScreen(false); &#125; private void evictAllNotInScreen(boolean removeAllReferences) &#123; if (mCaches != null) &#123; IDanmakuIterator it = mCaches.iterator(); while (it.hasNext()) &#123; BaseDanmaku danmaku = it.next(); ...... if (danmaku.isOutside()) &#123;//如果弹幕已经走完了，超过屏幕 entryRemoved(true, danmaku, null);//回收缓存 it.remove(); &#125; &#125; &#125; mRealSize = 0; &#125; //获得缓存实际大小占设置最大内存的百分比 public float getPoolPercent() &#123; if (mMaxSize == 0) &#123; return 0; &#125; return mRealSize / (float) mMaxSize; &#125; &#160; &#160; &#160; &#160;dispatchAction方法主要分为以下几种规则： 如果上一次buildCache完成后得到的缓存弹幕末尾项的时间（上面分析过，这个值存在mCacheTimer.currMillisecond中）和主定时器当前时间之间的时间差值已经大于一条弹幕时间, 则会清除所有不在屏幕内的缓存，然后重新buildCache建立缓存； 如果缓存弹幕的第一项出现时间大于当前时间超过半屏，并且总缓存大小在规定最大值一半以下, 就要重新建立缓存； 如果总缓存大小在规定最大值一半以上，并且上一次建立缓存距离现在已经超过两条弹幕时间了，就要清除超时缓存； 如果总缓存大小快达到规定最大值，就等待下一次清除超时缓存； 缓存的第一条弹幕已经过时了，并且缓存弹幕末尾时间和现在时间差值已经超过一条弹幕时间了，先清除过时缓存，再重组缓存； 如果缓存的最后一条弹幕时间距离现在还有双倍弹幕时间多，则啥都不做; 剩余情况就是重组缓存。 &#160; &#160; &#160; &#160;因为DISPATCH_ACTIONS消息是每隔半条弹幕时间发送一次，所以会不断执行dispatchAction方法。然后根据上述出现的情况不断BUILD_CACHES和CLEAR_TIMEOUT_CACHES,这样工作线程就形成了一套缓存机制。 绘制弹幕界面&#160; &#160; &#160; &#160;到此CacheManagingDrawTask的start方法就分析完了，继续回到DrawHandler的handleMessage方法，接着处理UPDATE消息：1234567891011//DrawHandler的handleMessage方法public void handleMessage(Message msg) &#123; case UPDATE: if (mUpdateInNewThread) &#123;//在DrawHandler构造方法里赋值的变量，只有当可用CPU个数大于3时才为true updateInNewThread();//四核，八核的请进 &#125; else &#123; updateInCurrentThread();//单核，双核的请进 &#125; break; ...... &#125; &#160; &#160; &#160; &#160;到这里，我们应该能猜到接下要进行应该就是绘制工作了。其实updateInNewThread和updateInCurrentThread做的事情是一样的，只不过其中一个新开了子线程去做这些事情。两者的工作原理都是更新定时器，然后postInvalidate，使DanmakuView重绘，然后再发UPDATE消息，重复上述过程。 &#160; &#160; &#160; &#160;鉴于目前四核手机已经烂大街了，我们也就挑个多核的方法进去看看：12345678910111213141516171819202122232425262728293031323334private void updateInNewThread() &#123; if (mThread != null) &#123; return; &#125; mThread = new UpdateThread("DFM Update") &#123; @Override public void run() &#123; long lastTime = SystemClock.uptimeMillis(); long dTime = 0; while (!isQuited() &amp;&amp; !quitFlag) &#123; long startMS = SystemClock.uptimeMillis(); dTime = SystemClock.uptimeMillis() - lastTime; long diffTime = mFrameUpdateRate - dTime;//mFrameUpdateRate 为16，之前计算过 if (diffTime &gt; 1) &#123;//如果间隔时间太短，则会延时，一定要等够16毫秒，达到绘制时间间隔 SystemClock.sleep(1); continue; &#125;d //上面逻辑是为了延时，稳定帧率 lastTime = startMS; long d = syncTimer(startMS);//同步主定时器时间 ...... d = mDanmakuView.drawDanmakus();//开始postInvalidate，绘制弹幕，同时返回绘制时间 //这种情况出现在绘制时间内，绘制时子线程在wait，等待绘制结束，然后返回差值必定大于警戒值100 if (d &gt; mCordonTime2) &#123; // this situation may be cuased by ui-thread waiting of DanmakuView, so we sync-timer at once timer.add(d);//绘制完成后更新主定时器时间 mDrawTimes.clear(); &#125; ...... &#125; &#125; &#125;; mThread.start();&#125; &#160; &#160; &#160; &#160;updateInNewThread主要做了两件事：延时然后同步主定时器时间，然后通知DanmakuView重绘。 &#160; &#160; &#160; &#160;我们先看同步主定时器时间：123456789101112131415161718192021222324252627282930313233 private final long syncTimer(long startMS) &#123; ...... long d = 0; long time = startMS - mTimeBase;//当前时间到初始时间的时间差 ...... long gapTime = time - timer.currMillisecond;//总时间差减去上一次绘制完成时间，得到绘制间隙时间 long averageTime = Math.max(mFrameUpdateRate, getAverageRenderingTime());//计算绘制间隙平均时间，大于等于16（getAverageRenderingTime方法是计算加入mDrawTimes队列的已经绘制过的时间总和除以帧数，得到平均时间，这个下面会讲到） //若果距离上次间隙时间过长||上次渲染时间大于第一警戒时间（40 ms）||上一步计算的绘制间隙平均时间大于第一警戒时间 if (gapTime &gt; 2000 || mRenderingState.consumingTime &gt; mCordonTime || averageTime &gt; mCordonTime) &#123; d = gapTime; gapTime = 0; &#125; else &#123;//如果是普通情况 d = averageTime + gapTime / mFrameUpdateRate;//将绘制间隙平均时间赋给d，后面的项值不大，可以忽略 d = Math.max(mFrameUpdateRate, d);//大于等于固定绘制间隔16 d = Math.min(mCordonTime, d);//小于第一警戒时间40 ...... &#125; ...... timer.add(d);//更新主定时器时间，加上计算的时间间隔...... return d; &#125; //计算平均绘制间隔时间 private synchronized long getAverageRenderingTime() &#123; int frames = mDrawTimes.size(); if(frames &lt;= 0) return 0; long dtime = mDrawTimes.getLast() - mDrawTimes.getFirst(); return dtime / frames; &#125; &#160; &#160; &#160; &#160;syncTimer主要是计算了一下绘制间隔时间，然后同步一下主定时器。 &#160; &#160; &#160; &#160;然后我们看看通知DanmakuView重绘部分：12345678910111213141516171819202122232425262728293031323334//DanmakuView的drawDanmakus方法 public long drawDanmakus() &#123; long stime = SystemClock.uptimeMillis(); lockCanvas();//再看看lockCanvas return SystemClock.uptimeMillis() - stime;//返回等待时间差 &#125; //DanmakuView的lockCanvas方法 private void lockCanvas() &#123; ...... postInvalidateCompat();//通知view重绘 synchronized (mDrawMonitor) &#123; while ((!mDrawFinished) &amp;&amp; (handler != null)) &#123;//mDrawFinished标志位为false，所以会进入循环。只有onDraw方法的绘制走完了才会将他置为true，才会跳出循环 try &#123; mDrawMonitor.wait(200);//onDraw没走完就会一直循环等待 &#125; catch (InterruptedException e) &#123; if (mDanmakuVisible == false || handler == null || handler.isStop()) &#123; break; &#125; else &#123; Thread.currentThread().interrupt(); &#125; &#125; &#125; mDrawFinished = false;//绘制结束后，将标志位置为false，一边下次进入方法后再次进入上述等待逻辑 &#125; &#125; private void postInvalidateCompat() &#123; mRequestRender = true;//将mRequestRender 标志位置为true，一遍onDraw方法逻辑执行 //通知view重绘 if(Build.VERSION.SDK_INT &gt;= 16) &#123; this.postInvalidateOnAnimation(); &#125; else &#123; this.postInvalidate(); &#125; &#125; &#160; &#160; &#160; &#160;这样就能保证保证每隔一定时间（这个时间通过syncTimer计算），更新主定时器（就是从0开始，往后每次加上（间隔时间 + 绘制时间）），然后执行postInvalidate通知DanmakuView重绘。 &#160; &#160; &#160; &#160;postInvalidate后，View重绘，会重走onDraw方法，所以我们进入DanmakuView的onDraw方法看看：1234567891011121314151617181920212223//DanmakuView的onDraw方法 protected void onDraw(Canvas canvas) &#123; if ((!mDanmakuVisible) &amp;&amp; (!mRequestRender)) &#123;//如果没有请求重绘则mRequestRender为false，不会绘制弹幕 super.onDraw(canvas); return; &#125; ...... if (handler != null) &#123; RenderingState rs = handler.draw(canvas);//DrawHandler的draw方法 ...... &#125; ...... //绘制结束后将mRequestRender 标志位重新设为false， //以便下一次发绘制消息时进入等待逻辑等候绘制结束，这个上面DanmakuView的drawDanmakus方法提到过 mRequestRender = false; unlockCanvasAndPost();//通知UpdateThread绘制完成 &#125; private void unlockCanvasAndPost() &#123; synchronized (mDrawMonitor) &#123; mDrawFinished = true;//将mDrawFinished 置为true，以便DanmakuView的lockCanvas方法跳出循环，这个上面也提到过 mDrawMonitor.notifyAll(); &#125; &#125; &#160; &#160; &#160; &#160;DanmakuView的onDraw回调逻辑会执行DrawHandler的draw方法，我们继续跟进去：1234567891011121314151617 public RenderingState draw(Canvas canvas) &#123;...... mDisp.setExtraData(canvas);//将canvas一些信息设置给AndroidDisplayer mRenderingState.set(drawTask.draw(mDisp));//绘制部分是drawTask.draw(mDisp) recordRenderingTime();//记录绘制结束时间 return mRenderingState; &#125; //还记得上面的DrawHandler的syncTimer方法吗？里面调用了getAverageRenderingTime计算绘制平均间隔时间， //其中用到的mDrawTimes变量就是在这里添加元素的 private synchronized void recordRenderingTime() &#123; long lastTime = SystemClock.uptimeMillis(); mDrawTimes.addLast(lastTime);//将绘制结束时间加入到类型为LinkedList的mDrawTimes集合中 int frames = mDrawTimes.size(); if (frames &gt; MAX_RECORD_SIZE) &#123;//最大容量为500个绘制时间，超出了则移除第一个 mDrawTimes.removeFirst(); &#125; &#125; &#160; &#160; &#160; &#160;上述逻辑中，我的注释部分先分析了记录绘制结束时间部分，填了上边syncTimer时的坑。&#160; &#160; &#160; &#160;然后应该进入主要绘制部分了drawTask.draw(mDisp)，也就是CacheManagingDrawTask的draw方法：123456789101112131415161718192021222324252627282930313233343536//CacheManagingDrawTask的draw方法 public RenderingState draw(AbsDisplayer displayer) &#123; RenderingState result = super.draw(displayer);//会调用父类的draw方法 ...... return result; &#125; //DrawTask的draw方法 public synchronized RenderingState draw(AbsDisplayer displayer) &#123; return drawDanmakus(displayer,mTimer);//又调用了drawDanmakus方法 &#125; //DrawTask的drawDanmakus方法 protected RenderingState drawDanmakus(AbsDisplayer disp, DanmakuTimer timer) &#123; ...... if (danmakuList != null) &#123; Canvas canvas = (Canvas) disp.getExtraData();//取出DanmakuView的canvas //当前时间 - 1屏弹幕时间 -100 （多减100是为了下次重新截取弹幕组时让绘制边界做到无缝衔接） long beginMills = timer.currMillisecond - mContext.mDanmakuFactory.MAX_DANMAKU_DURATION - 100; //当前时间 + 1屏弹幕时间 long endMills = timer.currMillisecond + mContext.mDanmakuFactory.MAX_DANMAKU_DURATION; //每过了一屏的弹幕时间，就会进入如下if逻辑，截取以当前时间为基准的前后两屏弹幕； //如果距离上次截取时间不到一屏弹幕时间，则不会进入if的逻辑 if(mLastBeginMills &gt; beginMills || timer.currMillisecond &gt; mLastEndMills) &#123; IDanmakus subDanmakus = danmakuList.sub(beginMills, endMills); if(subDanmakus != null) &#123; danmakus = subDanmakus; &#125; mLastBeginMills = beginMills; mLastEndMills = endMills; &#125; else &#123;//距离上次截取时间不到一屏时间 ...... &#125; if (danmakus != null &amp;&amp; !danmakus.isEmpty()) &#123;//开始绘制弹幕 RenderingState renderingState = mRenderingState = mRenderer.draw(mDisp, danmakus, mStartRenderTime); ...... &#125; &#125; &#160; &#160; &#160; &#160;我们可以看到第一次进入会截取以当前时间为基准的前后两屏弹幕。以后每过一屏弹幕时间，会重新截取当时时间为基准的前后两屏弹幕，如果不到一屏时间则不截取，还是以前的弹幕数据。 &#160; &#160; &#160; &#160;截取完弹幕数据后，就是绘制了，继续执行下面逻辑(mRenderer.draw(mDisp, danmakus, mStartRenderTime))，开始绘制工作：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//DanmakuRenderer的draw方法 public RenderingState draw(IDisplayer disp, IDanmakus danmakus, long startRenderTime) &#123; ...... IDanmakuIterator itr = danmakus.iterator(); ...... BaseDanmaku drawItem = null; while (itr.hasNext()) &#123; drawItem = itr.next(); ...... //如果弹幕还没有到出现时间，则检查它有没有缓存，如果没有则为它建立缓存 if (drawItem.isLate()) &#123; IDrawingCache&lt;?&gt; cache = drawItem.getDrawingCache(); if (mCacheManager != null &amp;&amp; (cache == null || cache.get() == null)) &#123; mCacheManager.addDanmaku(drawItem); &#125; break; &#125; ...... // measure 测量，我们之前prepareCache已经为他们在buildCache是测量过了 if (!drawItem.isMeasured()) &#123; drawItem.measure(disp, false); &#125; // layout 布局，计算弹幕在屏幕上应该显示的位置 mDanmakusRetainer.fix(drawItem, disp, mVerifier); // draw //绘制弹幕 if (!drawItem.isOutside() &amp;&amp; drawItem.isShown()) &#123; if (drawItem.lines == null &amp;&amp; drawItem.getBottom() &gt; disp.getHeight()) &#123; continue; // skip bottom outside danmaku ，忽略超过视图底部的弹幕 &#125; //开始绘制 int renderingType = drawItem.draw(disp); if(renderingType == IRenderer.CACHE_RENDERING) &#123;//如果是使用缓存bitmap绘制的 ...... &#125; else if(renderingType == IRenderer.TEXT_RENDERING) &#123;//如果使用缓存绘制失败，则会使用原声方法Canvas去draw ...... if (mCacheManager != null) &#123; mCacheManager.addDanmaku(drawItem);//再次为词条弹幕构建缓存，以便下次使用缓存bitmap绘制 &#125; &#125; ...... &#125; &#160; &#160; &#160; &#160;从截取的弹幕中遍历每一个，然后一一绘制。绘制步骤有如下几步： 如果弹幕还没有到出现时间，则检查它有没有缓存，如果没有则为它建立缓存； measure 测量，我们之前prepareCache已经为他们在buildCache时测量过了; layout 布局，计算弹幕在屏幕上应该显示的位置； draw 绘制弹幕。 &#160; &#160; &#160; &#160;我们一步一步分析：&#160; &#160; &#160; &#160;1）弹幕未到出现时间，检查是否建立缓存：12345678910111213141516171819202122232425262728293031//调用CacheManagingDrawTask的addDanmaku方法 public void addDanmaku(BaseDanmaku danmaku) &#123; if (mHandler != null) &#123; ...... //CacheHandler mHandler.obtainMessage(CacheHandler.ADD_DANMAKKU, danmaku).sendToTarget(); ...... &#125; &#125; //CacheHandler public void handleMessage(Message msg) &#123; case ADD_DANMAKKU: BaseDanmaku item = (BaseDanmaku) msg.obj; addDanmakuAndBuildCache(item);//调用了addDanmakuAndBuildCache方法 break; &#125; //调用了addDanmakuAndBuildCache方法 private final void addDanmakuAndBuildCache(BaseDanmaku danmaku) &#123; //过时了 || 并且弹幕时间不在3屏弹幕时间内(因为mCaches只缓存了3屏时间内的所有弹幕，上面说过的),并且它不是直播弹幕。则不建立缓存 if (danmaku.isTimeOut() || (danmaku.time &gt; mCacheTimer.currMillisecond + mContext.mDanmakuFactory.MAX_DANMAKU_DURATION &amp;&amp; !danmaku.isLive)) &#123; return; &#125; //优先级为0或者在过滤规则内，不建立缓存 if (danmaku.priority == 0 &amp;&amp; danmaku.isFiltered()) &#123; return; &#125; IDrawingCache&lt;?&gt; cache = danmaku.getDrawingCache(); if (cache == null || cache.get() == null) &#123;//如果弹幕没有缓存 buildCache(danmaku, true);//建立缓存（buildCache方法我们上面分析过，就是用来建立缓存的） &#125; &#125; &#160; &#160; &#160; &#160;2）测量，这个我们上面再buildCache时分析过了，不再赘述； &#160; &#160; &#160; &#160;3）布局：123456789101112131415161718192021//调用DanmakusRetainer的fix方法 public void fix(BaseDanmaku danmaku, IDisplayer disp, Verifier verifier) &#123; int type = danmaku.getType(); switch (type) &#123; case BaseDanmaku.TYPE_SCROLL_RL: rldrInstance.fix(danmaku, disp, verifier); break; case BaseDanmaku.TYPE_SCROLL_LR: lrdrInstance.fix(danmaku, disp, verifier); break; case BaseDanmaku.TYPE_FIX_TOP: ftdrInstance.fix(danmaku, disp, verifier); break; case BaseDanmaku.TYPE_FIX_BOTTOM: fbdrInstance.fix(danmaku, disp, verifier); break; case BaseDanmaku.TYPE_SPECIAL: danmaku.layout(disp, 0, 0); break; &#125; &#125; &#160; &#160; &#160; &#160;类型太多了，我们只分析TYPE_SCROLL_RL类型弹幕其他的就不分析，有兴趣的可以自己分析一下其他的。接着会调用AlignTopRetainer的fix方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133//保存需要显示的弹幕容器类（保存的一行只有一条弹幕，下面会说明的），内部持有一个以弹幕的y坐标排序的TreeSet集合，这个需要注意protected Danmakus mVisibleDanmakus = new Danmakus(Danmakus.ST_BY_YPOS); //AlignTopRetainer的fix方法 public void fix(BaseDanmaku drawItem, IDisplayer disp, Verifier verifier) &#123; if (drawItem.isOutside())//如果弹幕已经滚动到视图边界外，则不会为它布局 return; float topPos = 0;//弹幕的y坐标 int lines = 0;//弹幕在第几行显示 boolean shown = drawItem.isShown();//弹幕是否已经显示 boolean willHit = !shown &amp;&amp; !mVisibleDanmakus.isEmpty();//是否会和其他弹幕碰撞 boolean isOutOfVertialEdge = false;//弹幕y值是否超过试图高度 BaseDanmaku removeItem = null;//需要移除的弹幕 //为即将显示的弹幕确认位置 if (!shown) &#123; mCancelFixingFlag = false; // 确定弹幕位置开始 IDanmakuIterator it = mVisibleDanmakus.iterator(); //这四个变量分别为： //insertItem ---- 确认目标弹幕插入到哪一行的同行参考弹幕 //firstItem ---- 已经布局过的弹幕保存容器中的第一项 //lastItem ---- 已经布局过的弹幕保存容器中最后一项 //minRightRow ---- 已经布局过弹幕中x值最小的弹幕，即最左边的弹幕 BaseDanmaku insertItem = null, firstItem = null, lastItem = null, minRightRow = null; boolean overwriteInsert = false;//是否超出插入范围 //遍历已经绘制过的弹幕，因为mVisibleDanmakus 内弹幕以y值排序的，所以按y值从小到大遍历 while (!mCancelFixingFlag &amp;&amp; it.hasNext()) &#123; lines++;//每次循环都会将行号+1 BaseDanmaku item = it.next(); if(item == drawItem)&#123;//如果已经布局过了，说明已经存在自己位置了 insertItem = item;//将布局过的弹幕复制给参考弹幕insertItem lastItem = null;//置空 lastItem shown = true;//shown 置为true，以便末尾不再执行加入mVisibleDanmakus逻辑 willHit = false;//本身已经存在自己位置了，当然没有碰壁一说 break;//怕被下面干扰晕的可以跳出去继续看 &#125; if (firstItem == null)//找到已经布局过的弹幕第一项 firstItem = item; //如果插入目标弹幕后，y值超过了视图高度 if (drawItem.paintHeight + item.getTop() &gt; disp.getHeight()) &#123; overwriteInsert = true;//则将超出插入范围标签置为true break;//怕晕的跳出循环 &#125; //找出最左边的弹幕 if (minRightRow == null) &#123; minRightRow = item; &#125; else &#123; if (minRightRow.getRight() &gt;= item.getRight()) &#123; minRightRow = item; &#125; &#125; // 检查如果插入目标弹幕是否会和正在遍历的已经布局过的参考弹幕碰撞 willHit = DanmakuUtils.willHitInDuration(disp, item, drawItem, drawItem.getDuration(), drawItem.getTimer().currMillisecond); if (!willHit) &#123;//如果没有碰撞 insertItem = item;//则将它复制给参考弹幕insertItem break;//然后跳出循环，下去确定位置 &#125;/*如果有碰撞，则继续弹幕缩小添加范围，寻找可以添加的条件，最后出while循环，下去布局*/ lastItem = item;//暂时找到已经布局过的弹幕最后一项，然后继续循环 &#125; boolean checkEdge = true; if (insertItem != null) &#123;//已经布局过了||目标弹幕不会碰壁可以插入 if (lastItem != null)//目标弹幕插入，y值即为上一次遍历的弹幕的底部 topPos = lastItem.getBottom(); else//已经布局过了，则y的位置不变 topPos = insertItem.getTop(); if (insertItem != drawItem)&#123;//如果目标弹幕可以插入 //这里需要注意，因为一行可以放n多条弹幕，只要前后不碰撞就行； //所以下次我们在同一行插入弹幕判断碰壁时，当然要和这行最后一条弹幕去判断； //因此我们移除前一条弹幕，放入插入的目标弹幕，下次添加弹幕判断时就和目标弹幕判断，然后这么循环下去 removeItem = insertItem; shown = false;//置为false，以便mVisibleDanmakus 添加还未布局的新弹幕 &#125; &#125; else if (overwriteInsert &amp;&amp; minRightRow != null) &#123;//没有空行可以插入 topPos = minRightRow.getTop();//暂时放到最最左边的弹幕那一行（excuse me ？？？） checkEdge = false;//不做范围检查 shown = false; &#125; else if (lastItem != null) &#123;//找不到插入的位置 topPos = lastItem.getBottom();//暂时放到最低位置的弹幕下面，下面检测边界时会酌情河蟹 willHit = false;//置false碰壁标志 &#125; else if (firstItem != null) &#123;////mVisibleDanmakus只有第一条数据，截取弹幕集的第二条弹幕没有和第一条碰壁时 topPos = firstItem.getTop();//此时第二条弹幕和第一条在同一行 removeItem = firstItem; shown = false; &#125; else &#123;//mVisibleDanmakus 没有数据，截取弹幕集的第一条弹幕 topPos = 0;//第一条弹幕当然在最上面 &#125; if (checkEdge) &#123;//如果检查范围 //检查是否超出布局范围 isOutOfVertialEdge = isOutVerticalEdge(overwriteInsert, drawItem, disp, topPos, firstItem, lastItem); &#125; if (isOutOfVertialEdge) &#123;//如果超出布局范围，等待河蟹 topPos = 0; willHit = true; lines = 1; &#125; else if (removeItem != null) &#123;//上面可以插入目标弹幕的逻辑用上了 lines--;//因为参考弹幕和目标弹幕在同一行，但是每进入while循环一次就将行号+1，所有要减回去和参考弹幕保持相同行号 &#125; if (topPos == 0) &#123;//方便加入容器 shown = false; &#125; &#125; //这是河蟹规则，都是在设置DanmakuContext时指定的，比如最大行数限制，重复限制等等。 //这里限于篇幅已经太长了，也实在写不动了，就不再跟下去了。内部逻辑也不难，大家有兴趣可以自己看看。 if (verifier != null &amp;&amp; verifier.skipLayout(drawItem, topPos, lines, willHit)) &#123; return; &#125; if (isOutOfVertialEdge) &#123;//mVisibleDanmakus中所有弹幕绘制出来都超出范围了 clear(); &#125; //这才是真正确认弹幕位置的地方 drawItem.layout(disp, drawItem.getLeft(), topPos); if (!shown) &#123;//如果还未显示，则加入即将显示的容器中。可以看到，最终会把所有截取的弹幕加入到这个容器里 mVisibleDanmakus.removeItem(removeItem);//移除同一行之前的参考弹幕，保持保存的一行只有一条弹幕，上面说明过 mVisibleDanmakus.addItem(drawItem); &#125; &#125; //清除容器，重新放入新的内容 public void clear() &#123; mCancelFixingFlag = true; mVisibleDanmakus.clear(); &#125; &#160; &#160; &#160; &#160;这绝对是我写的注释最多的方法了ToT。。。。。。其实思路挺好理解的，通俗地讲就是这样的过程： 先添往最第一行添加一条弹幕，把它存到一个容器里（这个容器会把新添加进来的弹幕按照y值从小到大排序，而且容器只保存每一行的最后一条弹幕）。 然后添加第二条弹幕，从第一行开始添加，先判断和第一条弹幕会不会碰壁，如果不会碰壁则添加到这一行，然后容器内移除之前第一条的弹幕，保存这一条弹幕；如果会碰壁则添加到下一行，然后容器保存这条弹幕； 然后添加第三条，继续从第一行开始添加，先判断和第一条……(重复第二条的逻辑)……；。。。。。。 &#160; &#160; &#160; &#160;就是这么个思路，但是写起来真心不是随意就能写出来的。即使先不说写，把这个思路想出来，让我去设计一套规则，估计都相当困难啊。唉，人与人之间的差距始终在思维。。。。。。 &#160; &#160; &#160; &#160;扯远了，我们继续回归正题，上面逻辑完成了弹幕定位规则(内部那个layout接下来再讲)，限于篇幅，我只挑一个检查碰撞的代码贴出来分析，其它的请有兴趣者自行跟踪。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 public static boolean willHitInDuration(IDisplayer disp, BaseDanmaku d1, BaseDanmaku d2, long duration, long currTime) &#123;//disp, item, drawItem, drawItem.getDuration(), drawItem.getTimer().currMillisecond final int type1 = d1.getType(); final int type2 = d2.getType(); // allow hit if different type 不同类型的弹幕允许碰撞 if(type1 != type2) return false; if(d1.isOutside())&#123;//item已经跑出视图了，不存在碰撞问题 return false; &#125; long dTime = d2.time - d1.time; if (dTime &lt;= 0)//drawItem在item前面，已经碰撞了 return true; //两者出现时间已经相差一条弹幕时间了 || item超时跑出去了 || drawItem超时 ，都不会碰撞 if (Math.abs(dTime) &gt;= duration || d1.isTimeOut() || d2.isTimeOut()) &#123; return false; &#125;//item和drawItem都是顶部或者底部固定弹幕，因为在同一行，必定碰撞 if (type1 == BaseDanmaku.TYPE_FIX_TOP || type1 == BaseDanmaku.TYPE_FIX_BOTTOM) &#123; return true; &#125;//调用checkHitAtTime方法 return checkHitAtTime(disp, d1, d2, currTime) || checkHitAtTime(disp, d1, d2, d1.time + d1.getDuration()); &#125; //调用checkHitAtTime方法 private static boolean checkHitAtTime(IDisplayer disp, BaseDanmaku d1, BaseDanmaku d2, long time)&#123;//time = currTime || time = item.time + item.duration final float[] rectArr1 = d1.getRectAtTime(disp, time);//time获得item在视图的（l，t，r，b） final float[] rectArr2 = d2.getRectAtTime(disp, time);//time获得drawItem在视图的（l，t，r，b） if (rectArr1 == null || rectArr2 == null) return false; return checkHit(d1.getType(), d2.getType(), rectArr1, rectArr2); &#125; //调用checkHit方法 private static boolean checkHit(int type1, int type2, float[] rectArr1, float[] rectArr2) &#123; if(type1 != type2) return false; if (type1 == BaseDanmaku.TYPE_SCROLL_RL) &#123;//只要drawItem的left小于item的right就碰撞了 // hit if left2 &lt; right1 return rectArr2[0] &lt; rectArr1[2]; &#125; if (type1 == BaseDanmaku.TYPE_SCROLL_LR)&#123; // hit if right2 &gt; left1 return rectArr2[2] &gt; rectArr1[0]; &#125; return false; &#125; //R2LDanmaku的getRectAtTime方法 public float[] getRectAtTime(IDisplayer displayer, long time) &#123;//time = currTime || time = item.time + item.duration if (!isMeasured()) return null; float left = getAccurateLeft(displayer, time);//获得此时弹幕在视图的x坐标 if (RECT == null) &#123; RECT = new float[4]; &#125; RECT[0] = left;//left RECT[1] = y;//top RECT[2] = left + paintWidth;//right RECT[3] = y + paintHeight;//bottom return RECT; &#125; //R2LDanmaku的getAccurateLeft方法 protected float getAccurateLeft(IDisplayer displayer, long currTime) &#123;//currTime = timer.currTime || currTime = item.time + item.duration long elapsedTime = currTime - time;//当前时间 - 弹幕出现时间......//因此返回弹幕位于视图的x坐标，即视图宽度 - 弹幕已经显示了多少秒 * 每秒移动步长return displayer.getWidth() - elapsedTime * mStepX; &#125; &#160; &#160; &#160; &#160;检查碰撞逻辑比较简单，就是先根据当前时间就算出两条弹幕的位置（l1，t1，r1，b1），看看是否前面弹幕的 r1 小于后面弹幕的 l1；再根据前面弹幕的结束时间，计算出两条弹幕的位置（l2，t2，r2，b2）再次看看是否前面弹幕的 r2小于后面弹幕的 l2。只有两条都满足才不会碰撞。 &#160; &#160; &#160; &#160;好了检测碰撞就先到这里，然后继续回到AlignTopRetainer的fix方法，还有一个drawItem.layout(disp, drawItem.getLeft(), topPos);没讲呢，这才是真正确认弹幕位置的地方，继续查看L2RDanmaku的layout方法：1234567891011121314151617public void layout(IDisplayer displayer, float x, float y) &#123;//disp, drawItem.getLeft(), topPos if (mTimer != null) &#123; long currMS = mTimer.currMillisecond; long deltaDuration = currMS - time;//计算出出现时间和当前时间的时间差 if (deltaDuration &gt; 0 &amp;&amp; deltaDuration &lt; duration.value) &#123;//如果还没有到出现时间或者超出弹幕时间 this.x = getAccurateLeft(displayer, currMS);//计算出当前时间弹幕的x坐标，上面刚讲过 if (!this.isShown()) &#123; this.y = y;//把上面计算好的y值赋过来 this.setVisibility(true); &#125; mLastTime = currMS; return; &#125; mLastTime = currMS; &#125; this.setVisibility(false);&#125; &#160; &#160; &#160; &#160;这样弹幕的位置也就确定了，layout步骤就走完了。下一步就是draw步骤了。 &#160; &#160; &#160; &#160;4）绘制弹幕：&#160; &#160; &#160; &#160;赶紧回到DanmakuRenderer的draw方法，这个时候千万不要把自己搞晕了：123456789101112131415161718192021222324252627282930313233343536//DanmakuRenderer的draw方法 public RenderingState draw(IDisplayer disp, IDanmakus danmakus, long startRenderTime) &#123; ...... IDanmakuIterator itr = danmakus.iterator(); ...... BaseDanmaku drawItem = null; while (itr.hasNext()) &#123; drawItem = itr.next(); ...... ...检查是否建立缓存... ...... ...是否测量... ...layout布局... // draw //绘制弹幕 if (!drawItem.isOutside() &amp;&amp; drawItem.isShown()) &#123; if (drawItem.lines == null &amp;&amp; drawItem.getBottom() &gt; disp.getHeight()) &#123; continue; // skip bottom outside danmaku ，忽略超过视图底部的弹幕 &#125; //开始绘制 int renderingType = drawItem.draw(disp); if(renderingType == IRenderer.CACHE_RENDERING) &#123;//如果是使用缓存bitmap绘制的 ...... &#125; else if(renderingType == IRenderer.TEXT_RENDERING) &#123;//如果使用缓存绘制失败，则会使用原声方法Canvas去draw ...... if (mCacheManager != null) &#123; mCacheManager.addDanmaku(drawItem);//再次为词条弹幕构建缓存，以便下次使用缓存bitmap绘制 &#125; &#125; ...... &#125; &#160; &#160; &#160; &#160;继续跟踪int renderingType = drawItem.draw(disp) 这里：123456789101112131415161718192021222324252627282930313233343536//BaseDanmaku的draw方法 public int draw(IDisplayer displayer) &#123; return displayer.draw(this);//调用AndroidDisplayer的draw方法 &#125; //调用AndroidDisplayer的draw方法 public int draw(BaseDanmaku danmaku) &#123; float top = danmaku.getTop();//弹幕在视图的y值 float left = danmaku.getLeft();//弹幕在视图的x值 if (canvas != null) &#123; ...... // drawing cache boolean cacheDrawn = false; int result = IRenderer.CACHE_RENDERING; IDrawingCache&lt;?&gt; cache = danmaku.getDrawingCache(); if (cache != null) &#123;//如果弹幕有缓存 //取出缓存 DrawingCacheHolder holder = (DrawingCacheHolder) cache.get(); if (holder != null) &#123; //DrawingCacheHolder的draw方法，我们在上面的buildCache时分析过了，将每一条弹幕的bitmap绘制到视图的canvas上 cacheDrawn = holder.draw(canvas, left, top, alphaPaint); &#125; &#125; if (!cacheDrawn) &#123;//如果缓存绘制失败 ...... //则使用Android原生的canvas.drawText等方法绘制，drawDanmaku方法我们上面buildCache时也分析过 drawDanmaku(danmaku, canvas, left, top, false); result = IRenderer.TEXT_RENDERING; &#125; return result; &#125; return IRenderer.NOTHING_RENDERING; &#125; &#160; &#160; &#160; &#160;上面逻辑比较简单，先查看弹幕有没有缓存，如果有，就使用缓存绘制。在上面的buildCache时我们知道，缓存绘制的每一条弹幕都是一条bitmap，所以这里用缓存也是将bitmap绘制到视图的Canvas中。如果使用缓存绘制失败，会调用drawDanmaku方法，这个方法我们在上面的buildCache也分析过，则使用Android原生的canvas.drawText等绘制。 &#160; &#160; &#160; &#160;这样弹幕就被绘制到视图界面上了。 &#160; &#160; &#160; &#160;终于完了，以上就是DanmakuFlameMaster的流程分析过程了，分析的快吐学了ToT。。。。。。 TODO&#160; &#160; &#160; &#160;上面刚开始奖CacheManagingDrawTask时曾经说过，也可以不用CacheManagingDrawTask，直接使用DrawTask，只要将DanmakuView的mEnableDanmakuDrwaingCache变量改为false就可以了。这样改动之后就用不上工程里那些so库了，也就不用建立那么复杂的缓存机制。 &#160; &#160; &#160; &#160;还有一点区别就是使用CacheManagingDrawTask画出来的每一条弹幕都是bitmap，而用DrawTask的弹幕都是Canvas.drawText画出来的。 &#160; &#160; &#160; &#160;限于篇幅，DrawTask就不分析了，逻辑比CacheManagingDrawTask简单多了，大家有兴趣的自己看看。 结语&#160; &#160; &#160; &#160;DanmakuFlameMaster到此就分析完全了，简单总结一下流程就是： 加载弹幕资源 开启缓存机制，不断建立缓存和回收 开始绘制任务，根据定时器时间确定弹幕位置，绘制弹幕 &#160; &#160; &#160; &#160;这篇文章写的过程中也是十分蛋疼的，写的我差点over了。因为DanmakuFlameMaster源码实在太复杂了，坑非常多，所以很多细节都没有顾及。下次我绝对不会再写这么长的文章了，身体和脑力真心伤不起啊。赶紧休息一下~~~~~]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>弹幕</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android属性动画流程分析]]></title>
    <url>%2F2016%2F06%2F24%2FAndroid%E5%B1%9E%E6%80%A7%E5%8A%A8%E7%94%BB%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;摘要：Google在Android3.0之后给我们带来了属性动画，是真正意义上的改变属性。对比以前的Tween动画，只能作用于View，而且Tween动画改变的只是View的绘制效果，View真正的属性并没有改变。比如，一个按钮做平移的动画，虽然按钮的确做了平移，但按钮可点击的区域并没随着平移而改变，还是在原来的位置。而属性动画则可以改变真正的属性，从而实现按钮平移时点击区域也跟着平移。通俗点说，属性动画其实就是在一定时间内，按照一定规律来改变对象的属性，从而使对象展现出动画效果。 前言&#160; &#160; &#160; &#160; Google在Android3.0之后给我们带来了属性动画，是真正意义上的改变属性。对比以前的Tween动画，只能作用于View，而且Tween动画改变的只是View的绘制效果，View真正的属性并没有改变。比如，一个按钮做平移的动画，虽然按钮的确做了平移，但按钮可点击的区域并没随着平移而改变，还是在原来的位置。而属性动画则可以改变真正的属性，从而实现按钮平移时点击区域也跟着平移。通俗点说，属性动画其实就是在一定时间内，按照一定规律来改变对象的属性，从而使对象展现出动画效果。 基本用法&#160; &#160; &#160; &#160;我们要了解属性动画的原理，首先要知道他的用法。先整一个比较大众的吧：123456ObjectAnimator .ofInt(target,propName,values[]) .setInterpolator(LinearInterpolator) .setEvaluator(IntEvaluator) .setDuration(1000) .start(); &#160; &#160; &#160; &#160;这个应该都会吧，设置目标view，作用的属性，动画时长；设置插值器、估值器，这两个玩意设置最多的应该是插值器，估值器设置的应该比较少。这两个东西我们下面会逐一跟踪其踪迹；（当然还有对动画过程的回调监听，比如addListener，然后监听onAnimationStart、onAnimationEnd等等回调。不过我们这里没设，下面篇幅为讲到此处功能）；最后start,动画开始。 猜想与假设&#160; &#160; &#160; &#160; 一般的对象中的某个属性，如果要改变其值，要么是这个属性对外public，拿到对象后可以直接修改；要么这个对象的类有自己的get/set方法。但这都是理想的情况，如果不满足以上条件，如果想改变对象属性的值，就只能通过反射了。我们先这样假设，然后往下逐一验证。 &#160; &#160; &#160; &#160; 再者属性动画，我们断章取义一下，既然有动画这个词在内，就会有在规定时间内按固定规则对象属性的改变，就像函数一样，y=f(x)。属性动画也一样，y好比属性值property，x好比时间time，f关系就是插值器/估值器（Interpolator/TypeEvaluator）的作用。（先这么假设吧，是否合理我们往下深挖就知道了） 流程分析&#160; &#160; &#160; &#160; 我们按着上述猜测，然后进入验证阶段，直接整源码吧。源码文件不难找，并且都在两个固定目录，frameworks\base\core\java\android\animation\ 和 frameworks\base\core\java\android\view\animation\ 下。 &#160; &#160; &#160; &#160; 阅读源码不一定要把 源码 整个下载下来，比如有的在SDK里本来就有的文件，用 AS 或者 Eclipse 都可以直接看。不过我推荐有时间还是把重要的源码都下载下来，用Source Insight 或者 Sublime Text 来查看。&#160; &#160; &#160; &#160;其次，阅读源码一方面能让我们更清晰的理解Android SDK提供的API的流程原理，有助于开发人员更好的使用这些功能，遇到坑会有更好的解决方案。另一方面，也能够掌握其中的思想，因为一切业务都是思想的实体化，掌握了思想，才能在以后遇到问题或者需求的时候，能很快从脑中勾勒出解决思路，而不至于一脸懵逼无从下手。 &#160; &#160; &#160; &#160; 根据上述例子，一步一步分析。 主流程源码ofInt&#160; &#160; &#160; &#160;先从ofInt入手，例如这么用：.ofInt(view, “translationX”, 100)。挑一个简单明了的重载方法，其实其他的也是相同的道理。12345public static ObjectAnimator ofInt(Object target, String propertyName, int... values) &#123; ObjectAnimator anim = new ObjectAnimator(target, propertyName); anim.setIntValues(values); return anim;&#125; &#160; &#160; &#160; &#160;new了一个ObjectAnimator，构造方法传入target和propName，这么不难。12345678910111213141516171819202122232425262728293031 private ObjectAnimator(Object target, String propertyName) &#123; setTarget(target); setPropertyName(propertyName); &#125;//有两个方法，以此往下//设置目标对象target赋给属性动画全局变量mTarget public void setTarget(@Nullable Object target) &#123; final Object oldTarget = getTarget(); if (oldTarget != target) &#123; mTarget = target == null ? null : new WeakReference&lt;Object&gt;(target); // New target should cause re-initialization prior to starting mInitialized = false; &#125; &#125; //同上，将propertyName赋给mPropertyName public void setPropertyName(@NonNull String propertyName) &#123; // mValues could be null if this is being constructed piecemeal. Just record the // propertyName to be used later when setValues() is called if so. // 此时mValues为空，可以忽略判断逻辑 if (mValues != null) &#123; PropertyValuesHolder valuesHolder = mValues[0]; String oldName = valuesHolder.getPropertyName(); valuesHolder.setPropertyName(propertyName); mValuesMap.remove(oldName); mValuesMap.put(propertyName, valuesHolder); &#125; //赋值给全局变量 mPropertyName = propertyName; // New property/values/target should cause re-initialization prior to starting mInitialized = false; &#125; &#160; &#160; &#160; &#160;记录完target和propName，调用setIntValues。1234567891011121314public void setIntValues(int... values) &#123; // 此时mValues为空 if (mValues == null || mValues.length == 0) &#123; // No values yet - this animator is being constructed piecemeal. Init the values with // whatever the current propertyName is if (mProperty != null) &#123; // mProperty 也为空 setValues(PropertyValuesHolder.ofInt(mProperty, values)); &#125; else &#123; //因此会走到这里 setValues(PropertyValuesHolder.ofInt(mPropertyName, values)); &#125; &#125; else &#123; super.setIntValues(values); &#125;&#125; &#160; &#160; &#160; &#160;最后会走到setValues(PropertyValuesHolder.ofInt(mPropertyName, values)); 这里把我们传入的propName和values作为参数，又调用了PropertyValuesHolder的ofInt方法，我们先看里面这个，外面的那个setValues待会儿再看。123456789101112131415161718192021public static PropertyValuesHolder ofInt(String propertyName, int... values) &#123; return new IntPropertyValuesHolder(propertyName, values);&#125;//IntPropertyValuesHolder是PropertyValuesHolder的子类，也是个内部类public IntPropertyValuesHolder(String propertyName, int... values) &#123; super(propertyName);//这里会调用父类的构造 setIntValues(values);&#125;//父类一个参数构造方法如下，将propertyName赋给全局变量mPropertyName private PropertyValuesHolder(String propertyName) &#123; mPropertyName = propertyName;&#125;public void setIntValues(int... values) &#123; super.setIntValues(values);//同样会调用父类的setIntValues方法 mIntKeyframes = (Keyframes.IntKeyframes) mKeyframes;// 将父类方法得到的mKeyframes 再付给mIntKeyframes &#125;//父类setIntValues方法，为mValueType 赋值，同时为利用参数values调用KeyframeSet.ofInt(values)为mKeyframes赋值 public void setIntValues(int... values) &#123; mValueType = int.class; mKeyframes = KeyframeSet.ofInt(values);&#125; &#160; &#160; &#160; &#160; 我们可以看到，PropertyValuesHolder的ofInt方法让器内部存储了我们的propName，然后存储了我们的mValueType，即 int.class ，并且保存了一个新的变量 mIntKeyframes。 &#160; &#160; &#160; &#160; 这个mIntKeyframes由 KeyframeSet.ofInt(values) 得到，顾名思义，这玩意儿应该是关键帧之类的意思吧。我们联想一下关键帧，视频不就是一帧一帧的画面组成么，其中有参考帧和关键帧，且参考帧解码也依赖于关键帧，因此关键帧是视频图像流畅完整的保证（扯远了，我们先这么理解吧，然后再验证我们的猜测）。 &#160; &#160; &#160; &#160; mIntKeyframes类型为Keyframes，是个接口，实现类型为KeyframeSet。看着名字应该是关键帧集合吧，每个关键帧应该保存动画time/value(时间/值)对。那么继续验证，找到KeyframeSet.ofInt(values)方法：12345678910111213141516public static KeyframeSet ofInt(int... values) &#123; int numKeyframes = values.length; IntKeyframe keyframes[] = new IntKeyframe[Math.max(numKeyframes,2)];//最少应该有2帧 if (numKeyframes == 1) &#123;//如果只传了一个参数，比如我们前面给"translationX"属性的values传入移动300 keyframes[0] = (IntKeyframe) Keyframe.ofInt(0f);// 起始帧，属性保持原样 keyframes[1] = (IntKeyframe) Keyframe.ofInt(1f, values[0]);//结束帧，直接到达结果 &#125; else &#123;//可以设置某个属性的多个值，比如动态改变view的alpha值，例如1.0，0.8，0.4...... keyframes[0] = (IntKeyframe) Keyframe.ofInt(0f, values[0]);//起始帧 for (int i = 1; i &lt; numKeyframes; ++i) &#123;//后续n帧 //注意里面有个么一个片段:(float) i / (numKeyframes - 1)，这是按values个数等比例划分的 keyframes[i] = (IntKeyframe) Keyframe.ofInt((float) i / (numKeyframes - 1), values[i]); &#125; &#125; return new IntKeyframeSet(keyframes);&#125; &#160; &#160; &#160; &#160; 这里应该看到KeyframeSet.ofInt(values)方法，根据values的长度构造keyframes数组，然后分别通过Keyframe的ofInt方法，去构造keyframe对象。老规矩，继续看Keyframe.ofInt如何构造Keyframe：1234567891011121314151617181920int mValue; public static Keyframe ofInt(float fraction) &#123; return new IntKeyframe(fraction); &#125; //也就简单存了一下fraction，此时value默认值为0 IntKeyframe(float fraction) &#123; mFraction = fraction; mValueType = int.class; &#125; public static Keyframe ofInt(float fraction, int value) &#123; return new IntKeyframe(fraction, value); &#125; //也就简单存了一下fraction，value等 IntKeyframe(float fraction, int value) &#123; mFraction = fraction; mValue = value; mValueType = int.class; mHasValue = true; &#125; &#160; &#160; &#160; &#160;也就简单存了一下fraction和value，每个fraction对应相应的value。前面注释我特意写了个注意，这个fraction是按照帧的个数n存的，起始为0，结束为1，也就是个个数为n的等差数列an={0，1/(n-1), 2/(n-1), 3/(n-1)，……, (n-2)/(n-1), 1}。&#160; &#160; &#160; &#160;然后我们看它的 return new IntKeyframeSet(keyframes);12345678910111213//IntKeyframeSet构造方法，调其父类KeyframeSet构造方法public IntKeyframeSet(IntKeyframe... keyframes) &#123; super(keyframes);&#125;//KeyframeSet构造方法public KeyframeSet(Keyframe... keyframes) &#123; mNumKeyframes = keyframes.length; mKeyframes = new ArrayList&lt;Keyframe&gt;(); mKeyframes.addAll(Arrays.asList(keyframes)); mFirstKeyframe = mKeyframes.get(0); mLastKeyframe = mKeyframes.get(mNumKeyframes - 1); mInterpolator = mLastKeyframe.getInterpolator();//这个插值器是null，感觉没什么卵用&#125; &#160; &#160; &#160; &#160;存了有多少关键帧，开始帧，结束帧，以及插值器。 &#160; &#160; &#160; &#160;到这里PropertyValuesHolder.ofInt(mPropertyName, values)走完了，这个过程我们为PropertyValuesHolder对象赋了propName，valueType，keyframeSet，而keyframeSet中又存了Keyframe集合，keyframe中存储了（fraction , valuetype , value , hasValue）。 &#160; &#160; &#160; &#160;上面说过setValues(int… values) 会走setValues(PropertyValuesHolder.ofInt(mPropertyName, values));这一步，这是其父类ValueAnimator的方法，我们进入其父类看看：123456789101112public void setValues(PropertyValuesHolder... values) &#123; int numValues = values.length; mValues = values;//将刚才得到的PropertyValuesHolder存入 mValuesMap = new HashMap&lt;String, PropertyValuesHolder&gt;(numValues); //再包装一层map for (int i = 0; i &lt; numValues; ++i) &#123; PropertyValuesHolder valuesHolder = values[i]; mValuesMap.put(valuesHolder.getPropertyName(), valuesHolder); &#125; // New property/values/target should cause re-initialization prior to starting mInitialized = false;&#125; &#160; &#160; &#160; &#160;首先记录了mValues，注意这里的values是PropertyValuesHolder类型的，然后通过一个mValueMap记录：key为属性的名称，值为PropertyValuesHolder 。 &#160; &#160; &#160; &#160;到此ofInt流程算是走完了，小结一下：ofInt记录了target,propName,values（是将我们传入的int型values，辗转转化成了PropertyValuesHolder）,以及一个mValueMap，这个map的key是propName，value是PropertyValuesHolder，在PropertyValuesHolder内部又存储了proprName, valueType , keyframeSet等等。 setInterpolator&#160; &#160; &#160; &#160;设置插值器。1234567public void setInterpolator(TimeInterpolator value) &#123; if (value != null) &#123; mInterpolator = value; &#125; else &#123; mInterpolator = new LinearInterpolator(); &#125;&#125; &#160; &#160; &#160; &#160;也是父类ValueAnimator的方法，可以看到如果没有设置插值器，默认就是线性插值器LinearInterpolator。 setEvaluator&#160; &#160; &#160; &#160;设置估值器，这东西用的不多。12345public void setEvaluator(TypeEvaluator value) &#123; if (value != null &amp;&amp; mValues != null &amp;&amp; mValues.length &gt; 0) &#123; mValues[0].setEvaluator(value); &#125; &#125; &#160; &#160; &#160; &#160;mValues就是我们刚才ofInt里得到的PropertyValuesHolder对象，然后调用PropertyValuesHolder.setEvalutor：1234public void setEvaluator(TypeEvaluator evaluator) &#123; mEvaluator = evaluator;// 记录evaluator mKeyframes.setEvaluator(evaluator);// KeyframeSet再次记录evaluator&#125; &#160; &#160; &#160; &#160;KeyframeSet再次记录evaluator：123public void setEvaluator(TypeEvaluator evaluator) &#123; mEvaluator = evaluator; &#125; &#160; &#160; &#160; &#160;setEvaluator这一步就完了，也就是把估值器evaluator分别交给PropertyValuesHolder和KeyframeSet。 setDuration&#160; &#160; &#160; &#160; 设置动画时长。1234567891011121314151617181920212223//ObjectAnimator 的方法 public ObjectAnimator setDuration(long duration) &#123; super.setDuration(duration); //父类实现 return this; &#125; //VauleAnimator的方法 public ValueAnimator setDuration(long duration) &#123; if (duration &lt; 0) &#123; throw new IllegalArgumentException("Animators cannot have negative duration: " + duration); &#125; mUnscaledDuration = duration; updateScaledDuration(); return this; &#125;private static float sDurationScale = 1.0f;private long mDuration = (long)(300 * sDurationScale); private long mUnscaledDuration = 300; private void updateScaledDuration() &#123; mDuration = (long)(mUnscaledDuration * sDurationScale); &#125; &#160; &#160; &#160; &#160; 就是简单在mDuration中记录了一下动画的持续时间，这个sDurationScale默认为1，貌似是用于调整，观察动画的，比如你可以调整为10，动画就会慢10倍的播放。 start&#160; &#160; &#160; &#160; 以上的都比较简单，那么我们猜测start方法一定是巨复杂的。休息一下~&#160; &#160; &#160; &#160;OK，我们继续，走start()方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public void start() &#123; //... //省略干扰代码 //... super.start(); //最终会调用父类的方法&#125; //ValueAnimator的start方法 public void start() &#123; start(false); &#125; private void start(boolean playBackwards) &#123; if (Looper.myLooper() == null) &#123; throw new AndroidRuntimeException("Animators may only be run on Looper threads"); &#125; mPlayingBackwards = playBackwards; //动画是否reverse mCurrentIteration = 0;//记录当前的动画的执行次数（与setRepeatCount有关） mPlayingState = STOPPED;//动画的状态为STOPPED mStarted = true;//标志位 mStartedDelay = false; mPaused = false;//标志位 updateScaledDuration(); // in case the scale factor has changed since creation time AnimationHandler animationHandler = getOrCreateAnimationHandler();//生成一个AnimationHandler对象，getOrCreateAnimationHandler就是在当前线程变量ThreadLocal中取出来，没有的话，则创建一个，然后set进去。 animationHandler.mPendingAnimations.add(this);//AnimationHandler中包含一些List集合用于存储各种状态的ValueAnimator，将当前ValueAnimator对象，加入 animationHandler.mPendingAnimations 集合 if (mStartDelay == 0) &#123; //mStartDelay 默认为0，进入 // This sets the initial value of the animation, prior to actually starting it running setCurrentPlayTime(0); //这个接下来会说明 mPlayingState = STOPPED;//动画的状态为STOPPED mRunning = true;//标志位 notifyStartListeners();//回调监听动画的接口AnimatorListener的onAnimationStart方法，如果你设置了回调监听，此时就会进行回调。 &#125; animationHandler.start();//最后调用，这个会细说 &#125; protected static ThreadLocal&lt;AnimationHandler&gt; sAnimationHandler = new ThreadLocal&lt;AnimationHandler&gt;(); private static AnimationHandler getOrCreateAnimationHandler() &#123; AnimationHandler handler = sAnimationHandler.get(); if (handler == null) &#123; handler = new AnimationHandler(); sAnimationHandler.set(handler); &#125; return handler; &#125; private void notifyStartListeners() &#123; if (mListeners != null &amp;&amp; !mStartListenersCalled) &#123; ArrayList&lt;AnimatorListener&gt; tmpListeners = (ArrayList&lt;AnimatorListener&gt;) mListeners.clone(); int numListeners = tmpListeners.size(); for (int i = 0; i &lt; numListeners; ++i) &#123; tmpListeners.get(i).onAnimationStart(this); &#125; &#125; mStartListenersCalled = true; &#125; &#160; &#160; &#160; &#160;start方法做了一些状态和变量初始化，其他的都很好理解，有两处方法要细说：setCurrentPlayTime(0)和animationHandler.start()，我们先看setCurrentPlayTime(0)。12345678910public void setCurrentPlayTime(long playTime) &#123; initAnimation();//初始化动画，接下来看 long currentTime = AnimationUtils.currentAnimationTimeMillis();//得到当前时间 if (mPlayingState != RUNNING) &#123; //之前将mPlayingState设为了STOPPED mSeekTime = playTime;//为0 mPlayingState = SEEKED; //将mPlayingState 改为SEEKED &#125; mStartTime = currentTime - playTime;//起始时间为当前时间 doAnimationFrame(currentTime);//接下来也会看&#125; &#160; &#160; &#160; &#160; 先看initAnimation()：123456789void initAnimation() &#123; if (!mInitialized) &#123; int numValues = mValues.length; for (int i = 0; i &lt; numValues; ++i) &#123; mValues[i].init(); //mValues也就是之前ofInt方法得到的IntPropertyValueHolder对象 &#125; mInitialized = true; &#125;&#125; &#160; &#160; &#160; &#160; 接着找IntPropertyValueHolder的init方法，在其父类发现如下：1234567891011121314void init() &#123; if (mEvaluator == null) &#123; // We already handle int and float automatically, but not their Object // equivalents mEvaluator = (mValueType == Integer.class) ? sIntEvaluator : (mValueType == Float.class) ? sFloatEvaluator : null; &#125; if (mEvaluator != null) &#123; // KeyframeSet knows how to evaluate the common types - only give it a custom // evaluator if one has been set on this class mKeyframes.setEvaluator(mEvaluator); &#125;&#125; &#160; &#160; &#160; &#160; 其实就是遍历设置PropertyValuesHolder中的mEvaluator属性，默认根据valueType进行判断，IntEvaluator或者FloatEvaluator。 &#160; &#160; &#160; &#160;initAnimation()完了，然后看doAnimationFrame(currentTime)：123456789101112131415161718192021222324252627282930final boolean doAnimationFrame(long frameTime) &#123; if (mPlayingState == STOPPED) &#123; //上面已赋为SEEKED，所以不进入下面逻辑 mPlayingState = RUNNING; if (mSeekTime &lt; 0) &#123; mStartTime = frameTime; &#125; else &#123; mStartTime = frameTime - mSeekTime; // Now that we're playing, reset the seek time mSeekTime = -1; &#125; &#125; if (mPaused) &#123;//mPaused上面已赋为false if (mPauseTime &lt; 0) &#123; mPauseTime = frameTime; &#125; return false; &#125; else if (mResumed) &#123;//默认是false mResumed = false; if (mPauseTime &gt; 0) &#123; // Offset by the duration that the animation was paused mStartTime += (frameTime - mPauseTime); &#125; &#125; // The frame time might be before the start time during the first frame of // an animation. The "current time" must always be on or after the start // time to avoid animating frames at negative time intervals. In practice, this // is very rare and only happens when seeking backwards. final long currentTime = Math.max(frameTime, mStartTime); return animationFrame(currentTime);//最后只走了这个&#125; &#160; &#160; &#160; &#160; 继续跟animationFrame(currentTime)：1234567891011121314151617181920212223242526272829303132333435boolean animationFrame(long currentTime) &#123; boolean done = false; switch (mPlayingState) &#123;//上面已赋为SEEKED case RUNNING: case SEEKED: float fraction = mDuration &gt; 0 ? (float)(currentTime - mStartTime) / mDuration : 1f;//此时currentTime和mStartTime相等，fraction为0，刚开始嘛 if (fraction &gt;= 1f) &#123;//刚开始不会走这段逻辑 if (mCurrentIteration &lt; mRepeatCount || mRepeatCount == INFINITE) &#123; // Time to repeat if (mListeners != null) &#123; int numListeners = mListeners.size(); for (int i = 0; i &lt; numListeners; ++i) &#123; mListeners.get(i).onAnimationRepeat(this); &#125; &#125; if (mRepeatMode == REVERSE) &#123; mPlayingBackwards = !mPlayingBackwards; &#125; mCurrentIteration += (int)fraction; fraction = fraction % 1f; mStartTime += mDuration; &#125; else &#123; done = true; fraction = Math.min(fraction, 1.0f); &#125; &#125; if (mPlayingBackwards) &#123;//这个是false，因为没有设置reverse fraction = 1f - fraction; &#125; animateValue(fraction);//最后只会走这个，继续往下看 break; &#125; return done; &#125; &#160; &#160; &#160; &#160; 然后又到了这一步animateValue(fraction)，此时fraction是0，刚开始嘛。12345678910111213141516171819202122232425262728293031//这里会调用子类ObjectAnimator的animateValue方法 void animateValue(float fraction) &#123; final Object target = getTarget(); if (mTarget != null &amp;&amp; target == null) &#123; // We lost the target reference, cancel and clean up. cancel(); return; &#125; super.animateValue(fraction);//这里调用父类的方法 int numValues = mValues.length; for (int i = 0; i &lt; numValues; ++i) &#123; mValues[i].setAnimatedValue(target);//设置属性，下面会分析 &#125; &#125; //ValueAnimator的方法 void animateValue(float fraction) &#123; fraction = mInterpolator.getInterpolation(fraction);//插值器处理一下fraction mCurrentFraction = fraction; int numValues = mValues.length; for (int i = 0; i &lt; numValues; ++i) &#123; mValues[i].calculateValue(fraction);//之前ofInt得到的IntPropertyValueHolder对象的calculateValue方法 &#125; //UpdateListener监听接口开始回调，比较简单 if (mUpdateListeners != null) &#123; int numListeners = mUpdateListeners.size(); for (int i = 0; i &lt; numListeners; ++i) &#123; mUpdateListeners.get(i).onAnimationUpdate(this); &#125; &#125; &#125; &#160; &#160; &#160; &#160; 我们跟一下IntPropertyValueHolder的calculateValue方法：123void calculateValue(float fraction) &#123;//这个fraction是经过插值器处理过的fraction mIntAnimatedValue = mIntKeyframes.getIntValue(fraction);//这里注意是IntKeyFrameSet，千万不要看错方法了&#125; &#160; &#160; &#160; &#160;go on，IntKeyFrameSet的getIntValue方法，fraction是经过插值器处理过的fraction ：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Overridepublic int getIntValue(float fraction) &#123;//raction是经过插值器处理过的fraction if (mNumKeyframes == 2) &#123;//在ofInt里只设置了一个value，则只有两个关键帧，上面分析过 if (firstTime) &#123;//默认是true firstTime = false; firstValue = ((IntKeyframe) mKeyframes.get(0)).getIntValue();//取起始帧的value，为0 lastValue = ((IntKeyframe) mKeyframes.get(1)).getIntValue();//结束帧的value，即我们设进去的值 deltaValue = lastValue - firstValue;//计算delta值 &#125; if (mInterpolator != null) &#123;//这玩意儿是null，他是IntKeyFrameSet的，不是ObjectAnimator的 fraction = mInterpolator.getInterpolation(fraction); &#125; if (mEvaluator == null) &#123;//估值器，其实此处设与不设一样，实现都是firstValue + (int)(fraction * deltaValue)；这个很好看懂，想想函数y=f(X)之类，return的就是y return firstValue + (int)(fraction * deltaValue); &#125; else &#123;//扒一下IntEvaluator和上面那个一样 return ((Number)mEvaluator.evaluate(fraction, firstValue, lastValue)).intValue(); &#125; &#125; //下面逻辑是ofInt设置了多个value的 if (fraction &lt;= 0f) &#123;//小于区间范围，将第0和第1帧作为参考 final IntKeyframe prevKeyframe = (IntKeyframe) mKeyframes.get(0);//第1帧 final IntKeyframe nextKeyframe = (IntKeyframe) mKeyframes.get(1);//第2帧 int prevValue = prevKeyframe.getIntValue();//第1帧的属性值y1 int nextValue = nextKeyframe.getIntValue();//第2帧属性值y2 float prevFraction = prevKeyframe.getFraction();//第1帧的关系因子，x1 float nextFraction = nextKeyframe.getFraction();//第2帧的关系因子，x2 final TimeInterpolator interpolator = nextKeyframe.getInterpolator(); if (interpolator != null) &#123;//null fraction = interpolator.getInterpolation(fraction); &#125; //(x-x1)/(x2-x1) float intervalFraction = (fraction - prevFraction) / (nextFraction - prevFraction); //y=y1 + (x-x1)/(x2-x1)*(y2-y1),怎么样，直线方程既视感 return mEvaluator == null ? prevValue + (int)(intervalFraction * (nextValue - prevValue)) : ((Number)mEvaluator.evaluate(intervalFraction, prevValue, nextValue)). intValue(); &#125; else if (fraction &gt;= 1f) &#123;//大于区间范围，将倒1和倒2帧作为参考，一下逻辑同上 final IntKeyframe prevKeyframe = (IntKeyframe) mKeyframes.get(mNumKeyframes - 2); final IntKeyframe nextKeyframe = (IntKeyframe) mKeyframes.get(mNumKeyframes - 1); int prevValue = prevKeyframe.getIntValue(); int nextValue = nextKeyframe.getIntValue(); float prevFraction = prevKeyframe.getFraction(); float nextFraction = nextKeyframe.getFraction(); final TimeInterpolator interpolator = nextKeyframe.getInterpolator(); if (interpolator != null) &#123; fraction = interpolator.getInterpolation(fraction); &#125; float intervalFraction = (fraction - prevFraction) / (nextFraction - prevFraction); return mEvaluator == null ? prevValue + (int)(intervalFraction * (nextValue - prevValue)) : ((Number)mEvaluator.evaluate(intervalFraction, prevValue, nextValue)).intValue(); &#125; //正常范围 IntKeyframe prevKeyframe = (IntKeyframe) mKeyframes.get(0); for (int i = 1; i &lt; mNumKeyframes; ++i) &#123;//循环遍历 IntKeyframe nextKeyframe = (IntKeyframe) mKeyframes.get(i); if (fraction &lt; nextKeyframe.getFraction()) &#123;//这就比较好理解了，就进参考，往下逻辑同上 final TimeInterpolator interpolator = nextKeyframe.getInterpolator(); if (interpolator != null) &#123; fraction = interpolator.getInterpolation(fraction); &#125; float intervalFraction = (fraction - prevKeyframe.getFraction()) / (nextKeyframe.getFraction() - prevKeyframe.getFraction()); int prevValue = prevKeyframe.getIntValue(); int nextValue = nextKeyframe.getIntValue(); return mEvaluator == null ? prevValue + (int)(intervalFraction * (nextValue - prevValue)) : ((Number)mEvaluator.evaluate(intervalFraction, prevValue, nextValue)). intValue(); &#125; prevKeyframe = nextKeyframe; &#125; // shouldn't get here //确实不该到这儿，google注释都这么说了 return ((Number)mKeyframes.get(mNumKeyframes - 1).getValue()).intValue();&#125; &#160; &#160; &#160; &#160;看注释是不是有种学霸附体的感觉^ 。^至于第一种在ofInt里只设置了一个value，则只有两个关键帧，这个很好理解。可以比喻为：已知x和y成线性关系，公式为y=kx+b，告知其中一点x值为x1，求y1。一次函数的直视感。 &#160; &#160; &#160; &#160;第二种设置多个value也很好理解，因为动画是连续的，这是个连续函数，所以函数曲线是一条连续的线。每个关键帧都是连续函数上的固定点。虽然函数图像是连续的，但是他确是个分段函数，这些关键帧就是分段函数的拐点，而两个拐点之间的规则是一样的，就是我们定义的插值器interpolator 。&#160; &#160; &#160; &#160;动画之间都是连续的，如果要求出某个时间段对象的属性值，一定要参考距离它最近两帧。所以问题就转为已知两点坐标（x1，y1），（x2，y2），和另一点的x值，求其y值。果断高一数学的直线方程整起：$$ \dfrac{x-x1 }{x2-x1} = \dfrac{y-y1}{y2-y1} $$ $$ y=\dfrac{(x-x1) * (y2-y1) }{x2-x1} +y1 $$ &#160; &#160; &#160; &#160;IntKeyFrameSet的getIntValue方法就分析玩了，然后找到之前的逻辑。这样就求出属性值了，并把它赋给了ObjectAnimator的IntPropertyValueHolder类对象mValues的mIntAnimatedValue中。回到上面的子类animateValue(fraction)方法，还有一步mValues[i].setAnimatedValue(target)：123456789101112131415161718192021222324void setAnimatedValue(Object target) &#123; if (mIntProperty != null) &#123; mIntProperty.setValue(target, mIntAnimatedValue); return; &#125; if (mProperty != null) &#123; mProperty.set(target, mIntAnimatedValue); return; &#125; if (mJniSetter != 0) &#123; nCallIntMethod(target, mJniSetter, mIntAnimatedValue); return; &#125; if (mSetter != null) &#123; try &#123; mTmpValueArray[0] = mIntAnimatedValue; mSetter.invoke(target, mTmpValueArray); &#125; catch (InvocationTargetException e) &#123; Log.e("PropertyValuesHolder", e.toString()); &#125; catch (IllegalAccessException e) &#123; Log.e("PropertyValuesHolder", e.toString()); &#125; &#125;&#125; &#160; &#160; &#160; &#160;果然有反射，看来我们的猜测八九不离十。这样就把刚才计算的属性值设置给目标对象了。 &#160; &#160; &#160; &#160;确实有点晕了。。。。。回一下神，赶紧回到ObjectAnimator父类ValueAnimator的start方法里，还要继续分析第二个重要地方animationHandler.start()。animationHandler我们上面已经介绍了，存储在当前线程的ThreadLocal里面，里面放了一些集合用于存储各种状态的ObjectAnimator，我们当前的ObjectAnimator对象也存储在其mPendingAnimations的集合中（上面提到过~~）。123456789public void start() &#123; scheduleAnimation();&#125;private void scheduleAnimation() &#123; if (!mAnimationScheduled) &#123;//mAnimationScheduled默认false mChoreographer.postCallback(Choreographer.CALLBACK_ANIMATION, this, null);//Choreographer.CALLBACK_ANIMATION为1 mAnimationScheduled = true; &#125;&#125; &#160; &#160; &#160; &#160;要用到mChoreographer这个对象的postCallback方法，其中有一个参数是this；至于什么是Choreographer，暂时不用管；但是你需要知道一件事，其实我们的animationHandler是Runnable的子类，而 mChoreographer.postCallback(Choreographer.CALLBACK_ANIMATION, this, null);类似与handler发送消息，最终执行这个Runnable的run方法。&#160; &#160; &#160; &#160;Choreographer这个类里面障眼法太多了，就不贴了。绕来绕去，其实就是一句话，这里调用了animationHandler的 run方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127 public void run() &#123; mAnimationScheduled = false; doAnimationFrame(mChoreographer.getFrameTime()); &#125; private void doAnimationFrame(long frameTime) &#123; // mPendingAnimations holds any animations that have requested to be started // We're going to clear mPendingAnimations, but starting animation may // cause more to be added to the pending list (for example, if one animation // starting triggers another starting). So we loop until mPendingAnimations // is empty. while (mPendingAnimations.size() &gt; 0) &#123;//while循环，遍历所有在mPendingAnimations中的ObjectAnimator，依次调用anim.startAnimation(this); ArrayList&lt;ValueAnimator&gt; pendingCopy = (ArrayList&lt;ValueAnimator&gt;) mPendingAnimations.clone(); mPendingAnimations.clear(); int count = pendingCopy.size(); for (int i = 0; i &lt; count; ++i) &#123; ValueAnimator anim = pendingCopy.get(i); // If the animation has a startDelay, place it on the delayed list if (anim.mStartDelay == 0) &#123;//不延时的直接start anim.startAnimation(this); &#125; else &#123;//延时的先加入mDelayedAnims队列 mDelayedAnims.add(anim); &#125; &#125; &#125; // Next, process animations currently sitting on the delayed queue, adding // them to the active animations if they are ready //看有多少延时的，如果延时的时间到了就加入到准备队列mReadyAnims int numDelayedAnims = mDelayedAnims.size(); for (int i = 0; i &lt; numDelayedAnims; ++i) &#123; ValueAnimator anim = mDelayedAnims.get(i); if (anim.delayedAnimationFrame(frameTime)) &#123;//看方法注释就是说如果延时到了就该加入动画准备集合 mReadyAnims.add(anim); &#125; &#125; //准备队列的开始start int numReadyAnims = mReadyAnims.size(); if (numReadyAnims &gt; 0) &#123; for (int i = 0; i &lt; numReadyAnims; ++i) &#123; ValueAnimator anim = mReadyAnims.get(i); anim.startAnimation(this); anim.mRunning = true; mDelayedAnims.remove(anim); &#125; mReadyAnims.clear(); &#125; // Now process all active animations. The return value from animationFrame() // tells the handler whether it should now be ended //将animationHandler的mAnimations集合中的每个anim，加入到mTmpAnimations中； int numAnims = mAnimations.size(); for (int i = 0; i &lt; numAnims; ++i) &#123; mTmpAnimations.add(mAnimations.get(i)); &#125; //依次调用mTmpAnimations中的anim，anim.doAnimationFrame(frameTime)。doAnimationFrame（frameTime）上面已经分析过了，如果返回true，即doAnimationFrame的done为true，则将该动画加入到结束动画集合。 for (int i = 0; i &lt; numAnims; ++i) &#123; ValueAnimator anim = mTmpAnimations.get(i); if (mAnimations.contains(anim) &amp;&amp; anim.doAnimationFrame(frameTime)) &#123; mEndingAnims.add(anim); &#125; &#125; mTmpAnimations.clear(); //循环调用mEndingAnims， mEndingAnims.get(i).endAnimation(this);内部，会将动画移除mAnimations，回调动画监听接口onAnimationEnd；以及重置各种标志变量。 if (mEndingAnims.size() &gt; 0) &#123; for (int i = 0; i &lt; mEndingAnims.size(); ++i) &#123; mEndingAnims.get(i).endAnimation(this); &#125; mEndingAnims.clear(); &#125; // If there are still active or delayed animations, schedule a future call to // onAnimate to process the next frame of the animations. //如果mAnimations不为null，则再次调用scheduleAnimation(); if (!mAnimations.isEmpty() || !mDelayedAnims.isEmpty()) &#123; scheduleAnimation(); &#125; &#125; /** * Internal function called to process an animation frame on an animation that is currently * sleeping through its &lt;code&gt;startDelay&lt;/code&gt; phase. The return value indicates whether it * should be woken up and put on the active animations queue. * * @param currentTime The current animation time, used to calculate whether the animation * has exceeded its &lt;code&gt;startDelay&lt;/code&gt; and should be started. * @return True if the animation's &lt;code&gt;startDelay&lt;/code&gt; has been exceeded and the animation * should be added to the set of active animations. */private boolean delayedAnimationFrame(long currentTime) &#123; if (!mStartedDelay) &#123;//默认false mStartedDelay = true; mDelayStartTime = currentTime; &#125; if (mPaused) &#123;//默认false if (mPauseTime &lt; 0) &#123; mPauseTime = currentTime; &#125; return false; &#125; else if (mResumed) &#123;//默认false mResumed = false; if (mPauseTime &gt; 0) &#123; // Offset by the duration that the animation was paused mDelayStartTime += (currentTime - mPauseTime); &#125; &#125; long deltaTime = currentTime - mDelayStartTime;//延时间隔 if (deltaTime &gt; mStartDelay) &#123;//延时超了规定延时时间，应该执行了 // startDelay ended - start the anim and record the // mStartTime appropriately mStartTime = currentTime - (deltaTime - mStartDelay); mPlayingState = RUNNING; return true; &#125; return false;&#125;private void startAnimation(AnimationHandler handler) &#123; if (Trace.isTagEnabled(Trace.TRACE_TAG_VIEW)) &#123; Trace.asyncTraceBegin(Trace.TRACE_TAG_VIEW, getNameForTrace(), System.identityHashCode(this)); &#125; initAnimation(); handler.mAnimations.add(this); if (mStartDelay &gt; 0 &amp;&amp; mListeners != null) &#123; // Listeners were already notified in start() if startDelay is 0; this is // just for delayed animations notifyStartListeners(); &#125;&#125; &#160; &#160; &#160; &#160;scheduleAnimation()一旦调用，就像Handler不停发消息一样，AnimationHandler的run方法就会一直调用，mChoreographer.getFrameTime()控制动画时间段，然后一直调用AnimationHandler的doAnimationFrame方法，这个方法里面又调用了ValueAnimator的doAnimationFrame方法，这个方法上面分析过了，就是计算属性应该的值，然后反射设置；再startAnimation通知回调。这样动画就一帧一帧的执行了。 总结&#160; &#160; &#160; &#160;ofInt中实例化了一个ObjectAnimator对象，然后设置了target，propName，values(PropertyValuesHolder) ；然后分别在setInterpolator，setDuration设置了Interpolator和duration。其中setEvaluator是给PropertyValuesHolder，以及keyframeSet设置估值算法。 &#160; &#160; &#160; &#160;PropertyValueHolder实际上是IntPropertyValueHolder类型对象，包含propName,valueType,keyframeSet . &#160; &#160; &#160; &#160;keyframeset中存了Keyframe集合，keyframe中存储了（fraction , valuetype , value , hasValue）。 &#160; &#160; &#160; &#160;start()中：&#160; &#160; &#160; &#160;首先更新动画各种状态，然后初步计算fraction为(currentTime - mStartTime) / mDuration；然后将这个fraction交给我们的插值器计算后得到新的fraction，再将新的fraction交给我们的估值算法，估值算法根据开始、结束、fraction得到当前属性（动画作用的属性）应该的值，最大调用反射进行设置；&#160; &#160; &#160; &#160;start中还会根据动画的状态，如果没有结束，不断的调用AnimationHanlder的run方法;该方法内部利用mChoreographer不断的去重复第一步。 &#160; &#160; &#160; &#160;至此属性动画流程分析完了，也算马马虎虎，至少和我们的猜想八九不离十。看源码看的也快吐血了，这么长代码也只能了解个大概，以后有时间再好好整理一下思路。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>属性动画</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android跨进程通信机制Binder简要分析]]></title>
    <url>%2F2016%2F06%2F12%2FBinder%E7%AE%80%E8%A6%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160; 在Android系统中，每个应用都是由四大组件中之几组成，我们经常要去写一些Activity或者Service，这些Activity或者Service有时候会运行在不同的进程中，如果我们要让这些不同的Activity或者Service之间通信，就要用到Android提供的Binder机制了。 &#160;&#160;&#160;&#160;Binder其实也不是Android提出来的一套新的进程间通信机制，它是基于OpenBinder来实现的。OpenBinder最先是由Be Inc.开发的，接着Palm Inc.也着使用。现在OpenBinder的作者Dianne Hackborn就是在Google工作，负责Android平台的开发工作。 &#160; &#160; &#160; &#160; Android系统是基于Linux内核的，Linux已经拥有管道，system V IPC，socket等IPC手段，比如pipe管道（有血缘关系的进程），fifo有名管道（无血缘关系的进程），mmap/munmap内存共享映射，Unix Domain Socket等等。但是Android却采用了Binder这种不同上述几种类型的特有机制，至于原因肯定有很多，比如安全，实现方式合理性，内存优化，性能等等。（这一块儿坑的很多，需要积累很多linux系统相关内容，我也是恶补了许多知识。并且Binder远远不是三言两语能够概括清楚的，所以很多底层细节无法为大家展现，比如内核部分代码细节。既然不幸入坑了，我们就一步一步往下挖吧=。=） Binder通信模型Binder的优势实现方式&#160; &#160; &#160; &#160; Binder使用Client-Server通信方式：一个进程作为Server提供诸如视频/音频解码，视频捕获，地址本查询，网络连接等服务；多个进程作为Client向Server发起服务请求，获得所需要的服务。要想实现Client-Server通信据必须实现以下两点：一是server必须有确定的访问接入点或者说地址来接受Client的请求，并且Client可以通过某种途径获知Server的地址；二是制定Command-Reply协议来传输数据。例如在网络通信中Server的访问接入点就是Server主机的IP地址+端口号，传输协议为TCP协议。对Binder而言，Binder可以看成Server提供的实现某个特定服务的访问接入点， Client通过这个‘地址’向Server发送请求来使用该服务；对Client而言，Binder可以看成是通向Server的管道入口，要想和某个Server通信首先必须建立这个管道并获得管道入口。 性能优化&#160; &#160; &#160; &#160; 如果是传统的Linux IPC方式中，socket作为一款通用接口，其传输效率低，开销大，主要用在跨网络的进程间通信和本机上进程间的低速通信。消息队列和管道采用存储-转发方式，即数据先从发送方缓存区拷贝到内核开辟的缓存区中，然后再从内核缓存区拷贝到接收方缓存区，至少有两次拷贝过程。共享内存虽然无需拷贝，但控制复杂，难以使用。 &#160; &#160; &#160; &#160; 举个例子如，Client要将一块内存数据传递给Server，一般的做法是，Client将这块数据从它的进程空间拷贝到内核空间中，然后内核再将这个数据从内核空间拷贝到Server的进程空间，这样，Server就可以访问这个数据了。但是在这种方法中，执行了两次内存拷贝操作。所以Binder设计时采用了折衷的方式，只需要把Client进程空间的数据拷贝一次到内核空间，然后Server与内核共享这个数据就可以了，整个过程只需要执行一次内存拷贝，提高了效率。同时这样更有C/S架构的模型，方便管理。 安全性&#160; &#160; &#160; &#160; Android作为一个开放式，拥有众多开发者的的平台，应用程序的来源广泛，确保智能终端的安全是非常重要的。终端用户不希望从网上下载的程序在不知情的情况下偷窥隐私数据，连接无线网络，长期操作底层设备导致电池很快耗尽等等。传统IPC没有任何安全措施，完全依赖上层协议来确保。首先传统IPC的接收方无法获得对方进程可靠的UID/PID（用户ID/进程ID），从而无法鉴别对方身份。Android为每个安装好的应用程序分配了自己的UID，故进程的UID是鉴别进程身份的重要标志。使用传统IPC只能由用户在数据包里填入UID/PID，但这样不可靠，容易被恶意程序利用。可靠的身份标记只有由IPC机制本身在内核中添加。其次传统IPC访问接入点是开放的，无法建立私有通道。比如命名管道的名称，system V的键值，socket的ip地址或文件名都是开放的，只要知道这些接入点的程序都可以和对端建立连接，不管怎样都无法阻止恶意程序通过猜测接收方地址获得连接。 &#160; &#160; &#160; &#160; 基于以上原因，Android需要建立一套新的IPC机制来满足系统对通信方式的安全性，这就是Binder。Binder为发送发添加UID/PID身份，既支持实名Binder也支持匿名Binder，安全性高。 通信方式和引用&#160; &#160; &#160; &#160; 与其它IPC不同，Binder使用了面向对象的思想来描述作为访问接入点的Binder及其在Client中的入口：Binder是一个实体位于Server中的对象，该对象提供了一套方法用以实现对服务的请求，就象类的成员函数。遍布于client中的入口可以看成指向这个binder对象的‘指针’，一旦获得了这个‘指针’就可以调用该对象的方法访问server。在Client看来，通过Binder‘指针’调用其提供的方法和通过指针调用其它任何本地对象的方法并无区别，尽管前者的实体位于远端Server中，而后者实体位于本地内存中。‘指针’是C/C++的术语，换成java的说法是引用，即Client通过Binder的引用访问Server。如果借用Windows系统开发的一个术语‘句柄’也可以用来表述Binder在Client中的存在方式。从通信的角度看，Client中的Binder也可以看作是Server Binder的‘代理’，在本地代表远端Server为Client提供服务。本文中会使用‘引用’或‘句柄’这个两广泛使用的术语。 &#160; &#160; &#160; &#160; 面向对象思想的引入将进程间通信转化为通过对某个Binder对象的引用调用该对象的方法，而其独特之处在于Binder对象是一个可以跨进程引用的对象，它的实体位于一个进程中，而它的引用却遍布于系统的各个进程之中。最诱人的是，这个引用和Java里引用一样既可以是强类型，也可以是弱类型，而且可以从一个进程传给其它进程，让大家都能访问同一Server，就象将一个对象或引用赋值给另一个引用一样。Binder模糊了进程边界，淡化了进程间通信过程，整个系统仿佛运行于同一个面向对象的程序之中。形形色色的Binder对象以及星罗棋布的引用仿佛粘接各个应用程序的胶水，这也是Binder在英文里的原意。 &#160; &#160; &#160; &#160; 当然面向对象只是针对应用程序而言，对于Binder驱动和内核其它模块一样使用C语言实现，没有类和对象的概念。Binder驱动为面向对象的进程间通信提供底层支持。 Binder 通信模型&#160; &#160; &#160; &#160; 从英文字面上意思看，Binder具有粘结剂的意思，那么它把什么东西粘结在一起呢？在Android系统的Binder机制中，由一系统组件组成，分别是Client、Server、Service Manager和Binder驱动程序，其中Client、Server和Service Manager运行在用户空间，Binder驱动程序运行内核空间。Binder就是一种把这四个组件粘合在一起的粘结剂了，其中，核心组件便是Binder驱动程序了，Service Manager提供了辅助管理的功能，Client和Server正是在Binder驱动和Service Manager提供的基础设施上，进行Client-Server之间的通信。这四个角色的关系和互联网类似：Server是服务器，Client是客户终端，Service Manager是域名服务器（DNS），Binder驱动是路由器。 Binder驱动&#160; &#160; &#160; &#160; 和路由器一样，Binder驱动虽然默默无闻，却是通信的核心。尽管名叫‘驱动’，实际上和硬件设备没有任何关系，只是实现方式和设备驱动程序是一样的：它工作于内核态，提供open()，mmap()，poll()，ioctl()等标准文件操作，以字符驱动设备中的misc设备注册在设备目录/dev下，用户通过/dev/binder访问该它。驱动负责进程之间Binder通信的建立，Binder在进程之间的传递，Binder引用计数管理，数据包在进程之间的传递和交互等一系列底层支持。驱动和应用程序之间定义了一套接口协议，主要功能由ioctl()接口实现，不提供read()，write()接口，因为ioctl()灵活方便，且能够一次调用实现先写后读以满足同步交互，而不必分别调用write()和read()。Binder驱动的代码每个分支位置不一样，再加上我也没有下内核的代码，这里先给个4.4的Binder.c的地址,有兴趣的可以自己研究。 ServiceManager 与实名Binder&#160; &#160; &#160; &#160; 和DNS类似，ServiceManager的作用是将字符形式的Binder名字转化成Client中对该Binder的引用，使得Client能够通过Binder名字获得对Server中Binder实体的引用。注册了名字的Binder叫实名Binder，就象每个网站除了有IP地址外还有自己的网址。Server创建了Binder实体，为其取一个字符形式，可读易记的名字，将这个Binder连同名字以数据包的形式通过Binder驱动发送给ServiceManager，通知ServiceManager注册一个名叫张三的Binder，它位于某个Server中。驱动为这个穿过进程边界的Binder创建位于内核中的实体节点以及ServiceManager对实体的引用，将名字及新建的引用打包传递给ServiceManager。ServiceManager收数据包后，从中取出名字和引用填入一张查找表中。 &#160; &#160; &#160; &#160; 细心的读者可能会发现其中的蹊跷：ServiceManager是一个进程，Server是另一个进程，Server向ServiceManager注册Binder必然会涉及进程间通信。当前实现的是进程间通信却又要用到进程间通信，这就好象蛋可以孵出鸡前提却是要找只鸡来孵蛋。Binder的实现比较巧妙：预先创造一只鸡来孵蛋：ServiceManager和其它进程同样采用Binder通信，ServiceManager是Server端，有自己的Binder对象（实体），其它进程都是Client，需要通过这个Binder的引用来实现Binder的注册，查询和获取。ServiceManager提供的Binder比较特殊，它没有名字也不需要注册，当一个进程使用BINDER_SET_CONTEXT_MGR命令将自己注册成ServiceManager（会用到ioctl(fd, cmd, arg)函数，cmd为BINDER_SET_CONTEXT_MGR）时Binder驱动会自动为它创建Binder实体（这就是那只预先造好的鸡）。其次这个Binder的引用在所有Client中都固定为0而无须通过其它手段获得。也就是说，一个Server若要向ServiceManager注册自己Binder就必需通过0（即NULL指针）这个引用号和ServiceManager的Binder通信。类比网络通信，0号引用就好比域名服务器的地址，你必须预先手工或动态配置好。要注意这里说的Client是相对ServiceManager而言的，一个应用程序可能是个提供服务的Server，但对ServiceManager来说它仍然是个Client。 Client 获得实名Binder的引用&#160; &#160; &#160; &#160; Server向ServiceManager注册了Binder实体及其名字后，Client就可以通过名字获得该Binder的引用了。Client也利用保留的0号引用向ServiceManager请求访问某个Binder：我申请获得名字叫张三的Binder的引用。ServiceManager收到这个连接请求，从请求数据包里获得Binder的名字，在查找表里找到该名字对应的条目，从条目中取出Binder的引用，将该引用作为回复发送给发起请求的Client。从面向对象的角度，这个Binder对象现在有了两个引用：一个位于ServiceManager中，一个位于发起请求的Client中。如果接下来有更多的Client请求该Binder，系统中就会有更多的引用指向该Binder，就象java里一个对象存在多个引用一样。而且类似的这些指向Binder的引用是强类型，从而确保只要有引用Binder实体就不会被释放掉。通过以上过程可以看出，ServiceManager象个火车票代售点，收集了所有火车的车票，可以通过它购买到乘坐各趟火车的票-得到某个Binder的引用。 匿名 Binder&#160; &#160; &#160; &#160; 并不是所有Binder都需要注册给ServiceManager广而告之的。Server端可以通过已经建立的Binder连接将创建的Binder实体传给Client，当然这条已经建立的Binder连接必须是通过实名Binder实现。如果我们是从事application开发，跨进程的自己手写AIDL文件，或者相同进程的bindService自己添加一个继承Binder的子类，那么这个Binder没有向ServiceManager注册名字，所以是个匿名Binder。Client将会收到这个匿名Binder的引用，通过这个引用向位于Server中的实体发送请求。匿名Binder为通信双方建立一条私密通道，只要Server没有把匿名Binder发给别的进程，别的进程就无法通过穷举或猜测等任何方式获得该Binder的引用，向该Binder发送请求。 好了，理论性的科普先到这里，再继续下去估计要扑（pu）街（gai）了 =。=，先来美图放松放松。 Binder机制跨进程原理&#160; &#160; &#160; &#160; 上文给出了Binder的通信模型，指出了通信过程的四个角色: Client, Server, ServiceManager, driver; 但是我们仍然不清楚Client到底是如何与Server完成通信的,因为实现细节我们还没讲。 &#160; &#160; &#160; &#160; 虽然Binder用到了面向对象的思想，但并不限制应用程序一定要使用面向对象的语言，无论是C语言还是C++语言都可以很容易的使用Binder来通信。例如尽管Android主要使用java/C++，象ServiceManager这么重要的进程就是用C语言实现的。 &#160; &#160; &#160; &#160; Binder本质上只是一种底层通信方式，和具体服务没有关系。为了提供具体服务，Server必须提供一套接口函数以便Client通过远程访问使用各种服务。这时通常采用Proxy设计模式：将接口函数定义在一个抽象类中，Server和Client都会以该抽象类为基类实现所有接口函数，所不同的是Server端是真正的功能实现，而Client端是对这些函数远程调用请求的包装。如何将Binder和Proxy设计模式结合起来是应用程序实现面向对象Binder通信的根本问题。 Binder 在Server端的表述 – Binder实体&#160; &#160; &#160; &#160; 做为Proxy设计模式的基础，首先定义一个抽象接口类封装Server所有功能，其中包含一系列纯虚函数留待Server和Proxy各自实现（如果是java层则为接口方法，或者是AIDL文件，编译后大同小异）。由于这些函数需要跨进程调用，须为其一一编号，从而Server可以根据收到的编号决定调用哪个函数。其次就要引入Binder了。Server端定义另一个Binder抽象类处理来自Client的Binder请求数据包，其中最重要的成员是虚函数onTransact()。该函数分析收到的数据包，调用相应的接口函数处理请求。 &#160; &#160; &#160; &#160; 接下来采用继承方式以接口类和Binder抽象类为基类构建Binder在Server中的实体，实现基类里所有的虚函数，包括公共接口函数以及数据包处理函数：onTransact()。这个函数的输入是来自Client的binder_transaction_data结构的数据包（java层为Parcel对象，对应native层还是这个结构体某一部分）。前面提到，该结构里有个成员code，包含这次请求的接口函数编号。onTransact()将case-by-case地解析code值，从数据包里取出函数参数，调用接口类中相应的，已经实现的公共接口函数。函数执行完毕，如果需要返回数据就再构建一个binder_transaction_data包将返回数据包填入其中。 &#160; &#160; &#160; &#160; 那么各个Binder实体的onTransact()又是什么时候调用呢？这就需要驱动参与了。前面说过，Binder实体须要以Binde传输结构flat_binder_object形式发送给其它进程才能建立Binder通信，而Binder实体指针就存放在该结构的handle域中。驱动根据Binder位置数组从传输数据中获取该Binder的传输结构，为它创建位于内核中的Binder节点，将Binder实体指针记录在该节点中。如果接下来有其它进程向该Binder发送数据，驱动会根据节点中记录的信息将Binder实体指针填入binder_transaction_data的target.ptr中返回给接收线程。接收线程从数据包中取出该指针，reinterpret_cast成Binder抽象类并调用onTransact()函数。由于这是个虚函数，不同的Binder实体中有各自的实现，从而可以调用到不同Binder实体提供的onTransact()。 Binder 在Client端的表述 – Binder引用&#160; &#160; &#160; &#160; 做为Proxy设计模式的一部分，Client端的Binder同样要继承Server提供的公共接口类并实现公共函数。但这不是真正的实现，而是对远程函数调用的包装：将函数参数打包，通过Binder向Server发送申请并等待返回值。为此Client端的Binder还要知道Binder实体的相关信息，即对Binder实体的引用。该引用或是由ServiceManager转发过来的，对实名Binder的引用或是由另一个进程直接发送过来的，对匿名Binder的引用。 &#160; &#160; &#160; &#160; 由于继承了同样的公共接口类，Client Binder提供了与Server Binder一样的函数原型，使用户感觉不出Server是运行在本地还是远端。Client Binder中，公共接口函数的包装方式是：创建一个binder_transaction_data数据包，将其对应的编码填入code域，将调用该函数所需的参数填入data.buffer指向的缓存中，并指明数据包的目的地，那就是已经获得的对Binder实体的引用，填入数据包的target.handle中。注意这里和Server的区别：实际上target域是个联合体，包括ptr和handle两个成员，前者用于接收数据包的Server，指向 Binder实体对应的内存空间；后者用于作为请求方的Client，存放Binder实体的引用，告知驱动数据包将路由给哪个实体。数据包准备好后，通过驱动接口发送出去。经过BC_TRANSACTION/BC_REPLY回合完成函数的远程调用并得到返回值。 &#160; &#160; &#160; &#160; 以上主要是C++层实现步骤，java层也相同原理，只不过最终还会通过JNI调用C++层实现方法。&#160; &#160; &#160; &#160; 我们举个栗子来描述会更通俗易懂一些： &#160; &#160; &#160; &#160; 假设Client进程想要调用Server进程的object对象的一个方法add;&#160; &#160; &#160; &#160; 首先，Server进程要向ServiceManager注册；告诉自己是谁，自己有什么能力；在这个场景就是Server告诉ServiceManager，它叫zhangsan，它有一个object对象，可以执行add 操作；于是ServiceManager建立了一张表：zhangsan这个名字对应进程Server; &#160; &#160; &#160; &#160; 然后Client向ServiceManager查询：我需要联系一个名字叫做zhangsan的进程里面的object对象；这时候关键来了：进程之间通信的数据都会经过运行在内核空间里面的驱动，驱动在数据流过的时候做了一点手脚，它并不会给Client进程返回一个真正的object对象，而是返回一个看起来跟object一模一样的代理对象objectProxy，这个objectProxy也有一个add方法，但是这个add方法没有Server进程里面object对象的add方法那个能力；objectProxy的add只是一个傀儡，它唯一做的事情就是把参数包装然后交给驱动。(这里我们简化了ServiceManager的流程) &#160; &#160; &#160; &#160; 但是Client进程并不知道驱动返回给它的对象动过手脚，毕竟伪装的太像了，如假包换。Client开开心心地拿着objectProxy对象然后调用add方法；我们说过，这个add什么也不做，直接把参数做一些包装然后直接转发给Binder驱动。 &#160; &#160; &#160; &#160; 驱动收到这个消息，发现是这个objectProxy；一查表就明白了：我之前用objectProxy替换了object发送给Client了，它真正应该要访问的是object对象的add方法；于是Binder驱动通知Server进程，调用你的object对象的add方法，然后把结果发给我，Sever进程收到这个消息，照做之后将结果返回驱动，驱动然后把结果返回给Client进程；于是整个过程就完成了。 &#160; &#160; &#160; &#160; 由于驱动返回的objectProxy与Server进程里面原始的object是如此相似，给人感觉好像是直接把Server进程里面的对象object传递到了Client进程 ；因此，我们可以说Binder对象是可以进行跨进程传递的对象。 &#160; &#160; &#160; &#160; 但事实上我们知道，Binder跨进程传输并不是真的把一个对象传输到了另外一个进程；传输过程好像是Binder跨进程穿越的时候，它在一个进程留下了一个真身，在另外一个进程幻化出一个影子（这个影子可以很多个）；Client进程的操作其实是对于影子的操作，影子利用Binder驱动最终让真身完成操作。 &#160; &#160; &#160; &#160; 理解这一点非常重要；务必仔细体会。另外，Android系统实现这种机制使用的是代理模式, 对于Binder的访问，如果是在同一个进程（不需要跨进程），那么直接返回原始的Binder实体；如果在不同进程，那么就给他一个代理对象（影子）；我们在系统源码以及AIDL的生成代码里面可以看到很多这种实现。 &#160; &#160; &#160; &#160; 另外我们为了简化整个流程，隐藏了ServiceManager这一部分驱动进行的操作；实际上，由于ServiceManager与Server通常不在一个进程，Server进程向ServiceManager注册的过程也是跨进程通信，驱动也会对这个过程进行暗箱操作：ServiceManager中存在的Server端的对象实际上也是代理对象，后面Client向ServiceManager查询的时候，驱动会给Client返回另外一个代理对象。Sever进程的本地对象仅有一个，其他进程所拥有的全部都是它的代理。 &#160; &#160; &#160; &#160; 一句话总结就是：Client进程只不过是持有了Server端的代理；代理对象协助驱动完成了跨进程通信。 分析Java层的Binder&#160; &#160; &#160; &#160; Android系统的很多服务都是由C++层的Binder实现，过程复杂先不说，主要是代码量巨多，分析起来篇幅远远不够，即使不分析完全，估计粘一半代码都快吐血了，所以就选了java层的Binder，会重点分析AIDL过程。 &#160; &#160; &#160; &#160; 我们使用AIDL接口的时候，经常会接触到这些类：IBinder/IInterface/Binder/BinderProxy/Stub，相关功能如下： IBinder是一个接口，它代表了一种跨进程传输的能力；只要实现了这个接口，就能将这个对象进行跨进程传递；这是驱动底层支持的；在跨进程数据流经驱动的时候，驱动会识别IBinder类型的数据，从而自动完成不同进程Binder本地对象以及Binder代理对象的转换。 IBinder负责数据传输，那么client与server端的调用契约（这里不用接口避免混淆）呢？这里的IInterface代表的就是远程server对象具有什么能力。具体来说，就是aidl里面的接口。 Java层的Binder类，代表的其实就是Binder本地对象。BinderProxy类是Binder类的一个内部类，它代表远程进程的Binder对象的本地代理；这两个类都继承自IBinder, 因而都具有跨进程传输的能力；实际上，在跨越进程的时候，Binder驱动会自动完成这两个对象的转换。 在使用AIDL的时候，编译工具会给我们生成一个Stub的静态内部类；这个类继承了Binder, 说明它是一个Binder本地对象，它实现了IInterface接口，表明它具有远程Server承诺给Client的能力；Stub是一个抽象类，具体的IInterface的相关实现需要我们手动完成，这里使用了策略模式。 AIDL过程分析&#160; &#160; &#160; &#160; 现在我们通过一个AIDL的使用，分析一下整个通信过程中，各个角色到底做了什么，AIDL到底是如何完成通信的。&#160; &#160; &#160; &#160; 首先定一个一个简单的aidl接口：12345//ITest.aidlpackage com.windrunnerlihuan;interface ITest&#123; int add(int a, int b);&#125; 然后用编译工具编译之后，可以得到对应的ITest.java类：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package com.windrunnerlihuan;public interface ITest extends android.os.IInterface &#123; /** Local-side IPC implementation stub class. */ public static abstract class Stub extends android.os.Binder implements com.windrunnerlihuan.ITest &#123; private static final java.lang.String DESCRIPTOR = "com.windrunnerlihuan.ITest"; /** Construct the stub at attach it to the interface. */ public Stub() &#123; this.attachInterface(this, DESCRIPTOR); &#125; /** * Cast an IBinder object into an com.windrunnerlihuan.ITest interface, * generating a proxy if needed. */ public static com.windrunnerlihuan.ITest asInterface( android.os.IBinder obj) &#123; if ((obj == null)) &#123; return null; &#125; android.os.IInterface iin = obj.queryLocalInterface(DESCRIPTOR); if (((iin != null) &amp;&amp; (iin instanceof com.windrunnerlihuan.ITest))) &#123; return ((com.windrunnerlihuan.ITest) iin); &#125; return new com.windrunnerlihuan.ITest.Stub.Proxy(obj); &#125; @Override public android.os.IBinder asBinder() &#123; return this; &#125; @Override public boolean onTransact(int code, android.os.Parcel data, android.os.Parcel reply, int flags) throws android.os.RemoteException &#123; switch (code) &#123; case INTERFACE_TRANSACTION: &#123; reply.writeString(DESCRIPTOR); return true; &#125; case TRANSACTION_add: &#123; data.enforceInterface(DESCRIPTOR); int _arg0; _arg0 = data.readInt(); int _arg1; _arg1 = data.readInt(); int _result = this.add(_arg0, _arg1); reply.writeNoException(); reply.writeInt(_result); return true; &#125; &#125; return super.onTransact(code, data, reply, flags); &#125; private static class Proxy implements com.windrunnerlihuan.ITest &#123; private android.os.IBinder mRemote; Proxy(android.os.IBinder remote) &#123; mRemote = remote; &#125; @Override public android.os.IBinder asBinder() &#123; return mRemote; &#125; public java.lang.String getInterfaceDescriptor() &#123; return DESCRIPTOR; &#125; @Override public int add(int a, int b) throws android.os.RemoteException &#123; android.os.Parcel _data = android.os.Parcel.obtain(); android.os.Parcel _reply = android.os.Parcel.obtain(); int _result; try &#123; _data.writeInterfaceToken(DESCRIPTOR); _data.writeInt(a); _data.writeInt(b); mRemote.transact(Stub.TRANSACTION_add, _data, _reply, 0); _reply.readException(); _result = _reply.readInt(); &#125; finally &#123; _reply.recycle(); _data.recycle(); &#125; return _result; &#125; &#125; static final int TRANSACTION_add = (android.os.IBinder.FIRST_CALL_TRANSACTION + 0); &#125; public int add(int a, int b) throws android.os.RemoteException;&#125; &#160; &#160; &#160; &#160; 系统帮我们生成了这个文件之后，我们只需要继承ITest.Stub这个抽象类，实现它的方法，然后在Service 的onBind方法里面返回就实现了AIDL。这个Stub类非常重要，具体看看它做了什么。 &#160; &#160; &#160; &#160; Stub类继承自Binder，意味着这个Stub其实自己是一个Binder本地对象，然后实现了ITest接口，ITest本身是一个IInterface，因此他携带某种客户端需要的能力（这里是方法add)。此类有一个内部类Proxy，也就是Binder代理对象； &#160; &#160; &#160; &#160; 然后看看asInterface方法，我们在bind一个Service之后，在onServiceConnecttion的回调里面，就是通过这个方法拿到一个远程的service的，这个方法做了什么呢？123456789101112131415/** * Cast an IBinder object into an com.windrunnerlihuan.ITest interface, * generating a proxy if needed. */public static com.windrunnerlihuan.ITest asInterface( android.os.IBinder obj) &#123; if ((obj == null)) &#123; return null; &#125; android.os.IInterface iin = obj.queryLocalInterface(DESCRIPTOR); if (((iin != null) &amp;&amp; (iin instanceof com.windrunnerlihuan.ITest))) &#123; return ((com.windrunnerlihuan.ITest) iin); &#125; return new com.windrunnerlihuan.ITest.Stub.Proxy(obj);&#125; &#160; &#160; &#160; &#160; 首先看函数的参数IBinder类型的obj，这个对象是驱动给我们的，如果是Binder本地对象，那么它就是Binder类型，如果是Binder代理对象，那就是BinderProxy类型；然后，正如上面自动生成的文档所说，它会试着查找Binder本地对象，如果找到，说明Client和Server都在同一个进程，这个参数直接就是本地对象，直接强制类型转换然后返回，如果找不到，说明是远程对象（处于另外一个进程）那么就需要创建一个Binde代理对象，让这个Binder代理实现对于远程对象的访问。一般来说，如果是与一个远程Service对象进行通信，那么这里返回的一定是一个Binder代理对象，这个IBinder参数的实际上是BinderProxy; &#160; &#160; &#160; &#160; 再看看我们对于aidl的add 方法的实现；在Stub类里面，add是一个抽象方法，我们需要继承这个类并实现它；如果Client和Server在同一个进程，那么直接就是调用这个方法；那么，如果是远程调用，这中间发生了什么呢？Client是如何调用到Server的方法的？ &#160; &#160; &#160; &#160; 我们知道，对于远程方法的调用，是通过Binder代理完成的，在这个例子里面就是Proxy类；Proxy对于add方法的实现如下：123456789101112131415161718@Overridepublic int add(int a, int b) throws android.os.RemoteException &#123; android.os.Parcel _data = android.os.Parcel.obtain(); android.os.Parcel _reply = android.os.Parcel.obtain(); int _result; try &#123; _data.writeInterfaceToken(DESCRIPTOR); _data.writeInt(a); _data.writeInt(b); mRemote.transact(Stub.TRANSACTION_add, _data, _reply, 0); _reply.readException(); _result = _reply.readInt(); &#125; finally &#123; _reply.recycle(); _data.recycle(); &#125; return _result;&#125; &#160; &#160; &#160; &#160; 它首先用Parcel把数据序列化了，然后调用了transact方法；这个transact到底做了什么呢？这个Proxy类在asInterface方法里面被创建，前面提到过，如果是Binder代理那么说明驱动返回的IBinder实际是BinderProxy, 因此我们的Proxy类里面的mRemote实际类型应该是BinderProxy；我们看看BinderProxy的transact方法：(Binder.java的内部类)123456 public boolean transact(int code, Parcel data, Parcel reply, int flags) throws RemoteException &#123; Binder.checkParcel(this, code, data, "Unreasonably large binder buffer"); return transactNative(code, data, reply, flags);&#125; public native boolean transactNative(int code, Parcel data, Parcel reply, int flags) throws RemoteException; 这是一个本地方法；它的实现在native层，具体来说在frameworks/base/core/jni/android_util_Binder.cpp文件:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859static jboolean android_os_BinderProxy_transact(JNIEnv* env, jobject obj, jint code, jobject dataObj, jobject replyObj, jint flags) // throws RemoteException&#123; //这段不用看 if (dataObj == NULL) &#123; jniThrowNullPointerException(env, NULL); return JNI_FALSE; &#125; //把java对象解析为C++对象 Parcel* data = parcelForJavaObject(env, dataObj); if (data == NULL) &#123; return JNI_FALSE; &#125; //把java对象解析为C++对象 Parcel* reply = parcelForJavaObject(env, replyObj); if (reply == NULL &amp;&amp; replyObj != NULL) &#123; return JNI_FALSE; &#125; /*这部分得说明一下，这是个坑，有兴趣的可以挖：在获取Service Manager远程接口时，在JNI层中，创建了一个BpBinder对象，它的句柄值为0，它的地址保存在gBinderProxyOffsets.mObject中，因此，这里通过下面语句得到这个BpBinder对象的IBinder接口：*/ IBinder* target = (IBinder*) env-&gt;GetLongField(obj, gBinderProxyOffsets.mObject); if (target == NULL) &#123; jniThrowException(env, "java/lang/IllegalStateException", "Binder has been finalized!"); return JNI_FALSE; &#125; //打印LOG，忽略 ALOGV("Java code calling transact on %p in Java object %p with code %" PRId32 "\n", target, obj, code);//干扰视线的宏，忽略#if ENABLE_BINDER_SAMPLE // Only log the binder call duration for things on the Java-level main thread. // But if we don't const bool time_binder_calls = should_time_binder_calls(); int64_t start_millis; if (time_binder_calls) &#123; start_millis = uptimeMillis(); &#125;#endif //这段才是重点 //printf("Transact from Java code to %p sending: ", target); data-&gt;print(); status_t err = target-&gt;transact(code, *data, reply, flags); //if (reply) printf("Transact from Java code to %p received: ", target); reply-&gt;print();//下面都可以忽略了 #if ENABLE_BINDER_SAMPLE if (time_binder_calls) &#123; conditionally_log_binder_call(start_millis, target, code); &#125;#endif if (err == NO_ERROR) &#123; return JNI_TRUE; &#125; else if (err == UNKNOWN_TRANSACTION) &#123; return JNI_FALSE; &#125; signalExceptionForError(env, obj, err, true /*canThrowRemoteException*/); return JNI_FALSE;&#125; &#160; &#160; &#160; &#160; 我在注释中说这儿有个坑，有兴趣的就去挖吧，就是在获取Service Manager远程接口时，在JNI层中，创建了一个BpBinder对象，它的句柄值为0，它的地址保存在gBinderProxyOffsets.mObject中，因此，还得去找BpBinder的transact方法，BpBinder的位置在framework\native\libs\binder\BpBinder.cpp:1234567891011121314status_t BpBinder::transact( uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags)&#123; // Once a binder has died, it will never come back to life. if (mAlive) &#123; //唉，又是个坑，继续挖IPCThreadState status_t status = IPCThreadState::self()-&gt;transact( mHandle, code, data, reply, flags); if (status == DEAD_OBJECT) mAlive = 0; return status; &#125; return DEAD_OBJECT;&#125; &#160; &#160; &#160; &#160; 继续挖IPCThreadState，位于framework\native\libs\binder\IPCThreadState.cpp(这个self函数就不贴了，就是个单例写法):1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798status_t IPCThreadState::transact(int32_t handle, uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags)&#123; status_t err = data.errorCheck(); flags |= TF_ACCEPT_FDS; if (err == NO_ERROR) &#123; //调用writeTransactionData 发送数据 err = writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL); &#125; if ((flags &amp; TF_ONE_WAY) == 0) &#123; if (reply) &#123; err = waitForResponse(reply); &#125; else &#123; Parcel fakeReply; err = waitForResponse(&amp;fakeReply); &#125; ....等回复 err = waitForResponse(NULL, NULL); .... return err;&#125;再进一步，瞧瞧这个...status_t IPCThreadState::writeTransactionData(int32_t cmd, uint32_t binderFlags, int32_t handle, uint32_t code, const Parcel&amp; data, status_t* statusBuffer)&#123; binder_transaction_data tr; tr.target.handle = handle; tr.code = code; tr.flags = binderFlags; const status_t err = data.errorCheck(); if (err == NO_ERROR) &#123; tr.data_size = data.ipcDataSize(); tr.data.ptr.buffer = data.ipcData(); tr.offsets_size = data.ipcObjectsCount()*sizeof(size_t); tr.data.ptr.offsets = data.ipcObjects(); &#125; ....//上面把命令数据封装成binder_transaction_data，然后写到mOut中，mOut是命令的缓冲区，也是一个Parcel mOut.writeInt32(cmd); mOut.write(&amp;tr, sizeof(tr));//仅仅写到了Parcel中，Parcel好像没和/dev/binder设备有什么关联啊？//恩，那只能在另外一个地方写到binder设备中去了。难道是在？ return NO_ERROR;&#125;//说对了，就是在waitForResponse中status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult)&#123; int32_t cmd; int32_t err;while (1) &#123;//talkWithDriver，哈哈，应该是这里了 if ((err=talkWithDriver()) &lt; NO_ERROR) break; err = mIn.errorCheck(); if (err &lt; NO_ERROR) break; if (mIn.dataAvail() == 0) continue; //看见没？这里开始操作mIn了，看来talkWithDriver中//把mOut发出去，然后从driver中读到数据放到mIn中了。 cmd = mIn.readInt32(); switch (cmd) &#123; case BR_TRANSACTION_COMPLETE: if (!reply &amp;&amp; !acquireResult) goto finish; break; ..... return err;&#125;status_t IPCThreadState::talkWithDriver(bool doReceive)&#123;binder_write_read bwr; //中间东西太复杂了，不就是把mOut数据和mIn接收数据的处理后赋值给bwr吗？ status_t err; do &#123;//用ioctl来读写 if (ioctl(mProcess-&gt;mDriverFD, BINDER_WRITE_READ, &amp;bwr) &gt;= 0) err = NO_ERROR; else err = -errno; &#125; while (err == -EINTR);//到这里，回复数据就在bwr中了，bmr接收回复数据的buffer就是mIn提供的 if (bwr.read_consumed &gt; 0) &#123; mIn.setDataSize(bwr.read_consumed); mIn.setDataPosition(0); &#125;return NO_ERROR;&#125; &#160; &#160; &#160; &#160; 里面进行了一系列的函数调用，确实是个巨恶心的过程；要知道的是它最终调用到了talkWithDriver函数；看这个函数的名字就知道，通信过程要交给驱动完成了（驱动调用是个更恶心的过程，感觉真不是人看的。。。。。）；&#160; &#160; &#160; &#160; （以下过程限于篇幅和巨恶心的过程，所以简要概括，有兴趣的可以自己去挖坑=。=）&#160; &#160; &#160; &#160; 这个函数最后通过ioctl系统调用，Client进程陷入内核态，Client调用add方法的线程挂起等待返回；驱动完成一系列的操作之后唤醒Server进程，调用了Server进程本地对象的onTransact函数（实际上由Server端线程池完成）。我们再看Binder本地对象的onTransact方法（这里就是Stub类里面的此方法）：1234567891011121314151617181920212223@Overridepublic boolean onTransact(int code, android.os.Parcel data, android.os.Parcel reply, int flags) throws android.os.RemoteException &#123; switch (code) &#123; case INTERFACE_TRANSACTION: &#123; reply.writeString(DESCRIPTOR); return true; &#125; case TRANSACTION_add: &#123; data.enforceInterface(DESCRIPTOR); int _arg0; _arg0 = data.readInt(); int _arg1; _arg1 = data.readInt(); int _result = this.add(_arg0, _arg1); reply.writeNoException(); reply.writeInt(_result); return true; &#125; &#125; return super.onTransact(code, data, reply, flags);&#125; &#160; &#160; &#160; &#160; 在Server进程里面，onTransact根据调用号（每个AIDL函数都有一个编号，在跨进程的时候，不会传递函数，而是传递编号指明调用哪个函数）调用相关函数；在这个例子里面，调用了Binder本地对象的add方法；这个方法将结果返回给驱动，驱动唤醒挂起的Client进程里面的线程并将结果返回。于是一次跨进程调用就完成了。 &#160; &#160; &#160; &#160; 至此，你应该对AIDL这种通信方式里面的各个类以及各个角色有了一定的了解；它总是那么一种固定的模式：一个需要跨进程传递的对象一定继承自IBinder，如果是Binder本地对象，那么一定继承Binder实现IInterface，如果是代理对象，那么就实现了IInterface并持有了IBinder引用； &#160; &#160; &#160; &#160; Proxy与Stub不一样，虽然他们都既是Binder又是IInterface，不同的是Stub采用的是继承（is 关系），Proxy采用的是组合（has 关系）。他们均实现了所有的IInterface函数，不同的是，Stub又使用策略模式调用的是虚函数（待子类实现），而Proxy则使用组合模式。为什么Stub采用继承而Proxy采用组合？事实上，Stub本身is一个IBinder（Binder），它本身就是一个能跨越进程边界传输的对象，所以它得继承IBinder实现transact这个函数从而得到跨越进程的能力（这个能力由驱动赋予）。Proxy类使用组合，是因为他不关心自己是什么，它也不需要跨越进程传输，它只需要拥有这个能力即可，要拥有这个能力，只需要保留一个对IBinder的引用。如果把这个过程做一个类比，在封建社会，Stub好比皇帝，可以号令天下，他生而具有这个权利（不要说宣扬封建迷信。。）如果一个人也想号令天下，可以，“挟天子以令诸侯”。为什么不自己去当皇帝，其一，一般情况没必要，当了皇帝其实限制也蛮多的是不是？我现在既能掌管天下，又能不受约束（Java单继承）；其二，名不正言不顺啊，我本来特么就不是（Binder），你非要我是说不过去，搞不好还会造反。最后呢，如果想当皇帝也可以，那就是asBinder了。在Stub类里面，asBinder返回this，在Proxy里面返回的是持有的组合类IBinder的引用。 &#160; &#160; &#160; &#160; 再去翻阅系统的ActivityManagerServer的源码，就知道哪一个类是什么角色了：IActivityManager是一个IInterface，它代表远程Service具有什么能力，ActivityManagerNative指的是Binder本地对象（类似AIDL工具生成的Stub类），这个类是抽象类，它的实现是ActivityManagerService；因此对于AMS的最终操作都会进入ActivityManagerService这个真正实现；同时如果仔细观察，ActivityManagerNative.java里面有一个非公开类ActivityManagerProxy, 它代表的就是Binder代理对象；是不是跟AIDL模型一模一样呢？那么ActivityManager是什么？他不过是一个管理类而已，可以看到真正的操作都是转发给ActivityManagerNative进而交给他的实现ActivityManagerService 完成的。 结语&#160; &#160; &#160; &#160; 至此分析Binder就告一段落了，也是一个越往下挖越恶心的东西。等以后有机会系统学一下Linux内核的知识。虽然兴趣是最好的老师，但是学习也是个循序渐进的过程，不可能一口吃成大胖子。知识的积累也是需要脚踏实地，一步一个脚印。量变引起质变，见多识广之后才能厚积薄发，逐渐形成自己的思维，使自己成长的更强大。同时，积累过程最忌浮躁，不可因外界环境干扰心境，要始终勿忘初心，心态保持平静，不随波逐流，上善若水。]]></content>
      <categories>
        <category>Android技术点</category>
      </categories>
      <tags>
        <tag>Binder</tag>
        <tag>IPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[移动直播技术秒开优化经验]]></title>
    <url>%2F2016%2F06%2F06%2F%E7%A7%BB%E5%8A%A8%E7%9B%B4%E6%92%AD%E6%8A%80%E6%9C%AF%E7%A7%92%E5%BC%80%E4%BC%98%E5%8C%96%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;以下内容来自@高可用架构，这次当一回技术的搬运工，为大家分享七牛创始人之一————徐立的移动直播技术秒开优化经验。 &#160; &#160; &#160; &#160;徐立，七牛创始合伙人兼产品副总裁，负责七牛直播云的整体研发，是国内 Go / Docker / Container 技术早期布道者，&gt;Go / Containers / Distributed Systems 技术的忠实爱好者和实践者。曾合著国内第一本 Go 语言图书《Go 语言编程》，翻译《Go 语言程序设计》。 &#160; &#160; &#160; &#160;现今移动直播技术上的挑战要远远难于传统设备或电脑直播，其完整的处理环节包括但不限于：音视频采集、美颜/滤镜/特效处理、编码、封包、推流、转码、分发、解码/渲染/播放等。 直播常见的问题包括： 主播在不稳定的网络环境下如何稳定推流？ 偏远地区的观众如何高清流畅观看直播？ 直播卡顿时如何智能切换线路？ 如何精确度量直播质量指标并实时调整？ 移动设备上不同的芯片平台如何高性能编码和渲染视频？ 美颜等滤镜特效处理怎么做？ 如何实现播放秒开？ 如何保障直播持续播放流畅不卡顿？ 本次分享将为大家揭开移动直播核心技术的神秘面纱。 视频、直播等基础知识什么是视频？&#160; &#160; &#160; &#160;首先我们需要理解一个最基本的概念：视频。从感性的角度来看，视频就是一部充满趣味的影片，可以是电影，可以是短片，是一连贯的视觉冲击力表现丰富的画面和音频。但从理性的角度来看，视频是一种有结构的数据，用工程的语言解释，我们可以把视频剖析成如下结构：内容元素 ( Content ) 图像 ( Image ) 音频 ( Audio ) 元信息 ( Metadata ) 编码格式 ( Codec ) Video : H.264，H.265, … Audio : AAC， HE-AAC, … 容器封装 (Container) MP4，MOV，FLV，RM，RMVB，AVI，… 任何一个视频 Video 文件，从结构上讲，都是这样一种组成方式： 由图像和音频构成最基本的内容元素； 图像经过视频编码压缩格式处理（通常是 H.264）； 音频经过音频编码压缩格式处理（例如 AAC）； 注明相应的元信息（Metadata）； 最后经过一遍容器（Container）封装打包（例如 MP4），构成一个完整的视频文件。 &#160; &#160; &#160; &#160;如果觉得难以理解，可以想象成一瓶番茄酱。最外层的瓶子好比这个容器封装（Container），瓶子上注明的原材料和加工厂地等信息好比元信息（Metadata），瓶盖打开（解封装）后，番茄酱本身好比经过压缩处理过后的编码内容，番茄和调料加工成番茄酱的过程就好比编码（Codec），而原材料番茄和调料则好比最原本的内容元素（Content）。 视频的实时传输&#160; &#160; &#160; &#160;简而言之，理性的认知视频的结构后，有助于我们理解视频直播。如果视频是一种“有结构的数据”，那么视频直播无疑是实时传输这种“有结构的数据”（视频）的方式。 &#160; &#160; &#160; &#160;那么一个显而易见的问题是：如何实时（Real-Time）传输这种“有结构的数据”（视频）呢？ &#160; &#160; &#160; &#160;这里边一个悖论是：一个经过容器（Container）封装后的视频，一定是不可变的 ( Immutable ) 视频文件，不可变的 ( Immutable ) 的视频文件已经是一个生产结果，根据“相对论”，而这个生产结果显然不可能精确到实时的程度，它已经是一段时空的记忆。 &#160; &#160; &#160; &#160;因此视频直播，一定是一个 “边生产，边传输，边消费” 的过程。这意味着，我们需要更近一步了解视频从原始的内容元素 ( 图像和音频 ) 到成品 ( 视频文件 ) 之前的中间过程 ( 编码 )。 视频编码压缩&#160; &#160; &#160; &#160;不妨让我们来深入浅出理解视频编码压缩技术。 &#160; &#160; &#160; &#160;为了便于视频内容的存储和传输，通常需要减少视频内容的体积，也就是需要将原始的内容元素(图像和音频)经过压缩，压缩算法也简称编码格式。例如视频里边的原始图像数据会采用 H.264 编码格式进行压缩，音频采样数据会采用 AAC 编码格式进行压缩。 &#160; &#160; &#160; &#160;视频内容经过编码压缩后，确实有利于存储和传输; 不过当要观看播放时，相应地也需要解码过程。因此编码和解码之间，显然需要约定一种编码器和解码器都可以理解的约定。就视频图像编码和解码而言，这种约定很简单： 编码器将多张图像进行编码后生产成一段一段的 GOP ( Group of Pictures ) ， 解码器在播放时则是读取一段一段的 GOP 进行解码后读取画面再渲染显示。&#160; &#160; &#160; &#160;GOP ( Group of Pictures ) 是一组连续的画面，由一张 I 帧和数张 B / P 帧组成，是视频图像编码器和解码器存取的基本单位，它的排列顺序将会一直重复到影像结束。&#160; &#160; &#160; &#160;I 帧是内部编码帧（也称为关键帧），P 帧是前向预测帧（前向参考帧），B 帧是双向内插帧（双向参考帧）。简单地讲，I 帧是一个完整的画面，而 P 帧和 B 帧记录的是相对于 I 帧的变化。 如果没有 I 帧，P 帧和 B 帧就无法解码。&#160; &#160; &#160; &#160;小结一下，一个视频 ( Video ) ，其图像部分的数据是一组 GOP 的集合, 而单个 GOP 则是一组 I / P / B 帧图像的集合。 &#160; &#160; &#160; &#160;在这样的一种几何关系中，Video 好比一个 “物体”，GOP 好比 “分子”，I / P / B 帧的图像则好比 “原子”。 &#160; &#160; &#160; &#160;想象一下，如果我们把传输一个 “物体”，改成传输一个一个的 “原子”，将最小颗粒以光速传送，那么以人的生物肉眼来感知，将是一种怎样的体验？ 什么是视频直播？&#160; &#160; &#160; &#160;不难脑洞大开一下，直播就是这样的一种体验。视频直播技术，就是将视频内容的最小颗粒 ( I / P / B 帧，…)，基于时间序列，以光速进行传送的一种技术。 &#160; &#160; &#160; &#160;简而言之，直播就是将每一帧数据 ( Video / Audio / Data Frame )，打上时序标签 ( Timestamp ) 后进行流式传输的过程。发送端源源不断的采集音视频数据，经过编码、封包、推流，再经过中继分发网络进行扩散传播，播放端再源源不断地下载数据并按时序进行解码播放。如此就实现了 “边生产、边传输、边消费” 的直播过程。 &#160; &#160; &#160; &#160;理解以上两个关于视频和直播两个基础概念后，接下来我们就可以一窥直播的业务逻辑了。 直播的业务逻辑如下是一个最精简的一对多直播业务模型，以及各个层级之间的协议。各协议差异对比如下：以上就是关于直播技术的一些基础概念。下面我们进一步了解下影响人们视觉体验的直播性能指标。 影响视觉体验的直播性能指标&#160; &#160; &#160; &#160;直播第一个性能指标是延迟，延迟是数据从信息源发送到目的地所需的时间。根据爱因斯坦的狭义相对论，光速是所有能量、物质和信息运动所能达到的最高速度，这个结论给传播速度设定了上限。因此，即便我们肉眼感觉到的实时，实际上也是有一定的延迟。由于 RTMP/HLS 是基于 TCP 之上的应用层协议，TCP 三次握手，四次挥手，慢启动过程中的每一次往返来回，都会加上一次往返耗时 ( RTT )，这些交互过程都会增加延迟。其次根据 TCP 丢包重传特性，网络抖动可能导致丢包重传，也会间接导致延迟加大。&#160; &#160; &#160; &#160;一个完整的直播过程，包括但不限于以下环节：采集、处理、编码、封包、推流、传输、转码、分发、拉流、解码、播放。从推流到播放，再经过中间转发环节，延迟越低，则用户体验越好。 &#160; &#160; &#160; &#160;第二个直播性能指标卡顿，是指视频播放过程中出现画面滞帧，让人们明显感觉到“卡”。单位时间内的播放卡顿次数统计称之为卡顿率。 &#160; &#160; &#160; &#160;造成卡顿的因素有可能是推流端发送数据中断，也有可能是公网传输拥塞或网络抖动异常，也有可能是终端设备的解码性能太差。卡顿频次越少或没有，则说明用户体验越好。 &#160; &#160; &#160; &#160;第三个直播性能指标首屏耗时，指第一次点击播放后，肉眼看到画面所等待的时间。技术上指播放器解码第一帧渲染显示画面所花的耗时。通常说的 “秒开”，指点击播放后，一秒内即可看到播放画面。首屏打开越快，说明用户体验越好。 &#160; &#160; &#160; &#160;如上三个直播性能指标，分别对应一个低延迟、高清流畅、极速秒开 的用户体验诉求。了解这三个性能指标，对优化移动直播 APP 的用户体验至关重要。 &#160; &#160; &#160; &#160;那么移动直播场景下具体而言有哪些常见的坑呢？ &#160; &#160; &#160; &#160;根据实践总结下来的经验，移动平台上视频直播的坑主要可以总结为两方面：设备差异，以及网络环境这些场景下带来的技术考验。 移动直播场景的坑与规避措施不同芯片平台上的编码差异iOS 平台上无论硬编还是软编，由于是 Apple 一家公司出厂，几乎不存在因为芯片平台不同而导致的编码差异。 &#160; &#160; &#160; &#160;然而，在 Android 平台上，Android Framework SDK 提供的 MediaCodec 编码器，在不同的芯片平台上，差异表现很大， 不同的厂家使用不同的芯片，而不同的芯片平台上 Android MediaCodec 表现略有差异，通常实现全平台兼容的成本不低。 &#160; &#160; &#160; &#160;另外就是 Android MediaCodec 硬编层面的 H.264 编码画质参数是固定的 baseline，所以画质通常也一般。因此，在 Android 平台下，推荐是用软编，好处是画质可调控，兼容性也更好。 低端设备如何上高性能地采集和编码？&#160; &#160; &#160; &#160;例如 Camera 采集输出的可能是图片，一张图的体积并不会小，如果采集的频次很高，编码的帧率很高，每张图都经过编码器，那么编码器又可能会出现过载。 &#160; &#160; &#160; &#160;这个时候，可以考虑在编码前，不影响画质的前提下（前面我们讲过帧率的微观意义），进行选择性丢帧，以此降低编码环节的功耗开销。 弱网下如何保障高清流畅推流&#160; &#160; &#160; &#160;移动网络下，通常容易遇到网络不稳定，连接被重置，断线重连，一方面频繁重连，建立连接需要开销。另一方面尤其是发生 GPRS / 2G / 3G / 4G 切换时，带宽可能出现瓶颈。当带宽不够，帧率较高/码率较高的内容较难发送出去，这个时候就需要可变码率支持。 &#160; &#160; &#160; &#160;即在推流端，可检测网络状态和简单测速，动态来切换码率，以保障网络切换时的推流流畅。 &#160; &#160; &#160; &#160;其次编码、封包、推流 这一部分的逻辑也可以做微调，可以尝试选择性丢帧，比如优先丢视频参考帧（不丢 I 帧和音频帧 )，这样也可以减少要传输的数据内容，但同时又达到了不影响画质和版视听流畅的目的。 需要区分直播流的状态和业务状态&#160; &#160; &#160; &#160;直播是媒体流、APP 的交互是 API 信令流，两者的状态不能混为一谈。尤其是不能基于 APP 的交互的 API 状态来判断直播流的状态。以上是移动直播场景下常见的几个坑和规避措施。 移动直播场景其他优化措施一、怎么优化打开速度，达到传说中的 “秒开”？一、怎么优化打开速度，达到传说中的 “秒开”？ &#160; &#160; &#160; &#160;大家可能会看到，市面上某些手机直播 APP 的打开速度非常快，一点就开。而某些手机直播 APP，点击播放后要等好几秒以后才能播放。是什么原因导致如此的天壤之别呢？ &#160; &#160; &#160; &#160;大部分播放器都是拿到一个完成的 GOP 后才能解码播放，基于 FFmpeg 移植的播放器甚至需要等待音画时间戳同步后才能播放（如果一个直播里边没有音频只有视频相当于要等待音频超时后才能播放画面）。 “秒开”可以从以下几个方面考虑： 1. 改写播放器逻辑让播放器拿到第一个关键帧后就给予显示。&#160; &#160; &#160; &#160;GOP 的第一帧通常都是关键帧，由于加载的数据较少，可以达到 “首帧秒开”。 &#160; &#160; &#160; &#160;如果直播服务器支持 GOP 缓存，意味着播放器在和服务器建立连接后可立即拿到数据，从而省却跨地域和跨运营商的回源传输时间。 &#160; &#160; &#160; &#160;GOP 体现了关键帧的周期，也就是两个关键帧之间的距离，即一个帧组的最大帧数。假设一个视频的恒定帧率是 24fps（即1秒24帧图像），关键帧周期为 2s，那么一个 GOP 就是 48 张图像。一般而言，每一秒视频至少需要使用一个关键帧。 &#160; &#160; &#160; &#160;增加关键帧个数可改善画质（GOP 通常为 FPS 的倍数），但是同时增加了带宽和网络负载。这意味着，客户端播放器下载一个 GOP，毕竟该 GOP 存在一定的数据体积，如果播放端网络不佳，有可能不是能够快速在秒级以内下载完该 GOP，进而影响观感体验。 &#160; &#160; &#160; &#160;如果不能更改播放器行为逻辑为首帧秒开，直播服务器也可以做一些取巧处理，比如从缓存 GOP 改成缓存双关键帧（减少图像数量），这样可以极大程度地减少播放器加载 GOP 要传输的内容体积。 2. 在 APP 业务逻辑层面方面优化。&#160; &#160; &#160; &#160;比如提前做好 DNS 解析（省却几十毫秒），和提前做好测速选线（择取最优线路）。经过这样的预处理后，在点击播放按钮时，将极大提高下载性能。 &#160; &#160; &#160; &#160;一方面，可以围绕传输层面做性能优化；另一方面，可以围绕客户播放行为做业务逻辑优化。两者可以有效的互为补充，作为秒开的优化空间。 二、美颜等滤镜如何处理？&#160; &#160; &#160; &#160;在手机直播场景下，这就是一个刚需。没有美颜功能的手机直播 APP，主播基本不爱用。可以在采集画面后，将数据送给编码器之前，将数据源回调给滤镜处理程序，原始数据经过滤镜处理完后，再送回给编码器进行编码即可。 &#160; &#160; &#160; &#160;除了移动端可以做体验优化之外，直播流媒体服务端架构也可以降低延迟。例如收流服务器主动推送 GOP 至边缘节点，边缘节点缓存 GOP，播放端则可以快速加载，减少回源延迟。其次，可以贴近终端就近处理和分发 三、如何保障直播持续播放流畅不卡顿？&#160; &#160; &#160; &#160;“秒开”解决的是直播首次加载的播放体验，如何保障直播持续播放过程中的画面和声音视听流畅呢？因为，一个直播毕竟不是一个 HTTP 一样的一次性请求，而是一个 Socket 层面的长连接维持，直到直到主播主动终止推流。 &#160; &#160; &#160; &#160;上述我们讲过卡顿的定义：即播放时画面滞帧，触发了人们的视觉感受。在不考虑终端设备性能差异的情况下，针对网络传输层面的原因，我们看看如何保障一个持续的直播不卡顿。 &#160; &#160; &#160; &#160;这其实是一个直播过程中传输网络不可靠时的容错问题。例如，播放端临时断网了，但又快速恢复了，针对这种场景，播放端如果不做容错处理，很难不出现黑屏或是重新加载播放的现象。 &#160; &#160; &#160; &#160;为了容忍这种网络错误，并达到让终端用户无感知，客户端播放器可以考虑构建一个FIFO（先进先出）的缓冲队列，解码器从播放缓存队列读取数据，缓存队列从直播服务器源源不断的下载数据。通常，缓存队列的容量是以时间为单位（比如3s），在播放端网络不可靠时，客户端缓存区可以起到“断网无感”的过渡作用。 &#160; &#160; &#160; &#160;显然，这只是一个“缓兵之计”，如果直播服务器边缘节点出现故障，而此时客户端播放器又是长连接，在无法收到对端的连接断开信号，客户端的缓冲区容量再大也不管用了，这个时候就需要结合客户端业务逻辑来做调度。 &#160; &#160; &#160; &#160;重要的是客户端结合服务端，可以做精准调度。在初始化直播推流之前，例如基于 IP 地理位置和运营商的精确调度，分配线路质量最优的边缘接入节点。在直播推流的过程中，可以实时监测帧率反馈等质量数据，基于直播流的质量动态调整线路。 Q&amp;A1. 关键帧设置频率一般是多少？有没有根据接入动态设置？过长首屏秒会很难做到。 徐立：关键帧间隔越长，也就是 GOP 越长，理论上画面越高清。但是生成 HLS 直播时，最小切割粒度也是一个 GOP，所以针对交互直播，通常不建议 GOP 设置太长。直播一般 2 个关键帧间隔即可。比如帧率是 24fps， 那么 2 个关键帧的间隔就是 48fps ，这个 GOP 就是2s。 2. 七牛这个直播是用的网宿加速？有遇到什么坑没？ 徐立：七牛在直播方面主要是自建节点，也支持融合众多第三方 CDN 服务商，多样化的线路组合为客户提供更优质的服务。在和第三方 CDN 合作的过程中遇到的问题等有机会再做更细粒度的交流和分享。 3. RTMP 直播流除了优化线路外，还有什么加速手段吗？ 徐立：物理上优化线路，逻辑上优化策略，比如选择性丢帧，不影响编码画质的前提下减轻传输体积。 4. OBS 推流，播放端 HLS 出现视/音频不同步是哪个环节的问题？怎么优化？ 徐立：有可能是采集端的问题，如果是采集端编码环节就出现音画不同步，可以在收流服务器上做音画时间戳同步，这样是全局的校对。如果是播放端解码性能问题，那么需要调节播放逻辑，比如保证音画时间戳强一致性的前提下，选择性丢一部帧。 5. PPT 前几页中一个概念好像错了，I 帧不是关键帧，IDR 帧才是。IDR 帧是 I 帧，但是 I 帧不一定是 IDR 帧。只有 IDR 帧才是可重入的。 徐立：中文都把 I 帧翻译成关键帧了，不过既然提到了 IDR 帧，可以展开说明一下。所有的 IDR 帧都是 I 帧，但是并不是所有 I 帧都是 IDR 帧，IDR 帧是 I 帧的子集。I 帧严格定义是帧内编码帧，由于是一个全帧压缩编码帧，通常用 I 帧表示 “关键帧”。IDR 是基于 I 帧的一个 “扩展”，带了控制逻辑，IDR 图像都是 I 帧图像，当解码器解码到 IDR 图像时，会立即将参考帧队列清空，将已解码的数据全部输出或抛弃。重新查找参数集，开始一个新的序列。这样如果前一个序列出现重大错误，在这里可以获得重新同步的机会。IDR 图像之后的图像永远不会使用 IDR 之前的图像的数据来解码。 6. 有没有调研过 nginx rtmp module，为什么没有用，对它有什么评价？ 徐立：有调研过，nginx_rtmp_module 是单进程多线程，非 go 这种轻量级线程/协程用并发自然语义的方式编写流业务。nginx 原本的代码量较大（约 16 万行，但和直播业务相关的功能并不是很多）。且主要靠写 nginx.conf 做配置租户，通常单租户可以，但业务可扩展性方面不是很灵活，可满足基本需求，不满足高级功能。 7. 用到了那些开源软件？编码用的是 x264 吗？直播服务器你们自己开发还是开源的？ 徐立：直播服务器用 go 开发的，移动端编码优先硬编，软编用 x264 8. 请教一下用 OBS 推流到 nginx_rtmp_module 的时候是已经做了视频压缩了还是需要基于 OBS 再开发？ 徐立：OBS 把编码压缩都做了，不需要再开发。 9. 视频直播想在 HLS 流中无缝插入一段广告的 ts 文件，有问题想请教一下：1、这段 ts 的分辨率是否一定要和之前的视频流一致？2、pts 时间戳是否要和上一个 ts 递增？ 徐立：1、可以不一致。这种情况两段视频完全是独立状态，可以没有任何关系，只需要插入 discontinue 标记，播放器在识别到这个标记之后重置解码器参数就可以无缝播放，画面会很平滑的切换。2、不需要递增。举个例子，视频 A 正在直播，播放到 pts 在 5s 的时候，插入一个视频 B，需要先插入一个 discontinue，再插入 B，等 B 播放完之后，再插入一个 discontinue，再插入 A，这个时候 A 的 pts 可以和之前递增，也可以按照中间插入的 B 的时长做偏移，一般做点播和时移的时候 pts 会连续递增，直播的话会算上 B 的时长。 由于移动直播在实践上还有非常多细节，本文未能全部覆盖，感兴趣的朋友欢迎在文章最后留言讨论。 PPT 下载地址我备份了一分pdf，下载点这里]]></content>
      <categories>
        <category>科普分享</category>
      </categories>
      <tags>
        <tag>移动直播</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客搭建历程————————Github和Hexo]]></title>
    <url>%2F2016%2F05%2F27%2F%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[我的博客搭建历程，使用github pages + hexo，域名解析，配置插件 前言&#160; &#160; &#160; &#160;其实从去年开始我就想搭建一个自己的个人网站了，主要用于记录自己的技术积累和重点知识。但由于自己太懒，能力也太渣，所以就一直没有行动。直到今年，突然发现随着知识积累的增加，有好多重要的内容脑袋都记不下了，有的网页就直接存个标签，但是标签越存越多，但却很少再次去浏览，最后知识还是会淡忘，所以今天痛下决心，还是自己搭个博客吧，以挽回知识的流失，同时也可以向外部分享自己的一些见解。&#160; &#160; &#160; &#160;博客搭建过程也是一波三折，遇到各种坑爹问题，还好我没那么轻易放弃，最终在无数次失败之后终于成功了。 正文1. github配置1）首先需要有一个github账号，没有的话就得申请一个。然后新建一个代码仓库，注意，仓库名一定要是：你的github账号名字.github.io，比如我的是windrunnerlihuan.github.io。2）clone到本地，比如我的就是如下命令：git clone https://github.com/windrunnerlihuan/windrunnerlihuan.github.io.gitcd 进文件夹，我们先建个index.html用于测试：cd windrunnerlihuan.github.iovi index.html然后编辑：123456&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;github pages测试&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 然后按esc，再输入“:wq”回车保存。然后提交上去：git add .git commit -m “测试github pages”git push此时需要输入你的github账号和密码，输入就是了，成功后我们验证一下。然后打开浏览器，输入地址，比如我的就是：http://windrunnerlihuan.github.io如果能正确显示“github pages测试”字样，说明我们成功了。 2. hexo安装&#160; &#160; &#160; &#160;Hexo 是一个简单地、轻量地、基于Node的一个静态博客框架。通过Hexo我们可以快速创建自己的博客，仅需要几条命令就可以完成。 Hexo的官方网站：http://hexo.io/ ，也是基于Github构建的网站。 &#160; &#160; &#160; &#160;不过要使用hexo必须装node.js，这个easy，直接在官网https://nodejs.org/en/ 上下一个，首页就是下载页面，选一个你自己电脑平台对应的，比如我的是windows x64。如果不想看英文呢，还有个中文网站http://nodejs.cn/ ，几乎是把英文网站翻译了一遍，套路都一样。下载下来之后文件名叫node-v4.4.4-x64.msi，双击，一路next，傻瓜式安装。 接着安装hexo：1）打开控制台（按下win + r，输入cmd，回车）或者直接在git bash界面。输入 npm install -g hexo，然后开始安装hexo。成功后再输入 npm install hexo-deployer-git –save，安装发布工具。 2）接着进入我们从github上克隆下来的windrunnerlihuan.github.io文件，再次git bash here。输入 hexo init，初始化hexo。完成后我们测试一下：输入hexo serverbash控制台会打印出 “[info] Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.”这时端口4000被打开了，我们能过浏览器打开地址，http://localhost:4000/ 。 出现了默认的网页界面，是不是很容易呢！！ 3. hexo配置我们先看一下文件结构 scaffolds 脚手架，也就是一个工具模板 scripts 写文件的js，扩展hexo的功能 source 存放博客正文内容 source/_drafts 草稿箱 source/_posts 文件箱 themes 存放皮肤的目录 themes/landscape 默认的皮肤 _config.yml 全局的配置文件 db.json 静态常量 在这里，我们每次用到的就是_posts目录里的文件，而_config.yml文件和themes目录是第一次配置好就行了。 _posts目录：Hexo是一个静态博客框架，因此没有数据库。文章内容都是以文本文件方式进行存储的，直接存储在_posts的目录。Hexo天生集成了markdown，我们可以直接使用markdown语法格式写博客，例如:hello-world.md。新增加一篇文章，就在_posts目录，新建一个xxx.md的文件。 themes目录：是存放皮肤的，包括一套Javascript+CSS样式和基于EJS的模板设置。通过在themes目录下，新建一个子目录，就可以创建一套新的皮肤，当然我们也可以直接在landscape上面修改。 我们主要改的就是_config.yml全局配置文件，以及新建文章时要增加或者修改文章在source/-posts/目录下的md文件。 1）修改_config.yml全局配置文件 _config.yml是全局的配置文件：很多的网站配置都在这个文件中定义。 站点信息: 定义标题，作者，语言 URL: URL访问路径 文件目录: 正文的存储目录 写博客配置：文章标题，文章类型，外部链接等 目录和标签：默认分类，分类图，标签图 归档设置：归档的类型 服务器设置：IP，访问端口，日志输出 时间和日期格式： 时间显示格式，日期显示格式 分页设置：每页显示数量 评论：外挂的Disqus评论系统 插件和皮肤：换皮肤，安装插件 Markdown语言：markdown的标准 CSS的stylus格式：是否允许压缩 部署配置：github发布 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Site 站点信息title: this way, little boysubtitle: 無限大な夢のあとの 何もない世の中じゃdescription:author: Huan Lilanguage:timezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://windrunnerlihuan.comroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directory 文件目录source_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writing 写博客配置new_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tag 目录和标签default_category: uncategorizedcategory_map:tag_map:# Date / Time format 时间和日期## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination 分页设置## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Markdown Markdown语法## https://github.com/chjj/markedmarkdown: gfm: true pedantic: false sanitize: false tables: true breaks: true smartLists: true smartypants: true# Deployment 部署配置## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/windrunnerlihuan/windrunnerlihuan.github.io.git &#160; &#160; &#160; &#160;其他没啥改动的，就改个Site 站点信息，URL，还有Deployment 部署配置，前两个没啥难度，这个Deployment 部署配置很重要： 其中type一定要填git，因为以前低版本的hexo是github，不过现在早就废弃了。 其次repo填你github项目的clone地址，http/https/ssh类型都行，我的是https://github.com/windrunnerlihuan/windrunnerlihuan.github.io.git 和git@github.com:windrunnerlihuan/windrunnerlihuan.github.io.git ，我填的是第一个，因为填第二个后期发布时会验证公钥，需要用git bash生成公钥，然后贴到项目的Deploy keys里，比较麻烦，所以为了省事，我填了第一个。 2） 新建博客 接下来，我们开始新博客了，创建第一篇博客文章。Hexo建议通过命令行操作，当然你也可以直接在_posts目录下创建文件。 通过命令创建新文章，在bash 里输入 hexo new “新的开始”在source/_posts目录下就会生成文件：”新的开始.md”。 然后我们编辑 新的开始.md： 12345678910111213141516171819202122232425262728293031323334---title: 新的开始date: 2016-05-27 18:44:12tags:- 开始categories: 日志---这是**新的开始**，我用hexo创建了第一篇文章。## 引用# Swig语法&#123;% blockquote Seth Godin http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html Welcome to Island Marketing %&#125;Every interaction is both precious and an opportunity to delight.&#123;% endblockquote %&#125;# Markdown语法&gt; Every interaction is both precious and an opportunity to delight.## 链接# Swig语法&#123;% link gank日志 http://gank.io/ true 粉丝日志 %&#125;# Markdown语法[gank日志](http://gank.io/)## 图片# Swig语法&#123;% img /images/新的开始/meizi.jpg 400 600 这是一张图片 %&#125;# Markdown语法![这是一张图片](http://o7xxrho8u.bkt.clouddn.com/img/windrunnerlihuan%E5%8D%9A%E5%AE%A2/%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B/meizi.jpg)## 公式$$J\_\alpha(x)=\sum _&#123;m=0&#125;^\infty \frac&#123;(-1)^ m&#125;&#123;m! \, \Gamma (m + \alpha + 1)&#125;&#123;\left(&#123;\frac&#123;x&#125;&#123;2&#125;&#125;\right)&#125;^&#123;2 m + \alpha &#125;$$感觉非常好。 文章使用markedonw语法写的，还有很多普通或者高级的语法，这个可以问度娘或者谷歌，找找别人的帖子学学。 用命令行启动本地server：hexo server通过浏览器打开， http://localhost:4000/ ，就出现了我们新写的文章。 4. 发布到Github1) 静态化处理&#160; &#160; &#160; &#160;写完了文章，我们就可以发布了。要说明的一点是hexo的静态博客框架，那什么是静态博客呢？静态博客，是只包含html, javascript, css文件的网站，没有动态的脚本。虽然我们是用Node进行的开发，但博客的发布后就与Node无关了。在发布之前，我们要通过一条命令，把所有的文章都做静态化处理，就是生成对应的html, javascript, css，使得所有的文章都是由静态文件组成的。 输入静态化命令 ：hexo generate在本地目录下，会生成一个public的目录，里面包括了所有静态化的文件。 2) 发布到github接下来，我们把这个博客发布到github。现在检查我们上次填好的_config.yml文件，找到deploy的部分，设置github的项目地址。123deploy: type: git repo: https://github.com/windrunnerlihuan/windrunnerlihuan.github.io.git 然后执行部署命令：hexo deploy中间过程会让你再次输入github账号和密码，输入就行了成功后验证，浏览器里输入上次的地址：http://windrunnerlihuan.github.io 说明我们成功了 5. 独立购买域名与绑定&#160; &#160; &#160; &#160;有时候我们想做个人博客，想让自己的博客拥有自己的域名，那就得去买个域名了。这里我们可以通过万网申请购买：https://wanwang.aliyun.com/ 。流程很简单，基本上一路next。我也斥资45块买了个.com结尾的域名 windrunnerlihuan.com 。 然后就是解析域名了，不过解析之前你得知道你的gihub pages个人主页的ip：首先ping 一下个人主页地址 ，控制台里输入 ping www.windrunnerlihuan.github.io，然后会返回一个ip，然后用这个ip设置我们刚才买的域名解析，也是轻车熟路 最后最重要的是：1）在github项目中，新建一个文件CNAME，文件中写出你要绑定的域名windrunnerlihuan.com。通过浏览器，访问http://windrunnerlihuan.com，就打开了我们建好的博客站点。2）由于每次执行deploy的时候，github代码仓库所有的文件都会被覆盖，所以我们最好在source目录下创建这个CNAME文件，这样每次部署就不用动手创建了。 6. 个性化功能配置1） 替换皮肤博客系统流行的原因，是因为他的个人性，而皮肤正式个性化的一种体现。利用hexo替换皮肤，还是比较简单的，3步完成。 No.1 找到一个皮肤或者自己开发一个皮肤打开hexo的皮肤列表页面，你可以找到很多的皮肤，网页地址： https://github.com/tommy351/hexo/wiki/Themes 。 No.2放到themes目录下比如，我觉得pacman( https://github.com/A-limon/pacman )这个皮肤还不错，我就可以下载皮肤到themes目录下面。通过git命令下载皮肤git clone https://github.com/A-limon/pacman.git themes/pacman No.3在_config.yml指定皮肤编辑文件_config.yml，找到theme一行，改成 theme: pacman本地启动hexo服务器，打开浏览器 http://localhost:4000 新皮肤的效果还不错吧，然后静态化处理，再发布到github，就完成了站点的改版。 这里只介绍换皮肤，当然还有其他很多功能，不过对于我来说暂时没多大卵用，如果其他人有需求可以再google或者baidu搜，也都不难，很easy。 结语&#160; &#160; &#160; &#160;因为这是我第一次搭建个人博客，水平实在比较差，流程写的很粗燥，很多细节都没有涉及，望大家体谅，以后我会努力完善的，将后续文章做的更好。]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>博客搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新的开始]]></title>
    <url>%2F2016%2F05%2F27%2F%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[这是新的开始，我用hexo创建了第一篇文章。 通过下面的命令，就可以创建新文章12D:\workspace\javascript\nodejs-hexo&gt;hexo new 新的开始[info] File created at D:\workspace\javascript\nodejs-hexo\source\_posts\新的开始.md 感觉非常好。 引用Swig语法Every interaction is both precious and an opportunity to delight. Seth GodinWelcome to Island Marketing Markdown语法 Every interaction is both precious and an opportunity to delight. 代码块Swig语法.compactUnderscore.js12.compact([0, 1, false, 2, ‘’, 3]);=&gt; [1, 2, 3] Markdown语法12.compact([0, 1, false, 2, ‘’, 3]);=&gt; [1, 2, 3] 链接Swig语法gank日志 Markdown语法gank日志测试404页面 图片Swig语法 Markdown语法 公式$$J_\alpha(x)=\sum _{m=0}^\infty \frac{(-1)^ m}{m! \, \Gamma (m + \alpha + 1)}{\left({\frac{x}{2}}\right)}^{2 m + \alpha }$$]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>开始</tag>
      </tags>
  </entry>
</search>